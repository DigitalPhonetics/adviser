{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Introduction \u00b6 We present ADVISER - an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision) and socially-engaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The Python based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research. Guiding Principles \u00b6 Modularity \u00b6 In contrast to a traditional (rather static) pipeline approach which adheres to a fixed order of information flow, Adviser is implemented in an asynchronous way, using the publish-subscribe software pattern. This allows for parallel information flow which facilitates the combination of multiple modalities as well as the integration of additional modules. For each module in a classic dialog system (NLU, BST, dialog policy and NLG), we provide a handcrafted baseline module, additionally we provide a reinforcement learning based implementation for the policy. These can be used to quickly assemble a working dialog system or as implementation guidelines for custom modules. Additionally, because all modules inherit from the same abstract class, technical users can also easily write their own implementations or combinations of modules. Flexibility \u00b6 The publish-subscribe pattern allows great flexibility in terms of structure and scope of the dialog system. Users can easily realize anything from a simple text-based pipeline system to a full-fledged multimodal, multi-domain dialog system. Further, distributed systems are possible. Services are location-transparent and may thus be distributed across multiple machines. The central dialog system discovers local and remote services and provides synchronization guarantees for dialog initialization and termination. This is useful for resource-heavy tasks such as speech synthesis. Transparency \u00b6 We provide a utility to draw the dialog graph, showing the information flow between services and any inconsistencies in publish/subscribe connections. User-friendly at different levels \u00b6 technical users have the full flexibility to explore and extend the back-end; non-technical users can use the provided code base for building systems; students from different disciplines could easily learn the concepts and explore human machine interaction. Support \u00b6 You can ask questions by sending emails to adviser-support@ims.uni-stuttgart.de . You can also post bug reports and feature requests in GitHub issues. How To Cite \u00b6 If you use or reimplement any of this source code, please cite the following paper: @InProceedings { title = {ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents } , author = {Chia-Yu Li and Daniel Ortega and Dirk V{\\\"{a}}th and Florian Lux and Lindsey Vanderlyn and Maximilian Schmidt and Michael Neumann and Moritz V{\\\"{o}}lkel and Pavel Denisov and Sabrina Jenne and Zorica Karacevic and Ngoc Thang Vu}, booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020) - System Demonstrations}, publisher = {Association for Computational Linguistics}, location = {Seattle, Washington, USA}, year = {2020} } License \u00b6 Adviser is published under the GNU GPL 3 license.","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#introduction","text":"We present ADVISER - an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision) and socially-engaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The Python based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.","title":"Introduction"},{"location":"#guiding-principles","text":"","title":"Guiding Principles"},{"location":"#modularity","text":"In contrast to a traditional (rather static) pipeline approach which adheres to a fixed order of information flow, Adviser is implemented in an asynchronous way, using the publish-subscribe software pattern. This allows for parallel information flow which facilitates the combination of multiple modalities as well as the integration of additional modules. For each module in a classic dialog system (NLU, BST, dialog policy and NLG), we provide a handcrafted baseline module, additionally we provide a reinforcement learning based implementation for the policy. These can be used to quickly assemble a working dialog system or as implementation guidelines for custom modules. Additionally, because all modules inherit from the same abstract class, technical users can also easily write their own implementations or combinations of modules.","title":"Modularity"},{"location":"#flexibility","text":"The publish-subscribe pattern allows great flexibility in terms of structure and scope of the dialog system. Users can easily realize anything from a simple text-based pipeline system to a full-fledged multimodal, multi-domain dialog system. Further, distributed systems are possible. Services are location-transparent and may thus be distributed across multiple machines. The central dialog system discovers local and remote services and provides synchronization guarantees for dialog initialization and termination. This is useful for resource-heavy tasks such as speech synthesis.","title":"Flexibility"},{"location":"#transparency","text":"We provide a utility to draw the dialog graph, showing the information flow between services and any inconsistencies in publish/subscribe connections.","title":"Transparency"},{"location":"#user-friendly-at-different-levels","text":"technical users have the full flexibility to explore and extend the back-end; non-technical users can use the provided code base for building systems; students from different disciplines could easily learn the concepts and explore human machine interaction.","title":"User-friendly at different levels"},{"location":"#support","text":"You can ask questions by sending emails to adviser-support@ims.uni-stuttgart.de . You can also post bug reports and feature requests in GitHub issues.","title":"Support"},{"location":"#how-to-cite","text":"If you use or reimplement any of this source code, please cite the following paper: @InProceedings { title = {ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents } , author = {Chia-Yu Li and Daniel Ortega and Dirk V{\\\"{a}}th and Florian Lux and Lindsey Vanderlyn and Maximilian Schmidt and Michael Neumann and Moritz V{\\\"{o}}lkel and Pavel Denisov and Sabrina Jenne and Zorica Karacevic and Ngoc Thang Vu}, booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020) - System Demonstrations}, publisher = {Association for Computational Linguistics}, location = {Seattle, Washington, USA}, year = {2020} }","title":"How To Cite"},{"location":"#license","text":"Adviser is published under the GNU GPL 3 license.","title":"License"},{"location":"faq/","text":"FAQ \u00b6 Who contributed to ADvISER? \u00b6 Chia-Yu Li Daniel Ortega Dirk V\u00e4th Florian Lux Gianna Weber Lindsey Vanderlyn Maximilian Schmidt Michael Neumann Moritz V\u00f6lkel Pavel Denisov Sabrina Jenne Zorica Kacarevic Ngoc Thang Vu How shall I cite ADvISER \u00b6 Please see here . Who can I contact in case of problems or questions? \u00b6 You can ask questions by sending emails to adviser-support@ims.uni-stuttgart.de . You can also post bug reports and feature requests in GitHub issues. Can I contribute to the project? \u00b6 You can post bug reports and feature requests in GitHub issues. You can find the code to ADvISER in our Git repository . What are the main features of the system\u2019s framework? \u00b6 Which User Actions and System Actions are currently supported by the system? \u00b6 User Actions \u00b6 Inform : User informs the system about a constraint/entity name NegativeInform : User informs the system they do not want a particular value Request : User asks the system for information about an entity Hello : User issues a greeting Bye : User says bye; this ends the dialog Thanks : User says thanks Affirm : User agrees with the last system confirm request Deny : User disagrees with the last system confirm request RequestAlternatives : User asks for an alternative offer from the system Ack : User likes the system's proposed offer Bad : User input could not be recognized SelectDomain : User has provided a domain keyword System Actions \u00b6 Welcome : Issue system greeting InformByName : Propose an entity to the user InformByAlternatives : Propose an alternate entity if the user isn't satisfied with the first Request : Ask for more information from the user Confirm : Ask the user to confirm a proposed value for a slot Select : Provide the user with 2 or 3 options and ask the user to select the correct one RequestMore : Ask the user if there is anything else the system can provide Bad : If the system could not understand the user Bye : Say goodbye What Emotions and Engagements are currently supported by the system? \u00b6 User Emotions \u00b6 happy angry neutral User Engagement \u00b6 high low Which domains are currently supported by ADvISER? \u00b6 ADvISER currently supports the following domains: IMS Lecturers Providing information about lecturers teaching at the IMS (for privacy reasons, our database includes fictive information about lecturers and related contact information, however, it serves as an example for a real-world application). Weather Providing information about the weather. Mensa Providing information about the menu at the dining hall of the University of Stuttgart. World Knowledge QA Providing information about common world knowledge. Can ADvISER be extended by new modules? \u00b6 Please follow our advanced tutorial .","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#who-contributed-to-adviser","text":"Chia-Yu Li Daniel Ortega Dirk V\u00e4th Florian Lux Gianna Weber Lindsey Vanderlyn Maximilian Schmidt Michael Neumann Moritz V\u00f6lkel Pavel Denisov Sabrina Jenne Zorica Kacarevic Ngoc Thang Vu","title":"Who contributed to ADvISER?"},{"location":"faq/#how-shall-i-cite-adviser","text":"Please see here .","title":"How shall I cite ADvISER"},{"location":"faq/#who-can-i-contact-in-case-of-problems-or-questions","text":"You can ask questions by sending emails to adviser-support@ims.uni-stuttgart.de . You can also post bug reports and feature requests in GitHub issues.","title":"Who can I contact in case of problems or questions?"},{"location":"faq/#can-i-contribute-to-the-project","text":"You can post bug reports and feature requests in GitHub issues. You can find the code to ADvISER in our Git repository .","title":"Can I contribute to the project?"},{"location":"faq/#what-are-the-main-features-of-the-systems-framework","text":"","title":"What are the main features of the system\u2019s framework?"},{"location":"faq/#which-user-actions-and-system-actions-are-currently-supported-by-the-system","text":"","title":"Which User Actions and System Actions are currently supported by the system?"},{"location":"faq/#user-actions","text":"Inform : User informs the system about a constraint/entity name NegativeInform : User informs the system they do not want a particular value Request : User asks the system for information about an entity Hello : User issues a greeting Bye : User says bye; this ends the dialog Thanks : User says thanks Affirm : User agrees with the last system confirm request Deny : User disagrees with the last system confirm request RequestAlternatives : User asks for an alternative offer from the system Ack : User likes the system's proposed offer Bad : User input could not be recognized SelectDomain : User has provided a domain keyword","title":"User Actions"},{"location":"faq/#system-actions","text":"Welcome : Issue system greeting InformByName : Propose an entity to the user InformByAlternatives : Propose an alternate entity if the user isn't satisfied with the first Request : Ask for more information from the user Confirm : Ask the user to confirm a proposed value for a slot Select : Provide the user with 2 or 3 options and ask the user to select the correct one RequestMore : Ask the user if there is anything else the system can provide Bad : If the system could not understand the user Bye : Say goodbye","title":"System Actions"},{"location":"faq/#what-emotions-and-engagements-are-currently-supported-by-the-system","text":"","title":"What Emotions and Engagements are currently supported by the system?"},{"location":"faq/#user-emotions","text":"happy angry neutral","title":"User Emotions"},{"location":"faq/#user-engagement","text":"high low","title":"User Engagement"},{"location":"faq/#which-domains-are-currently-supported-by-adviser","text":"ADvISER currently supports the following domains: IMS Lecturers Providing information about lecturers teaching at the IMS (for privacy reasons, our database includes fictive information about lecturers and related contact information, however, it serves as an example for a real-world application). Weather Providing information about the weather. Mensa Providing information about the menu at the dining hall of the University of Stuttgart. World Knowledge QA Providing information about common world knowledge.","title":"Which domains are currently supported by ADvISER?"},{"location":"faq/#can-adviser-be-extended-by-new-modules","text":"Please follow our advanced tutorial .","title":"Can ADvISER be extended by new modules?"},{"location":"getting-started/","text":"Getting Started \u00b6 Installing the Adviser Toolkit \u00b6 Get the code and follow the install instructions. Testing Your Installation \u00b6 Open a terminal Activate your virtual environment for Adviser (as created in the install instructions) Navigate to the adviser folder containing the run_chat.py file Execute python run_chat.py mensa This will start a new dialog with a domain about the food plan of the University of Stuttgart's dining hall Try chatting with the system e.g. type something like What's the vegetarian main dish today In case you encounter any errors, try to make sure your setup is correct. If the problem persists, feel free to write an email to support , providing the full stack trace and, if possible, the dialog turn history. Creating and Running Your Own Dialog System \u00b6 To setup your own text-based dialog system (in this example, for a domain about lecturer information from the IMS Institute at the Unviersity of Stuttgart), it's as simple as creating a new file in the 'adviser' folder (where the run_chat.py file is located), naming it e.g. mydiasys.py , and adding the following content: import sys import os from typing import List from utils.domain.jsonlookupdomain import JSONLookupDomain from services.service import Service , PublishSubscribe , DialogSystem from services.domain_tracker import DomainTracker from services.nlu.nlu import HandcraftedNLU from services.nlg.nlg import HandcraftedNLG from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.hci import ConsoleInput , ConsoleOutput from utils.logger import DiasysLogger , LogLevel # create modules lecturers_domain = JSONLookupDomain ( 'ImsLecturers' ) nlu = HandcraftedNLU ( domain = lecturers_domain ) bst = HandcraftedBST ( domain = lecturers_domain ) policy = HandcraftedPolicy ( domain = lecturers_domain ) nlg = HandcraftedNLG ( domain = lecturers_domain ) d_tracker = DomainTracker ( domains = [ lecturers_domain ]) # Input modules (just allow access to terminal for text based dialog) user_in = ConsoleInput ( domain = \"\" ) user_out = ConsoleOutput ( domain = \"\" ) logger = DiasysLogger ( console_log_lvl = LogLevel . DIALOGS ) ds = DialogSystem ( services = [ d_tracker , user_in , user_out , nlu , bst , policy , nlg ], debug_logger = logger ) error_free = ds . is_error_free_messaging_pipeline () if not error_free : ds . print_inconsistencies () ds . draw_system_graph ( name = \"testgraph\" ) # start dialog for _ in range ( 1 ): ds . run_dialog ({ 'gen_user_utterance' : \"\" }) ds . shutdown () To run this code, execute python mydiasys.py . You can try e.g. utterances like I want information about a Digital Phonetics lecturer and continue the dialog from there.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#installing-the-adviser-toolkit","text":"Get the code and follow the install instructions.","title":"Installing the Adviser Toolkit"},{"location":"getting-started/#testing-your-installation","text":"Open a terminal Activate your virtual environment for Adviser (as created in the install instructions) Navigate to the adviser folder containing the run_chat.py file Execute python run_chat.py mensa This will start a new dialog with a domain about the food plan of the University of Stuttgart's dining hall Try chatting with the system e.g. type something like What's the vegetarian main dish today In case you encounter any errors, try to make sure your setup is correct. If the problem persists, feel free to write an email to support , providing the full stack trace and, if possible, the dialog turn history.","title":"Testing Your Installation"},{"location":"getting-started/#creating-and-running-your-own-dialog-system","text":"To setup your own text-based dialog system (in this example, for a domain about lecturer information from the IMS Institute at the Unviersity of Stuttgart), it's as simple as creating a new file in the 'adviser' folder (where the run_chat.py file is located), naming it e.g. mydiasys.py , and adding the following content: import sys import os from typing import List from utils.domain.jsonlookupdomain import JSONLookupDomain from services.service import Service , PublishSubscribe , DialogSystem from services.domain_tracker import DomainTracker from services.nlu.nlu import HandcraftedNLU from services.nlg.nlg import HandcraftedNLG from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.hci import ConsoleInput , ConsoleOutput from utils.logger import DiasysLogger , LogLevel # create modules lecturers_domain = JSONLookupDomain ( 'ImsLecturers' ) nlu = HandcraftedNLU ( domain = lecturers_domain ) bst = HandcraftedBST ( domain = lecturers_domain ) policy = HandcraftedPolicy ( domain = lecturers_domain ) nlg = HandcraftedNLG ( domain = lecturers_domain ) d_tracker = DomainTracker ( domains = [ lecturers_domain ]) # Input modules (just allow access to terminal for text based dialog) user_in = ConsoleInput ( domain = \"\" ) user_out = ConsoleOutput ( domain = \"\" ) logger = DiasysLogger ( console_log_lvl = LogLevel . DIALOGS ) ds = DialogSystem ( services = [ d_tracker , user_in , user_out , nlu , bst , policy , nlg ], debug_logger = logger ) error_free = ds . is_error_free_messaging_pipeline () if not error_free : ds . print_inconsistencies () ds . draw_system_graph ( name = \"testgraph\" ) # start dialog for _ in range ( 1 ): ds . run_dialog ({ 'gen_user_utterance' : \"\" }) ds . shutdown () To run this code, execute python mydiasys.py . You can try e.g. utterances like I want information about a Digital Phonetics lecturer and continue the dialog from there.","title":"Creating and Running Your Own Dialog System"},{"location":"api/examples/","text":"Examples \u00b6 \u00b6 qa special \u00b6 worldknowledge special \u00b6 domain \u00b6 WorldKnowledgeDomain \u00b6 Question answering for the world knowledge domain. Attributes artificial_id_counter (int): pseudo identifier for each entry name_lex (Dict[str,str]): lexicon for matching topic's names to their KG entity __init__ ( self ) special \u00b6 Calls super class' constructor and loads name lexicon Source code in adviser/examples/qa/worldknowledge/domain.py 41 42 43 44 45 46 47 48 49 def __init__ ( self ): \"\"\"Calls super class' constructor and loads name lexicon\"\"\" LookupDomain . __init__ ( self , 'CSQA' , 'World Knowledge' ) self . artificial_id_counter = 1 #int: lexicon for matching topic's names to their KG entity self . name_lex = self . _init_name_lexicon () \"\"\"Dict[str,str]: lexicon for matching topic's names to their KG entity.\"\"\" find_entities ( self , constraints ) \u00b6 Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict slot-value mapping of constraints required Source code in adviser/examples/qa/worldknowledge/domain.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def find_entities ( self , constraints : dict ): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): slot-value mapping of constraints \"\"\" assert 'relation' in constraints assert 'topic' in constraints assert 'direction' in constraints topics = self . _find_topic_entities ( constraints [ 'topic' ]) if not topics : return [] answers = [] for topic_id , topic_label in topics : answer_ids = [] if constraints [ 'direction' ] == 'out' : answer_ids = self . _perform_out_query ( constraints [ 'relation' ], topic_id ) for answer_id in answer_ids : answers . append ({ 'subject' : topic_label , 'predicate' : constraints [ 'relation' ], 'object' : answer_id }) self . artificial_id_counter += 1 else : answer_ids = self . _perform_in_query ( constraints [ 'relation' ], topic_id ) for answer_id in answer_ids : answers . append ({ 'subject' : answer_id , 'predicate' : constraints [ 'relation' ], 'object' : topic_label }) self . artificial_id_counter += 1 return answers find_info_about_entity ( self , entity_id , requested_slots ) \u00b6 Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/examples/qa/worldknowledge/domain.py 133 134 135 136 137 138 139 140 141 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" raise BaseException ( 'should not be called' ) get_domain_name ( self ) \u00b6 Return the domain name of the current ontology. Returns: Type Description object Source code in adviser/examples/qa/worldknowledge/domain.py 143 144 def get_domain_name ( self ): return \"qa\" get_informable_slots ( self ) \u00b6 Returns a list of all informable slots. Source code in adviser/examples/qa/worldknowledge/domain.py 154 155 156 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return [ 'relation' , 'topic' , 'direction' ] get_keyword ( self ) \u00b6 Source code in adviser/examples/qa/worldknowledge/domain.py 179 180 def get_keyword ( self ): return 'world knowledge' get_mandatory_slots ( self ) \u00b6 Returns a list of all mandatory slots. Source code in adviser/examples/qa/worldknowledge/domain.py 158 159 160 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" return [ 'relation' , 'topic' , 'direction' ] get_possible_values ( self , slot ) \u00b6 Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/examples/qa/worldknowledge/domain.py 162 163 164 165 166 167 168 169 170 171 172 173 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" # 'assert False, \"this method should not be called\"' raise BaseException ( 'should not be called' ) get_primary_key ( self ) \u00b6 Returns the slot name that will be used as the 'name' of an entry Source code in adviser/examples/qa/worldknowledge/domain.py 175 176 177 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" return 'artificial_id' get_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the user. Source code in adviser/examples/qa/worldknowledge/domain.py 146 147 148 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return [ 'subject' , 'predicate' , 'object' , 'object_type' ] get_system_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the system. Source code in adviser/examples/qa/worldknowledge/domain.py 150 151 152 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return [ 'relation' , 'topic' , 'direction' ] get_root_dir () \u00b6 Source code in adviser/examples/qa/worldknowledge/domain.py 27 28 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))) multinlg \u00b6 Handcrafted (i.e. template-based) Natural Language Generation Module MultiNLG \u00b6 Extension of the handcrafted NLG by allowing multiple system acts. This change is necessary for QA, since the policy publishes multiple system acts. __init__ ( self , domain , template_file = None , sub_topic_domains = {}, logger =< DiasysLogger adviser ( NOTSET ) > , template_file_german = None , language = None ) special \u00b6 Source code in adviser/examples/qa/worldknowledge/multinlg.py 39 40 41 42 43 def __init__ ( self , domain : Domain , template_file : str = None , sub_topic_domains : Dict [ str , str ] = {}, logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): # only calls the super class' constructor HandcraftedNLG . __init__ ( self , domain , template_file , sub_topic_domains , logger , template_file_german , language ) publish_system_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/examples/qa/worldknowledge/multinlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result neuralmodels special \u00b6 director \u00b6 Classifier \u00b6 Neural network for predicting the relation's direction (outgoing or incoming). The model uses a question encoder to classify the question as one of the two classes \"outgoing\" or \"incoming\" . !!! attributes hidden_dim ( int ): Size of the Bi_LSTM 's hidden layer out_dim (int): Size of the output layer (here: 2) diminisher (nn.Module): Fine-tuning embedding layer, good for reducing Bi-LSTM' s size lstm ( nn . Module ): Bi - LSTM for encoding a question hidden2tag ( nn . Module ): Output layer __init__ ( self , emb_dim , lstm_out_dim , num_classes ) special Initialises all required elements of the neural network. Parameters: Name Type Description Default emb_dim int Output size of the fine-tuning embedding layer required lstm_out_dim int Output size of the Bi-LSTM required num_classes int Size of the output layer (in this context, always 2) required Source code in adviser/examples/qa/worldknowledge/neuralmodels/director.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , emb_dim : int , lstm_out_dim : int , num_classes : int ): \"\"\"Initialises all required elements of the neural network. Args: emb_dim: Output size of the fine-tuning embedding layer lstm_out_dim: Output size of the Bi-LSTM num_classes: Size of the output layer (in this context, always 2) \"\"\" super ( Classifier , self ) . __init__ () self . hidden_dim = lstm_out_dim self . out_dim = num_classes self . diminisher = nn . Linear ( 768 , emb_dim ) self . lstm = nn . LSTM ( emb_dim , lstm_out_dim , bidirectional = True ) self . hidden2tag = nn . Linear ( lstm_out_dim * 2 , self . out_dim ) forward ( self , embeds ) Application of the neural network on a given input question. Parameters: Name Type Description Default embeds Tensor Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| required Returns: Type Description Tensor Probabilities of the two classes \"incoming\" and \"outgoing\" Source code in adviser/examples/qa/worldknowledge/neuralmodels/director.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def forward ( self , embeds : torch . Tensor ) -> torch . Tensor : \"\"\"Application of the neural network on a given input question. Args: embeds: Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| Returns: Probabilities of the two classes \"incoming\" and \"outgoing\" \"\"\" embeds = self . diminisher ( embeds ) lstm_out , _ = self . lstm ( embeds ) tag_space = self . hidden2tag ( lstm_out [ 0 ]) tag_scores = F . log_softmax ( tag_space , dim = 1 ) return tag_scores simpledot \u00b6 SimpleDot \u00b6 Neural network for predicting the relation of a question. The simple dot approach compares a question with each possible relation candidate by taking the ( simple ) dot product between the encoded question and every encoded relation . !!! attributes softmax ( bool ): whether or not the scores should be converted to probabilities hidden ( int ): size of the hidden layer relations_tensor ( torch . autograd . Variable ): embeddings for all relation descriptions diminisher ( nn . Module ): Fine - tuning embedding layer , good for reducing Bi - LSTMs ' size lstm_question ( nn . Module ): Bi - LSTM for encoding a question lstm_relation ( nn . Module ): Bi - LSTM for encoding a relation __init__ ( self , emb_dim , hidden_dim , softmax = True ) special Initialises all required elements of the neural network. Parameters: Name Type Description Default emb_dim int Output size of the fine-tuning embedding layer required hidden_dim int Output size of the Bi-LSTM required softmax bool Whether or not a softmax is applied on the output layer True Source code in adviser/examples/qa/worldknowledge/neuralmodels/simpledot.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , emb_dim : int , hidden_dim : int , softmax : bool = True ): \"\"\"Initialises all required elements of the neural network. Args: emb_dim: Output size of the fine-tuning embedding layer hidden_dim: Output size of the Bi-LSTM softmax: Whether or not a softmax is applied on the output layer \"\"\" super ( SimpleDot , self ) . __init__ () self . softmax = softmax self . hidden = hidden_dim self . relations_tensor = self . _initialise_relations_tensor () self . diminisher = nn . Linear ( 768 , emb_dim ) self . lstm_question = nn . LSTM ( emb_dim , hidden_dim , bidirectional = True ) self . lstm_relation = nn . LSTM ( emb_dim , hidden_dim , bidirectional = True ) forward ( self , embeds ) Application of the neural network on a given input question. Parameters: Name Type Description Default embeds Tensor Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| required Returns: Type Description Tensor Probabilities for the relation classes Source code in adviser/examples/qa/worldknowledge/neuralmodels/simpledot.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def forward ( self , embeds : torch . Tensor ) -> torch . Tensor : \"\"\"Application of the neural network on a given input question. Args: embeds: Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| Returns: Probabilities for the relation classes \"\"\" embeds = self . diminisher ( embeds ) relations_embeds = self . diminisher ( self . relations_tensor ) question_out , _ = self . lstm_question ( embeds ) # T_Q x B x H relation_out , _ = self . lstm_relation ( relations_embeds ) # T_R x R x H last_question_out = question_out [ 0 ][:, self . hidden :] # B x H last_relation_out = relation_out [ 0 ][:, self . hidden :] # R x H # relation prediction matrix = last_relation_out . repeat ( last_question_out . size ( 0 ), 1 , 1 ) # B x R x H vector = last_question_out rel_scores = torch . bmm ( matrix , vector . unsqueeze ( 2 )) . squeeze () if self . softmax : rel_scores = F . log_softmax ( rel_scores , dim = 1 ) return rel_scores get_root_dir () \u00b6 Source code in adviser/examples/qa/worldknowledge/neuralmodels/simpledot.py 8 9 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))))) tagger \u00b6 Tagger \u00b6 Neural network for predicting the topic entities. The model uses a question encoder and classifies each token using the BIO tag set . !!! attributes hidden_dim ( int ): Size of the Bi_LSTM 's hidden layer diminisher (nn.Module): Fine-tuning embedding layer, good for reducing Bi-LSTM' s size lstm ( nn . Module ): Bi - LSTM hidden2label ( nn . Module ): Output layer __init__ ( self , emb_dim , hidden_dim ) special Initialises all required elements of the neural network. Parameters: Name Type Description Default emb_dim int Output size of the fine-tuning embedding layer required hidden_dim int Hidden layer size of the Bi-LSTM required Source code in adviser/examples/qa/worldknowledge/neuralmodels/tagger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , emb_dim : int , hidden_dim : int ): \"\"\"Initialises all required elements of the neural network. Args: emb_dim: Output size of the fine-tuning embedding layer hidden_dim: Hidden layer size of the Bi-LSTM \"\"\" super ( Tagger , self ) . __init__ () self . hidden_dim = hidden_dim self . diminisher = nn . Linear ( 768 , emb_dim ) self . lstm = nn . LSTM ( emb_dim , hidden_dim , bidirectional = True ) self . hidden2label = nn . Linear ( hidden_dim * 2 , len ( TAGS )) forward ( self , embeds ) Application of the neural network on a given input question. Parameters: Name Type Description Default embeds Tensor Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| required Returns: Type Description Tensor Probabilities of each BIO tag for all tokens Source code in adviser/examples/qa/worldknowledge/neuralmodels/tagger.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def forward ( self , embeds : torch . Tensor ) -> torch . Tensor : \"\"\"Application of the neural network on a given input question. Args: embeds: Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| Returns: Probabilities of each BIO tag for all tokens \"\"\" embeds = self . diminisher ( embeds ) lstm_out , _ = self . lstm ( embeds ) label_space = self . hidden2label ( lstm_out [ 1 :]) label_scores = F . log_softmax ( label_space , dim = 2 ) return label_scores extract_entities ( tokens , tag_idxs ) \u00b6 Extracts entities using the predicted BIO tags for each token Parameters: Name Type Description Default tokens List[str] question's tokens required tag_idxs List[int] index of the BIO tag for each token in the question required Returns: Type Description List[List[str]] List of entities, i.e. list of connected tokens Source code in adviser/examples/qa/worldknowledge/neuralmodels/tagger.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def extract_entities ( tokens : List [ str ], tag_idxs : List [ int ]) -> List [ List [ str ]]: \"\"\"Extracts entities using the predicted BIO tags for each token Arguments: tokens: question's tokens tag_idxs: index of the BIO tag for each token in the question Returns: List of entities, i.e. list of connected tokens \"\"\" entities = [] curr_entity = [] tags = [ TAGS [ tag_idx ] for tag_idx in tag_idxs ] for i , ( token , tag ) in enumerate ( zip ( tokens , tags )): if tag == 'I' : curr_entity . append ( token ) continue else : if curr_entity : entities . append ( curr_entity ) curr_entity = [] if tag == 'B' : curr_entity . append ( token ) if curr_entity : entities . append ( curr_entity ) return entities policyqa \u00b6 QaPolicy \u00b6 Policy module for question answering. Provides a simple rule - based policy for question answering . The QA module assumes that the user acts contain information about relation , topic entities and relation direction . Adequate answers are looked up in the knowledge and published . The difference to the default HandcraftedPolicy is that no BST is needed and that multiple system acts can be published . __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Source code in adviser/examples/qa/worldknowledge/policyqa.py 41 42 43 def __init__ ( self , domain : LookupDomain , logger : DiasysLogger = DiasysLogger ()): # only call super class' constructor Service . __init__ ( self , domain = domain , debug_logger = logger ) generate_sys_acts ( self , * args , ** kwargs ) \u00b6 Source code in adviser/examples/qa/worldknowledge/policyqa.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result semanticparser \u00b6 QuestionParser \u00b6 Semantic parsing module for question answering !!! attributes device ( torch . device ) : PyTorch device object , either CPU or GPU nn_relation ( nn . Module ) : neural network for relation prediction nn_entity ( nn . Module ) : neural network for topic entity prediction nn_direction ( nn . Module ) : neural network for relation direction prediction tags ( List [ str ] ) : relation tags max_seq_len ( int ) : maximum number of tokens per question embedding_creator ( BertEmbedding ) : object creating BERT embeddings __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > , device = 'cpu' ) special \u00b6 Creates neural networks for semantic parsing and other required utils Parameters: Name Type Description Default domain LookupDomain the QA domain required logger DiasysLogger the logger <DiasysLogger adviser (NOTSET)> device str PyTorch device name 'cpu' Source code in adviser/examples/qa/worldknowledge/semanticparser.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , domain : LookupDomain , \\ logger : DiasysLogger = DiasysLogger (), device : str = 'cpu' ): \"\"\"Creates neural networks for semantic parsing and other required utils Args: domain: the QA domain logger: the logger device: PyTorch device name \"\"\" Service . __init__ ( self , domain = domain , debug_logger = logger ) self . device = torch . device ( device ) self . nn_relation = self . _load_relation_model () self . nn_entity = self . _load_entity_model () self . nn_direction = self . _load_direction_model () self . tags = self . _load_tag_set () self . max_seq_len = 40 self . embedding_creator = BertEmbedding ( max_seq_length = self . max_seq_len ) parse_user_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/examples/qa/worldknowledge/semanticparser.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result get_root_dir () \u00b6 Source code in adviser/examples/qa/worldknowledge/semanticparser.py 39 40 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))) webapi special \u00b6 mensa special \u00b6 domain \u00b6 MensaDomain \u00b6 Domain for the Mensa API !!! attributes parser ( MensaParser ) : HTML file parser for dynamically building a pseudo database last_results ( List [ dict ] ) : Current results which the user might request info about __init__ ( self ) special \u00b6 Source code in adviser/examples/webapi/mensa/domain.py 43 44 45 46 def __init__ ( self ): LookupDomain . __init__ ( self , 'MensaAPI' , 'Mensa Food' ) self . parser = MensaParser () self . last_results = [] find_entities ( self , constraints , requested_slots =< tuple_iterator object at 0x7f91b50be4c0 > ) \u00b6 Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict Slot-value mapping of constraints. If empty, all entities in the database will be returned. required requested_slots Iterable list of slots that should be returned in addition to the system requestable slots and the primary key <tuple_iterator object at 0x7f91b50be4c0> Source code in adviser/examples/webapi/mensa/domain.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): Slot-value mapping of constraints. If empty, all entities in the database will be returned. requested_slots (Iterable): list of slots that should be returned in addition to the system requestable slots and the primary key \"\"\" if 'day' in constraints : meals = self . parser . get_meals ( constraints [ 'day' ]) results = [ meal . as_dict () for meal in meals ] for slot in constraints : if slot == 'day' : continue results = [ candidate for candidate in results if candidate [ slot ] == constraints [ slot ]] for i , result in enumerate ( results ): result [ 'artificial_id' ] = i + 1 if list ( requested_slots ): cleaned_results = [{ slot : result_dict [ slot ] for slot in requested_slots } for result_dict in results ] else : cleaned_results = results self . last_results = results return cleaned_results else : return [] find_info_about_entity ( self , entity_id , requested_slots ) \u00b6 Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/examples/webapi/mensa/domain.py 75 76 77 78 79 80 81 82 83 84 85 def find_info_about_entity ( self , entity_id : str , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" result = { slot : self . last_results [ int ( entity_id ) - 1 ][ slot ] for slot in requested_slots } result [ 'artificial_id' ] = entity_id return [ result ] get_informable_slots ( self ) \u00b6 Returns a list of all informable slots. Source code in adviser/examples/webapi/mensa/domain.py 95 96 97 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return [ 'day' , 'type' , 'vegan' , 'vegetarian' , 'fish' , 'pork' ] get_keyword ( self ) \u00b6 Source code in adviser/examples/webapi/mensa/domain.py 120 121 def get_keyword ( self ): return 'mensa' get_mandatory_slots ( self ) \u00b6 Returns a list of all mandatory slots. Source code in adviser/examples/webapi/mensa/domain.py 99 100 101 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" return [ 'day' ] get_possible_values ( self , slot ) \u00b6 Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/examples/webapi/mensa/domain.py 103 104 105 106 107 108 109 110 111 112 113 114 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" assert slot in SLOT_VALUES return SLOT_VALUES [ slot ] get_primary_key ( self ) \u00b6 Returns the slot name that will be used as the 'name' of an entry Source code in adviser/examples/webapi/mensa/domain.py 116 117 118 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" return 'artificial_id' get_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the user. Source code in adviser/examples/webapi/mensa/domain.py 87 88 89 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return [ 'name' , 'type' , 'price' , 'allergens' , 'vegan' , 'vegetarian' , 'fish' , 'pork' ] get_system_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the system. Source code in adviser/examples/webapi/mensa/domain.py 91 92 93 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return [ 'day' , 'type' , 'vegan' , 'vegetarian' , 'fish' , 'pork' ] nlu \u00b6 MensaNLU \u00b6 Adapted handcrafted NLU for the mensa domain. The default handcrafted NLU is adapted to automatically add the user act request(name). This is necessary because the name is not the primary key, i.e. it is not printed by default once an element is found. To force the Policy to automatically inform about the name, too, a request for the name is added in each turn. __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Source code in adviser/examples/webapi/mensa/nlu.py 42 43 44 def __init__ ( self , domain : LookupDomain , logger : DiasysLogger = DiasysLogger ()): # only calls super class' constructor HandcraftedNLU . __init__ ( self , domain , logger ) extract_user_acts ( self , * args , ** kwargs ) \u00b6 Source code in adviser/examples/webapi/mensa/nlu.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result parser \u00b6 Allergen \u00b6 This enum provides the allergens used in the mensa menu. __new__ ( cls , value ) special \u00b6 Source code in adviser/examples/webapi/mensa/parser.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc DishType \u00b6 This enum provides the dish types used in the mensa menu. __new__ ( cls , value ) special \u00b6 Source code in adviser/examples/webapi/mensa/parser.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc from_website_name ( website_name ) staticmethod \u00b6 Converts the type as listed on the website into the type used in the dialog system. Parameters: Name Type Description Default website_name str The name as used in the response to the POST request. required Returns: Type Description DishType The corresponding enum member. Source code in adviser/examples/webapi/mensa/parser.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 @staticmethod def from_website_name ( website_name : str ) -> 'DishType' : \"\"\"Converts the type as listed on the website into the type used in the dialog system. Args: website_name: The name as used in the response to the POST request. Returns: The corresponding enum member. \"\"\" if website_name == 'STARTER' : return DishType . Starter elif website_name == 'BUFFET' : return DishType . Buffet elif website_name == 'MAIN DISH' : return DishType . MainDish elif website_name == 'SIDE DISH' : return DishType . SideDish elif website_name == 'DESSERT' : return DishType . Dessert Location \u00b6 This enum provides the possible mensa locations. __new__ ( cls , value ) special \u00b6 Source code in adviser/examples/webapi/mensa/parser.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc Meal \u00b6 __init__ ( self , name , day , prices , price_quantity , allergens , vegan , vegetarian , fish , pork , dish_type ) special \u00b6 The class for a meal consisting of a name and several properties (slot-value pairs). Parameters: Name Type Description Default name str The name of the meal. required day str The day on which the meal is offered. required prices Tuple[float] The price for students and guests. required price_quantity str The unit for which the price is valid. required allergens List[adviser.examples.webapi.mensa.parser.Allergen] The allergens of this meal. required vegan bool Whether the meal is vegan or not. required vegetarian bool Whether the meal is vegetarian or not. required fish bool Whether the meal contains fish or not. required pork bool Whether the meal contains pork or not. required dish_type DishType The type of the dish. (Starter, Buffet, Main Dish, Side Dish or Buffet) required Source code in adviser/examples/webapi/mensa/parser.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , name : str , day : str , prices : Tuple [ float ], price_quantity : str , \\ allergens : List [ Allergen ], vegan : bool , vegetarian : bool , fish : bool , pork : bool , \\ dish_type : DishType ): \"\"\"The class for a meal consisting of a name and several properties (slot-value pairs). Args: name: The name of the meal. day: The day on which the meal is offered. prices: The price for students and guests. price_quantity: The unit for which the price is valid. allergens: The allergens of this meal. vegan: Whether the meal is vegan or not. vegetarian: Whether the meal is vegetarian or not. fish: Whether the meal contains fish or not. pork: Whether the meal contains pork or not. dish_type: The type of the dish. (Starter, Buffet, Main Dish, Side Dish or Buffet) \"\"\" self . name = name self . day = day self . prices = prices self . price_quantity = price_quantity self . allergens = allergens self . vegan = vegan self . vegetarian = vegetarian self . fish = fish self . pork = pork self . dish_type = dish_type __repr__ ( self ) special \u00b6 The string representation of the meal. Source code in adviser/examples/webapi/mensa/parser.py 162 163 164 165 def __repr__ ( self ) -> str : \"\"\"The string representation of the meal.\"\"\" return str ( self ) __str__ ( self ) special \u00b6 The string representation of the meal. Source code in adviser/examples/webapi/mensa/parser.py 154 155 156 157 158 159 160 def __str__ ( self ) -> str : \"\"\"The string representation of the meal.\"\"\" return ( f \"Meal(name= { self . name } , day= { self . day } , prices= { self . prices } , \\ price_quantity= { self . price_quantity } , \" f \"allergens= { self . allergens } , vegan= { self . vegan } , vegetarian= { self . vegetarian } , \" f \"fish= { self . fish } , pork= { self . pork } , dish_type= { self . dish_type } )\" ) as_dict ( self ) \u00b6 A dict representation of the meal. Source code in adviser/examples/webapi/mensa/parser.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def as_dict ( self ) -> Dict [ str , str ]: \"\"\"A dict representation of the meal.\"\"\" return { 'name' : self . name , 'day' : self . day , 'type' : self . dish_type . value , 'price' : str ( self . prices [ 0 ]), 'allergens' : ', ' . join ([ allergen . value for allergen in self . allergens ]) if \\ self . allergens is not None else 'none' , 'vegan' : str ( self . vegan ) . lower (), 'vegetarian' : str ( self . vegetarian ) . lower (), 'fish' : str ( self . fish ) . lower (), 'pork' : str ( self . pork ) . lower () } MensaParser \u00b6 __init__ ( self , cache = True ) special \u00b6 The class to issue post requests and parse the response. Will also take care of caching the parser's results. Parameters: Name Type Description Default cache bool Whether to cache results or not. True Source code in adviser/examples/webapi/mensa/parser.py 169 170 171 172 173 174 175 176 177 178 179 180 181 def __init__ ( self , cache : bool = True ): \"\"\" The class to issue post requests and parse the response. Will also take care of caching the parser's results. Args: cache (bool): Whether to cache results or not. \"\"\" #: dict of str: storgae to cache parsed meals self . storage = {} self . cache = cache get_meals ( self , date , use_cache = True ) \u00b6 Gets the meals for a specified day by either looking them up in the cache or by issuing and parsing a post request . Args : date ( str ): The date for which the data will be returned . Can be a string in the format 'Y-m-d' or one of today , tomorrow and monday - sunday . use_cache ( bool ): If False will always query the server instead of using the cache . Returns: Type Description List[adviser.examples.webapi.mensa.parser.Meal] :obj: list of Meal: List of meals for specified date Source code in adviser/examples/webapi/mensa/parser.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def get_meals ( self , date : str , use_cache : bool = True ) -> List [ Meal ]: \"\"\" Gets the meals for a specified day by either looking them up in the cache or by issuing and parsing a post request. Args: date (str): The date for which the data will be returned. Can be a string in the format 'Y-m-d' or one of today, tomorrow and monday-sunday. use_cache (bool): If False will always query the server instead of using the cache. Returns: :obj:`list` of Meal: List of meals for specified date \"\"\" date = self . _parse_date ( date ) if use_cache and date . date () in self . storage : # NOTE data could be deprecated return self . storage [ date . date ()] else : # issue request to server return self . _parse ( date ) ParseDateError \u00b6 This exception is raised when the date cannot be parsed. weather special \u00b6 domain \u00b6 WeatherDomain \u00b6 Domain for the Weather API. !!! attributes last_results ( List [ dict ] ) : Current results which the user might request info about __init__ ( self ) special \u00b6 Source code in adviser/examples/webapi/weather/domain.py 37 38 39 40 def __init__ ( self ): LookupDomain . __init__ ( self , 'WeatherAPI' , 'Weather' ) self . last_results = [] find_entities ( self , constraints , requested_slots =< tuple_iterator object at 0x7f91b46470d0 > ) \u00b6 Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict Slot-value mapping of constraints. If empty, all entities in the database will be returned. required requested_slots Iterable list of slots that should be returned in addition to the system requestable slots and the primary key <tuple_iterator object at 0x7f91b46470d0> Source code in adviser/examples/webapi/weather/domain.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): Slot-value mapping of constraints. If empty, all entities in the database will be returned. requested_slots (Iterable): list of slots that should be returned in addition to the system requestable slots and the primary key \"\"\" if 'location' in constraints and 'date' in constraints : forecast = self . _query ( constraints [ 'location' ], constraints [ 'date' ]) if forecast is None : return [] temperature = int ( ' %.0f ' % ( float ( forecast [ 'main' ][ 'temp' ]) - 273.15 )) description = forecast [ 'weather' ][ 0 ][ 'description' ] result_dict = { 'artificial_id' : str ( len ( self . last_results )), 'temperature' : temperature , 'description' : description , 'location' : constraints [ 'location' ], 'date' : constraints [ 'date' ], } if any ( True for _ in requested_slots ): cleaned_result_dict = { slot : result_dict [ slot ] for slot in requested_slots } else : cleaned_result_dict = result_dict self . last_results . append ( cleaned_result_dict ) return [ cleaned_result_dict ] else : return [] find_info_about_entity ( self , entity_id , requested_slots ) \u00b6 Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/examples/webapi/weather/domain.py 76 77 78 79 80 81 82 83 84 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" return [ self . last_results [ int ( entity_id )]] get_informable_slots ( self ) \u00b6 Returns a list of all informable slots. Source code in adviser/examples/webapi/weather/domain.py 94 95 96 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return [ 'location' , 'date' ] get_keyword ( self ) \u00b6 Source code in adviser/examples/webapi/weather/domain.py 148 149 def get_keyword ( self ): return 'weather' get_mandatory_slots ( self ) \u00b6 Returns a list of all mandatory slots. Source code in adviser/examples/webapi/weather/domain.py 98 99 100 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" return [ 'location' , 'date' ] get_possible_values ( self , slot ) \u00b6 Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/examples/webapi/weather/domain.py 102 103 104 105 106 107 108 109 110 111 112 113 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" raise BaseException ( 'all slots in this domain do not have a fixed set of ' 'values, so this method should never be called' ) get_primary_key ( self ) \u00b6 Returns the slot name that will be used as the 'name' of an entry Source code in adviser/examples/webapi/weather/domain.py 115 116 117 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" return 'artificial_id' get_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the user. Source code in adviser/examples/webapi/weather/domain.py 86 87 88 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return [ 'temperature' , 'description' ] get_system_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the system. Source code in adviser/examples/webapi/weather/domain.py 90 91 92 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return [ 'location' , 'date' ] nlg \u00b6 WeatherNLG \u00b6 Simple NLG for the weather domain __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Source code in adviser/examples/webapi/weather/nlg.py 31 32 33 def __init__ ( self , domain , logger = DiasysLogger ()): # only calls super class' constructor super ( WeatherNLG , self ) . __init__ ( domain , debug_logger = logger ) generate_system_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/examples/webapi/weather/nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result nlu \u00b6 WeatherNLU \u00b6 Very simple NLU for the weather domain. __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Source code in adviser/examples/webapi/weather/nlu.py 45 46 47 def __init__ ( self , domain , logger = DiasysLogger ()): # only calls super class' constructor super ( WeatherNLU , self ) . __init__ ( domain , debug_logger = logger ) extract_user_acts ( self , * args , ** kwargs ) \u00b6 Source code in adviser/examples/webapi/weather/nlu.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"Examples"},{"location":"api/examples/#examples","text":"","title":"Examples"},{"location":"api/examples/#adviser.examples","text":"","title":"adviser.examples"},{"location":"api/examples/#adviser.examples.qa","text":"","title":"qa"},{"location":"api/examples/#adviser.examples.qa.worldknowledge","text":"","title":"worldknowledge"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain","text":"","title":"domain"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain","text":"Question answering for the world knowledge domain. Attributes artificial_id_counter (int): pseudo identifier for each entry name_lex (Dict[str,str]): lexicon for matching topic's names to their KG entity","title":"WorldKnowledgeDomain"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.__init__","text":"Calls super class' constructor and loads name lexicon Source code in adviser/examples/qa/worldknowledge/domain.py 41 42 43 44 45 46 47 48 49 def __init__ ( self ): \"\"\"Calls super class' constructor and loads name lexicon\"\"\" LookupDomain . __init__ ( self , 'CSQA' , 'World Knowledge' ) self . artificial_id_counter = 1 #int: lexicon for matching topic's names to their KG entity self . name_lex = self . _init_name_lexicon () \"\"\"Dict[str,str]: lexicon for matching topic's names to their KG entity.\"\"\"","title":"__init__()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.find_entities","text":"Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict slot-value mapping of constraints required Source code in adviser/examples/qa/worldknowledge/domain.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def find_entities ( self , constraints : dict ): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): slot-value mapping of constraints \"\"\" assert 'relation' in constraints assert 'topic' in constraints assert 'direction' in constraints topics = self . _find_topic_entities ( constraints [ 'topic' ]) if not topics : return [] answers = [] for topic_id , topic_label in topics : answer_ids = [] if constraints [ 'direction' ] == 'out' : answer_ids = self . _perform_out_query ( constraints [ 'relation' ], topic_id ) for answer_id in answer_ids : answers . append ({ 'subject' : topic_label , 'predicate' : constraints [ 'relation' ], 'object' : answer_id }) self . artificial_id_counter += 1 else : answer_ids = self . _perform_in_query ( constraints [ 'relation' ], topic_id ) for answer_id in answer_ids : answers . append ({ 'subject' : answer_id , 'predicate' : constraints [ 'relation' ], 'object' : topic_label }) self . artificial_id_counter += 1 return answers","title":"find_entities()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.find_info_about_entity","text":"Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/examples/qa/worldknowledge/domain.py 133 134 135 136 137 138 139 140 141 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" raise BaseException ( 'should not be called' )","title":"find_info_about_entity()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_domain_name","text":"Return the domain name of the current ontology. Returns: Type Description object Source code in adviser/examples/qa/worldknowledge/domain.py 143 144 def get_domain_name ( self ): return \"qa\"","title":"get_domain_name()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_informable_slots","text":"Returns a list of all informable slots. Source code in adviser/examples/qa/worldknowledge/domain.py 154 155 156 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return [ 'relation' , 'topic' , 'direction' ]","title":"get_informable_slots()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_keyword","text":"Source code in adviser/examples/qa/worldknowledge/domain.py 179 180 def get_keyword ( self ): return 'world knowledge'","title":"get_keyword()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_mandatory_slots","text":"Returns a list of all mandatory slots. Source code in adviser/examples/qa/worldknowledge/domain.py 158 159 160 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" return [ 'relation' , 'topic' , 'direction' ]","title":"get_mandatory_slots()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_possible_values","text":"Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/examples/qa/worldknowledge/domain.py 162 163 164 165 166 167 168 169 170 171 172 173 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" # 'assert False, \"this method should not be called\"' raise BaseException ( 'should not be called' )","title":"get_possible_values()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_primary_key","text":"Returns the slot name that will be used as the 'name' of an entry Source code in adviser/examples/qa/worldknowledge/domain.py 175 176 177 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" return 'artificial_id'","title":"get_primary_key()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_requestable_slots","text":"Returns a list of all slots requestable by the user. Source code in adviser/examples/qa/worldknowledge/domain.py 146 147 148 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return [ 'subject' , 'predicate' , 'object' , 'object_type' ]","title":"get_requestable_slots()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.WorldKnowledgeDomain.get_system_requestable_slots","text":"Returns a list of all slots requestable by the system. Source code in adviser/examples/qa/worldknowledge/domain.py 150 151 152 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return [ 'relation' , 'topic' , 'direction' ]","title":"get_system_requestable_slots()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.domain.get_root_dir","text":"Source code in adviser/examples/qa/worldknowledge/domain.py 27 28 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))))","title":"get_root_dir()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.multinlg","text":"Handcrafted (i.e. template-based) Natural Language Generation Module","title":"multinlg"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.multinlg.MultiNLG","text":"Extension of the handcrafted NLG by allowing multiple system acts. This change is necessary for QA, since the policy publishes multiple system acts.","title":"MultiNLG"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.multinlg.MultiNLG.__init__","text":"Source code in adviser/examples/qa/worldknowledge/multinlg.py 39 40 41 42 43 def __init__ ( self , domain : Domain , template_file : str = None , sub_topic_domains : Dict [ str , str ] = {}, logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): # only calls the super class' constructor HandcraftedNLG . __init__ ( self , domain , template_file , sub_topic_domains , logger , template_file_german , language )","title":"__init__()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.multinlg.MultiNLG.publish_system_utterance","text":"Source code in adviser/examples/qa/worldknowledge/multinlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"publish_system_utterance()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels","text":"","title":"neuralmodels"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.director","text":"","title":"director"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.director.Classifier","text":"Neural network for predicting the relation's direction (outgoing or incoming). The model uses a question encoder to classify the question as one of the two classes \"outgoing\" or \"incoming\" . !!! attributes hidden_dim ( int ): Size of the Bi_LSTM 's hidden layer out_dim (int): Size of the output layer (here: 2) diminisher (nn.Module): Fine-tuning embedding layer, good for reducing Bi-LSTM' s size lstm ( nn . Module ): Bi - LSTM for encoding a question hidden2tag ( nn . Module ): Output layer __init__ ( self , emb_dim , lstm_out_dim , num_classes ) special Initialises all required elements of the neural network. Parameters: Name Type Description Default emb_dim int Output size of the fine-tuning embedding layer required lstm_out_dim int Output size of the Bi-LSTM required num_classes int Size of the output layer (in this context, always 2) required Source code in adviser/examples/qa/worldknowledge/neuralmodels/director.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , emb_dim : int , lstm_out_dim : int , num_classes : int ): \"\"\"Initialises all required elements of the neural network. Args: emb_dim: Output size of the fine-tuning embedding layer lstm_out_dim: Output size of the Bi-LSTM num_classes: Size of the output layer (in this context, always 2) \"\"\" super ( Classifier , self ) . __init__ () self . hidden_dim = lstm_out_dim self . out_dim = num_classes self . diminisher = nn . Linear ( 768 , emb_dim ) self . lstm = nn . LSTM ( emb_dim , lstm_out_dim , bidirectional = True ) self . hidden2tag = nn . Linear ( lstm_out_dim * 2 , self . out_dim ) forward ( self , embeds ) Application of the neural network on a given input question. Parameters: Name Type Description Default embeds Tensor Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| required Returns: Type Description Tensor Probabilities of the two classes \"incoming\" and \"outgoing\" Source code in adviser/examples/qa/worldknowledge/neuralmodels/director.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def forward ( self , embeds : torch . Tensor ) -> torch . Tensor : \"\"\"Application of the neural network on a given input question. Args: embeds: Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| Returns: Probabilities of the two classes \"incoming\" and \"outgoing\" \"\"\" embeds = self . diminisher ( embeds ) lstm_out , _ = self . lstm ( embeds ) tag_space = self . hidden2tag ( lstm_out [ 0 ]) tag_scores = F . log_softmax ( tag_space , dim = 1 ) return tag_scores","title":"Classifier"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.simpledot","text":"","title":"simpledot"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.simpledot.SimpleDot","text":"Neural network for predicting the relation of a question. The simple dot approach compares a question with each possible relation candidate by taking the ( simple ) dot product between the encoded question and every encoded relation . !!! attributes softmax ( bool ): whether or not the scores should be converted to probabilities hidden ( int ): size of the hidden layer relations_tensor ( torch . autograd . Variable ): embeddings for all relation descriptions diminisher ( nn . Module ): Fine - tuning embedding layer , good for reducing Bi - LSTMs ' size lstm_question ( nn . Module ): Bi - LSTM for encoding a question lstm_relation ( nn . Module ): Bi - LSTM for encoding a relation __init__ ( self , emb_dim , hidden_dim , softmax = True ) special Initialises all required elements of the neural network. Parameters: Name Type Description Default emb_dim int Output size of the fine-tuning embedding layer required hidden_dim int Output size of the Bi-LSTM required softmax bool Whether or not a softmax is applied on the output layer True Source code in adviser/examples/qa/worldknowledge/neuralmodels/simpledot.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , emb_dim : int , hidden_dim : int , softmax : bool = True ): \"\"\"Initialises all required elements of the neural network. Args: emb_dim: Output size of the fine-tuning embedding layer hidden_dim: Output size of the Bi-LSTM softmax: Whether or not a softmax is applied on the output layer \"\"\" super ( SimpleDot , self ) . __init__ () self . softmax = softmax self . hidden = hidden_dim self . relations_tensor = self . _initialise_relations_tensor () self . diminisher = nn . Linear ( 768 , emb_dim ) self . lstm_question = nn . LSTM ( emb_dim , hidden_dim , bidirectional = True ) self . lstm_relation = nn . LSTM ( emb_dim , hidden_dim , bidirectional = True ) forward ( self , embeds ) Application of the neural network on a given input question. Parameters: Name Type Description Default embeds Tensor Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| required Returns: Type Description Tensor Probabilities for the relation classes Source code in adviser/examples/qa/worldknowledge/neuralmodels/simpledot.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def forward ( self , embeds : torch . Tensor ) -> torch . Tensor : \"\"\"Application of the neural network on a given input question. Args: embeds: Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| Returns: Probabilities for the relation classes \"\"\" embeds = self . diminisher ( embeds ) relations_embeds = self . diminisher ( self . relations_tensor ) question_out , _ = self . lstm_question ( embeds ) # T_Q x B x H relation_out , _ = self . lstm_relation ( relations_embeds ) # T_R x R x H last_question_out = question_out [ 0 ][:, self . hidden :] # B x H last_relation_out = relation_out [ 0 ][:, self . hidden :] # R x H # relation prediction matrix = last_relation_out . repeat ( last_question_out . size ( 0 ), 1 , 1 ) # B x R x H vector = last_question_out rel_scores = torch . bmm ( matrix , vector . unsqueeze ( 2 )) . squeeze () if self . softmax : rel_scores = F . log_softmax ( rel_scores , dim = 1 ) return rel_scores","title":"SimpleDot"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.simpledot.get_root_dir","text":"Source code in adviser/examples/qa/worldknowledge/neuralmodels/simpledot.py 8 9 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))))","title":"get_root_dir()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.tagger","text":"","title":"tagger"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.tagger.Tagger","text":"Neural network for predicting the topic entities. The model uses a question encoder and classifies each token using the BIO tag set . !!! attributes hidden_dim ( int ): Size of the Bi_LSTM 's hidden layer diminisher (nn.Module): Fine-tuning embedding layer, good for reducing Bi-LSTM' s size lstm ( nn . Module ): Bi - LSTM hidden2label ( nn . Module ): Output layer __init__ ( self , emb_dim , hidden_dim ) special Initialises all required elements of the neural network. Parameters: Name Type Description Default emb_dim int Output size of the fine-tuning embedding layer required hidden_dim int Hidden layer size of the Bi-LSTM required Source code in adviser/examples/qa/worldknowledge/neuralmodels/tagger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , emb_dim : int , hidden_dim : int ): \"\"\"Initialises all required elements of the neural network. Args: emb_dim: Output size of the fine-tuning embedding layer hidden_dim: Hidden layer size of the Bi-LSTM \"\"\" super ( Tagger , self ) . __init__ () self . hidden_dim = hidden_dim self . diminisher = nn . Linear ( 768 , emb_dim ) self . lstm = nn . LSTM ( emb_dim , hidden_dim , bidirectional = True ) self . hidden2label = nn . Linear ( hidden_dim * 2 , len ( TAGS )) forward ( self , embeds ) Application of the neural network on a given input question. Parameters: Name Type Description Default embeds Tensor Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| required Returns: Type Description Tensor Probabilities of each BIO tag for all tokens Source code in adviser/examples/qa/worldknowledge/neuralmodels/tagger.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def forward ( self , embeds : torch . Tensor ) -> torch . Tensor : \"\"\"Application of the neural network on a given input question. Args: embeds: Tensor containing the embeddings of shape |Token| x |Batch| x |Embedding Size| Returns: Probabilities of each BIO tag for all tokens \"\"\" embeds = self . diminisher ( embeds ) lstm_out , _ = self . lstm ( embeds ) label_space = self . hidden2label ( lstm_out [ 1 :]) label_scores = F . log_softmax ( label_space , dim = 2 ) return label_scores","title":"Tagger"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.neuralmodels.tagger.extract_entities","text":"Extracts entities using the predicted BIO tags for each token Parameters: Name Type Description Default tokens List[str] question's tokens required tag_idxs List[int] index of the BIO tag for each token in the question required Returns: Type Description List[List[str]] List of entities, i.e. list of connected tokens Source code in adviser/examples/qa/worldknowledge/neuralmodels/tagger.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def extract_entities ( tokens : List [ str ], tag_idxs : List [ int ]) -> List [ List [ str ]]: \"\"\"Extracts entities using the predicted BIO tags for each token Arguments: tokens: question's tokens tag_idxs: index of the BIO tag for each token in the question Returns: List of entities, i.e. list of connected tokens \"\"\" entities = [] curr_entity = [] tags = [ TAGS [ tag_idx ] for tag_idx in tag_idxs ] for i , ( token , tag ) in enumerate ( zip ( tokens , tags )): if tag == 'I' : curr_entity . append ( token ) continue else : if curr_entity : entities . append ( curr_entity ) curr_entity = [] if tag == 'B' : curr_entity . append ( token ) if curr_entity : entities . append ( curr_entity ) return entities","title":"extract_entities()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.policyqa","text":"","title":"policyqa"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.policyqa.QaPolicy","text":"Policy module for question answering. Provides a simple rule - based policy for question answering . The QA module assumes that the user acts contain information about relation , topic entities and relation direction . Adequate answers are looked up in the knowledge and published . The difference to the default HandcraftedPolicy is that no BST is needed and that multiple system acts can be published .","title":"QaPolicy"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.policyqa.QaPolicy.__init__","text":"Source code in adviser/examples/qa/worldknowledge/policyqa.py 41 42 43 def __init__ ( self , domain : LookupDomain , logger : DiasysLogger = DiasysLogger ()): # only call super class' constructor Service . __init__ ( self , domain = domain , debug_logger = logger )","title":"__init__()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.policyqa.QaPolicy.generate_sys_acts","text":"Source code in adviser/examples/qa/worldknowledge/policyqa.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"generate_sys_acts()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.semanticparser","text":"","title":"semanticparser"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.semanticparser.QuestionParser","text":"Semantic parsing module for question answering !!! attributes device ( torch . device ) : PyTorch device object , either CPU or GPU nn_relation ( nn . Module ) : neural network for relation prediction nn_entity ( nn . Module ) : neural network for topic entity prediction nn_direction ( nn . Module ) : neural network for relation direction prediction tags ( List [ str ] ) : relation tags max_seq_len ( int ) : maximum number of tokens per question embedding_creator ( BertEmbedding ) : object creating BERT embeddings","title":"QuestionParser"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.semanticparser.QuestionParser.__init__","text":"Creates neural networks for semantic parsing and other required utils Parameters: Name Type Description Default domain LookupDomain the QA domain required logger DiasysLogger the logger <DiasysLogger adviser (NOTSET)> device str PyTorch device name 'cpu' Source code in adviser/examples/qa/worldknowledge/semanticparser.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , domain : LookupDomain , \\ logger : DiasysLogger = DiasysLogger (), device : str = 'cpu' ): \"\"\"Creates neural networks for semantic parsing and other required utils Args: domain: the QA domain logger: the logger device: PyTorch device name \"\"\" Service . __init__ ( self , domain = domain , debug_logger = logger ) self . device = torch . device ( device ) self . nn_relation = self . _load_relation_model () self . nn_entity = self . _load_entity_model () self . nn_direction = self . _load_direction_model () self . tags = self . _load_tag_set () self . max_seq_len = 40 self . embedding_creator = BertEmbedding ( max_seq_length = self . max_seq_len )","title":"__init__()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.semanticparser.QuestionParser.parse_user_utterance","text":"Source code in adviser/examples/qa/worldknowledge/semanticparser.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"parse_user_utterance()"},{"location":"api/examples/#adviser.examples.qa.worldknowledge.semanticparser.get_root_dir","text":"Source code in adviser/examples/qa/worldknowledge/semanticparser.py 39 40 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))))","title":"get_root_dir()"},{"location":"api/examples/#adviser.examples.webapi","text":"","title":"webapi"},{"location":"api/examples/#adviser.examples.webapi.mensa","text":"","title":"mensa"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain","text":"","title":"domain"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain","text":"Domain for the Mensa API !!! attributes parser ( MensaParser ) : HTML file parser for dynamically building a pseudo database last_results ( List [ dict ] ) : Current results which the user might request info about","title":"MensaDomain"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.__init__","text":"Source code in adviser/examples/webapi/mensa/domain.py 43 44 45 46 def __init__ ( self ): LookupDomain . __init__ ( self , 'MensaAPI' , 'Mensa Food' ) self . parser = MensaParser () self . last_results = []","title":"__init__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.find_entities","text":"Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict Slot-value mapping of constraints. If empty, all entities in the database will be returned. required requested_slots Iterable list of slots that should be returned in addition to the system requestable slots and the primary key <tuple_iterator object at 0x7f91b50be4c0> Source code in adviser/examples/webapi/mensa/domain.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): Slot-value mapping of constraints. If empty, all entities in the database will be returned. requested_slots (Iterable): list of slots that should be returned in addition to the system requestable slots and the primary key \"\"\" if 'day' in constraints : meals = self . parser . get_meals ( constraints [ 'day' ]) results = [ meal . as_dict () for meal in meals ] for slot in constraints : if slot == 'day' : continue results = [ candidate for candidate in results if candidate [ slot ] == constraints [ slot ]] for i , result in enumerate ( results ): result [ 'artificial_id' ] = i + 1 if list ( requested_slots ): cleaned_results = [{ slot : result_dict [ slot ] for slot in requested_slots } for result_dict in results ] else : cleaned_results = results self . last_results = results return cleaned_results else : return []","title":"find_entities()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.find_info_about_entity","text":"Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/examples/webapi/mensa/domain.py 75 76 77 78 79 80 81 82 83 84 85 def find_info_about_entity ( self , entity_id : str , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" result = { slot : self . last_results [ int ( entity_id ) - 1 ][ slot ] for slot in requested_slots } result [ 'artificial_id' ] = entity_id return [ result ]","title":"find_info_about_entity()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.get_informable_slots","text":"Returns a list of all informable slots. Source code in adviser/examples/webapi/mensa/domain.py 95 96 97 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return [ 'day' , 'type' , 'vegan' , 'vegetarian' , 'fish' , 'pork' ]","title":"get_informable_slots()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.get_keyword","text":"Source code in adviser/examples/webapi/mensa/domain.py 120 121 def get_keyword ( self ): return 'mensa'","title":"get_keyword()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.get_mandatory_slots","text":"Returns a list of all mandatory slots. Source code in adviser/examples/webapi/mensa/domain.py 99 100 101 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" return [ 'day' ]","title":"get_mandatory_slots()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.get_possible_values","text":"Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/examples/webapi/mensa/domain.py 103 104 105 106 107 108 109 110 111 112 113 114 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" assert slot in SLOT_VALUES return SLOT_VALUES [ slot ]","title":"get_possible_values()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.get_primary_key","text":"Returns the slot name that will be used as the 'name' of an entry Source code in adviser/examples/webapi/mensa/domain.py 116 117 118 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" return 'artificial_id'","title":"get_primary_key()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.get_requestable_slots","text":"Returns a list of all slots requestable by the user. Source code in adviser/examples/webapi/mensa/domain.py 87 88 89 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return [ 'name' , 'type' , 'price' , 'allergens' , 'vegan' , 'vegetarian' , 'fish' , 'pork' ]","title":"get_requestable_slots()"},{"location":"api/examples/#adviser.examples.webapi.mensa.domain.MensaDomain.get_system_requestable_slots","text":"Returns a list of all slots requestable by the system. Source code in adviser/examples/webapi/mensa/domain.py 91 92 93 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return [ 'day' , 'type' , 'vegan' , 'vegetarian' , 'fish' , 'pork' ]","title":"get_system_requestable_slots()"},{"location":"api/examples/#adviser.examples.webapi.mensa.nlu","text":"","title":"nlu"},{"location":"api/examples/#adviser.examples.webapi.mensa.nlu.MensaNLU","text":"Adapted handcrafted NLU for the mensa domain. The default handcrafted NLU is adapted to automatically add the user act request(name). This is necessary because the name is not the primary key, i.e. it is not printed by default once an element is found. To force the Policy to automatically inform about the name, too, a request for the name is added in each turn.","title":"MensaNLU"},{"location":"api/examples/#adviser.examples.webapi.mensa.nlu.MensaNLU.__init__","text":"Source code in adviser/examples/webapi/mensa/nlu.py 42 43 44 def __init__ ( self , domain : LookupDomain , logger : DiasysLogger = DiasysLogger ()): # only calls super class' constructor HandcraftedNLU . __init__ ( self , domain , logger )","title":"__init__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.nlu.MensaNLU.extract_user_acts","text":"Source code in adviser/examples/webapi/mensa/nlu.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"extract_user_acts()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser","text":"","title":"parser"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Allergen","text":"This enum provides the allergens used in the mensa menu.","title":"Allergen"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Allergen.__new__","text":"Source code in adviser/examples/webapi/mensa/parser.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.DishType","text":"This enum provides the dish types used in the mensa menu.","title":"DishType"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.DishType.__new__","text":"Source code in adviser/examples/webapi/mensa/parser.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.DishType.from_website_name","text":"Converts the type as listed on the website into the type used in the dialog system. Parameters: Name Type Description Default website_name str The name as used in the response to the POST request. required Returns: Type Description DishType The corresponding enum member. Source code in adviser/examples/webapi/mensa/parser.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 @staticmethod def from_website_name ( website_name : str ) -> 'DishType' : \"\"\"Converts the type as listed on the website into the type used in the dialog system. Args: website_name: The name as used in the response to the POST request. Returns: The corresponding enum member. \"\"\" if website_name == 'STARTER' : return DishType . Starter elif website_name == 'BUFFET' : return DishType . Buffet elif website_name == 'MAIN DISH' : return DishType . MainDish elif website_name == 'SIDE DISH' : return DishType . SideDish elif website_name == 'DESSERT' : return DishType . Dessert","title":"from_website_name()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Location","text":"This enum provides the possible mensa locations.","title":"Location"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Location.__new__","text":"Source code in adviser/examples/webapi/mensa/parser.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Meal","text":"","title":"Meal"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Meal.__init__","text":"The class for a meal consisting of a name and several properties (slot-value pairs). Parameters: Name Type Description Default name str The name of the meal. required day str The day on which the meal is offered. required prices Tuple[float] The price for students and guests. required price_quantity str The unit for which the price is valid. required allergens List[adviser.examples.webapi.mensa.parser.Allergen] The allergens of this meal. required vegan bool Whether the meal is vegan or not. required vegetarian bool Whether the meal is vegetarian or not. required fish bool Whether the meal contains fish or not. required pork bool Whether the meal contains pork or not. required dish_type DishType The type of the dish. (Starter, Buffet, Main Dish, Side Dish or Buffet) required Source code in adviser/examples/webapi/mensa/parser.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , name : str , day : str , prices : Tuple [ float ], price_quantity : str , \\ allergens : List [ Allergen ], vegan : bool , vegetarian : bool , fish : bool , pork : bool , \\ dish_type : DishType ): \"\"\"The class for a meal consisting of a name and several properties (slot-value pairs). Args: name: The name of the meal. day: The day on which the meal is offered. prices: The price for students and guests. price_quantity: The unit for which the price is valid. allergens: The allergens of this meal. vegan: Whether the meal is vegan or not. vegetarian: Whether the meal is vegetarian or not. fish: Whether the meal contains fish or not. pork: Whether the meal contains pork or not. dish_type: The type of the dish. (Starter, Buffet, Main Dish, Side Dish or Buffet) \"\"\" self . name = name self . day = day self . prices = prices self . price_quantity = price_quantity self . allergens = allergens self . vegan = vegan self . vegetarian = vegetarian self . fish = fish self . pork = pork self . dish_type = dish_type","title":"__init__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Meal.__repr__","text":"The string representation of the meal. Source code in adviser/examples/webapi/mensa/parser.py 162 163 164 165 def __repr__ ( self ) -> str : \"\"\"The string representation of the meal.\"\"\" return str ( self )","title":"__repr__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Meal.__str__","text":"The string representation of the meal. Source code in adviser/examples/webapi/mensa/parser.py 154 155 156 157 158 159 160 def __str__ ( self ) -> str : \"\"\"The string representation of the meal.\"\"\" return ( f \"Meal(name= { self . name } , day= { self . day } , prices= { self . prices } , \\ price_quantity= { self . price_quantity } , \" f \"allergens= { self . allergens } , vegan= { self . vegan } , vegetarian= { self . vegetarian } , \" f \"fish= { self . fish } , pork= { self . pork } , dish_type= { self . dish_type } )\" )","title":"__str__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.Meal.as_dict","text":"A dict representation of the meal. Source code in adviser/examples/webapi/mensa/parser.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def as_dict ( self ) -> Dict [ str , str ]: \"\"\"A dict representation of the meal.\"\"\" return { 'name' : self . name , 'day' : self . day , 'type' : self . dish_type . value , 'price' : str ( self . prices [ 0 ]), 'allergens' : ', ' . join ([ allergen . value for allergen in self . allergens ]) if \\ self . allergens is not None else 'none' , 'vegan' : str ( self . vegan ) . lower (), 'vegetarian' : str ( self . vegetarian ) . lower (), 'fish' : str ( self . fish ) . lower (), 'pork' : str ( self . pork ) . lower () }","title":"as_dict()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.MensaParser","text":"","title":"MensaParser"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.MensaParser.__init__","text":"The class to issue post requests and parse the response. Will also take care of caching the parser's results. Parameters: Name Type Description Default cache bool Whether to cache results or not. True Source code in adviser/examples/webapi/mensa/parser.py 169 170 171 172 173 174 175 176 177 178 179 180 181 def __init__ ( self , cache : bool = True ): \"\"\" The class to issue post requests and parse the response. Will also take care of caching the parser's results. Args: cache (bool): Whether to cache results or not. \"\"\" #: dict of str: storgae to cache parsed meals self . storage = {} self . cache = cache","title":"__init__()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.MensaParser.get_meals","text":"Gets the meals for a specified day by either looking them up in the cache or by issuing and parsing a post request . Args : date ( str ): The date for which the data will be returned . Can be a string in the format 'Y-m-d' or one of today , tomorrow and monday - sunday . use_cache ( bool ): If False will always query the server instead of using the cache . Returns: Type Description List[adviser.examples.webapi.mensa.parser.Meal] :obj: list of Meal: List of meals for specified date Source code in adviser/examples/webapi/mensa/parser.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def get_meals ( self , date : str , use_cache : bool = True ) -> List [ Meal ]: \"\"\" Gets the meals for a specified day by either looking them up in the cache or by issuing and parsing a post request. Args: date (str): The date for which the data will be returned. Can be a string in the format 'Y-m-d' or one of today, tomorrow and monday-sunday. use_cache (bool): If False will always query the server instead of using the cache. Returns: :obj:`list` of Meal: List of meals for specified date \"\"\" date = self . _parse_date ( date ) if use_cache and date . date () in self . storage : # NOTE data could be deprecated return self . storage [ date . date ()] else : # issue request to server return self . _parse ( date )","title":"get_meals()"},{"location":"api/examples/#adviser.examples.webapi.mensa.parser.ParseDateError","text":"This exception is raised when the date cannot be parsed.","title":"ParseDateError"},{"location":"api/examples/#adviser.examples.webapi.weather","text":"","title":"weather"},{"location":"api/examples/#adviser.examples.webapi.weather.domain","text":"","title":"domain"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain","text":"Domain for the Weather API. !!! attributes last_results ( List [ dict ] ) : Current results which the user might request info about","title":"WeatherDomain"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.__init__","text":"Source code in adviser/examples/webapi/weather/domain.py 37 38 39 40 def __init__ ( self ): LookupDomain . __init__ ( self , 'WeatherAPI' , 'Weather' ) self . last_results = []","title":"__init__()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.find_entities","text":"Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict Slot-value mapping of constraints. If empty, all entities in the database will be returned. required requested_slots Iterable list of slots that should be returned in addition to the system requestable slots and the primary key <tuple_iterator object at 0x7f91b46470d0> Source code in adviser/examples/webapi/weather/domain.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): Slot-value mapping of constraints. If empty, all entities in the database will be returned. requested_slots (Iterable): list of slots that should be returned in addition to the system requestable slots and the primary key \"\"\" if 'location' in constraints and 'date' in constraints : forecast = self . _query ( constraints [ 'location' ], constraints [ 'date' ]) if forecast is None : return [] temperature = int ( ' %.0f ' % ( float ( forecast [ 'main' ][ 'temp' ]) - 273.15 )) description = forecast [ 'weather' ][ 0 ][ 'description' ] result_dict = { 'artificial_id' : str ( len ( self . last_results )), 'temperature' : temperature , 'description' : description , 'location' : constraints [ 'location' ], 'date' : constraints [ 'date' ], } if any ( True for _ in requested_slots ): cleaned_result_dict = { slot : result_dict [ slot ] for slot in requested_slots } else : cleaned_result_dict = result_dict self . last_results . append ( cleaned_result_dict ) return [ cleaned_result_dict ] else : return []","title":"find_entities()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.find_info_about_entity","text":"Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/examples/webapi/weather/domain.py 76 77 78 79 80 81 82 83 84 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" return [ self . last_results [ int ( entity_id )]]","title":"find_info_about_entity()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.get_informable_slots","text":"Returns a list of all informable slots. Source code in adviser/examples/webapi/weather/domain.py 94 95 96 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return [ 'location' , 'date' ]","title":"get_informable_slots()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.get_keyword","text":"Source code in adviser/examples/webapi/weather/domain.py 148 149 def get_keyword ( self ): return 'weather'","title":"get_keyword()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.get_mandatory_slots","text":"Returns a list of all mandatory slots. Source code in adviser/examples/webapi/weather/domain.py 98 99 100 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" return [ 'location' , 'date' ]","title":"get_mandatory_slots()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.get_possible_values","text":"Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/examples/webapi/weather/domain.py 102 103 104 105 106 107 108 109 110 111 112 113 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" raise BaseException ( 'all slots in this domain do not have a fixed set of ' 'values, so this method should never be called' )","title":"get_possible_values()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.get_primary_key","text":"Returns the slot name that will be used as the 'name' of an entry Source code in adviser/examples/webapi/weather/domain.py 115 116 117 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" return 'artificial_id'","title":"get_primary_key()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.get_requestable_slots","text":"Returns a list of all slots requestable by the user. Source code in adviser/examples/webapi/weather/domain.py 86 87 88 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return [ 'temperature' , 'description' ]","title":"get_requestable_slots()"},{"location":"api/examples/#adviser.examples.webapi.weather.domain.WeatherDomain.get_system_requestable_slots","text":"Returns a list of all slots requestable by the system. Source code in adviser/examples/webapi/weather/domain.py 90 91 92 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return [ 'location' , 'date' ]","title":"get_system_requestable_slots()"},{"location":"api/examples/#adviser.examples.webapi.weather.nlg","text":"","title":"nlg"},{"location":"api/examples/#adviser.examples.webapi.weather.nlg.WeatherNLG","text":"Simple NLG for the weather domain","title":"WeatherNLG"},{"location":"api/examples/#adviser.examples.webapi.weather.nlg.WeatherNLG.__init__","text":"Source code in adviser/examples/webapi/weather/nlg.py 31 32 33 def __init__ ( self , domain , logger = DiasysLogger ()): # only calls super class' constructor super ( WeatherNLG , self ) . __init__ ( domain , debug_logger = logger )","title":"__init__()"},{"location":"api/examples/#adviser.examples.webapi.weather.nlg.WeatherNLG.generate_system_utterance","text":"Source code in adviser/examples/webapi/weather/nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"generate_system_utterance()"},{"location":"api/examples/#adviser.examples.webapi.weather.nlu","text":"","title":"nlu"},{"location":"api/examples/#adviser.examples.webapi.weather.nlu.WeatherNLU","text":"Very simple NLU for the weather domain.","title":"WeatherNLU"},{"location":"api/examples/#adviser.examples.webapi.weather.nlu.WeatherNLU.__init__","text":"Source code in adviser/examples/webapi/weather/nlu.py 45 46 47 def __init__ ( self , domain , logger = DiasysLogger ()): # only calls super class' constructor super ( WeatherNLU , self ) . __init__ ( domain , debug_logger = logger )","title":"__init__()"},{"location":"api/examples/#adviser.examples.webapi.weather.nlu.WeatherNLU.extract_user_acts","text":"Source code in adviser/examples/webapi/weather/nlu.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"extract_user_acts()"},{"location":"api/services/","text":"Services \u00b6 \u00b6 backchannel special \u00b6 acoustic_backchanneller \u00b6 AcousticBackchanneller \u00b6 AcousticBackchanneller predicts a backchannel given the last user utterance. The model can predict: No backchannel (0), Assessment (1), Continuer (2) The backchannel realization is added in the NLG module. __init__ ( self ) special \u00b6 Source code in adviser/services/backchannel/acoustic_backchanneller.py 42 43 44 45 46 def __init__ ( self ): Service . __init__ ( self ) self . speech_in_dir = os . path . dirname ( os . path . abspath ( __file__ )) + '/' self . trained_model_path = os . path . join ( 'resources' , 'models' , 'backchannel' ) + '/pytorch_acoustic_backchanneller.pt' self . load_model () backchannel_prediction ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/backchannel/acoustic_backchanneller.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result load_model ( self ) \u00b6 The PyTorch Backchannel model is instantiated and the pretrained parameters are loaded. Source code in adviser/services/backchannel/acoustic_backchanneller.py 48 49 50 51 52 53 54 55 56 def load_model ( self ): \"\"\" The PyTorch Backchannel model is instantiated and the pretrained parameters are loaded. Returns: \"\"\" self . model = PytorchAcousticBackchanneler () self . model . load_state_dict ( torch . load ( self . trained_model_path )) self . model . eval () split_input_data ( self , mfcc_features ) \u00b6 Preprocess and segmentation of MFCC features of the user's speech. Segmentation is done every 150ms without overlapping. Parameters: Name Type Description Default mfcc_features numpy.array mffcc features of users speech required Returns: Type Description new_data (list) segmented mfcc features Source code in adviser/services/backchannel/acoustic_backchanneller.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def split_input_data ( self , mfcc_features ): \"\"\" Preprocess and segmentation of MFCC features of the user's speech. Segmentation is done every 150ms without overlapping. Args: mfcc_features (numpy.array): mffcc features of users speech Returns: new_data (list): segmented mfcc features \"\"\" input_height = 150 # this stands for 150ms input_length = mfcc_features . shape [ 0 ] zero_shape = list ( mfcc_features . shape ) zero_shape [ 0 ] = input_height ranges = list ( reversed ([ idx for idx in range ( input_length - 1 , 0 , - input_height )])) new_data = [] for r in ranges : if r < input_height : zero_data = np . zeros ( zero_shape ) zero_data [ - r :, :] = mfcc_features [: r , :] new_data . append ( zero_data ) else : new_data . append ( mfcc_features [ r - input_height : r , :]) return ( new_data ) PytorchAcousticBackchanneler \u00b6 PytorchAcousticBackchanneler \u00b6 Class for defining the Deep Backchannel model in PyTorch __init__ ( self , parameters = [], load_params = False ) special \u00b6 Defines the elements/layers of the neural network as well as loads the pretrained parameters The model is constituted by two parallel CNNs followed by a concatenation, a FFN and a softmax layer. Parameters: Name Type Description Default parameters list list of pre-trained parameters to be used for prediction [] load_params bool Bool to signal if params should be loaded False Source code in adviser/services/backchannel/PytorchAcousticBackchanneler.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , parameters : list = [], load_params : bool = False ): \"\"\" Defines the elements/layers of the neural network as well as loads the pretrained parameters The model is constituted by two parallel CNNs followed by a concatenation, a FFN and a softmax layer. Args: parameters (list): list of pre-trained parameters to be used for prediction load_params (bool): Bool to signal if params should be loaded \"\"\" super ( PytorchAcousticBackchanneler , self ) . __init__ () # First CNN cnn = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = ( 11 , 13 ), stride = ( 3 , 1 )) if load_params : weights = np . transpose ( parameters [ 0 ][ 0 ], ( 3 , 2 , 0 , 1 )) cnn . weight = torch . nn . Parameter ( torch . tensor ( weights ) . float ()) cnn . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 0 ][ 1 ]) . float ()) self . cnn1 = nn . Sequential ( cnn , nn . ReLU (), nn . MaxPool2d (( 23 , 1 )) ) # Second CNN cnn = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = ( 12 , 13 ), stride = ( 3 , 1 )) if load_params : weights = np . transpose ( parameters [ 1 ][ 0 ], ( 3 , 2 , 0 , 1 )) cnn . weight = torch . nn . Parameter ( torch . tensor ( weights ) . float ()) cnn . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 1 ][ 1 ]) . float ()) self . cnn2 = nn . Sequential ( cnn , nn . ReLU (), nn . MaxPool2d (( 23 , 1 )) ) # Linear layer self . linear1 = nn . Linear ( in_features = 64 , out_features = 100 ) if load_params : self . linear1 . weight = torch . nn . Parameter ( torch . tensor ( parameters [ 2 ][ 0 ] . T ) . float ()) self . linear1 . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 2 ][ 1 ]) . float ()) self . relu = nn . ReLU () self . dropout = nn . Dropout ( 0.5 ) # Softmax self . linear2 = nn . Linear ( in_features = 100 , out_features = 3 ) if load_params : self . linear2 . weight = torch . nn . Parameter ( torch . tensor ( parameters [ 3 ][ 0 ] . T ) . float ()) self . linear2 . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 3 ][ 1 ]) . float ()) self . softmax = nn . Softmax ( dim = 1 ) forward ( self , feat_inputs ) \u00b6 PyTorch forward method used for training and prediction. It defines the interaction between layers. Parameters: Name Type Description Default feat_inputs numpy array It contains the network's input. required Returns: Type Description out (torch.tensor) Network's output Source code in adviser/services/backchannel/PytorchAcousticBackchanneler.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , feat_inputs ): \"\"\" PyTorch forward method used for training and prediction. It defines the interaction between layers. Args: feat_inputs (numpy array): It contains the network's input. Returns: out (torch.tensor): Network's output \"\"\" feat_inputs = torch . tensor ( feat_inputs ) . float () feat_inputs = feat_inputs . unsqueeze ( 1 ) cnn_1 = self . cnn1 ( feat_inputs ) cnn_1 = cnn_1 . flatten ( 1 ) cnn_2 = self . cnn2 ( feat_inputs ) . flatten ( 1 ) out = torch . cat (( cnn_1 , cnn_2 ), 1 ) out = self . linear1 ( out ) out = self . relu ( out ) out = self . dropout ( out ) out = self . linear2 ( out ) out = self . softmax ( out ) return out bst special \u00b6 bst \u00b6 HandcraftedBST \u00b6 A rule-based approach to belief state tracking. __init__ ( self , domain = None , logger = None ) special \u00b6 Source code in adviser/services/bst/bst.py 33 34 35 36 def __init__ ( self , domain = None , logger = None ): Service . __init__ ( self , domain = domain ) self . logger = logger self . bs = BeliefState ( domain ) dialog_start ( self ) \u00b6 Restets the belief state so it is ready for a new dialog Returns: Type Description (dict) a dictionary with a single entry where the key is 'beliefstate'and the value is a new BeliefState object Source code in adviser/services/bst/bst.py 69 70 71 72 73 74 75 76 77 78 def dialog_start ( self ): \"\"\" Restets the belief state so it is ready for a new dialog Returns: (dict): a dictionary with a single entry where the key is 'beliefstate'and the value is a new BeliefState object \"\"\" # initialize belief state self . bs = BeliefState ( self . domain ) update_bst ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/bst/bst.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result domain_tracker special \u00b6 domain_tracker \u00b6 The console module provides ADVISER services for tracking current domain DomainTracker \u00b6 Responsible for selecting which domain should be active at a given time. Current implmentation uses keywords to switch domains. __init__ ( self , domains , greet_on_first_turn = False ) special \u00b6 Source code in adviser/services/domain_tracker/domain_tracker.py 34 35 36 37 38 def __init__ ( self , domains : List [ Domain ], greet_on_first_turn : bool = False ): Service . __init__ ( self , domain = \"\" ) self . domains = domains self . current_domain = None self . greet_on_first_turn = greet_on_first_turn dialog_start ( self ) \u00b6 Resets the domain tracker for the start of a new dialog Source code in adviser/services/domain_tracker/domain_tracker.py 40 41 42 43 44 45 def dialog_start ( self ): \"\"\" Resets the domain tracker for the start of a new dialog \"\"\" self . turn = 0 self . current_domain = None domains_to_str ( self ) \u00b6 Method to create the greeting on the first turn, grammatically joins the names of possible domains into a string Returns: Type Description (str) String representing a list of all domain names the system can talk about Source code in adviser/services/domain_tracker/domain_tracker.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def domains_to_str ( self ): \"\"\" Method to create the greeting on the first turn, grammatically joins the names of possible domains into a string Returns: (str): String representing a list of all domain names the system can talk about \"\"\" if len ( self . domains ) == 1 : return self . domains [ 0 ] . get_display_name () elif len ( self . domains ) == 2 : return \" and \" . join ([ d . get_display_name () for d in self . domains ]) else : return \", \" . join ([ d . get_display_name () for d in self . domains ][: - 1 ]) + f \", and { self . domains [ - 1 ] . get_display_name () } \" select_domain ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/domain_tracker/domain_tracker.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result emotion special \u00b6 EmotionRecognition \u00b6 Emotion recognition module. EmotionRecognition \u00b6 Emotion recognition module. This module receives acoustic features, loads pretrained models and outputs predictions of emotional states. It can easily be extended/adapted to use different models and facial features in addition. __init__ ( self ) special \u00b6 Emotion recognition module. On initialization all necessary models are loaded. Source code in adviser/services/emotion/EmotionRecognition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self ): \"\"\" Emotion recognition module. On initialization all necessary models are loaded. \"\"\" Service . __init__ ( self ) self . emotion_dir = os . path . dirname ( os . path . abspath ( __file__ )) self . model_path = os . path . abspath ( os . path . join ( self . emotion_dir , \"..\" , \"..\" , \"resources\" , \"models\" , \"emotion\" ) ) def load_args ( emo_representation ): arg_dict = pickle . load ( open ( os . path . join ( self . model_path , f ' { emo_representation } _args.pkl' ), 'rb' ) ) return arg_dict def load_model ( emo_representation , arg_dict ): ARGS = arg_dict [ 'args' ] model = cnn ( kernel_size = ( ARGS . height , arg_dict [ 'D_in' ]), D_out = arg_dict [ 'D_out' ], args = ARGS ) model . load_state_dict ( torch . load ( os . path . join ( self . model_path , f ' { emo_representation } _model_params.pt' ), map_location = torch . device ( 'cpu' ) ) ) model . eval () return model self . emo_representations = [ 'category' , 'arousal' , 'valence' ] self . models = {} self . args = {} for emo_representation in self . emo_representations : self . args [ emo_representation ] = load_args ( emo_representation ) self . models [ emo_representation ] = load_model ( emo_representation , self . args [ emo_representation ] ) self . arousal_mapping = { 0 : 'low' , 1 : 'medium' , 2 : 'high' } self . valence_mapping = { 0 : 'negative' , 1 : 'neutral' , 2 : 'positive' } self . category_mapping = { 0 : EmotionType . Angry , 1 : EmotionType . Happy , 2 : EmotionType . Neutral , 3 : EmotionType . Sad } predict_from_audio ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/emotion/EmotionRecognition.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result engagement special \u00b6 engagement_tracker \u00b6 EngagementTracker \u00b6 Start feature extraction with OpenFace. Requires OpenFace to be installed - instructions can be found in tool/openface.txt __init__ ( self , domain = '' , camera_id = 0 , openface_port = 6004 , delay = 2 , identifier = None ) special \u00b6 Parameters: Name Type Description Default camera_id int index of the camera you want to use (if you only have one camera: 0) 0 Source code in adviser/services/engagement/engagement_tracker.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , domain = \"\" , camera_id : int = 0 , openface_port : int = 6004 , delay : int = 2 , identifier = None ): \"\"\" Args: camera_id: index of the camera you want to use (if you only have one camera: 0) \"\"\" Service . __init__ ( self , domain = \"\" , identifier = identifier ) self . camera_id = camera_id self . openface_port = openface_port self . openface_running = False self . threshold = delay # provide number of seconds as parameter, one second = 15 frames ctx = Context . instance () self . openface_endpoint = ctx . socket ( zmq . PAIR ) self . openface_endpoint . bind ( f \"tcp://127.0.0.1: { self . openface_port } \" ) startExtraction = f \" { os . path . join ( get_root_dir (), 'tools/OpenFace/build/bin/FaceLandmarkVidZMQ' ) } -device { self . camera_id } -port 6004\" # todo config open face port self . p_openface = subprocess . Popen ( startExtraction . split (), stdout = subprocess . PIPE ) # start OpenFace self . extracting = False self . extractor_thread = None dialog_end ( self ) \u00b6 This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. Source code in adviser/services/engagement/engagement_tracker.py 145 146 147 148 149 def dialog_end ( self ): # Set openface to non-publishing mode and wait until it is ready self . openface_endpoint . send ( bytes ( f \"OPENFACE_END\" , encoding = \"ascii\" )) if self . extractor_thread : self . extractor_thread . join () dialog_exit ( self ) \u00b6 This function is called when the dialog system is shutting down. You should overwrite this function to stop your threads and cleanup any open resources. Source code in adviser/services/engagement/engagement_tracker.py 151 152 153 def dialog_exit ( self ): # close openface process self . p_openface . kill () dialog_start ( self ) \u00b6 This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/engagement/engagement_tracker.py 69 70 71 72 73 74 75 76 77 78 79 80 def dialog_start ( self ): # Set openface to publishing mode and wait until it is ready self . openface_endpoint . send ( bytes ( f \"OPENFACE_START\" , encoding = \"ascii\" )) self . extracting = False while not self . extracting : msg = self . openface_endpoint . recv () # receive started signal msg = msg . decode ( \"utf-8\" ) if msg == \"OPENFACE_STARTED\" : print ( \"START EXTRACTION\" ) self . extracting = True self . extractor_thread = Thread ( target = self . publish_gaze_directions ) self . extractor_thread . start () publish_gaze_directions ( self ) \u00b6 Meant to be used in a thread. Runs an inifinte loop polling features from OpenFace library, parsing them and extracting engagement features. Calls yield_gaze_direction to publish the polled and processed engagement features. Source code in adviser/services/engagement/engagement_tracker.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def publish_gaze_directions ( self ): \"\"\" Meant to be used in a thread. Runs an inifinte loop polling features from OpenFace library, parsing them and extracting engagement features. Calls `yield_gaze_direction` to publish the polled and processed engagement features. \"\"\" x_coordinates = [] y_coordinates = [] norm = 0.0 # center point of screen; should be close(r) to 0 looking = True while self . extracting : req = self . openface_endpoint . send ( bytes ( f \"OPENFACE_PULL\" , encoding = \"ascii\" )) msg = self . openface_endpoint . recv () try : msg = msg . decode ( \"utf-8\" ) if msg == \"OPENFACE_ENDED\" : self . extracting = False msg_data = json . loads ( msg ) gaze_x = msg_data [ \"gaze\" ][ \"angle\" ][ \"x\" ] gaze_y = msg_data [ \"gaze\" ][ \"angle\" ][ \"y\" ] gaze_x = sqrt ( gaze_x ** 2 ) # gaze_angle_x (left-right movement), square + root is done to yield only positive values gaze_y = sqrt ( gaze_y ** 2 ) # gaze_angle_y (up-down movement) x_coordinates . append ( gaze_x ) y_coordinates . append ( gaze_y ) current = ( len ( x_coordinates )) - 1 if current > self . threshold : previous_x = mean ( x_coordinates [ current - ( self . threshold + 1 ): current ]) # obtain the average of previous frames previous_y = mean ( y_coordinates [ current - ( self . threshold + 1 ): current ]) difference_x = sqrt (( norm - previous_x ) ** 2 ) # compare current frame to average of previous frames difference_y = sqrt (( norm - previous_y ) ** 2 ) # print(difference_x, difference_y) if difference_x < 0.15 and difference_y < 0.15 : # check whether difference between current and previous frames exceeds certain threshold (regulates tolerance/strictness) if looking != True : looking = True self . yield_gaze_direction ( engagement = EngagementType . High , gaze_direction = ( gaze_x , gaze_y )) else : if looking != False : looking = False self . yield_gaze_direction ( engagement = EngagementType . Low , gaze_direction = ( gaze_x , gaze_y )) except : # import traceback # traceback.print_exc() pass yield_gaze_direction ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/engagement/engagement_tracker.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result get_root_dir () \u00b6 Source code in adviser/services/engagement/engagement_tracker.py 39 40 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))) hci special \u00b6 console \u00b6 The console module provides ADVISER modules that access the console for input and output. ConsoleInput \u00b6 Gets the user utterance from the console. Waits for the built-in input function to return a non-empty text. __init__ ( self , domain = None , conversation_log_dir = None , language = None ) special \u00b6 Source code in adviser/services/hci/console.py 40 41 42 43 44 45 def __init__ ( self , domain : Domain = None , conversation_log_dir : str = None , language : Language = None ): Service . __init__ ( self , domain = domain ) # self.language = language self . language = Language . ENGLISH self . conversation_log_dir = conversation_log_dir self . interaction_count = 0 dialog_start ( self ) \u00b6 This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/hci/console.py 49 50 def dialog_start ( self ): self . interaction_count = 0 get_user_input ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/console.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result ConsoleOutput \u00b6 Writes the system utterance to the console. __init__ ( self , domain = None ) special \u00b6 Source code in adviser/services/hci/console.py 113 114 def __init__ ( self , domain : Domain = None ): Service . __init__ ( self , domain = domain ) print_sys_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/console.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result gui \u00b6 GUIServer \u00b6 Service for the React-based Web-UI. Run this as a remote service: * run this file seperately, will start the GUI Server * run the dialog system in another python instance, add a RemoteService with identifier GUIServer __init__ ( self , socketio , identifier = 'GUIServer' , logger = None ) special \u00b6 Source code in adviser/services/hci/gui.py 62 63 64 65 def __init__ ( self , socketio , identifier = \"GUIServer\" , logger : DiasysLogger = None ): super () . __init__ ( domain = '' , identifier = identifier ) self . socketio = socketio self . logger = logger forward_message_to_react ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/gui.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result start_dialog ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/gui.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result user_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/gui.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result get_root_dir () \u00b6 Source code in adviser/services/hci/gui.py 32 33 def get_root_dir (): return os . path . abspath ( os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"..\" , \"..\" )) speech special \u00b6 cleaners \u00b6 This file is derived from https://github.com/keithito/tacotron. basic_cleaners ( text ) \u00b6 Basic pipeline that lowercases and collapses whitespace without transliteration. Source code in adviser/services/hci/speech/cleaners.py 208 209 210 211 212 def basic_cleaners ( text ): \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\" text = lowercase ( text ) text = collapse_whitespace ( text ) return text collapse_whitespace ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 200 201 def collapse_whitespace ( text ): return re . sub ( _whitespace_re , ' ' , text ) convert_to_ascii ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 204 205 def convert_to_ascii ( text ): return unidecode ( text ) custom_english_cleaners ( text ) \u00b6 Custom pipeline for English text, including number and abbreviation expansion. Source code in adviser/services/hci/speech/cleaners.py 254 255 256 257 258 259 260 261 262 263 264 265 266 def custom_english_cleaners ( text ): \"\"\"Custom pipeline for English text, including number and abbreviation expansion.\"\"\" text = convert_to_ascii ( text ) text = expand_email ( text ) text = expand_acronym ( text ) text = lowercase ( text ) text = expand_numbers ( text ) text = expand_abbreviations ( text ) text = expand_symbols ( text ) text = remove_unnecessary_symbols ( text ) text = uppercase ( text ) text = collapse_whitespace ( text ) return text english_cleaners ( text ) \u00b6 Pipeline for English text, including number and abbreviation expansion. Source code in adviser/services/hci/speech/cleaners.py 223 224 225 226 227 228 229 230 def english_cleaners ( text ): \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\" text = convert_to_ascii ( text ) text = lowercase ( text ) text = expand_numbers ( text ) text = expand_abbreviations ( text ) text = collapse_whitespace ( text ) return text expand_abbreviations ( text ) \u00b6 Preprocesses a text to turn abbreviations into forms that the TTS can pronounce properly text (string): Text to be preprocessed Source code in adviser/services/hci/speech/cleaners.py 137 138 139 140 141 142 143 144 145 def expand_abbreviations ( text ): \"\"\" Preprocesses a text to turn abbreviations into forms that the TTS can pronounce properly text (string): Text to be preprocessed \"\"\" for regex , replacement in _abbreviations : text = re . sub ( regex , replacement , text ) return text expand_acronym ( text ) \u00b6 Preprocesses a text to turn acronyms into forms that the TTS can pronounce properly text (string): Text to be preprocessed Source code in adviser/services/hci/speech/cleaners.py 171 172 173 174 175 176 177 178 179 def expand_acronym ( text ): \"\"\" Preprocesses a text to turn acronyms into forms that the TTS can pronounce properly text (string): Text to be preprocessed \"\"\" for word , replacement in _acronym : text = re . sub ( word , replacement , text ) return text expand_email ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 188 189 190 def expand_email ( text ): text = re . sub ( _email_re , _expand_email , text ) return text expand_numbers ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 192 193 def expand_numbers ( text ): return normalize_numbers ( text ) expand_symbols ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 240 241 242 243 244 245 246 def expand_symbols ( text ): # added text = re . sub ( \";\" , \",\" , text ) text = re . sub ( \":\" , \",\" , text ) text = re . sub ( \"-\" , \" \" , text ) text = re . sub ( \"&\" , \"and\" , text ) return text lowercase ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 196 197 def lowercase ( text ): return text . lower () normalize_numbers ( text ) \u00b6 Normalizes numbers in an utterance as preparation for TTS text (string): Text to be preprocessed Source code in adviser/services/hci/speech/cleaners.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def normalize_numbers ( text ): \"\"\" Normalizes numbers in an utterance as preparation for TTS text (string): Text to be preprocessed \"\"\" text = re . sub ( _comma_number_re , _remove_commas , text ) text = re . sub ( _pounds_re , r '\\1 pounds' , text ) text = re . sub ( _dollars_re , _expand_dollars , text ) text = re . sub ( _decimal_number_re , _expand_decimal_point , text ) text = re . sub ( _ordinal_re , _expand_ordinal , text ) text = re . sub ( _ID_number_re , _expand_ID_number , text ) text = re . sub ( _number_re , _expand_number , text ) return text remove_unnecessary_symbols ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 233 234 235 236 237 def remove_unnecessary_symbols ( text ): # added text = re . sub ( r '[()[]<>\"]+' , '' , text ) text = re . sub ( r '/' , ' ' , text ) return text transliteration_cleaners ( text ) \u00b6 Pipeline for non-English text that transliterates to ASCII. Source code in adviser/services/hci/speech/cleaners.py 215 216 217 218 219 220 def transliteration_cleaners ( text ): \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\" text = convert_to_ascii ( text ) text = lowercase ( text ) text = collapse_whitespace ( text ) return text uppercase ( text ) \u00b6 Source code in adviser/services/hci/speech/cleaners.py 249 250 251 def uppercase ( text ): # added return text . upper () FeatureExtractor \u00b6 Feature extraction with openSMILE. This module provides a feature extractor which uses the openSMILE toolkit to extract features from raw audio. The user utterance which is represented as a numpy array in-memory needs to be written to a temporary file first, so that openSMILE can read it. SpeechFeatureExtractor \u00b6 SpeechFeatureExtractor calls openSMILE to extract features from audio. Note : openSMILE will be downloaded & compiled to tools / opensmile if not found there . __init__ ( self ) special \u00b6 SpeechFeatureExtractor. The following things are setup on initialization: * directory for temporary audio files * path to openSMILE config files * path to openSMILE executable Source code in adviser/services/hci/speech/FeatureExtractor.py 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self ): \"\"\" SpeechFeatureExtractor. The following things are setup on initialization: * directory for temporary audio files * path to openSMILE config files * path to openSMILE executable \"\"\" Service . __init__ ( self ) self . speech_out_dir = os . path . join ( \"resources\" , \"tmp_audio_and_features\" ) self . cfg_dir = os . path . join ( \"resources\" , \"opensmile_config\" ) self . openSmile_path = get_opensmile_executable_path () extract_wav_file_features ( self , features , new_audio_file ) \u00b6 Extracting acoustic features using openSMILE. Parameters: Name Type Description Default features str path to openSMILE's feature config required new_audio_file str path to audio file required Returns: Type Description numpy.ndarray extracted features for the audio file Source code in adviser/services/hci/speech/FeatureExtractor.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def extract_wav_file_features ( self , features , new_audio_file ): \"\"\"Extracting acoustic features using openSMILE. Args: features (str): path to openSMILE's feature config new_audio_file (str): path to audio file Returns: numpy.ndarray: extracted features for the audio file \"\"\" output_file = new_audio_file + '.csv' config_file = os . path . join ( self . cfg_dir , features + \".conf\" ) f = open ( os . devnull , 'w' ) try : # OpenSMILE command to extract features # SMILExtract -C <configfile> -I <input_file> \u2212O <output_file> command = ' ' . join ([ self . openSmile_path , '-C' , config_file , '-I' , new_audio_file , '-csvoutput' , output_file , '-headercsv' , '0' , '-timestampcsv' , '0' , '-instname' , '0' ]) subprocess . call ( command , stdout = f , stderr = f , shell = True ) return self . preprocess_csv ( output_file ) except OSError as err : print ( command ) print ( \"OS error: {0} \" . format ( err )) preprocess_csv ( self , csv_file ) \u00b6 Get features from csv file and normalize them if necessary. openSMILE feature are written to temporary csv file. This function reads them into a numpy array, removes instance names and could do a normalization step if needed. This is not implemented right now. Parameters: Name Type Description Default csv_file str path to csv file required Returns: Type Description numpy.ndarray raw feature values Source code in adviser/services/hci/speech/FeatureExtractor.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def preprocess_csv ( self , csv_file ): \"\"\"Get features from csv file and normalize them if necessary. openSMILE feature are written to temporary csv file. This function reads them into a numpy array, removes instance names and could do a normalization step if needed. This is not implemented right now. Args: csv_file (str): path to csv file Returns: numpy.ndarray: raw feature values \"\"\" feats = np . genfromtxt ( csv_file , delimiter = ';' ) if len ( feats . shape ) == 1 : # reshape one-dimensional features, e.g. gemaps feats = feats . reshape ( 1 , - 1 ) # take everything except first column which is the instance name feats = feats [:, 1 :] # StandardScaler normalizes feats to zero mean and unit variance # for frame-wise LLDs, it will standardize across frames # for gemaps (or functionals in general), it's not possible to scale # ideal setup: fit StandardScaler on training set across samples # and apply it with .transform() to new samples # scaler = preprocessing.StandardScaler() self . remove_file ( csv_file ) return feats remove_file ( self , file_name ) \u00b6 Remove specified file. Parameters: Name Type Description Default file_name str full path of file which shall be removed required Source code in adviser/services/hci/speech/FeatureExtractor.py 94 95 96 97 98 99 100 101 102 103 104 def remove_file ( self , file_name ): \"\"\" Remove specified file. Args: file_name (str): full path of file which shall be removed \"\"\" try : os . remove ( file_name ) except FileNotFoundError as error : self . logger . error ( error ) raise ( error ) speech_to_features ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/speech/FeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result speech_utility \u00b6 Utility for the emotion recognition script that needs the utterance a s file delete_file ( filepath ) \u00b6 Deletes the file at the given path to clean up the audio file once it's not needed anymore. This is why unique filenames are important. filepath (string): path to the file that is to be deleted Source code in adviser/services/hci/speech/speech_utility.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def delete_file ( filepath ): \"\"\" Deletes the file at the given path to clean up the audio file once it's not needed anymore. This is why unique filenames are important. filepath (string): path to the file that is to be deleted \"\"\" if os . path . exists ( filepath ): os . remove ( filepath ) else : print ( \"The file cannot be deleted, as it was not found. \" \"Please check the provided path for errors: \\n {} \" . format ( filepath )) sound_array_to_file ( filepath , sampling_rate , sound_as_array ) \u00b6 Saves the recording of the recorder to a file Turns the audio from the recorder service into a wav file for processing with opensmile c++ scripts filepath (string): full path, including filename and .wav suffix at an arbitrary location. Careful: python takes paths as relative to the main script. The name should be unique, to ensure files don't get mixed up if there are multiple calls in short time and one file might get overwriteen or deleted before it's done being processed. sampling_rate (int): the sampling rate of the audio, as published by the recorder sound_as_array (np.array): the audio in form of an array as published by the recorder Source code in adviser/services/hci/speech/speech_utility.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def sound_array_to_file ( filepath , sampling_rate , sound_as_array ): \"\"\" Saves the recording of the recorder to a file Turns the audio from the recorder service into a wav file for processing with opensmile c++ scripts filepath (string): full path, including filename and .wav suffix at an arbitrary location. Careful: python takes paths as relative to the main script. The name should be unique, to ensure files don't get mixed up if there are multiple calls in short time and one file might get overwriteen or deleted before it's done being processed. sampling_rate (int): the sampling rate of the audio, as published by the recorder sound_as_array (np.array): the audio in form of an array as published by the recorder \"\"\" librosa . output . write_wav ( filepath , sound_as_array , sampling_rate ) SpeechInputDecoder \u00b6 SpeechInputDecoder \u00b6 __init__ ( self , domain = '' , identifier = None , conversation_log_dir = None , use_cuda = False ) special \u00b6 Transforms spoken input from the user to text for further processing. Parameters: Name Type Description Default domain Domain Needed for Service, but has no meaning here '' identifier string Needed for Service None conversation_log_dir str If this is provided, logfiles will be placed by this Service into the specified directory. None use_cuda boolean Whether or not to run the computations on a GPU False Source code in adviser/services/hci/speech/SpeechInputDecoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , domain : Domain = \"\" , identifier = None , conversation_log_dir : str = None , use_cuda = False ): \"\"\" Transforms spoken input from the user to text for further processing. Args: domain (Domain): Needed for Service, but has no meaning here identifier (string): Needed for Service conversation_log_dir (string): If this is provided, logfiles will be placed by this Service into the specified directory. use_cuda (boolean): Whether or not to run the computations on a GPU \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) self . conversation_log_dir = conversation_log_dir # load model model_dir = os . path . join ( get_root_dir (), \"resources\" , \"models\" , \"speech\" , \"multi_en_20190916\" ) self . model , conf = load_trained_model ( os . path . join ( model_dir , \"model.bin\" )) self . vocab = conf . char_list # setup beam search self . bs = BeamSearch ( scorers = self . model . scorers (), weights = { \"decoder\" : 1.0 , \"ctc\" : 0.0 }, sos = self . model . sos , eos = self . model . eos , beam_size = 4 , vocab_size = len ( self . vocab ), pre_beam_score_key = \"decoder\" ) self . bs . __class__ = BatchBeamSearch # choose hardware to run on if use_cuda : self . device = \"cuda\" else : self . device = \"cpu\" self . model . to ( self . device ) self . bs . to ( self . device ) # change from training mode to eval mode self . model . eval () self . bs . eval () # scale and offset for feature normalization # follows https://github.com/kaldi-asr/kaldi/blob/33255ed224500f55c8387f1e4fa40e08b73ff48a/src/transform/cmvn.cc#L92-L111 norm = torch . load ( os . path . join ( model_dir , \"cmvn.bin\" )) count = norm [ 0 ][ - 1 ] mean = norm [ 0 ][: - 1 ] / count var = ( norm [ 1 ][: - 1 ] / count ) - mean * mean self . scale = 1.0 / torch . sqrt ( var ) self . offset = - ( mean * self . scale ) features_to_text ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/speech/SpeechInputDecoder.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result get_root_dir () \u00b6 Source code in adviser/services/hci/speech/SpeechInputDecoder.py 34 35 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))) SpeechInputFeatureExtractor \u00b6 SpeechInputFeatureExtractor \u00b6 __init__ ( self , domain = '' ) special \u00b6 Given a sound, this service extracts features and passes them on to the decoder for ASR Parameters: Name Type Description Default domain Domain Needed for Service, no meaning here '' Source code in adviser/services/hci/speech/SpeechInputFeatureExtractor.py 32 33 34 35 36 37 38 39 def __init__ ( self , domain : Domain = \"\" ): \"\"\" Given a sound, this service extracts features and passes them on to the decoder for ASR Args: domain (Domain): Needed for Service, no meaning here \"\"\" Service . __init__ ( self , domain = domain ) speech_to_features ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/speech/SpeechInputFeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result speech_to_mfcc ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/speech/SpeechInputFeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result SpeechOutputGenerator \u00b6 SpeechOutputGenerator \u00b6 __init__ ( self , domain = '' , identifier = None , use_cuda = False , sub_topic_domains = {}) special \u00b6 Text To Speech Module that reads out the system utterance. Parameters: Name Type Description Default domain Domain Needed for Service, no meaning here '' identifier str Needed for Service None use_cuda boolean Whether or not to perform computations on GPU. Highly recommended if available False sub_topic_domains Dict[str, str] see services.service.Service constructor for more details {} Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , domain : Domain = \"\" , identifier : str = None , use_cuda = False , sub_topic_domains : Dict [ str , str ] = {}): \"\"\" Text To Speech Module that reads out the system utterance. Args: domain (Domain): Needed for Service, no meaning here identifier (string): Needed for Service use_cuda (boolean): Whether or not to perform computations on GPU. Highly recommended if available sub_topic_domains: see `services.service.Service` constructor for more details \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier , sub_topic_domains = sub_topic_domains ) self . models_directory = os . path . join ( get_root_dir (), \"resources\" , \"models\" , \"speech\" ) # The following lines can be changed to incorporate different models. # This is the only thing that needs to be changed for that, everything else should be dynamic. self . transcription_type = \"phn\" self . dict_path = os . path . join ( self . models_directory , \"phn_train_no_dev_pytorch_train_fastspeech.v4\" , \"data\" , \"lang_1phn\" , \"train_no_dev_units.txt\" ) self . model_path = os . path . join ( self . models_directory , \"phn_train_no_dev_pytorch_train_fastspeech.v4\" , \"exp\" , \"phn_train_no_dev_pytorch_train_fastspeech.v4\" , \"results\" , \"model.last1.avg.best\" ) self . vocoder_path = os . path . join ( self . models_directory , \"ljspeech.parallel_wavegan.v1\" , \"checkpoint-400000steps.pkl\" ) self . vocoder_conf = os . path . join ( self . models_directory , \"ljspeech.parallel_wavegan.v1\" , \"config.yml\" ) # define device to run the synthesis on if use_cuda : self . device = torch . device ( \"cuda\" ) else : self . device = torch . device ( \"cpu\" ) # define end to end TTS model self . input_dimensions , self . output_dimensions , self . train_args = get_model_conf ( self . model_path ) model_class = dynamic_import . dynamic_import ( self . train_args . model_module ) model = model_class ( self . input_dimensions , self . output_dimensions , self . train_args ) torch_load ( self . model_path , model ) self . model = model . eval () . to ( self . device ) self . inference_args = Namespace ( ** { \"threshold\" : 0.5 , \"minlenratio\" : 0.0 , \"maxlenratio\" : 10.0 }) # define neural vocoder with open ( self . vocoder_conf ) as vocoder_config_file : self . config = yaml . load ( vocoder_config_file , Loader = yaml . Loader ) vocoder = ParallelWaveGANGenerator ( ** self . config [ \"generator_params\" ]) vocoder . load_state_dict ( torch . load ( self . vocoder_path , map_location = \"cpu\" )[ \"model\" ][ \"generator\" ]) vocoder . remove_weight_norm () self . vocoder = vocoder . eval () . to ( self . device ) with open ( self . dict_path ) as dictionary_file : lines = dictionary_file . readlines () lines = [ line . replace ( \" \\n \" , \"\" ) . split ( \" \" ) for line in lines ] self . char_to_id = { c : int ( i ) for c , i in lines } self . g2p = G2p () # download the pretrained Punkt tokenizer from NLTK. This is done only # the first time the code is executed on a machine, if it has been done # before, this line will be skipped and output a warning. We will probably # redirect warnings into a file rather than std_err in the future, since # there's also a lot of pytorch warnings going on etc. nltk . download ( 'punkt' , quiet = True ) generate_speech ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result preprocess_text_input ( self , text ) \u00b6 Clean the text and then convert it to id sequence. Parameters: Name Type Description Default text string The text to preprocess required Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def preprocess_text_input ( self , text ): \"\"\" Clean the text and then convert it to id sequence. Args: text (string): The text to preprocess \"\"\" text = custom_english_cleaners ( text ) # cleans the text if self . transcription_type == \"phn\" : # depending on the model type, different preprocessing is needed. text = filter ( lambda s : s != \" \" , self . g2p ( text )) text = \" \" . join ( text ) char_sequence = text . split ( \" \" ) else : char_sequence = list ( text ) id_sequence = [] for c in char_sequence : if c . isspace (): id_sequence += [ self . char_to_id [ \"<space>\" ]] elif c not in self . char_to_id . keys (): id_sequence += [ self . char_to_id [ \"<unk>\" ]] else : id_sequence += [ self . char_to_id [ c ]] id_sequence += [ self . input_dimensions - 1 ] # <eos> return torch . LongTensor ( id_sequence ) . view ( - 1 ) . to ( self . device ) get_root_dir () \u00b6 Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 39 40 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))) SpeechOutputPlayer \u00b6 SpeechOutputPlayer \u00b6 __init__ ( self , domain = '' , conversation_log_dir = None , identifier = None ) special \u00b6 Service that plays the system utterance as sound Parameters: Name Type Description Default domain Domain Needed for Service, but has no meaning here '' conversation_log_dir str If this is provided it will create log files in the specified directory. None identifier str Needed for Service. None Source code in adviser/services/hci/speech/SpeechOutputPlayer.py 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , domain : Domain = \"\" , conversation_log_dir : str = None , identifier : str = None ): \"\"\" Service that plays the system utterance as sound Args: domain (Domain): Needed for Service, but has no meaning here conversation_log_dir (string): If this is provided it will create log files in the specified directory. identifier (string): Needed for Service. \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) self . conversation_log_dir = conversation_log_dir self . interaction_count = 0 speak ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/speech/SpeechOutputPlayer.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result SpeechRecorder \u00b6 SpeechRecorder \u00b6 __init__ ( self , domain = '' , conversation_log_dir = None , enable_plotting = False , threshold = 8000 , voice_privacy = False , identifier = None ) special \u00b6 A service that can record a microphone upon a key pressing event and publish the result as an array. The end of the utterance is detected automatically, also the voice can be masked to alleviate privacy issues. Parameters: Name Type Description Default domain Union[str, utils.domain.domain.Domain] I don't know why this is here. Service needs it, but it means nothing in this context. '' conversation_log_dir str If this parameter is given, log files of the conversation will be created in this directory None enable_plotting bool If this is set to True, the recorder is no longer real time able and thus the recordings don't work properly. This is just to be used to tune the threshold for the end of utterance detection, not during deployment. False threshold int The threshold below which the assumption of the end of utterance detection is silence 8000 voice_privacy bool Whether or not to enable the masking of the users voice False identifier str I don't know why this is here. Service needs it. None Source code in adviser/services/hci/speech/SpeechRecorder.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , domain : Union [ str , Domain ] = \"\" , conversation_log_dir : str = None , enable_plotting : bool = False , threshold : int = 8000 , voice_privacy : bool = False , identifier : str = None ) -> None : \"\"\" A service that can record a microphone upon a key pressing event and publish the result as an array. The end of the utterance is detected automatically, also the voice can be masked to alleviate privacy issues. Args: domain (Domain): I don't know why this is here. Service needs it, but it means nothing in this context. conversation_log_dir (string): If this parameter is given, log files of the conversation will be created in this directory enable_plotting (boolean): If this is set to True, the recorder is no longer real time able and thus the recordings don't work properly. This is just to be used to tune the threshold for the end of utterance detection, not during deployment. threshold (int): The threshold below which the assumption of the end of utterance detection is silence voice_privacy (boolean): Whether or not to enable the masking of the users voice identifier (string): I don't know why this is here. Service needs it. \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) self . conversation_log_dir = conversation_log_dir self . recording_indicator = False self . audio_interface = pyaudio . PyAudio () self . push_to_talk_listener = keyboard . Listener ( on_press = self . start_recording ) self . threshold = threshold self . enable_plotting = enable_plotting self . voice_privacy = voice_privacy record_user_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/speech/SpeechRecorder.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result start_recorder ( self ) \u00b6 Starts the listener and outputs that the speech recorder is ready for use Source code in adviser/services/hci/speech/SpeechRecorder.py 137 138 139 140 141 142 143 def start_recorder ( self ): \"\"\" Starts the listener and outputs that the speech recorder is ready for use \"\"\" self . push_to_talk_listener . start () print ( \"To speak to the system, tap your right [CTRL] or [CMD] key. \\n \" \"It will try to automatically detect when your utterance is over. \\n \" ) start_recording ( self , key ) \u00b6 This method is a callback of the push to talk key listener. It calls the recorder, if it's not already recording. Parameters: Name Type Description Default key Key The pressed key required Source code in adviser/services/hci/speech/SpeechRecorder.py 126 127 128 129 130 131 132 133 134 135 def start_recording ( self , key ): \"\"\" This method is a callback of the push to talk key listener. It calls the recorder, if it's not already recording. Args: key (Key): The pressed key \"\"\" if ( key is keyboard . Key . cmd_r or key is keyboard . Key . ctrl_r ) and not self . recording_indicator : self . record_user_utterance () threshold_plotter_generator ( self ) \u00b6 Generates a plotter to visualize when the signal is above the set threshold Returns: Type Description function Plots the threshold with the current continuous waveform Source code in adviser/services/hci/speech/SpeechRecorder.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def threshold_plotter_generator ( self ): \"\"\" Generates a plotter to visualize when the signal is above the set threshold Returns: function: Plots the threshold with the current continuous waveform \"\"\" import matplotlib matplotlib . use ( 'TkAgg' ) plt . figure ( figsize = ( 10 , 2 )) plt . axhline ( y = self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . axhline ( y =- self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . pause ( 0.000000000001 ) def threshold_plotter ( data ): plt . clf () plt . tight_layout () plt . axis ([ 0 , len ( data ), - 20000 , 20000 ]) plt . plot ( data , color = 'b' ) plt . axhline ( y = self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . axhline ( y =- self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . pause ( 0.000000000001 ) return threshold_plotter voice_sanitizer ( audio ) \u00b6 While this is by no means a good voice sanitizer, it works as a proof of concept. It randomly shifts the spectrogram of a speakers utterance up or down, making automatic speaker identification much harder while keeping impact on asr performance as low as possible. The use should be turned off by default. Parameters: Name Type Description Default audio np.array The audio represented as array required Returns: Type Description np.array The mutated audio as array Source code in adviser/services/hci/speech/SpeechRecorder.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def voice_sanitizer ( audio ): \"\"\" While this is by no means a good voice sanitizer, it works as a proof of concept. It randomly shifts the spectrogram of a speakers utterance up or down, making automatic speaker identification much harder while keeping impact on asr performance as low as possible. The use should be turned off by default. Args: audio (np.array): The audio represented as array Returns: np.array: The mutated audio as array \"\"\" spectrogram = librosa . stft ( audio ) voice_shift = np . random . randint ( 3 , 6 ) if np . random . choice ([ True , False ]): for frequency_index , _ in enumerate ( spectrogram ): # mutate the voice to be higher try : spectrogram [ len ( spectrogram ) - ( frequency_index + 1 )] = spectrogram [ len ( spectrogram ) - ( frequency_index + 1 + voice_shift )] except IndexError : pass else : for frequency_index , _ in enumerate ( spectrogram ): # mutate the voice to be lower try : spectrogram [ frequency_index ] = spectrogram [ frequency_index + voice_shift ] except IndexError : pass return librosa . istft ( spectrogram ) video special \u00b6 FeatureExtractor \u00b6 Feature extraction with openSMILE VideoFeatureExtractor \u00b6 TODO __init__ ( self , domain = '' ) special \u00b6 Source code in adviser/services/hci/video/FeatureExtractor.py 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , domain : Domain = \"\" ): Service . __init__ ( self , domain = domain ) self . module_dir = os . path . dirname ( os . path . abspath ( __file__ )) # # CLAHE (Contrast Limited Adaptive Histogram Equalization) self . CLAHE = cv2 . createCLAHE ( clipLimit = 2.0 , tileGridSize = ( 8 , 8 )) # for detecting faces (returns coordinates of rectangle(s) of face area(s)) self . DETECTOR = dlib . get_frontal_face_detector () # facial landmark predictor predictor_file = os . path . abspath ( os . path . join ( self . module_dir , '..' , '..' , '..' , 'resources' , 'models' , 'video' , 'shape_predictor_68_face_landmarks.dat' )) self . PREDICTOR = dlib . shape_predictor ( predictor_file ) extract_fl_features ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/video/FeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result VideoInput \u00b6 VideoInput \u00b6 Captures frames with a specified capture interval between two consecutive dialog turns and returns a list of frames. __init__ ( self , domain = None , camera_id = 0 , capture_interval = 1000000.0 , identifier = None ) special \u00b6 Parameters: Name Type Description Default camera_id int device id (if only 1 camera device is connected, id is 0, if two are connected choose between 0 and 1, ...) 0 capture_interval int try to capture a frame every x microseconds - is a lower bound, no hard time guarantees (e.g. 5e5 -> every >= 0.5 seconds) 1000000.0 Source code in adviser/services/hci/video/VideoInput.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , domain = None , camera_id : int = 0 , capture_interval : int = 10e5 , identifier : str = None ): \"\"\" Args: camera_id (int): device id (if only 1 camera device is connected, id is 0, if two are connected choose between 0 and 1, ...) capture_interval (int): try to capture a frame every x microseconds - is a lower bound, no hard time guarantees (e.g. 5e5 -> every >= 0.5 seconds) \"\"\" Service . __init__ ( self , domain , identifier = identifier ) self . cap = cv2 . VideoCapture ( camera_id ) # get handle to camera device if not self . cap . isOpened (): self . cap . open () # open self . terminating = Event () self . terminating . clear () self . capture_thread = Thread ( target = self . capture ) # create thread object for capturing self . capture_interval = capture_interval capture ( self ) \u00b6 Continuous video capture, meant to be run in a loop. Calls publish_img once per interval tick to publish the captured image. Source code in adviser/services/hci/video/VideoInput.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def capture ( self ): \"\"\" Continuous video capture, meant to be run in a loop. Calls `publish_img` once per interval tick to publish the captured image. \"\"\" while self . cap . isOpened () and not self . terminating . isSet (): start_time = datetime . datetime . now () # Capture frame-by-frame # cap.read() returns a bool (true when frame was read correctly) ret , frame = self . cap . read () # Our operations on the frame come here if ret : # rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) self . publish_img ( rgb_img = frame ) end_time = datetime . datetime . now () time_diff = end_time - start_time wait_seconds = ( self . capture_interval - time_diff . microseconds ) * 1e-6 # note: time to wait for next capture to match specified sampling rate in seconds if wait_seconds > 0.0 : time . sleep ( wait_seconds ) if self . cap . isOpened (): self . cap . release () dialog_end ( self ) \u00b6 This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. Source code in adviser/services/hci/video/VideoInput.py 76 77 def dialog_end ( self ): self . terminating . set () dialog_start ( self ) \u00b6 This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/hci/video/VideoInput.py 79 80 81 82 def dialog_start ( self ): if not self . capture_thread . is_alive (): print ( \"Starting video capture...\" ) self . capture_thread . start () publish_img ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/hci/video/VideoInput.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result nlg special \u00b6 affective_nlg \u00b6 Handcrafted (i.e. template-based) Natural Language Generation Module HandcraftedEmotionNLG \u00b6 A child of the HandcraftedNLG, the HandcraftedEmotionNLG can choose between multiple affective response templates for each sys_act dependingon the current sys_emotion __init__ ( self , domain , sub_topic_domains = {}, template_file = None , logger =< DiasysLogger adviser ( NOTSET ) > , template_file_german = None , emotions = [], debug_logger = None ) special \u00b6 Source code in adviser/services/nlg/affective_nlg.py 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , domain : Domain , sub_topic_domains = {}, template_file : str = None , logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , emotions : List [ str ] = [], debug_logger = None ): \"\"\"Constructor mainly extracts methods and rules from the template file\"\"\" Service . __init__ ( self , domain = domain , sub_topic_domains = sub_topic_domains , debug_logger = debug_logger ) self . domain = domain self . template_filename = template_file self . templates = {} self . logger = logger self . emotions = emotions self . _initialise_templates () generate_system_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/nlg/affective_nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result bc_nlg \u00b6 Handcrafted (i.e. template-based) Natural Language Generation Module with backchannel BackchannelHandcraftedNLG \u00b6 Handcrafted (i.e. template-based) Natural Language Generation Module A rule - based approach on natural language generation . The rules have to be specified within a template file using the ADVISER NLG syntax . Python methods that are called within a template file must be specified in the HandcraftedNLG class by using the prefix \"_template_\" . For example , the method \"_template_genitive_s\" can be accessed in the template file via calling { genitive_s ( name ) } !!! attributes domain ( Domain ) : the domain template_filename ( str ) : the NLG template filename templates ( TemplateFile ) : the parsed and ready - to - go NLG template file template_english ( str ) : the name of the English NLG template file template_german ( str ) : the name of the German NLG template file language ( Language ) : the language of the dialogue __init__ ( self , domain , sub_topic_domains = {}, template_file = None , logger =< DiasysLogger adviser ( NOTSET ) > , template_file_german = None , language = None ) special \u00b6 Source code in adviser/services/nlg/bc_nlg.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , domain : Domain , sub_topic_domains : Dict [ str , str ] = {}, template_file : str = None , logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): \"\"\"Constructor mainly extracts methods and rules from the template file\"\"\" HandcraftedNLG . __init__ ( self , domain , template_file = None , logger = DiasysLogger (), template_file_german = None , language = None , sub_topic_domains = sub_topic_domains ) # class_int_mapping = {0: b'no_bc', 1: b'assessment', 2: b'continuer'} self . backchannels = { 0 : [ '' ], 1 : [ 'Okay. ' , 'Yeah. ' ], 2 : [ 'Um-hum. ' , 'Uh-huh. ' ] } publish_system_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/nlg/bc_nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result nlg \u00b6 Handcrafted (i.e. template-based) Natural Language Generation Module HandcraftedNLG \u00b6 Handcrafted (i.e. template-based) Natural Language Generation Module A rule - based approach on natural language generation . The rules have to be specified within a template file using the ADVISER NLG syntax . Python methods that are called within a template file must be specified in the HandcraftedNLG class by using the prefix \"_template_\" . For example , the method \"_template_genitive_s\" can be accessed in the template file via calling { genitive_s ( name ) } !!! attributes domain ( Domain ) : the domain template_filename ( str ) : the NLG template filename templates ( TemplateFile ) : the parsed and ready - to - go NLG template file template_english ( str ) : the name of the English NLG template file template_german ( str ) : the name of the German NLG template file language ( Language ) : the language of the dialogue __init__ ( self , domain , template_file = None , sub_topic_domains = {}, logger =< DiasysLogger adviser ( NOTSET ) > , template_file_german = None , language = None ) special \u00b6 Constructor mainly extracts methods and rules from the template file Source code in adviser/services/nlg/nlg.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , domain : Domain , template_file : str = None , sub_topic_domains : Dict [ str , str ] = {}, logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): \"\"\"Constructor mainly extracts methods and rules from the template file\"\"\" Service . __init__ ( self , domain = domain , sub_topic_domains = sub_topic_domains ) self . language = language if language else Language . ENGLISH self . template_english = template_file # TODO: at some point if we expand languages, maybe make kwargs? --LV self . template_german = template_file_german self . domain = domain self . template_filename = None self . templates = None self . logger = logger self . language = Language . ENGLISH self . _initialise_language ( self . language ) generate_system_utterance ( self , sys_act = None ) \u00b6 Main function of the NLG module Takes a system act, searches for a fitting rule, applies it and returns the message. Overwrite this function if you inherit from the NLG module. Parameters: Name Type Description Default sys_act SysAct The system act None Returns: Type Description str The utterance generated by applying a fitting template Source code in adviser/services/nlg/nlg.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def generate_system_utterance ( self , sys_act : SysAct = None ) -> str : \"\"\"Main function of the NLG module Takes a system act, searches for a fitting rule, applies it and returns the message. Overwrite this function if you inherit from the NLG module. Args: sys_act (SysAct): The system act Returns: The utterance generated by applying a fitting template \"\"\" rule_found = True message = \"\" try : message = self . templates . create_message ( sys_act ) except BaseException as error : rule_found = False self . logger . error ( error ) raise ( error ) # inform if no applicable rule could be found in the template file if not rule_found : self . logger . info ( 'Could not find a fitting rule for the given system act!' ) self . logger . info ( \"System Action: \" + str ( sys_act . type ) + \" - Slots: \" + str ( sys_act . slot_values )) # self.logger.dialog_turn(\"System Action: \" + message) return message publish_system_utterance ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/nlg/nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result templates special \u00b6 builtinfunctions \u00b6 ForEntryFunction \u00b6 __init__ ( self , global_memory ) special \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 43 44 45 def __init__ ( self , global_memory ): Function . __init__ ( self , 'for_entry(slots, function, separator_first, separator_last)' ) self . global_memory = global_memory apply ( self , parameters = None ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def apply ( self , parameters : Memory = None ) -> str : function = parameters . get_function ( parameters . variables [ 1 ] . value ) extra_arguments = [ variable . value for variable in parameters . variables [ 4 :]] texts : List [ str ] = [] for slot_value_pair in parameters . variables [ 0 ] . value : memory = self . _build_memory ( slot_value_pair [ 0 ], slot_value_pair [ 1 ], extra_arguments ) if not function . is_applicable ( memory ): raise BaseException ( f 'The function { function . function_name } could not be called ' f 'from the for_entry function' ) texts . append ( function . apply ( memory )) return self . _create_text_from_elements ( texts , parameters . variables [ 2 ] . value , parameters . variables [ 3 ] . value ) is_applicable ( self , parameters ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 47 48 def is_applicable ( self , parameters : Memory ) -> bool : return len ( parameters . variables ) >= 4 ForEntryListFunction \u00b6 __init__ ( self , global_memory ) special \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 128 129 130 131 def __init__ ( self , global_memory : GlobalMemory ): Function . __init__ ( self , 'for_entry_list(slots, function, value_sep, value_sep_last, ' 'slot_sep, slot_sep_last)' ) self . global_memory = global_memory apply ( self , parameters = None ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def apply ( self , parameters : Memory = None ) -> str : function = parameters . get_function ( parameters . variables [ 1 ] . value ) extra_arguments = [ variable . value for variable in parameters . variables [ 6 :]] texts_per_slot : List [ str ] = [] for slot_values_pair in parameters . variables [ 0 ] . value : slot_texts : List [ str ] = [] for value in slot_values_pair [ 1 ]: memory = self . _build_memory ( slot_values_pair [ 0 ], value , extra_arguments ) if not function . is_applicable ( memory ): raise BaseException ( f 'The function { function . function_name } could not be ' f 'called from the for_entry_list function' ) slot_texts . append ( function . apply ( memory )) text = self . _create_text_from_elements ( slot_texts , parameters . variables [ 2 ] . value , parameters . variables [ 3 ] . value ) texts_per_slot . append ( text ) return self . _create_text_from_elements ( texts_per_slot , parameters . variables [ 4 ] . value , parameters . variables [ 5 ] . value ) is_applicable ( self , parameters ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 133 134 def is_applicable ( self , parameters : Memory ) -> bool : return len ( parameters . variables ) >= 6 ForFunction \u00b6 __init__ ( self , global_memory ) special \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 86 87 88 def __init__ ( self , global_memory ): Function . __init__ ( self , 'for(values, function, separator_first, separator_last, *args)' ) self . global_memory = global_memory apply ( self , parameters = None ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def apply ( self , parameters : Memory = None ) -> str : function = parameters . get_function ( parameters . variables [ 1 ] . value ) extra_arguments = [ variable . value for variable in parameters . variables [ 4 :]] texts : List [ str ] = [] for value in parameters . variables [ 0 ] . value : memory = self . _build_memory ( value , extra_arguments ) if not function . is_applicable ( memory ): raise BaseException ( f 'The function { function . function_name } could not be called ' f 'from the for function' ) texts . append ( function . apply ( memory )) return self . _create_text_from_elements ( texts , parameters . variables [ 2 ] . value , parameters . variables [ 3 ] . value ) is_applicable ( self , parameters ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 90 91 def is_applicable ( self , parameters : Memory ) -> bool : return len ( parameters . variables ) >= 4 PythonFunction \u00b6 __init__ ( self , function_name , function_to_call , obligatory_arguments = []) special \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 27 28 29 30 31 def __init__ ( self , function_name : str , function_to_call : Callable , obligatory_arguments : List [ object ] = []): Function . __init__ ( self , f ' { function_name } ()' ) self . function = function_to_call self . obligatory_arguments = obligatory_arguments apply ( self , parameters = None ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 36 37 38 39 def apply ( self , parameters : Memory = None ) -> str : arguments = self . obligatory_arguments . copy () arguments . extend ([ variable . value for variable in parameters . variables ]) return self . function ( * arguments ) is_applicable ( self , parameters ) \u00b6 Source code in adviser/services/nlg/templates/builtinfunctions.py 33 34 def is_applicable ( self , parameters : Memory ) -> bool : return True parsing special \u00b6 automaton \u00b6 ModifiedPushdownAutomaton \u00b6 __init__ ( self , start_state , accept_states , state_descriptions ) special Source code in adviser/services/nlg/templates/parsing/automaton.py 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , start_state : State , accept_states : List [ State ], state_descriptions : List [ StateDescription ]): self . start_state = start_state self . accept_states = accept_states self . state_descriptions = state_descriptions self . state_transition_mapping = self . _create_state_transition_mapping () self . state_default_transition_mapping = self . _create_state_default_transition_mapping () self . stack = AutomatonStack () parse ( self , input_tape ) Source code in adviser/services/nlg/templates/parsing/automaton.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def parse ( self , input_tape : str ) -> List [ object ]: self . stack . clear () current_state = self . start_state input_tape_index = 0 for input_char in input_tape : try : configuration = Configuration ( current_state , input_char ) transition = self . _find_transition ( configuration ) current_state = self . _apply_transition ( transition , configuration ) input_tape_index += 1 except ParsingError as error : print ( 'State:' , current_state . name ) print ( 'Index:' , input_tape_index ) print ( 'Original Input:' , input_tape ) raise error if current_state not in self . accept_states : print ( 'State:' , current_state . name ) raise ParsingError ( f 'Parser was not in a final state after the input tape was read.' ) return self . stack . data_stack [:] configuration \u00b6 Configuration \u00b6 __init__ ( self , state , character ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 37 38 39 def __init__ ( self , state : State , character : str ): self . state = state self . character = character DefaultTransition \u00b6 __init__ ( self , state ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 54 55 def __init__ ( self , state : State ): Transition . __init__ ( self , Configuration ( state , None )) SimpleForwardDefaultTransition \u00b6 __init__ ( self , state ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 67 68 def __init__ ( self , state : State ): DefaultTransition . __init__ ( self , state ) get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 70 71 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : return Configuration ( input_configuration . state , input_configuration . character ) perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 73 74 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): pass State \u00b6 __eq__ ( self , other ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 29 30 def __eq__ ( self , other ): return isinstance ( other , State ) and other . name == self . name __hash__ ( self ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 32 33 def __hash__ ( self ): return hash ( self . name ) __init__ ( self , name ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 26 27 def __init__ ( self , name ): self . name = name StateDescription \u00b6 __init__ ( self , default_state , default_transition , transitions ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 59 60 61 62 63 def __init__ ( self , default_state : State , default_transition : DefaultTransition , transitions : List [ Transition ]): self . default_state = default_state self . default_transition = default_transition self . transitions = transitions Transition \u00b6 __init__ ( self , input_configuration ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 43 44 def __init__ ( self , input_configuration : Configuration ): self . input_configuration = input_configuration get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 46 47 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : raise NotImplementedError () perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 49 50 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): raise NotImplementedError () TransitionWithAction \u00b6 __init__ ( self , input_configuration , output_configuration , action ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 90 91 92 93 94 def __init__ ( self , input_configuration : Configuration , output_configuration : Configuration , action : Callable [[ AutomatonStack ], None ]): Transition . __init__ ( self , input_configuration ) self . output_configuration = output_configuration self . action = action get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 96 97 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : return self . output_configuration perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 99 100 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): self . action ( stack ) TransitionWithoutAction \u00b6 __init__ ( self , input_configuration , output_configuration ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 78 79 80 def __init__ ( self , input_configuration : Configuration , output_configuration : Configuration ): Transition . __init__ ( self , input_configuration ) self . output_configuration = output_configuration get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 82 83 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : return self . output_configuration perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 85 86 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): pass exceptions \u00b6 ParsingError \u00b6 __init__ ( self , message ) special Source code in adviser/services/nlg/templates/parsing/exceptions.py 21 22 def __init__ ( self , message ): Exception . __init__ ( self , message ) parsers special \u00b6 messageparser special \u00b6 messageparser MessageParser __init__ ( self ) special Source code in adviser/services/nlg/templates/parsing/parsers/messageparser/messageparser.py 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self ): ModifiedPushdownAutomaton . __init__ ( self , StartState (), [ AcceptState ()], [ StartStateDescription (), AcceptStateDescription (), MessageStateDescription (), EscapeStateDescription (), CodeStateDescription (), AdviserStateDescription (), PythonStateDescription (), PythonClosingBraceStateDescription (), CodeStringStateDescription () ]) stack \u00b6 AutomatonStack \u00b6 __init__ ( self ) special Source code in adviser/services/nlg/templates/parsing/stack.py 24 25 26 27 def __init__ ( self ): # self.char_stack = [] # the automaton's stack self . data_stack = [] # the stack in which custom data structures can be stored self . levels = [[]] # multiple automaton stacks are possible here add_char ( self , stack_char ) Source code in adviser/services/nlg/templates/parsing/stack.py 29 30 31 32 def add_char ( self , stack_char : str ): if not self . levels : raise ParsingError ( 'No more levels left on the stack' ) self . levels [ - 1 ] . append ( stack_char ) add_data ( self , data ) Source code in adviser/services/nlg/templates/parsing/stack.py 34 35 def add_data ( self , data : object ): self . data_stack . append ( data ) add_level ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 43 44 def add_level ( self ): self . levels . append ([]) clear ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 56 57 58 def clear ( self ): self . data_stack = [] self . levels = [[]] fetch_data ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 40 41 def fetch_data ( self ) -> object : return self . data_stack [ - 1 ] get_current_content ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 46 47 48 49 def get_current_content ( self ) -> str : if not self . levels : raise ParsingError ( 'No more levels left on the stack' ) return '' . join ( self . levels [ - 1 ]) pop_data ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 37 38 def pop_data ( self ) -> object : return self . data_stack . pop ( - 1 ) remove_level ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 51 52 53 54 def remove_level ( self ): if not self . levels : raise ParsingError ( 'No more levels to remove from the stack' ) self . levels . pop () preprocessing \u00b6 templatefile \u00b6 TemplateFile \u00b6 Interprets a template file !!! attributes global_memory {GlobalMemory} -- memory that can be accessed at all times in the tempaltes __init__ ( self , filename , domain ) special \u00b6 Source code in adviser/services/nlg/templates/templatefile.py 63 64 65 66 67 68 def __init__ ( self , filename : str , domain : JSONLookupDomain ): self . global_memory = GlobalMemory ( domain ) self . _add_built_in_functions () tfr = _TemplateFileReader ( filename ) self . _templates = self . _create_template_dict ( tfr . get_templates ()) self . _add_functions_to_global_memory ( tfr . get_functions ()) add_python_function ( self , function_name , python_function , obligatory_arguments = []) \u00b6 Add a python function to the global memory of the template file interpreter Keyword Arguments: obligatory_arguments {List[object]} -- objects that are always passed as first arguments to the python function, e.g. \"self\" (default: {[]}) Source code in adviser/services/nlg/templates/templatefile.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def add_python_function ( self , function_name : str , python_function : Callable [[ object ], str ], obligatory_arguments : List [ object ] = []): \"\"\"Add a python function to the global memory of the template file interpreter Arguments: function_name {str} -- name under which the function can be accessed in template file python_function {Callable[[object], str]} -- python function which is called when being accessed in the template file Keyword Arguments: obligatory_arguments {List[object]} -- objects that are always passed as first arguments to the python function, e.g. \"self\" (default: {[]}) \"\"\" self . global_memory . add_function ( PythonFunction ( function_name , python_function , obligatory_arguments )) create_message ( self , sys_act ) \u00b6 Iterates through all possible templates and applies the first one to fit the system act Exceptions: Type Description BaseException when no template could be applied Returns: Type Description str str -- the message returned by the template Source code in adviser/services/nlg/templates/templatefile.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def create_message ( self , sys_act : SysAct ) -> str : \"\"\"Iterates through all possible templates and applies the first one to fit the system act Arguments: sys_act {SysAct} -- the system act to find a template for Raises: BaseException: when no template could be applied Returns: str -- the message returned by the template \"\"\" slots = self . _create_memory_from_sys_act ( sys_act ) for template in self . _templates [ sys_act . type . value ]: if template . is_applicable ( slots ): return template . apply ( slots ) raise BaseException ( 'No template was found for the given system act.' ) nlu special \u00b6 nlu \u00b6 HandcraftedNLU \u00b6 Class for Handcrafted Natural Language Understanding Module (HDC-NLU). HDC-NLU is a rule-based approach to recognize the user acts as well as their respective slots and values from the user input (i.e. text) by means of regular expressions. HDC-NLU is domain-independet. The regular expressions of are read from JSON files. There exist a JSON file that stores general rules (GeneralRules.json), i.e. rules that apply to any domain, e.g. rules to detect salutation (Hello, Hi). There are two more files per domain that contain the domain-specific rules for request and inform user acts, e.g. ImsCoursesInformRules.json and ImsCoursesRequestRules.json. The output during dialog interaction of this module is a semantic representation of the user input. \"I am looking for pizza\" --> inform(slot=food,value=italian) See the regex_generator under tools, if the existing regular expressions need to be changed or a new domain should be added. __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > , language = None ) special \u00b6 Loads - domain key - informable slots - requestable slots - domain-independent regular expressions - domain-specific regualer espressions It sets the previous system act to None Source code in adviser/services/nlu/nlu.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , domain : JSONLookupDomain , logger : DiasysLogger = DiasysLogger (), language : Language = None ): \"\"\" Loads - domain key - informable slots - requestable slots - domain-independent regular expressions - domain-specific regualer espressions It sets the previous system act to None Args: domain {domain.jsonlookupdomain.JSONLookupDomain} -- Domain \"\"\" Service . __init__ ( self , domain = domain ) self . logger = logger self . language = language if language else Language . ENGLISH # Getting domain information self . domain_name = domain . get_domain_name () self . domain_key = domain . get_primary_key () # Getting lists of informable and requestable slots self . USER_INFORMABLE = domain . get_informable_slots () self . USER_REQUESTABLE = domain . get_requestable_slots () # Getting the relative path where regexes are stored self . base_folder = os . path . join ( get_root_dir (), 'resources' , 'nlu_regexes' ) # Setting previous system act to None to signal the first turn # self.prev_sys_act = None self . sys_act_info = { 'last_act' : None , 'lastInformedPrimKeyVal' : None , 'lastRequestSlot' : None } self . language = Language . ENGLISH self . _initialize () extract_user_acts ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/nlu/nlu.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result start_dialog ( self ) \u00b6 Sets the previous system act as None. This function is called when the dialog starts Returns: Type Description dict Empty dictionary Source code in adviser/services/nlu/nlu.py 390 391 392 393 394 395 396 397 398 399 400 def start_dialog ( self ) -> dict : \"\"\" Sets the previous system act as None. This function is called when the dialog starts Returns: Empty dictionary \"\"\" self . sys_act_info = { 'last_act' : None , 'lastInformedPrimKeyVal' : None , 'lastRequestSlot' : None } get_root_dir () \u00b6 Source code in adviser/services/nlu/nlu.py 35 36 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))) policy special \u00b6 affective_policy \u00b6 EmotionPolicy \u00b6 Module for deciding what type of emotional response/ engagement level of response, the system should give __init__ ( self , domain = None , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Initializes the policy Parameters: Name Type Description Default domain JSONLookupDomain the domain that the affective policy should operate in None Source code in adviser/services/policy/affective_policy.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , domain : JSONLookupDomain = None , logger : DiasysLogger = DiasysLogger ()): \"\"\" Initializes the policy Arguments: domain (JSONLookupDomain): the domain that the affective policy should operate in \"\"\" self . first_turn = True Service . __init__ ( self , domain = domain ) self . logger = logger choose_sys_emotion ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/policy/affective_policy.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result dialog_start ( self ) \u00b6 This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/policy/affective_policy.py 46 47 def dialog_start ( self ): pass policy_api \u00b6 HandcraftedPolicy \u00b6 Handcrafted policy for API domains Differs from the default HandcraftedPolicy class by taking into account mandatory slots, i.e. slots which have to be informed about before an API can even be called. The class is currently a copy of an older version of the HandcraftedPolicy class with the required changes for API usage. The classes will probably be merged in the future. __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Initializes the policy Source code in adviser/services/policy/policy_api.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , domain : LookupDomain , logger : DiasysLogger = DiasysLogger ()): \"\"\" Initializes the policy Arguments: domain {domain.lookupdomain.LookupDomain} -- Domain \"\"\" self . first_turn = True Service . __init__ ( self , domain = domain ) self . last_action = None self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation self . domain_key = domain . get_primary_key () self . logger = logger choose_sys_act ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/policy/policy_api.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result dialog_start ( self ) \u00b6 This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/policy/policy_api.py 124 125 126 127 128 def dialog_start ( self ): self . first_turn = True self . last_action = None self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation policy_handcrafted \u00b6 HandcraftedPolicy \u00b6 Base class for handcrafted policies. Provides a simple rule-based policy. Can be used for any domain where a user is trying to find an entity (eg. a course from a module handbook) from a database by providing constraints (eg. semester the course is offered) or where a user is trying to find out additional information about a named entity. Output is a system action such as: * inform : provides information on an entity * request : request more information from the user * bye : issue parting message and end dialog In order to create your own policy, you can inherit from this class. Make sure to overwrite the choose_sys_act -method with whatever additionally rules/functionality required. __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > , max_turns = 25 ) special \u00b6 Initializes the policy Source code in adviser/services/policy/policy_handcrafted.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , domain : JSONLookupDomain , logger : DiasysLogger = DiasysLogger (), max_turns : int = 25 ): \"\"\" Initializes the policy Arguments: domain {domain.jsonlookupdomain.JSONLookupDomain} -- Domain \"\"\" self . first_turn = True Service . __init__ ( self , domain = domain ) self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation self . domain_key = domain . get_primary_key () self . logger = logger self . max_turns = max_turns choose_sys_act ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/policy/policy_handcrafted.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result dialog_start ( self ) \u00b6 resets the policy after each dialog Source code in adviser/services/policy/policy_handcrafted.py 68 69 70 71 72 73 74 75 def dialog_start ( self ): \"\"\" resets the policy after each dialog \"\"\" self . turns = 0 self . first_turn = True self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation rl special \u00b6 dqn \u00b6 DQN \u00b6 Simple Deep Q-Network __init__ ( self , state_dim , action_dim , hidden_layer_sizes = [ 300 , 300 ], dropout_rate = 0.0 ) special \u00b6 Initialize a DQN Network with an arbitrary amount of linear hidden layers Source code in adviser/services/policy/rl/dqn.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , state_dim : int , action_dim : int , hidden_layer_sizes : List [ int ] = [ 300 , 300 ], dropout_rate : float = 0.0 ): \"\"\" Initialize a DQN Network with an arbitrary amount of linear hidden layers \"\"\" super ( DQN , self ) . __init__ () print ( \"Architecture: DQN\" ) self . dropout_rate = dropout_rate # create layers self . layers = nn . ModuleList () current_input_dim = state_dim for layer_size in hidden_layer_sizes : self . layers . append ( nn . Linear ( current_input_dim , layer_size )) self . layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . layers . append ( nn . Dropout ( p = dropout_rate )) current_input_dim = layer_size # output layer self . layers . append ( nn . Linear ( current_input_dim , action_dim )) forward ( self , state_batch ) \u00b6 Forward pass: calculate Q(state) for all actions Parameters: Name Type Description Default state_batch FloatTensor tensor of size batch_size x state_dim required Returns: Type Description output tensor of size batch_size x action_dim Source code in adviser/services/policy/rl/dqn.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def forward ( self , state_batch : torch . FloatTensor ): \"\"\" Forward pass: calculate Q(state) for all actions Args: state_batch (torch.FloatTensor): tensor of size batch_size x state_dim Returns: output: tensor of size batch_size x action_dim \"\"\" output = state_batch for layer in self . layers : output = layer ( output ) return output DuelingDQN \u00b6 Dueling DQN network architecture Splits network into value- and advantage stream (V(s) and A(s,a)), recombined in final layer to form Q-value again: Q(s,a) = V(s) + A(s,a). __init__ ( self , state_dim , action_dim , shared_layer_sizes = [ 128 ], value_layer_sizes = [ 128 ], advantage_layer_sizes = [ 128 ], dropout_rate = 0.0 ) special \u00b6 Source code in adviser/services/policy/rl/dqn.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def __init__ ( self , state_dim : int , action_dim : int , shared_layer_sizes : List [ int ] = [ 128 ], value_layer_sizes : List [ int ] = [ 128 ], advantage_layer_sizes : List [ int ] = [ 128 ], dropout_rate : float = 0.0 ): super ( DuelingDQN , self ) . __init__ () print ( \"ARCHITECTURE: Dueling\" ) self . dropout_rate = dropout_rate # configure layers self . shared_layers = nn . ModuleList () self . value_layers = nn . ModuleList () self . advantage_layers = nn . ModuleList () # shared layer: state_dim -> shared_layer_sizes[-1] shared_layer_dim = state_dim for layer_size in shared_layer_sizes : self . shared_layers . append ( nn . Linear ( shared_layer_dim , layer_size )) self . shared_layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . shared_layers . append ( nn . Dropout ( p = dropout_rate )) shared_layer_dim = layer_size # value layer: shared_layer_sizes[-1] -> 1 value_layer_dim = shared_layer_dim for layer_size in value_layer_sizes : self . value_layers . append ( nn . Linear ( value_layer_dim , layer_size )) self . value_layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . value_layers . append ( nn . Dropout ( p = dropout_rate )) value_layer_dim = layer_size self . value_layers . append ( nn . Linear ( value_layer_dim , 1 )) # advantage layer: shared_layer_sizes[-1] -> actions advantage_layer_dim = shared_layer_dim for layer_size in advantage_layer_sizes : self . advantage_layers . append ( nn . Linear ( advantage_layer_dim , layer_size )) self . advantage_layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . advantage_layers . append ( nn . Dropout ( p = dropout_rate )) advantage_layer_dim = layer_size self . advantage_layers . append ( nn . Linear ( advantage_layer_dim , action_dim )) forward ( self , state_batch ) \u00b6 Forward pass: calculate Q(state) for all actions Parameters: Name Type Description Default input torch.FloatTensor tensor of size batch_size x state_dim required Returns: Type Description tensor of size batch_size x action_dim Source code in adviser/services/policy/rl/dqn.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def forward ( self , state_batch : torch . FloatTensor ): \"\"\" Forward pass: calculate Q(state) for all actions Args: input (torch.FloatTensor): tensor of size batch_size x state_dim Returns: tensor of size batch_size x action_dim \"\"\" shared_output = state_batch # shared layer representation for layer in self . shared_layers : shared_output = layer ( shared_output ) # value stream value_stream = shared_output for layer in self . value_layers : value_stream = layer ( value_stream ) # advantage stream advantage_stream = shared_output for layer in self . advantage_layers : advantage_stream = layer ( advantage_stream ) # combine value and advantage streams into Q values result = value_stream + advantage_stream - advantage_stream . mean () return result NetArchitecture \u00b6 Network architecture for DQN vanilla: normal MLP dueling: splits network into value- and advantage stream, recombined in final layer __new__ ( cls , value ) special \u00b6 Source code in adviser/services/policy/rl/dqn.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc dqnpolicy \u00b6 DQNPolicy \u00b6 __init__ ( self , domain , architecture =< NetArchitecture . DUELING : 'dueling' > , hidden_layer_sizes = [ 256 , 700 , 700 ], shared_layer_sizes = [ 256 ], value_layer_sizes = [ 300 , 300 ], advantage_layer_sizes = [ 400 , 400 ], lr = 0.0001 , discount_gamma = 0.99 , target_update_rate = 3 , replay_buffer_size = 8192 , batch_size = 64 , buffer_cls =< class ' services . policy . rl . experience_buffer . NaivePrioritizedBuffer '>, eps_start=0.3, eps_end=0.0, l2_regularisation=0.0, gradient_clipping=5.0, p_dropout=0.0, training_frequency=2, train_dialogs=1000, include_confreq=False, logger=<DiasysLogger adviser (NOTSET)>, max_turns=25, summary_writer=None, device=device(type=' cpu ')) special \u00b6 Parameters: Name Type Description Default target_update_rate int if 1, vanilla dqn update if > 1, double dqn with specified target update rate 3 Source code in adviser/services/policy/rl/dqnpolicy.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def __init__ ( self , domain : JSONLookupDomain , architecture : NetArchitecture = NetArchitecture . DUELING , hidden_layer_sizes : List [ int ] = [ 256 , 700 , 700 ], # vanilla architecture shared_layer_sizes : List [ int ] = [ 256 ], value_layer_sizes : List [ int ] = [ 300 , 300 ], advantage_layer_sizes : List [ int ] = [ 400 , 400 ], # dueling architecture lr : float = 0.0001 , discount_gamma : float = 0.99 , target_update_rate : int = 3 , replay_buffer_size : int = 8192 , batch_size : int = 64 , buffer_cls : Type [ Buffer ] = NaivePrioritizedBuffer , eps_start : float = 0.3 , eps_end : float = 0.0 , l2_regularisation : float = 0.0 , gradient_clipping : float = 5.0 , p_dropout : float = 0.0 , training_frequency : int = 2 , train_dialogs : int = 1000 , include_confreq : bool = False , logger : DiasysLogger = DiasysLogger (), max_turns : int = 25 , summary_writer : SummaryWriter = None , device = torch . device ( 'cpu' )): \"\"\" Args: target_update_rate: if 1, vanilla dqn update if > 1, double dqn with specified target update rate \"\"\" RLPolicy . __init__ ( self , domain , buffer_cls = buffer_cls , buffer_size = replay_buffer_size , batch_size = batch_size , discount_gamma = discount_gamma , include_confreq = include_confreq , logger = logger , max_turns = max_turns , device = device ) Service . __init__ ( self , domain = domain ) self . writer = summary_writer self . training_frequency = training_frequency self . train_dialogs = train_dialogs self . lr = lr self . gradient_clipping = gradient_clipping if gradient_clipping > 0.0 and self . logger : self . logger . info ( \"Gradient Clipping: \" + str ( gradient_clipping )) self . target_update_rate = target_update_rate self . epsilon_start = eps_start self . epsilon_end = eps_end # Select network architecture if architecture == NetArchitecture . VANILLA : if self . logger : self . logger . info ( \"Architecture: Vanilla\" ) self . model = DQN ( self . state_dim , self . action_dim , hidden_layer_sizes = hidden_layer_sizes , dropout_rate = p_dropout ) else : if self . logger : self . logger . info ( \"Architecture: Dueling\" ) self . model = DuelingDQN ( self . state_dim , self . action_dim , shared_layer_sizes = shared_layer_sizes , value_layer_sizes = value_layer_sizes , advantage_layer_sizes = advantage_layer_sizes , dropout_rate = p_dropout ) # Select network update self . target_model = None if target_update_rate > 1 : if self . logger : self . logger . info ( \"Update: Double\" ) if architecture == NetArchitecture . VANILLA : self . target_model = copy . deepcopy ( self . model ) elif self . logger : self . logger . info ( \"Update: Vanilla\" ) self . optim = optim . Adam ( self . model . parameters (), lr = lr , weight_decay = l2_regularisation ) self . loss_fun = nn . SmoothL1Loss ( reduction = 'none' ) # self.loss_fun = nn.MSELoss(reduction='none') self . train_call_count = 0 self . total_train_dialogs = 0 self . epsilon = self . epsilon_start self . turns = 0 self . cumulative_train_dialogs = - 1 choose_sys_act ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/policy/rl/dqnpolicy.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result dialog_end ( self ) \u00b6 clean up needed at the end of a dialog Source code in adviser/services/policy/rl/dqnpolicy.py 163 164 165 166 167 168 169 170 def dialog_end ( self ): \"\"\" clean up needed at the end of a dialog \"\"\" self . end_dialog ( self . sim_goal ) if self . is_training : self . total_train_dialogs += 1 self . train_batch () dialog_start ( self , dialog_start = False ) \u00b6 This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/policy/rl/dqnpolicy.py 121 122 123 124 125 126 127 128 129 130 def dialog_start ( self , dialog_start = False ): self . turns = 0 self . last_sys_act = None if self . is_training : self . cumulative_train_dialogs += 1 self . sys_state = { \"lastInformedPrimKeyVal\" : None , \"lastActionInformNone\" : False , \"offerHappened\" : False , 'informedPrimKeyValsSinceNone' : []} end ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/policy/rl/dqnpolicy.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result eps_scheduler ( self ) \u00b6 Linear epsilon decay Source code in adviser/services/policy/rl/dqnpolicy.py 377 378 379 380 381 382 383 384 def eps_scheduler ( self ): \"\"\" Linear epsilon decay \"\"\" if self . is_training : self . epsilon = max ( 0 , self . epsilon_start - ( self . epsilon_start - self . epsilon_end ) * float ( self . num_dialogs ) / float ( self . train_dialogs )) if self . writer is not None : self . writer . add_scalar ( 'train/eps' , self . epsilon , self . total_train_dialogs ) eval ( self , eval = True ) \u00b6 Sets module and its subgraph to eval mode Source code in adviser/services/policy/rl/dqnpolicy.py 425 426 427 428 429 430 431 def eval ( self , eval = True ): \"\"\" Sets module and its subgraph to eval mode \"\"\" super ( DQNPolicy , self ) . eval () self . is_training = False self . model . eval () if self . target_model is not None : self . target_model . eval () load ( self , path = 'models/dqn' , version = '1.0' ) \u00b6 Load model weights Parameters: Name Type Description Default path str path to model folder 'models/dqn' version str appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) '1.0' Source code in adviser/services/policy/rl/dqnpolicy.py 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def load ( self , path : str = os . path . join ( 'models' , 'dqn' ), version : str = \"1.0\" ): \"\"\" Load model weights Args: path (str): path to model folder version (str): appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) \"\"\" model_file = os . path . join ( path , \"rlpolicy_\" + self . domain . get_domain_name () + \"_\" + version + \".pt\" ) if not os . path . isfile ( model_file ): raise FileNotFoundError ( \"Could not find DQN policy weight file \" , model_file ) self . model = torch . load ( model_file ) self . logger . info ( \"Loaded DQN weights from file \" + model_file ) if self . target_model is not None : self . target_model . load_state_dict ( self . model . state_dict ()) loss ( self , s_batch , a_batch , s2_batch , r_batch , t_batch , gamma ) \u00b6 Calculate TD-loss for given experience tuples Parameters: Name Type Description Default s_batch FloatTensor states (dimension batch x state_dim) required a_batch LongTensor actions (dimension batch x 1) required s2_batch FloatTensor next states (dimension: batch x state_dim) required r_batch FloatTensor rewards (dimension batch x 1) required t_batch FloatTensor indicator {0,1} for terminal states (dimension: batch x 1) required gamma float discount factor required Returns: Type Description TD-loss Source code in adviser/services/policy/rl/dqnpolicy.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def loss ( self , s_batch : torch . FloatTensor , a_batch : torch . LongTensor , s2_batch : torch . FloatTensor , r_batch : torch . FloatTensor , t_batch : torch . FloatTensor , gamma : float ): \"\"\" Calculate TD-loss for given experience tuples Args: s_batch (torch.FloatTensor): states (dimension batch x state_dim) a_batch (torch.LongTensor): actions (dimension batch x 1) s2_batch (torch.FloatTensor): next states (dimension: batch x state_dim) r_batch (torch.FloatTensor): rewards (dimension batch x 1) t_batch (torch.FloatTensor): indicator {0,1} for terminal states (dimension: batch x 1) gamma (float): discount factor Returns: TD-loss \"\"\" # forward value torch . autograd . set_grad_enabled ( True ) q_val = self . _forward ( s_batch , a_batch ) # forward target torch . autograd . set_grad_enabled ( False ) if self . target_model is None : q_target = self . _forward_target ( s2_batch , r_batch , t_batch , gamma ) else : q_target = self . _forward_target_ddqn ( s2_batch , r_batch , t_batch , gamma ) torch . autograd . set_grad_enabled ( True ) # loss loss = self . loss_fun ( q_val , q_target ) return loss save ( self , path = 'models/dqn' , version = '1.0' ) \u00b6 Save model weights Parameters: Name Type Description Default path str path to model folder 'models/dqn' version str appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) '1.0' Source code in adviser/services/policy/rl/dqnpolicy.py 386 387 388 389 390 391 392 393 394 395 396 397 398 def save ( self , path : str = os . path . join ( 'models' , 'dqn' ), version : str = \"1.0\" ): \"\"\" Save model weights Args: path (str): path to model folder version (str): appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) \"\"\" if not os . path . exists ( path ): os . makedirs ( path , exist_ok = True ) model_file = os . path . join ( path , \"rlpolicy_\" + self . domain . get_domain_name () + \"_\" + version + \".pt\" ) torch . save ( self . model , model_file ) select_action_eps_greedy ( self , state_vector ) \u00b6 Epsilon-greedy policy. Parameters: Name Type Description Default state_vector FloatTensor current state (dimension 1 x state_dim) required Returns: Type Description action index for action selected by the agent for the current state Source code in adviser/services/policy/rl/dqnpolicy.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def select_action_eps_greedy ( self , state_vector : torch . FloatTensor ): \"\"\" Epsilon-greedy policy. Args: state_vector (torch.FloatTensor): current state (dimension 1 x state_dim) Returns: action index for action selected by the agent for the current state \"\"\" self . eps_scheduler () # epsilon greedy exploration if self . is_training and common . random . random () < self . epsilon : next_action_idx = common . random . randint ( 0 , self . action_dim - 1 ) else : torch . autograd . set_grad_enabled ( False ) q_values = self . model ( state_vector ) next_action_idx = q_values . squeeze ( dim = 0 ) . max ( dim = 0 )[ 1 ] . item () torch . autograd . set_grad_enabled ( True ) return next_action_idx train ( self , train = True ) \u00b6 Sets module and its subgraph to training mode Source code in adviser/services/policy/rl/dqnpolicy.py 417 418 419 420 421 422 423 def train ( self , train = True ): \"\"\" Sets module and its subgraph to training mode \"\"\" super ( DQNPolicy , self ) . train () self . is_training = True self . model . train () if self . target_model is not None : self . target_model . train () train_batch ( self ) \u00b6 Train on a minibatch drawn from the experience buffer. Source code in adviser/services/policy/rl/dqnpolicy.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def train_batch ( self ): \"\"\" Train on a minibatch drawn from the experience buffer. \"\"\" if not self . is_training : return if len ( self . buffer ) >= self . batch_size * 10 and \\ self . total_train_dialogs % self . training_frequency == 0 : self . train_call_count += 1 s_batch , a_batch , r_batch , s2_batch , t_batch , indices , importance_weights = \\ self . buffer . sample () self . optim . zero_grad () torch . autograd . set_grad_enabled ( True ) s_batch . requires_grad_ () gamma = torch . tensor ([ self . discount_gamma ] * self . batch_size , dtype = torch . float , device = self . device ) . view ( self . batch_size , 1 ) # calculate loss loss = self . loss ( s_batch , a_batch , s2_batch , r_batch , t_batch , gamma ) if importance_weights is not None : loss = loss * importance_weights for i in range ( self . batch_size ): # importance weighting # update priorities self . buffer . update ( i , loss [ i ] . item ()) loss = loss . mean () loss . backward () # clip gradients if self . gradient_clipping > 0.0 : nn . utils . clip_grad_norm_ ( self . model . parameters (), self . gradient_clipping ) # update weights self . optim . step () current_loss = loss . item () torch . autograd . set_grad_enabled ( False ) if self . writer is not None : # plot loss self . writer . add_scalar ( 'train/loss' , current_loss , self . train_call_count ) # plot min/max gradients max_grad_norm = - 1.0 min_grad_norm = 1000000.0 for param in self . model . parameters (): if param . grad is not None : # TODO decide on norm current_grad_norm = torch . norm ( param . grad , 2 ) if current_grad_norm > max_grad_norm : max_grad_norm = current_grad_norm if current_grad_norm < min_grad_norm : min_grad_norm = current_grad_norm self . writer . add_scalar ( 'train/min_grad' , min_grad_norm , self . train_call_count ) self . writer . add_scalar ( 'train/max_grad' , max_grad_norm , self . train_call_count ) # update target net if self . target_model is not None and \\ self . train_call_count % self . target_update_rate == 0 : self . target_model . load_state_dict ( self . model . state_dict ()) experience_buffer \u00b6 Buffer \u00b6 Base class for experience replay buffers Initializes the memory, provides a print function for the memory contents and a method to insert new items into the buffer. Sampling has to be implemented by child classes. __init__ ( self , buffer_size , batch_size , state_dim , discount_gamma = 0.99 , device = device ( type = 'cpu' )) special \u00b6 Source code in adviser/services/policy/rl/experience_buffer.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , buffer_size : int , batch_size : int , state_dim : int , discount_gamma : float = 0.99 , device = torch . device ( 'cpu' )): assert buffer_size >= batch_size , 'the buffer hast to be larger than the batch size' self . device = device self . buffer_size = buffer_size self . batch_size = batch_size self . discount_gamma = discount_gamma # construct memory self . mem_state = torch . empty ( buffer_size , state_dim , dtype = torch . float , device = device ) self . mem_action = torch . empty ( buffer_size , 1 , dtype = torch . long , device = device ) self . mem_reward = torch . empty ( buffer_size , 1 , dtype = torch . float , device = device ) self . mem_next_state = torch . empty ( buffer_size , state_dim , dtype = torch . float , device = device ) self . mem_terminal = torch . empty ( buffer_size , 1 , dtype = torch . float , device = device ) self . write_pos = 0 self . last_write_pos = 0 self . buffer_count = 0 self . _reset () __len__ ( self ) special \u00b6 Returns the number of items currently inside the buffer Source code in adviser/services/policy/rl/experience_buffer.py 145 146 147 def __len__ ( self ): \"\"\" Returns the number of items currently inside the buffer \"\"\" return self . buffer_count print_contents ( self , max_size = None ) \u00b6 Print contents of the experience replay memory. Parameters: Name Type Description Default max_size int restrict the number of printed items to this number (if not None) None Source code in adviser/services/policy/rl/experience_buffer.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def print_contents ( self , max_size : int = None ): \"\"\" Print contents of the experience replay memory. Args: max_size (int): restrict the number of printed items to this number (if not None) \"\"\" # how many entries to print print_items = len ( self ) if max_size is not None : print_items = min ( print_items , max_size ) print ( \"# REPLAY BUFFER CAPACITY: \" , self . buffer_size ) print ( \"# CURRENT ITEM COUNT\" , len ( self )) for i in range ( print_items ): print ( \"entry \" , i ) print ( \" action\" , self . mem_action [ i ]) print ( \" reward\" , self . mem_reward [ i ]) print ( \" terminal\" , self . mem_terminal [ i ]) print ( '---------' ) sample ( self ) \u00b6 Sample from buffer, has to be implemented by subclasses Source code in adviser/services/policy/rl/experience_buffer.py 149 150 151 def sample ( self ): \"\"\" Sample from buffer, has to be implemented by subclasses \"\"\" raise NotImplementedError store ( self , state , action , reward , terminal = False ) \u00b6 Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Parameters: Name Type Description Default state FloatTensor this turn's state tensor, or None if terminal = True required action LongTensor this turn's action index (int), or None if terminal = True required reward float this turn's reward (float) required terminal bool indicates whether episode finished (boolean) False Source code in adviser/services/policy/rl/experience_buffer.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def store ( self , state : torch . FloatTensor , action : torch . LongTensor , reward : float , terminal : bool = False ): \"\"\" Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Args: state (torch.tensor): this turn's state tensor, or None if terminal = True action (torch.tensor): this turn's action index (int), or None if terminal = True reward (torch.tensor): this turn's reward (float) terminal (bool): indicates whether episode finished (boolean) \"\"\" reward /= 20.0 if isinstance ( self . last_state , type ( None )): # and terminal == False: # first turn of trajectory, don't record since s' is needed self . last_state = state self . last_action = action self . last_reward = reward return False else : if terminal == True : if self . episode_length > 0 : # update last state's reward and set it to terminal self . mem_terminal [ self . last_write_pos ] = float ( True ) self . mem_reward [ self . last_write_pos ] += reward self . _reset () return False else : # in-between turn of trajectory: record self . mem_state [ self . write_pos ] = \\ self . last_state . clone () . detach () self . mem_action [ self . write_pos ][ 0 ] = self . last_action self . mem_reward [ self . write_pos ][ 0 ] = self . last_reward self . mem_next_state [ self . write_pos ] = state . clone () . detach () self . mem_terminal [ self . write_pos ] = float ( False ) # update last encountered state self . last_state = state . clone () . detach () self . last_action = action self . last_reward = reward # update write index self . last_write_pos = self . write_pos self . write_pos = ( self . write_pos + 1 ) % self . buffer_size if self . buffer_count < self . buffer_size : self . buffer_count += 1 self . episode_length += 1 return True NaivePrioritizedBuffer \u00b6 Prioritized experience replay buffer. Assigns sampling probabilities dependent on TD-error of the transitions. __init__ ( self , buffer_size , batch_size , state_dim , sample_last_transition = False , regularisation = 1e-05 , exponent = 0.6 , beta = 0.4 , discount_gamma = 0.99 , device = device ( type = 'cpu' )) special \u00b6 Source code in adviser/services/policy/rl/experience_buffer.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def __init__ ( self , buffer_size : int , batch_size : int , state_dim : int , sample_last_transition : bool = False , regularisation : float = 0.00001 , exponent : float = 0.6 , beta : float = 0.4 , discount_gamma : float = 0.99 , device = torch . device ( 'cpu' )): super ( NaivePrioritizedBuffer , self ) . __init__ ( buffer_size , batch_size , state_dim , discount_gamma = discount_gamma , device = device ) print ( \" REPLAY MEMORY: NAIVE Prioritized\" ) self . probs = [ 0.0 ] * buffer_size self . regularisation = regularisation self . exponent = exponent self . beta = beta self . max_p = 1.0 self . sample_last_transition = sample_last_transition sample ( self ) \u00b6 Sample from buffer. Returns: Type Description states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, importance weights Source code in adviser/services/policy/rl/experience_buffer.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def sample ( self ): \"\"\" Sample from buffer. Returns: states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, importance weights \"\"\" batch_size = self . batch_size batch_write_pos = 0 data_indices = torch . empty ( self . batch_size , dtype = torch . long , device = self . device ) probabilities = torch . empty ( self . batch_size , dtype = torch . float , device = self . device ) indices = [] self . sample_last_transition = True p_normed = np . array ( self . probs [: self . buffer_count ]) / np . linalg . norm ( self . probs [: self . buffer_count ], ord = 1 ) indices = common . numpy . random . choice ( list ( range ( self . buffer_count )), size = self . batch_size , p = p_normed ) if self . sample_last_transition : # include last transition (was at tree.write - 1) # -> see Sutton: A deeper look at experience replay data_indices [ 0 ] = self . last_write_pos probabilities [ 0 ] = self . probs [ self . last_write_pos ] # correct size of batch batch_size = batch_size - 1 batch_write_pos += 1 # TODO add option to sample each segment uniformly for i in range ( batch_write_pos , self . batch_size ): data_indices [ i ] = int ( indices [ i ]) probabilities [ i ] = self . probs [ data_indices [ i ]] # assemble batch from data indices s_batch = self . mem_state . index_select ( 0 , data_indices ) a_batch = self . mem_action . index_select ( 0 , data_indices ) r_batch = self . mem_reward . index_select ( 0 , data_indices ) t_batch = self . mem_terminal . index_select ( 0 , data_indices ) s2_batch = self . mem_next_state . index_select ( 0 , data_indices ) # calculate importance sampling weights importance_weights = float ( len ( self )) * probabilities importance_weights = importance_weights . pow ( - self . beta ) importance_weights = importance_weights / importance_weights . max ( dim = 0 )[ 0 ] . item () return s_batch , a_batch , r_batch , s2_batch , t_batch , data_indices , \\ importance_weights . view ( - 1 , 1 ) store ( self , state , action , reward , terminal = False ) \u00b6 Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Newly added experience tuples will be assigned maximum priority. Parameters: Name Type Description Default state FloatTensor this turn's state tensor, or None if terminal = True required action LongTensor this turn's action index (int), or None if terminal = True required reward float this turn's reward (float) required terminal bool indicates whether episode finished (boolean) False Source code in adviser/services/policy/rl/experience_buffer.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def store ( self , state : torch . FloatTensor , action : torch . LongTensor , reward : float , terminal : bool = False ): \"\"\" Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Newly added experience tuples will be assigned maximum priority. Args: state: this turn's state tensor, or None if terminal = True action: this turn's action index (int), or None if terminal = True reward: this turn's reward (float) terminal: indicates whether episode finished (boolean) \"\"\" if super ( NaivePrioritizedBuffer , self ) . store ( state , action , reward , terminal = terminal ): # create new tree node only if something new was added to the buffers self . probs [ self . last_write_pos ] = self . _priority_to_probability ( self . max_p ) update ( self , idx , error ) \u00b6 Update the priority of transition with index idx Source code in adviser/services/policy/rl/experience_buffer.py 252 253 254 255 256 257 def update ( self , idx : int , error : float ): \"\"\" Update the priority of transition with index idx \"\"\" p = self . _priority_to_probability ( error ) if p > self . max_p : self . max_p = p self . probs [ idx ] = p UniformBuffer \u00b6 Experience replay buffer with uniformly random sampling __init__ ( self , buffer_size , batch_size , state_dim , discount_gamma = 0.99 , sample_last_transition = True , device = device ( type = 'cpu' )) special \u00b6 Parameters: Name Type Description Default sample_last_transition bool if True, a batch will always include the most recent transition (see Sutton: A deeper look at experience replay) True Source code in adviser/services/policy/rl/experience_buffer.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def __init__ ( self , buffer_size : int , batch_size : int , state_dim : int , discount_gamma : float = 0.99 , sample_last_transition : bool = True , device = torch . device ( 'cpu' )): \"\"\" Args: sample_last_transition (bool): if True, a batch will always include the most recent transition (see Sutton: A deeper look at experience replay) \"\"\" super ( UniformBuffer , self ) . __init__ ( buffer_size , batch_size , state_dim , discount_gamma = discount_gamma , device = device ) print ( \" REPLAY MEMORY: Uniform\" ) self . sample_last_transition = sample_last_transition sample ( self ) \u00b6 Sample from buffer. Returns: Type Description states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, None Source code in adviser/services/policy/rl/experience_buffer.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def sample ( self ): \"\"\" Sample from buffer. Returns: states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, None \"\"\" # create random indices data_indices = [] if self . sample_last_transition : # include last transition (was at write - 1) # - see Sutton: A deeper look at experience replay if self . write_pos - 1 < 0 : # last transition filled the capacity of the buffer data_indices = [ self . buffer_size - 1 ] else : data_indices = [ self . write_pos - 1 ] data_indices . extend ([ common . random . randint ( 0 , self . buffer_count - 1 ) for i in range ( self . batch_size - int ( self . sample_last_transition ))]) data_indices = torch . tensor ( data_indices , dtype = torch . long , device = self . device ) state_batch = self . mem_state . index_select ( 0 , data_indices ) action_batch = self . mem_action . index_select ( 0 , data_indices ) reward_batch = self . mem_reward . index_select ( 0 , data_indices ) next_state_batch = self . mem_next_state . index_select ( 0 , data_indices ) terminal_batch = self . mem_terminal . index_select ( 0 , data_indices ) return state_batch , action_batch , reward_batch , next_state_batch , terminal_batch , \\ data_indices , None policy_rl \u00b6 RLPolicy \u00b6 Base class for Reinforcement Learning based policies. Functionality provided includes the setup of state- and action spaces, conversion of BeliefState objects into pytorch tensors, updating the last performed system actions and informed entities, populating the experience replay buffer, extraction of most probable user hypothesis and candidate action expansion. Output of an agent is a candidate action like inform_food which is then populated with the most probable slot/value pair from the beliefstate and database candidates by the expand_system_action -function to become inform(slot=food,value=italian) . In order to create your own policy, you can inherit from this class. Make sure to call the turn_end -function after each system turn and the end_dialog -function after each completed dialog. __init__ ( self , domain , buffer_cls =< class ' services . policy . rl . experience_buffer . UniformBuffer '>, buffer_size=6000, batch_size=64, discount_gamma=0.99, max_turns=25, include_confreq=False, logger=<DiasysLogger adviser (NOTSET)>, include_select=False, device=device(type=' cpu ')) special \u00b6 Creates state- and action spaces, initializes experience replay buffers. Keyword Arguments: subgraph {[type]} -- [see services.Module] (default: {None}) buffer_cls {services.policy.rl.experience_buffer.Buffer} -- [Experience replay buffer class , not an instance - will be initialized by this constructor!] (default: {UniformBuffer}) buffer_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {6000}) batch_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {64}) discount_gamma {float} -- [Discount factor] (default: {0.99}) include_confreq {bool} -- [Use confirm_request actions] (default: {False}) Source code in adviser/services/policy/rl/policy_rl.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , domain : JSONLookupDomain , buffer_cls = UniformBuffer , buffer_size = 6000 , batch_size = 64 , discount_gamma = 0.99 , max_turns : int = 25 , include_confreq = False , logger : DiasysLogger = DiasysLogger (), include_select : bool = False , device = torch . device ( 'cpu' )): \"\"\" Creates state- and action spaces, initializes experience replay buffers. Arguments: domain {domain.jsonlookupdomain.JSONLookupDomain} -- Domain Keyword Arguments: subgraph {[type]} -- [see services.Module] (default: {None}) buffer_cls {services.policy.rl.experience_buffer.Buffer} -- [Experience replay buffer *class*, **not** an instance - will be initialized by this constructor!] (default: {UniformBuffer}) buffer_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {6000}) batch_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {64}) discount_gamma {float} -- [Discount factor] (default: {0.99}) include_confreq {bool} -- [Use confirm_request actions] (default: {False}) \"\"\" self . device = device self . sys_state = { \"lastInformedPrimKeyVal\" : None , \"lastActionInformNone\" : False , \"offerHappened\" : False , 'informedPrimKeyValsSinceNone' : []} self . max_turns = max_turns self . logger = logger self . domain = domain # setup evaluator for training self . evaluator = ObjectiveReachedEvaluator ( domain , logger = logger ) self . buffer_size = buffer_size self . batch_size = batch_size self . discount_gamma = discount_gamma self . writer = None # get state size self . state_dim = self . beliefstate_dict_to_vector ( BeliefState ( domain ) . _init_beliefstate ()) . size ( 1 ) self . logger . info ( \"state space dim: \" + str ( self . state_dim )) # get system action list self . actions = [ \"inform_byname\" , # TODO rename to 'bykey' \"inform_alternatives\" , \"reqmore\" ] # TODO badaction for req_slot in self . domain . get_system_requestable_slots (): self . actions . append ( 'request#' + req_slot ) self . actions . append ( 'confirm#' + req_slot ) if include_select : self . actions . append ( 'select#' + req_slot ) if include_confreq : for conf_slot in self . domain . get_system_requestable_slots (): if not req_slot == conf_slot : # skip case where confirm slot = request slot self . actions . append ( 'confreq#' + conf_slot + '#' + req_slot ) self . action_dim = len ( self . actions ) # don't include closingmsg in learnable actions self . actions . append ( 'closingmsg' ) # self.actions.append(\"closingmsg\") self . logger . info ( \"action space dim: \" + str ( self . action_dim )) self . primary_key = self . domain . get_primary_key () # init replay memory self . buffer = buffer_cls ( buffer_size , batch_size , self . state_dim , discount_gamma = discount_gamma , device = device ) self . sys_state = {} self . last_sys_act = None action_idx ( self , action_name ) \u00b6 Returns the action index for the specified action name Source code in adviser/services/policy/rl/policy_rl.py 140 141 142 def action_idx ( self , action_name : str ): \"\"\" Returns the action index for the specified action name \"\"\" return self . actions . index ( action_name ) action_name ( self , action_idx ) \u00b6 Returns the action name for the specified action index Source code in adviser/services/policy/rl/policy_rl.py 136 137 138 def action_name ( self , action_idx : int ): \"\"\" Returns the action name for the specified action index \"\"\" return self . actions [ action_idx ] beliefstate_dict_to_vector ( self , beliefstate ) \u00b6 Converts the beliefstate dict to a torch tensor Parameters: Name Type Description Default beliefstate BeliefState dict of belief (with at least beliefs and system keys) required Returns: Type Description belief tensor with dimension 1 x state_dim Source code in adviser/services/policy/rl/policy_rl.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def beliefstate_dict_to_vector ( self , beliefstate : BeliefState ): \"\"\" Converts the beliefstate dict to a torch tensor Args: beliefstate: dict of belief (with at least beliefs and system keys) Returns: belief tensor with dimension 1 x state_dim \"\"\" belief_vec = [] # add user acts belief_vec += [ 1 if act in beliefstate [ 'user_acts' ] else 0 for act in UserActionType ] # handle none actions belief_vec . append ( 1 if sum ( belief_vec ) == 0 else 1 ) # add informs (including special flag if slot not mentioned) for slot in sorted ( self . domain . get_informable_slots ()): values = self . domain . get_possible_values ( slot ) + [ \"dontcare\" ] if slot not in beliefstate [ 'informs' ]: # add **NONE** value first, then 0.0 for all others belief_vec . append ( 1.0 ) # also add value for don't care belief_vec += [ 0 for i in range ( len ( values ))] else : # add **NONE** value first belief_vec . append ( 0.0 ) bs_slot = beliefstate [ 'informs' ][ slot ] belief_vec += [ bs_slot [ value ] if value in bs_slot else 0.0 for value in values ] # add requests for slot in sorted ( self . domain . get_requestable_slots ()): if slot in beliefstate [ 'requests' ]: belief_vec . append ( 1.0 ) else : belief_vec . append ( 0.0 ) # append system features belief_vec . append ( float ( self . sys_state [ 'lastActionInformNone' ])) belief_vec . append ( float ( self . sys_state [ 'offerHappened' ])) candidate_count = beliefstate [ 'num_matches' ] # buckets for match count: 0, 1, 2-4, >4 belief_vec . append ( float ( candidate_count == 0 )) belief_vec . append ( float ( candidate_count == 1 )) belief_vec . append ( float ( 2 <= candidate_count <= 4 )) belief_vec . append ( float ( candidate_count > 4 )) belief_vec . append ( float ( beliefstate [ \"discriminable\" ])) # convert to torch tensor return torch . tensor ([ belief_vec ], dtype = torch . float , device = self . device ) end_dialog ( self , sim_goal ) \u00b6 Call this function when a dialog ended Source code in adviser/services/policy/rl/policy_rl.py 544 545 546 547 548 549 550 551 552 553 554 def end_dialog ( self , sim_goal : Goal ): \"\"\" Call this function when a dialog ended \"\"\" if sim_goal is None : # real user interaction, no simulator - don't have to evaluate # anything, just reset counters return final_reward , success = self . evaluator . get_final_reward ( sim_goal , logging = False ) if self . is_training : self . buffer . store ( None , None , final_reward , terminal = True ) expand_system_action ( self , action_idx , beliefstate ) \u00b6 Expands an action index to a real sytem act Source code in adviser/services/policy/rl/policy_rl.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def expand_system_action ( self , action_idx : int , beliefstate : BeliefState ): \"\"\" Expands an action index to a real sytem act \"\"\" action_name = self . action_name ( action_idx ) if 'request#' in action_name : return self . _expand_request ( action_name ) elif 'select#' in action_name : return self . _expand_select ( action_name , beliefstate ) elif 'confirm#' in action_name : return self . _expand_confirm ( action_name , beliefstate ) elif 'confreq#' in action_name : return self . _expand_confreq ( action_name , beliefstate ) elif action_name == 'inform_byname' : return self . _expand_informbyname ( beliefstate ) elif action_name == 'inform_alternatives' : return self . _expand_informbyalternatives ( beliefstate ) elif action_name == 'closingmsg' : return self . _expand_bye () elif action_name == 'repeat' : return self . last_sys_act elif action_name == 'reqmore' : return self . _expand_reqmore () elif self . logger : self . logger . warning ( \"RL POLICY: system action not supported: \" + action_name ) return None turn_end ( self , beliefstate , state_vector , sys_act_idx ) \u00b6 Call this function after a turn is done by the system Source code in adviser/services/policy/rl/policy_rl.py 520 521 522 523 524 525 526 527 528 529 530 531 532 def turn_end ( self , beliefstate : BeliefState , state_vector : torch . FloatTensor , sys_act_idx : int ): \"\"\" Call this function after a turn is done by the system \"\"\" self . last_sys_act = self . expand_system_action ( sys_act_idx , beliefstate ) if self . logger : self . logger . dialog_turn ( \"system action > \" + str ( self . last_sys_act )) self . _update_system_belief ( beliefstate , self . last_sys_act ) turn_reward = self . evaluator . get_turn_reward () if self . is_training : self . buffer . store ( state_vector , sys_act_idx , turn_reward , terminal = False ) train_dqnpolicy \u00b6 This script can be executed to train a DQN policy. \u00b6 It will create a policy model (file ending with .pt). \u00b6 You need to execute this script before you can interact with the RL agent. \u00b6 \u00b6 get_root_dir () \u00b6 Source code in adviser/services/policy/rl/train_dqnpolicy.py 31 32 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))) train ( domain_name , log_to_file , seed , train_epochs , train_dialogs , eval_dialogs , max_turns , train_error_rate , test_error_rate , lr , eps_start , grad_clipping , buffer_classname , buffer_size , use_tensorboard ) \u00b6 Training loop for the RL policy, for information on the parameters, look at the descriptions of commandline arguments in the \"if main\" below Source code in adviser/services/policy/rl/train_dqnpolicy.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def train ( domain_name : str , log_to_file : bool , seed : int , train_epochs : int , train_dialogs : int , eval_dialogs : int , max_turns : int , train_error_rate : float , test_error_rate : float , lr : float , eps_start : float , grad_clipping : float , buffer_classname : str , buffer_size : int , use_tensorboard : bool ): \"\"\" Training loop for the RL policy, for information on the parameters, look at the descriptions of commandline arguments in the \"if main\" below \"\"\" common . init_random ( seed = seed ) file_log_lvl = LogLevel . DIALOGS if log_to_file else LogLevel . NONE logger = DiasysLogger ( console_log_lvl = LogLevel . RESULTS , file_log_lvl = file_log_lvl ) if buffer_classname == \"prioritized\" : buffer_cls = NaivePrioritizedBuffer elif buffer_classname == \"uniform\" : buffer_cls = UniformBuffer domain = JSONLookupDomain ( name = domain_name ) bst = HandcraftedBST ( domain = domain , logger = logger ) user = HandcraftedUserSimulator ( domain , logger = logger ) # noise = SimpleNoise(domain=domain, train_error_rate=train_error_rate, # test_error_rate=test_error_rate, logger=logger) policy = DQNPolicy ( domain = domain , lr = lr , eps_start = eps_start , gradient_clipping = grad_clipping , buffer_cls = buffer_cls , replay_buffer_size = buffer_size , train_dialogs = train_dialogs , logger = logger ) evaluator = PolicyEvaluator ( domain = domain , use_tensorboard = use_tensorboard , experiment_name = domain_name , logger = logger ) ds = DialogSystem ( services = [ user , bst , policy , evaluator ], protocol = 'tcp' ) # ds.draw_system_graph() error_free = ds . is_error_free_messaging_pipeline () if not error_free : ds . print_local_inconsistencies () for j in range ( train_epochs ): # START TRAIN EPOCH evaluator . train () policy . train () evaluator . start_epoch () for episode in range ( train_dialogs ): if episode % 100 == 0 : print ( \"DIALOG\" , episode ) logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () policy . save () # START EVAL EPOCH evaluator . eval () policy . eval () evaluator . start_epoch () for episode in range ( eval_dialogs ): logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () ds . shutdown () service \u00b6 DialogSystem \u00b6 This class will constrct a dialog system from the list of services provided to the constructor. It will also handle synchronization for initalization of services before dialog start / after dialog end / on system shutdown and lets you discover potential conflicts in you messaging pipeline. This class is also used to communicate / synchronize with services running on different nodes. __init__ ( self , services , sub_port = 65533 , pub_port = 65534 , reg_port = 65535 , protocol = 'tcp' , debug_logger = None ) special \u00b6 Parameters: Name Type Description Default services List[Union[adviser.services.service.Service, adviser.services.service.RemoteService]] List of all (remote) services to connect to. Only once they're specified here will they start listening for messages. required sub_port(int) subscriber port required sub_addr(str) IP-address or domain name of proxy subscriber interface (e.g. 127.0.0.1 for your local machine) required pub_port(int) publisher port required pub_addr(str) IP-address or domain name of proxy publisher interface (e.g. 127.0.0.1 for your local machine) required reg_port int registration port for remote services 65535 protocol(str) communication protol, either 'inproc' or 'tcp' or ipc required debug_logger DiasysLogger If not None , all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the DialogSystem even if they are never forwarded (as expected) to your Service None Source code in adviser/services/service.py 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 def __init__ ( self , services : List [ Union [ Service , RemoteService ]], sub_port : int = 65533 , pub_port : int = 65534 , reg_port : int = 65535 , protocol : str = 'tcp' , debug_logger : DiasysLogger = None ): \"\"\" Args: services (List[Union[Service, RemoteService]]): List of all (remote) services to connect to. Only once they're specified here will they start listening for messages. sub_port(int): subscriber port sub_addr(str): IP-address or domain name of proxy subscriber interface (e.g. 127.0.0.1 for your local machine) pub_port(int): publisher port pub_addr(str): IP-address or domain name of proxy publisher interface (e.g. 127.0.0.1 for your local machine) reg_port (int): registration port for remote services protocol(str): communication protol, either 'inproc' or 'tcp' or `ipc` debug_logger (DiasysLogger): If not `None`, all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the `DialogSystem` even if they are never forwarded (as expected) to your `Service` \"\"\" # node-local topics self . debug_logger = debug_logger self . protocol = protocol self . _sub_topics = {} self . _pub_topics = {} self . _remote_identifiers = set () self . _services = [] # collects names and instances of local services self . _start_dialog_services = set () # collects names of local services that subscribe to dialog_start # node-local sockets self . _domains = set () # start proxy thread self . _proxy_dev = ProcessProxy ( in_type = zmq . XSUB , out_type = zmq . XPUB ) # , mon_type=zmq.XSUB) self . _proxy_dev . bind_in ( f \" { protocol } ://127.0.0.1: { pub_port } \" ) self . _proxy_dev . bind_out ( f \" { protocol } ://127.0.0.1: { sub_port } \" ) self . _proxy_dev . start () self . _sub_port = sub_port self . _pub_port = pub_port # thread control self . _start_topics = set () self . _end_topics = set () self . _terminate_topics = set () self . _stopEvent = threading . Event () # control channels ctx = Context . instance () self . _control_channel_pub = ctx . socket ( zmq . PUB ) self . _control_channel_pub . sndhwm = 1100000 self . _control_channel_pub . connect ( f \" { protocol } ://127.0.0.1: { pub_port } \" ) self . _control_channel_sub = ctx . socket ( zmq . SUB ) # register services (local and remote) remote_services = {} for service in services : if isinstance ( service , Service ): # register local service service_name = type ( service ) . __name__ if service . _identifier is None else service . _identifier service . _init_pubsub () self . _add_service_info ( service_name , service . _domain_name , service . _sub_topics , service . _pub_topics , service . _start_topic , service . _end_topic , service . _terminate_topic ) service . _register_with_dialogsystem () elif isinstance ( service , RemoteService ): remote_services [ getattr ( service , 'identifier' )] = service self . _register_remote_services ( remote_services , reg_port ) self . _control_channel_sub . connect ( f \" { protocol } ://127.0.0.1: { sub_port } \" ) self . _setup_dialog_end_listener () time . sleep ( 0.25 ) draw_system_graph ( self , name = 'system' , format = 'png' , show = True ) \u00b6 Draws a graph of the system as a directed graph. Services are represented by nodes, messages by directed edges (from publisher to subscriber). Warnings are drawn as yellow edges (and the missing subscribers represented by an 'UNCONNECTED SERVICES' node), errors as red edges (and the missing publishers represented by the 'UNCONNECTED SERVICES' node as well). Will mark remote services with blue. Parameters: Name Type Description Default name str used to construct the name of your output file 'system' format str output file format (e.g. png, pdf, jpg, ...) 'png' show bool if True, the graph image will be opened in your default image viewer application True Requires graphviz library (pip install graphviz) Source code in adviser/services/service.py 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 def draw_system_graph ( self , name : str = 'system' , format : str = \"png\" , show : bool = True ): \"\"\" Draws a graph of the system as a directed graph. Services are represented by nodes, messages by directed edges (from publisher to subscriber). Warnings are drawn as yellow edges (and the missing subscribers represented by an 'UNCONNECTED SERVICES' node), errors as red edges (and the missing publishers represented by the 'UNCONNECTED SERVICES' node as well). Will mark remote services with blue. Args: name (str): used to construct the name of your output file format (str): output file format (e.g. png, pdf, jpg, ...) show (bool): if True, the graph image will be opened in your default image viewer application Requires: graphviz library (pip install graphviz) \"\"\" from graphviz import Digraph g = Digraph ( name = name , format = format ) # collect all services, errors and warnings services = set () for service_set in self . _pub_topics . values (): services = services . union ( service_set ) for service_set in self . _sub_topics . values (): services = services . union ( service_set ) errors , warnings = self . list_inconsistencies () # add services as nodes for service in services : if service in self . _remote_identifiers : g . node ( service , color = '#1f618d' , style = 'filled' , fontcolor = 'white' , shape = 'box' ) # remote service else : g . node ( service , color = '#1c2833' , shape = 'box' ) # local service if len ( errors ) > 0 or len ( warnings ) > 0 : g . node ( 'UNCONNECTED SERVICES' , style = 'filled' , color = '#922b21' , fontcolor = 'white' , shape = 'box' ) # draw connections from publisher to subscribers as edges for topic in self . _pub_topics : publishers = self . _pub_topics [ topic ] receivers = self . _sub_topics [ topic ] if topic in self . _sub_topics else [] for receiver in receivers : for publisher in publishers : g . edge ( publisher , receiver , label = topic ) # draw warnings and errors as edges to node 'UNCONNECTED SERVICES' for topic in errors : receivers = errors [ topic ] for receiver in receivers : g . edge ( 'UNCONNECTED SERVICES' , receiver , color = '#c34400' , fontcolor = '#c34400' , label = topic ) for topic in warnings : publishers = warnings [ topic ] for publisher in publishers : g . edge ( publisher , 'UNCONNECTED SERVICES' , color = '#e37c02' , fontcolor = '#e37c02' , label = topic ) # draw graph g . render ( view = show , cleanup = True ) is_error_free_messaging_pipeline ( self ) \u00b6 Checks the current messaging pipeline for potential errors. (Potential) Errors are defined in this context as subscribed topics without publishers. Returns: Type Description bool True, if no potential errors could be found - else, False Notes Call this method after instantiating all services. Lists only node-local (or process-local) inconsistencies. Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. Source code in adviser/services/service.py 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 def is_error_free_messaging_pipeline ( self ) -> bool : \"\"\" Checks the current messaging pipeline for potential errors. (Potential) Errors are defined in this context as subscribed topics without publishers. Returns: True, if no potential errors could be found - else, False Notes: * Call this method after instantiating all services. * Lists only node-local (or process-local) inconsistencies. * Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. \"\"\" return len ( self . list_inconsistencies ()[ 0 ]) == 0 list_inconsistencies ( self ) \u00b6 Checks for potential errors in the current messaging pipleline: e.g. len(list_inconsistencies()[0]) == 0 -> error free pipeline (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Returns: Type Description A touple of dictionaries the first dictionary contains potential errors (with the mapping topics -> subsribing services) the second dictionary contains warnings (with the mapping topics -> publishing services). Notes Call this method after instantiating all services. Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. Source code in adviser/services/service.py 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 def list_inconsistencies ( self ): \"\"\" Checks for potential errors in the current messaging pipleline: e.g. len(list_inconsistencies()[0]) == 0 -> error free pipeline (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Returns: A touple of dictionaries: * the first dictionary contains potential errors (with the mapping topics -> subsribing services) * the second dictionary contains warnings (with the mapping topics -> publishing services). Notes: * Call this method after instantiating all services. * Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. \"\"\" # look for subscribers w/o publishers by checking topic prefixes errors = {} for sub_topic in self . _sub_topics : found_pub = False for pub_topic in self . _pub_topics : if pub_topic . startswith ( sub_topic ): found_pub = True break if not found_pub : errors [ sub_topic ] = self . _sub_topics [ sub_topic ] # look for publishers w/o subscribers by checking topic prefixes warnings = {} for pub_topic in self . _pub_topics : found_sub = False for sub_topic in self . _sub_topics : if pub_topic . startswith ( sub_topic ): found_sub = True break if not found_sub : warnings [ pub_topic ] = self . _pub_topics [ pub_topic ] return errors , warnings list_published_topics ( self ) \u00b6 Get all declared publisher topics. Returns: Type Description A dictionary with mapping topic (str) -> publishing services (Set[str]). Note Call this method after instantiating all services. Even though a publishing topic might be listed here, there is no guarantee that its publisher(s) might ever publish to it. Source code in adviser/services/service.py 852 853 854 855 856 857 858 859 860 861 862 863 def list_published_topics ( self ): \"\"\" Get all declared publisher topics. Returns: A dictionary with mapping topic (str) -> publishing services (Set[str]). Note: * Call this method after instantiating all services. * Even though a publishing topic might be listed here, there is no guarantee that its publisher(s) might ever publish to it. \"\"\" return copy . deepcopy ( self . _pub_topics ) # copy s.t. no user changes this list list_subscribed_topics ( self ) \u00b6 Get all declared subscribed topics. Returns: Type Description A dictionary with mapping topic (str) -> subscribing services (Set[str]). Notes Call this method after instantiating all services. Source code in adviser/services/service.py 865 866 867 868 869 870 871 872 873 874 def list_subscribed_topics ( self ): \"\"\" Get all declared subscribed topics. Returns: A dictionary with mapping topic (str) -> subscribing services (Set[str]). Notes: * Call this method after instantiating all services. \"\"\" return copy . deepcopy ( self . _sub_topics ) # copy s.t. no user changes this list print_inconsistencies ( self ) \u00b6 Checks for potential errors in the current messaging pipleline: e.g. len(list_local_inconsistencies()[0]) == 0 -> error free pipeline and prints them to the console. (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Notes Call this method after instantiating all services. Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. Source code in adviser/services/service.py 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 def print_inconsistencies ( self ): \"\"\" Checks for potential errors in the current messaging pipleline: e.g. len(list_local_inconsistencies()[0]) == 0 -> error free pipeline and prints them to the console. (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Notes: * Call this method after instantiating all services. * Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. \"\"\" # console colors WARNING = ' \\033 [93m' ERROR = ' \\033 [91m' ENDC = ' \\033 [0m' errors , warnings = self . list_inconsistencies () print ( ERROR ) print ( \"(Potential) Errors (subscribed topics without publishers):\" ) for topic in errors : print ( f \" topic: ' { topic } ', subscribed to in services: { errors [ topic ] } \" ) print ( ENDC ) print ( WARNING ) print ( \"Warnings (published topics without subscribers):\" ) for topic in warnings : print ( f \" topic: ' { topic } ', published in services: { warnings [ topic ] } \" ) print ( ENDC ) run_dialog ( self , start_signals = { 'dialog_end' : False }) \u00b6 Run a complete dialog (blocking). Dialog will be started via messages to the topics specified in start_signals . The dialog will end on receiving any Topic.DIALOG_END message with value 'True', so make sure at least one service in your dialog graph will publish this message eventually. Parameters: Name Type Description Default start_signals dict mapping from topic -> value Publishes the value given for each topic to the respective topic. Use this to trigger the start of your dialog system. {'dialog_end': False} Source code in adviser/services/service.py 838 839 840 841 842 843 844 845 846 847 848 849 850 def run_dialog ( self , start_signals : dict = { Topic . DIALOG_END : False }): \"\"\" Run a complete dialog (blocking). Dialog will be started via messages to the topics specified in `start_signals`. The dialog will end on receiving any `Topic.DIALOG_END` message with value 'True', so make sure at least one service in your dialog graph will publish this message eventually. Args: start_signals (Dict[str, Any]): mapping from topic -> value Publishes the value given for each topic to the respective topic. Use this to trigger the start of your dialog system. \"\"\" self . _start_dialog ( start_signals ) self . _end_dialog () shutdown ( self ) \u00b6 Shutdown dialog system. This will trigger terminate messages to be sent to all registered services to stop their listener loops. Should be called in the end before exiting your program. Blocks until all services sent ACK's confirming they're stopped. Source code in adviser/services/service.py 780 781 782 783 784 785 786 787 788 789 def shutdown ( self ): \"\"\" Shutdown dialog system. This will trigger `terminate` messages to be sent to all registered services to stop their listener loops. Should be called in the end before exiting your program. Blocks until all services sent ACK's confirming they're stopped. \"\"\" self . _stopEvent . set () for terminate_topic in self . _terminate_topics : _send_msg ( self . _control_channel_pub , terminate_topic , True ) _recv_ack ( self . _control_channel_sub , terminate_topic ) stop ( self ) \u00b6 Set stop event (can be queried by services via the terminating() function) Source code in adviser/services/service.py 771 772 773 774 def stop ( self ): \"\"\" Set stop event (can be queried by services via the `terminating()` function) \"\"\" self . _stopEvent . set () pass terminating ( self ) \u00b6 Returns True if the system is stopping, else False Source code in adviser/services/service.py 776 777 778 def terminating ( self ): \"\"\" Returns True if the system is stopping, else False \"\"\" return self . _stopEvent . is_set () RemoteService \u00b6 This is a placeholder to be used in the service list argument when constructing a DialogSystem : * Run the real Service instance on a remote node, give it a *UNIQUE* identifier * call run_standalone() on this instance * Instantiate a remote service on the node about to run the DialogSystem , assign the *SAME* identifier to it * add it to the DialogSystem service list * Now, when calling the constructor of DialogSystem`, you should see messages informing you about the successfull connection, or if the system is still trying to connect, it will block until connected to the remote service. __init__ ( self , identifier ) special \u00b6 Parameters: Name Type Description Default identifier str the UNIQUE identifier to call the remote service instance required Source code in adviser/services/service.py 94 95 96 97 98 99 def __init__ ( self , identifier : str ): \"\"\" Args: identifier (str): the *UNIQUE* identifier to call the remote service instance \"\"\" self . identifier = identifier Service \u00b6 Service base class. Inherit from this class, if you want to publish / subscribe to topics (Don't forget to call the super constructor!) . You may decorate arbitrary functions in the child class with the services.service.PublishSubscribe decorator for this purpose. A Service will only start listening to messages once it is added to a DialogSystem (or calling run_standalone() in the remote case and adding a corresponding RemoteService to the DialogSystem ). __init__ ( self , domain = '' , sub_topic_domains = {}, pub_topic_domains = {}, ds_host_addr = '127.0.0.1' , sub_port = 65533 , pub_port = 65534 , protocol = 'tcp' , debug_logger = None , identifier = None ) special \u00b6 Create a new service instance (call this super constructor from your inheriting classes!) . Parameters: Name Type Description Default domain Union[str, utils.domain.domain.Domain] The domain(-name) of your service (or empty string, if domain-agnostic). If a domain(-name) is set, it will automatically filter out all messages from other domains. If no domain(-name) is set, messages from all domains will be received. '' sub_topic_domains Dict[str, str] change subscribed to topics to listen to a specific domain (e.g. 'erase'/append a domain for a specific topic) {} pub_topic_domains Dict[str, str] change published topics to a specific domain (e.g. 'erase'/append a domain for a specific topic) {} ds_host_addr str IP-address of the parent DialogSystem (default: localhost) '127.0.0.1' sub_port int subscriber port following zmq's XSUB/XPUB pattern 65533 pub_port int publisher port following zmq's XSUB/XPUB pattern 65534 protocol str communication protocol with DialogSystem - has to match! Possible options: tcp , inproc , ipc 'tcp' debug_logger DiasysLogger If not None , all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the DialogSystem even if they are never forwarded (as expected) to your Service . None identifier str Set this to a UNIQUE identifier per service to be run remotely. See RemoteService for more details. None Source code in adviser/services/service.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self , domain : Union [ str , Domain ] = \"\" , sub_topic_domains : Dict [ str , str ] = {}, pub_topic_domains : Dict [ str , str ] = {}, ds_host_addr : str = \"127.0.0.1\" , sub_port : int = 65533 , pub_port : int = 65534 , protocol : str = \"tcp\" , debug_logger : DiasysLogger = None , identifier : str = None ): \"\"\" Create a new service instance *(call this super constructor from your inheriting classes!)*. Args: domain (Union[str, Domain]): The domain(-name) of your service (or empty string, if domain-agnostic). If a domain(-name) is set, it will automatically filter out all messages from other domains. If no domain(-name) is set, messages from all domains will be received. sub_topic_domains (Dict[str, str]): change subscribed to topics to listen to a specific domain (e.g. 'erase'/append a domain for a specific topic) pub_topic_domains (Dict[str, str]): change published topics to a specific domain (e.g. 'erase'/append a domain for a specific topic) ds_host_addr (str): IP-address of the parent `DialogSystem` (default: localhost) sub_port (int): subscriber port following zmq's XSUB/XPUB pattern pub_port (int): publisher port following zmq's XSUB/XPUB pattern protocol (string): communication protocol with `DialogSystem` - has to match! Possible options: `tcp`, `inproc`, `ipc` debug_logger (DiasysLogger): If not `None`, all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the `DialogSystem` even if they are never forwarded (as expected) to your `Service`. identifier (str): Set this to a *UNIQUE* identifier per service to be run remotely. See `RemoteService` for more details. \"\"\" self . is_training = False self . domain = domain # get domain name (gets appended to all sub/pub topics so that different domain topics don't get shared) if domain is not None : self . _domain_name = domain . get_domain_name () if isinstance ( domain , Domain ) else domain else : self . _domain_name = \"\" self . _sub_topic_domains = sub_topic_domains self . _pub_topic_domains = pub_topic_domains # socket information self . _host_addr = ds_host_addr self . _sub_port = sub_port self . _pub_port = pub_port self . _protocol = protocol self . _identifier = identifier self . debug_logger = debug_logger self . _sub_topics = set () self . _pub_topics = set () self . _publish_sockets = dict () self . _internal_start_topics = dict () self . _internal_end_topics = dict () self . _internal_terminate_topics = dict () # NOTE: class name + memory pointer make topic unique (required, e.g. for running mutliple instances of same module!) self . _start_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /START\" self . _end_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /END\" self . _terminate_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /TERMINATE\" self . _train_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /TRAIN\" self . _eval_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /EVAL\" dialog_end ( self ) \u00b6 This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. Source code in adviser/services/service.py 331 332 333 334 def dialog_end ( self ): \"\"\" This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. \"\"\" pass dialog_exit ( self ) \u00b6 This function is called when the dialog system is shutting down. You should overwrite this function to stop your threads and cleanup any open resources. Source code in adviser/services/service.py 336 337 338 339 def dialog_exit ( self ): \"\"\" This function is called when the dialog system is shutting down. You should overwrite this function to stop your threads and cleanup any open resources. \"\"\" pass dialog_start ( self ) \u00b6 This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/service.py 326 327 328 329 def dialog_start ( self ): \"\"\" This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. \"\"\" pass eval ( self ) \u00b6 Sets module to eval mode Source code in adviser/services/service.py 345 346 347 def eval ( self ): \"\"\" Sets module to eval mode \"\"\" self . is_training = False get_all_published_topics ( self ) \u00b6 Returns: Type Description Set of all topics published to by this Service Source code in adviser/services/service.py 391 392 393 394 395 396 def get_all_published_topics ( self ): \"\"\" Returns: Set of all topics published to by this `Service` \"\"\" return copy . deepcopy ( self . _pub_topics ) get_all_subscribed_topics ( self ) \u00b6 Returns: Type Description Set of all topics subscribed to by this Service Source code in adviser/services/service.py 384 385 386 387 388 389 def get_all_subscribed_topics ( self ): \"\"\" Returns: Set of all topics subscribed to by this `Service` \"\"\" return copy . deepcopy ( self . _sub_topics ) run_standalone ( self , host_reg_port = 65535 ) \u00b6 Run this service as a standalone serivce (without a DialogSystem ) on a remote node. Use a RemoteService with corresponding identifier on the DialogSystem node to connect both. Note: this call is blocking! Parameters: Name Type Description Default host_reg_port int The port on the DialogSystem node listening for Service register requests 65535 Source code in adviser/services/service.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def run_standalone ( self , host_reg_port : int = 65535 ): \"\"\" Run this service as a standalone serivce (without a `DialogSystem`) on a remote node. Use a `RemoteService` with *corresponding identifier* on the `DialogSystem` node to connect both. Note: this call is blocking! Args: host_reg_port (int): The port on the `DialogSystem` node listening for `Service` register requests \"\"\" assert self . _identifier is not None , \"running a service on a remote node requires a unique identifier\" print ( \"Waiting for dialog system host...\" ) # send service info to dialog system node self . _init_pubsub () ctx = Context . instance () sync_endpoint = ctx . socket ( zmq . REQ ) sync_endpoint . connect ( f \"tcp:// { self . _host_addr } : { host_reg_port } \" ) data = pickle . dumps (( self . _domain_name , self . _sub_topics , self . _pub_topics , self . _start_topic , self . _end_topic , self . _terminate_topic )) sync_endpoint . send_multipart (( bytes ( f \"REGISTER_ { self . _identifier } \" , encoding = \"ascii\" ), data )) # wait for registration confirmation registered = False while not registered : msg = sync_endpoint . recv () msg = msg . decode ( \"utf-8\" ) if msg . startswith ( \"ACK_REGISTER_\" ): remote_service_identifier = msg [ len ( \"ACK_REGISTER_\" ):] if remote_service_identifier == self . _identifier : self . _register_with_dialogsystem () sync_endpoint . send_multipart ( ( bytes ( f \"CONF_REGISTER_ { self . _identifier } \" , encoding = \"ascii\" ), pickle . dumps ( True ))) registered = True print ( f \"Done\" ) train ( self ) \u00b6 Sets module to training mode Source code in adviser/services/service.py 341 342 343 def train ( self ): \"\"\" Sets module to training mode \"\"\" self . is_training = True PublishSubscribe ( sub_topics = [], pub_topics = [], queued_sub_topics = []) \u00b6 Decorator function for services. To be able to publish / subscribe to / from topics, your class is required to inherit from services.service.Service. Then, decorate any function you like. Your function will be called as soon as: * at least one message is received for each topic in sub_topics (only latest message will be forwarded, others dropped) * at least one message is received for each topic in queued_sub_topics (all messages since the previous function call will be forwarded as a list) Parameters: Name Type Description Default sub_topics(List[str or utils.topics.Topic] The topics you want to get the latest messages from. If multiple messages are received until your function is called, you will only receive the value of the latest message, previously received values will be discarded. required pub_topics(List[str or utils.topics.Topic] The topics you want to publish messages to. required queued_sub_topics(List[str or utils.topics.Topic] The topics you want to get all messages from. If multiple messages are received until your function is called, you will receive all values since the previous function call as a list. required Notes Subscription topic names have to match your function keywords Your function should return a dictionary with the keys matching your publish topics names and the value being any arbitrary python object or primitive type you want to send sub_topics and queued_sub_topics have to be disjoint! If you need timestamps for your messages, specify a 'timestamps' argument in your subscribing function. It will be filled by a dictionary providing timestamps for each received value, indexed by name. Technical notes: * Data will be automatically pickled / unpickled during send / receive to reduce meassage size. However, some python objects are not serializable (e.g. database connections) for good reasons and will throw an error if you try to publish them. * The domain name of your service class will be appended to your publish topics. Subscription topics are prefix-matched, so you will receive all messages from 'topic/suffix' if you subscibe to 'topic'. Source code in adviser/services/service.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 def PublishSubscribe ( sub_topics : List [ str ] = [], pub_topics : List [ str ] = [], queued_sub_topics : List [ str ] = []): \"\"\" Decorator function for services. To be able to publish / subscribe to / from topics, your class is required to inherit from services.service.Service. Then, decorate any function you like. Your function will be called as soon as: * at least one message is received for each topic in sub_topics (only latest message will be forwarded, others dropped) * at least one message is received for each topic in queued_sub_topics (all messages since the previous function call will be forwarded as a list) Args: sub_topics(List[str or utils.topics.Topic]): The topics you want to get the latest messages from. If multiple messages are received until your function is called, you will only receive the value of the latest message, previously received values will be discarded. pub_topics(List[str or utils.topics.Topic]): The topics you want to publish messages to. queued_sub_topics(List[str or utils.topics.Topic]): The topics you want to get all messages from. If multiple messages are received until your function is called, you will receive all values since the previous function call as a list. Notes: * Subscription topic names have to match your function keywords * Your function should return a dictionary with the keys matching your publish topics names and the value being any arbitrary python object or primitive type you want to send * sub_topics and queued_sub_topics have to be disjoint! * If you need timestamps for your messages, specify a 'timestamps' argument in your subscribing function. It will be filled by a dictionary providing timestamps for each received value, indexed by name. Technical notes: * Data will be automatically pickled / unpickled during send / receive to reduce meassage size. However, some python objects are not serializable (e.g. database connections) for good reasons and will throw an error if you try to publish them. * The domain name of your service class will be appended to your publish topics. Subscription topics are prefix-matched, so you will receive all messages from 'topic/suffix' if you subscibe to 'topic'. \"\"\" def wrapper ( func ): def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result # declare function as publish / subscribe functions and attach the respective topics delegate . pubsub = True delegate . sub_topics = sub_topics delegate . queued_sub_topics = queued_sub_topics delegate . pub_topics = pub_topics # check arguments: is subsriber interested in timestamps? delegate . timestamp_enabled = 'timestamps' in inspect . getfullargspec ( func )[ 0 ] return delegate return wrapper simulator special \u00b6 This package contains the handcrafted user simulatod and related services. emotion_simulator \u00b6 EmotionSimulator \u00b6 Class which generates user emotion/engagements. Currently outputs either a user defined or random emotion/engagement level and was designed to test the affective services work correctly. However, in the future it could be extended to be more realistic. __init__ ( self , domain = None , logger = None , random = True , static_emotion =< EmotionType . Neutral : 'neutral' > , static_engagement =< EngagementType . High : 'high' > ) special \u00b6 Source code in adviser/services/simulator/emotion_simulator.py 36 37 38 39 40 41 42 43 44 def __init__ ( self , domain : JSONLookupDomain = None , logger : DiasysLogger = None , random : bool = True , static_emotion : EmotionType = EmotionType . Neutral , static_engagement : EngagementType = EngagementType . High ): Service . __init__ ( self , domain = domain ) self . domain = domain self . logger = logger self . random = random self . engagement = static_engagement self . emotion = static_emotion send_emotion ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/simulator/emotion_simulator.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result goal \u00b6 This module provides the Goal class and related stuff. Constraint \u00b6 __eq__ ( self , other ) special \u00b6 Constraint should be equal if the slot and value is the same. Source code in adviser/services/simulator/goal.py 41 42 43 44 45 46 def __eq__ ( self , other ): \"\"\"Constraint should be equal if the slot and value is the same.\"\"\" if isinstance ( other , Constraint ): return ( self . slot == other . slot and self . value == other . value ) return False __getitem__ ( self , key ) special \u00b6 Source code in adviser/services/simulator/goal.py 48 49 50 51 52 53 54 55 56 def __getitem__ ( self , key ): if not isinstance ( key , int ): raise TypeError if key == 0 : return self . slot elif key == 1 : return self . value else : raise IndexError __hash__ ( self ) special \u00b6 Source code in adviser/services/simulator/goal.py 61 62 def __hash__ ( self ): return hash ( self . slot ) * hash ( self . value ) __init__ ( self , slot , value ) special \u00b6 The class for a constraint as used in the goal. Parameters: Name Type Description Default slot str The slot. required value str The value. required Source code in adviser/services/simulator/goal.py 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , slot , value ): \"\"\" The class for a constraint as used in the goal. Args: slot (str): The slot. value (str): The value. \"\"\" self . slot = slot self . value = value __repr__ ( self ) special \u00b6 Source code in adviser/services/simulator/goal.py 58 59 def __repr__ ( self ): return \"Constraint(slot= {} , value= {} )\" . format ( self . slot , self . value ) Goal \u00b6 __init__ ( self , domain , parameters = None ) special \u00b6 The class representing a goal, therefore containing requests and constraints. Parameters: Name Type Description Default domain JSONLookupDomain The domain for which the goal will be instantiated. It will only work within this domain. required parameters dict The parameters for the goal defined by a key=value mapping: 'MinVenues' (int) allows to set a minimum number of venues which fulfill the constraints of the goal, 'MinConstraints' (int) and 'MaxConstraints' (int) set the minimum and maximum amount of constraints respectively, 'MinRequests' (int) and 'MaxRequests' (int) set the minimum and maximum amount of requests respectively and 'Reachable' (float) allows to specify how many (in percent) of all generated goals are definitely fulfillable (i.e. there exists a venue for the current goal) or not (doesn't have to be fulfillable). Although the parameter 'Reachable' equals 1.0 implicitly states that 'MinVenues' equals 1 or more, the implementation looks different, is more efficient and takes all goals into consideration (since 'Reachable' is a float (percentage of generated goals)). On the other hand, setting 'MinVenues' to any number bigger than 0 forces every goal to be fulfillable. None Source code in adviser/services/simulator/goal.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , domain : JSONLookupDomain , parameters = None ): \"\"\" The class representing a goal, therefore containing requests and constraints. Args: domain (JSONLookupDomain): The domain for which the goal will be instantiated. It will only work within this domain. parameters (dict): The parameters for the goal defined by a key=value mapping: 'MinVenues' (int) allows to set a minimum number of venues which fulfill the constraints of the goal, 'MinConstraints' (int) and 'MaxConstraints' (int) set the minimum and maximum amount of constraints respectively, 'MinRequests' (int) and 'MaxRequests' (int) set the minimum and maximum amount of requests respectively and 'Reachable' (float) allows to specify how many (in percent) of all generated goals are definitely fulfillable (i.e. there exists a venue for the current goal) or not (doesn't have to be fulfillable). Although the parameter 'Reachable' equals 1.0 implicitly states that 'MinVenues' equals 1 or more, the implementation looks different, is more efficient and takes all goals into consideration (since 'Reachable' is a float (percentage of generated goals)). On the other hand, setting 'MinVenues' to any number bigger than 0 forces every goal to be fulfillable. \"\"\" self . domain = domain self . parameters = parameters or {} # cache inform and request slots # make sure to copy the list (shallow is sufficient) self . inf_slots = sorted ( list ( domain . get_informable_slots ())[:]) # make sure that primary key is never a constraint if self . domain . get_primary_key () in self . inf_slots : self . inf_slots . remove ( self . domain . get_primary_key ()) # TODO sometimes ask for specific primary key with very small probability (instead of any other constraints?) # pylint: disable=line-too-long self . inf_slot_values = {} for slot in self . inf_slots : self . inf_slot_values [ slot ] = sorted ( domain . get_possible_values ( slot )[:]) self . req_slots = sorted ( domain . get_requestable_slots ()[:]) # self.req_slots_without_informables = sorted(list( # set(self.req_slots).difference(self.inf_slots))) # make sure that primary key is never a request as it is added anyway if self . domain . get_primary_key () in self . req_slots : self . req_slots . remove ( self . domain . get_primary_key ()) self . constraints = [] self . requests = {} self . excluded_inf_slot_values = {} self . missing_informs = [] __repr__ ( self ) special \u00b6 Source code in adviser/services/simulator/goal.py 257 258 def __repr__ ( self ): return \"Goal(constraints= {} , requests= {} )\" . format ( self . constraints , self . requests ) fulfill_request ( self , slot , value ) \u00b6 Fulfills a request, i.e. sets value for request slot . Parameters: Name Type Description Default slot str The request slot which will be filled. required value str The value the request slot will be filled with. required Source code in adviser/services/simulator/goal.py 280 281 282 283 284 285 286 287 288 289 290 def fulfill_request ( self , slot , value ): \"\"\" Fulfills a request, i.e. sets ``value`` for request ``slot``. Args: slot (str): The request slot which will be filled. value (str): The value the request slot will be filled with. \"\"\" if slot in self . requests : self . requests [ slot ] = value get_constraint ( self , slot ) \u00b6 Gets the value for a given constraint slot . Parameters: Name Type Description Default slot str The constraint slot which will be looked up. required Returns: Type Description bool The constraint value . Source code in adviser/services/simulator/goal.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def get_constraint ( self , slot ): \"\"\" Gets the value for a given constraint ``slot``. Args: slot (str): The constraint ``slot`` which will be looked up. Returns: bool: The constraint ``value``. \"\"\" for _constraint in self . constraints : if _constraint . slot == slot : return _constraint . value return 'dontcare' init ( self , random_goal = True , constraints = None , requests = None ) \u00b6 Initializes a goal randomly OR using the given constraints and requests. Parameters: Name Type Description Default random_goal bool If True, a goal will be drawn randomly from available constraints and requests (considering the parameters given in the constructor, if any). However if constraints and requests are given and both don't equal None, this parameter is considered as False. If False, the given constraints and requests are used. True constraints List[Constraint] The constraints which will be used for the goal. None requests Dict[str, Union[None,str]] The requests which will be used for the goal. None Source code in adviser/services/simulator/goal.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def init ( self , random_goal = True , constraints = None , requests = None ) -> None : \"\"\" Initializes a goal randomly OR using the given constraints and requests. Args: random_goal (bool): If True, a goal will be drawn randomly from available constraints and requests (considering the parameters given in the constructor, if any). However if constraints and requests are given and both don't equal None, this parameter is considered as False. If False, the given constraints and requests are used. constraints (List[Constraint]): The constraints which will be used for the goal. requests (Dict[str, Union[None,str]]): The requests which will be used for the goal. \"\"\" # reset goal self . constraints = [] self . requests = {} self . excluded_inf_slot_values = { key : set () for key in self . inf_slot_values } # TODO implement possibility to pass either constraints or requests as a parameter if random_goal and constraints is None and requests is None : self . _init_random_goal () else : self . _init_from_parameters ( constraints , requests ) # make sure that primary key is always requested self . requests [ self . domain . get_primary_key ()] = None self . missing_informs = [ UserAct ( act_type = UserActionType . Inform , slot = _constraint . slot , value = _constraint . value ) for _constraint in self . constraints ] is_fulfilled ( self ) \u00b6 Checks whether all requests have been fulfilled. Returns: Type Description bool True if all requests have been fulfilled, False otherwise. .. note:: Does not check whether the venue (issued by the system) fulfills the constraints since it's the system's task to give an appropriate venue by requesting the user's constraints. Source code in adviser/services/simulator/goal.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def is_fulfilled ( self ): \"\"\" Checks whether all requests have been fulfilled. Returns: bool: ``True`` if all requests have been fulfilled, ``False`` otherwise. .. note:: Does not check whether the venue (issued by the system) fulfills the constraints since it's the system's task to give an appropriate venue by requesting the user's constraints. \"\"\" for slot , value in self . requests . items (): assert slot != self . domain . get_primary_key () or value != 'none' # TODO remove later if value is None : return False return True is_inconsistent_constraint ( self , constraint ) \u00b6 Checks whether the given constraint is consistent with the goal. A constraint is also consistent if it's value is 'dontcare' in the current goal. Parameters: Name Type Description Default constraint Constraint The constraint which will be checked for consistency. required Returns: Type Description bool True if values match or value in goal is 'dontcare', False otherwise. Source code in adviser/services/simulator/goal.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def is_inconsistent_constraint ( self , constraint ): \"\"\" Checks whether the given constraint is consistent with the goal. A constraint is also consistent if it's value is 'dontcare' in the current goal. Args: constraint (Constraint): The constraint which will be checked for consistency. Returns: bool: True if values match or value in goal is 'dontcare', False otherwise. \"\"\" for _constraint in self . constraints : if _constraint . slot == constraint . slot and ( _constraint . value != constraint . value \\ and _constraint . value != 'dontcare' ): return True return False is_inconsistent_constraint_strict ( self , constraint ) \u00b6 Checks whether the given constraint is strictly consistent with the goal, whereby 'dontcare' is treated as a different value (no match). Parameters: Name Type Description Default constraint Constraint The constraint which will be checked for consistency. required Returns: Type Description bool True if values match, False otherwise. See Also [ is_inconsistent_constraint ][adviser.services.simulator.goal.Goal.is_inconsistent_constraint] Source code in adviser/services/simulator/goal.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def is_inconsistent_constraint_strict ( self , constraint ): \"\"\" Checks whether the given constraint is strictly consistent with the goal, whereby 'dontcare' is treated as a different value (no match). Args: constraint (Constraint): The constraint which will be checked for consistency. Returns: bool: True if values match, False otherwise. !!! seealso \"See Also\" [`is_inconsistent_constraint`][adviser.services.simulator.goal.Goal.is_inconsistent_constraint] \"\"\" for _constraint in self . constraints : if _constraint . slot == constraint . slot and _constraint . value == constraint . value : return False # here there are only two possibilities: the constraint is implicitly 'dontcare' because # it is not explicitly listed and the given constraint is either 1) 'dontcare' or 2) not return constraint . value != 'dontcare' reset ( self ) \u00b6 Resets all requests of the goal. Source code in adviser/services/simulator/goal.py 250 251 252 253 def reset ( self ): \"\"\"Resets all requests of the goal.\"\"\" # reset goal -> empty all requests self . requests = dict . fromkeys ( self . requests ) update_constraint ( self , slot , value ) \u00b6 Update a given constraint slot with value . Parameters: Name Type Description Default slot str The constraint slot which will be updated. required value str The value with which the constraint will be updated. required Returns: Type Description bool True if update was successful, i.e. the constraint slot is included in the goal, False otherwise. Source code in adviser/services/simulator/goal.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 def update_constraint ( self , slot , value ): \"\"\" Update a given constraint ``slot`` with ``value``. Args: slot (str): The constraint *slot* which will be updated. value (str): The *value* with which the constraint will be updated. Returns: bool: ``True`` if update was successful, i.e. the constraint ``slot`` is included in the goal, ``False`` otherwise. \"\"\" for _constraint in self . constraints : if _constraint . slot == slot : _constraint . value = value return True return False simulator \u00b6 This module provides the agenda-based user model for the handcrafted simulator. Agenda \u00b6 A stack-like object representing an agenda. Actions can be pushed on and popped off the agenda. __bool__ ( self ) special \u00b6 Source code in adviser/services/simulator/simulator.py 681 682 def __bool__ ( self ): return bool ( self . stack ) __contains__ ( self , value ) special \u00b6 Source code in adviser/services/simulator/simulator.py 675 676 def __contains__ ( self , value ): return value in self . stack __init__ ( self ) special \u00b6 Source code in adviser/services/simulator/simulator.py 669 670 def __init__ ( self ): self . stack = [] __iter__ ( self ) special \u00b6 Source code in adviser/services/simulator/simulator.py 672 673 def __iter__ ( self ): return iter ( self . stack ) __len__ ( self ) special \u00b6 Source code in adviser/services/simulator/simulator.py 678 679 def __len__ ( self ): return len ( self . stack ) __repr__ ( self ) special \u00b6 Source code in adviser/services/simulator/simulator.py 684 685 def __repr__ ( self ): return repr ( self . stack ) __str__ ( self ) special \u00b6 Source code in adviser/services/simulator/simulator.py 687 688 def __str__ ( self ): return str ( self . stack ) clean ( self , goal ) \u00b6 Cleans the agenda, i.e. makes sure that actions are consistent with goal and in the correct order. Parameters: Name Type Description Default goal Goal The goal which is needed to determine the consistent actions. required Source code in adviser/services/simulator/simulator.py 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 def clean ( self , goal : Goal ): \"\"\"Cleans the agenda, i.e. makes sure that actions are consistent with goal and in the correct order. Args: goal (Goal): The goal which is needed to determine the consistent actions. \"\"\" cleaned_stack = [] # reverse order since most recent actions are on top of agenda for action in self . stack [:: - 1 ]: if action not in cleaned_stack : # NOTE sufficient if there is only one slot per (request) action # remove accomplished requests if ( action . type is not UserActionType . Request or ( action . slot in goal . requests and goal . requests [ action . slot ] is None ) or action . slot not in goal . requests ): # make sure to remove \"old\" inform actions if action . type is UserActionType . Inform : if not goal . is_inconsistent_constraint ( Constraint ( action . slot , action . value )): cleaned_stack . insert ( 0 , action ) else : cleaned_stack . insert ( 0 , action ) self . stack = cleaned_stack clear ( self ) \u00b6 Empties the agenda. Source code in adviser/services/simulator/simulator.py 766 767 768 def clear ( self ): \"\"\"Empties the agenda.\"\"\" self . stack . clear () contains_action_of_type ( self , act_type , consider_dontcare = True ) \u00b6 Checks whether agenda contains actions of a specific type. Parameters: Name Type Description Default act_type UserActionType The action type (intent) for which the agenda will be checked. required consider_dontcare bool If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. True Returns: Type Description (bool) True if agenda contains act_type , False otherwise. Source code in adviser/services/simulator/simulator.py 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 def contains_action_of_type ( self , act_type : UserActionType , consider_dontcare = True ): \"\"\"Checks whether agenda contains actions of a specific type. Args: act_type (UserActionType): The action type (intent) for which the agenda will be checked. consider_dontcare (bool): If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. Returns: (bool): True if agenda contains *act_type*, False otherwise. \"\"\" for _action in self . stack : if not consider_dontcare and _action . value == 'dontcare' : continue if _action . type == act_type : return True return False fill_with_constraints ( self , goal ) \u00b6 Adds all inform actions to the agenda necessary to fulfill the goal . Generally there is no need to add all constraints from the goal to the agenda apart from the initialisation. Parameters: Name Type Description Default goal Goal The current goal of the (simulated) user for which actions will be pushed to the agenda. required Source code in adviser/services/simulator/simulator.py 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 def fill_with_constraints ( self , goal : Goal ): \"\"\" Adds all inform actions to the agenda necessary to fulfill the *goal*. Generally there is no need to add all constraints from the goal to the agenda apart from the initialisation. Args: goal (Goal): The current goal of the (simulated) user for which actions will be pushed to the agenda. \"\"\" # add informs from goal for constraint in goal . constraints : self . stack . append ( UserAct ( act_type = UserActionType . Inform , slot = constraint . slot , value = constraint . value , score = 1.0 )) fill_with_requests ( self , goal , exclude_name = True ) \u00b6 Adds all request actions to the agenda necessary to fulfill the goal . Parameters: Name Type Description Default goal Goal The current goal of the (simulated) user for which actions will be pushed to the agenda. required exclude_name bool whehter or not to include an action to request an entities name. True Source code in adviser/services/simulator/simulator.py 838 839 840 841 842 843 844 845 846 847 848 849 850 851 def fill_with_requests ( self , goal : Goal , exclude_name : bool = True ): \"\"\"Adds all request actions to the agenda necessary to fulfill the *goal*. Args: goal (Goal): The current goal of the (simulated) user for which actions will be pushed to the agenda. exclude_name (bool): whehter or not to include an action to request an entities name. \"\"\" # add requests and make sure to add the name at the end (i.e. ask first for name) for key , value in goal . requests . items (): if (( key != 'name' and exclude_name ) or not exclude_name ) and value is None : self . stack . append ( UserAct ( act_type = UserActionType . Request , slot = key , value = value , score = 1.0 )) get_actions ( self , num_actions ) \u00b6 Retrieves num_actions actions from the agenda. Parameters: Name Type Description Default num_actions int Amount of actions which will be retrieved from the agenda. required Returns: Type Description (List[UserAct]) list of num_actions user actions. Source code in adviser/services/simulator/simulator.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 def get_actions ( self , num_actions : int ): \"\"\"Retrieves *num_actions* actions from the agenda. Args: num_actions (int): Amount of actions which will be retrieved from the agenda. Returns: (List[UserAct]): list of *num_actions* user actions. \"\"\" if num_actions < 0 or num_actions > len ( self . stack ): num_actions = len ( self . stack ) return [ self . stack . pop () for _ in range ( 0 , num_actions )] get_actions_of_type ( self , act_type =< enum 'UserActionType' > , consider_dontcare = True ) \u00b6 Get actions of a specific type from the agenda. Parameters: Name Type Description Default act_type UserActionType The action type (intent) for which the agenda will be checked. <enum 'UserActionType'> consider_dontcare bool If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. True Returns: Type Description (Iterable[UserAct]) A list of user actions of the given type/intent. Source code in adviser/services/simulator/simulator.py 798 799 800 801 802 803 804 805 806 807 808 809 810 811 def get_actions_of_type ( self , act_type = UserActionType , consider_dontcare = True ): \"\"\"Get actions of a specific type from the agenda. Args: act_type (UserActionType): The action type (intent) for which the agenda will be checked. consider_dontcare (bool): If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. Returns: (Iterable[UserAct]): A list of user actions of the given type/intent. \"\"\" return filter ( lambda x : x . type == act_type and ( consider_dontcare or x . value != 'dontcare' ), self . stack ) init ( self , goal ) \u00b6 Initializes the agenda given a goal. For this purpose, inform actions for constraints in the goal and request actions for requests in the goal are added such that the informs are handled first followed by the requests. Parameters: Name Type Description Default goal Goal The goal for which the agenda will be initialized. required Source code in adviser/services/simulator/simulator.py 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 def init ( self , goal ): \"\"\" Initializes the agenda given a goal. For this purpose, inform actions for constraints in the goal and request actions for requests in the goal are added such that the informs are handled first followed by the requests. Args: goal (Goal): The goal for which the agenda will be initialized. \"\"\" self . stack . clear () # populate agenda according to goal # NOTE don't push bye action here since bye action could be poppped with another (missing) # request, but user should not end dialog before having the goal fulfilled # NOTE do not add requests to agenda since system can't handle inform and request action in # same turn currently! # self.fill_with_requests(goal) self . fill_with_constraints ( goal ) is_empty ( self ) \u00b6 Checks whether the agenda is empty. Returns: Type Description (bool) True if agenda is empty, False otherwise. Source code in adviser/services/simulator/simulator.py 770 771 772 773 774 775 776 777 def is_empty ( self ): \"\"\"Checks whether the agenda is empty. Returns: (bool): True if agenda is empty, False otherwise. \"\"\" return len ( self . stack ) == 0 push ( self , item ) \u00b6 Pushes item onto the agenda. Parameters: Name Type Description Default item The goal for which the agenda will be initialized. required Source code in adviser/services/simulator/simulator.py 712 713 714 715 716 717 718 719 720 721 722 def push ( self , item ): \"\"\"Pushes *item* onto the agenda. Args: item: The goal for which the agenda will be initialized. \"\"\" if isinstance ( item , list ): self . stack += item else : self . stack . append ( item ) remove_actions ( self , act_type , slot , value = None ) \u00b6 Removes actions of a specific type, slot and optionally value from the agenda. All arguments (value only if given) have to match in conjunction. Parameters: Name Type Description Default act_type UserActionType The action type (intent) which will be removed from the agenda. required slot str The action type (intent) which will be removed from the agenda. required value str The action type (intent) which will be removed from the agenda. None Source code in adviser/services/simulator/simulator.py 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 def remove_actions ( self , act_type : UserActionType , slot : str , value : str = None ): \"\"\"Removes actions of a specific type, slot and optionally value from the agenda. All arguments (value only if given) have to match in conjunction. Args: act_type (UserActionType): The action type (intent) which will be removed from the agenda. slot (str): The action type (intent) which will be removed from the agenda. value (str): The action type (intent) which will be removed from the agenda. \"\"\" if value is None : self . stack = list ( filter ( lambda x : x . type != act_type or x . slot != slot , self . stack )) else : self . stack = list ( filter ( lambda x : x . type != act_type or x . slot != slot or x . value != value , self . stack )) remove_actions_of_type ( self , act_type ) \u00b6 Removes actions of a specific type from the agenda. Parameters: Name Type Description Default act_type UserActionType The action type (intent) which will be removed from the agenda. required Source code in adviser/services/simulator/simulator.py 813 814 815 816 817 818 819 820 def remove_actions_of_type ( self , act_type : UserActionType ): \"\"\"Removes actions of a specific type from the agenda. Args: act_type (UserActionType): The action type (intent) which will be removed from the agenda. \"\"\" self . stack = list ( filter ( lambda x : x . type != act_type , self . stack )) HandcraftedUserSimulator \u00b6 The class for a handcrafted (agenda-based) user simulator. !!! args domain (Domain): The domain for which the user simulator will be instantiated. It will use this domain to generate the goals. __init__ ( self , domain , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Source code in adviser/services/simulator/simulator.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , domain : Domain , logger : DiasysLogger = DiasysLogger ()): super ( HandcraftedUserSimulator , self ) . __init__ ( domain ) # possible system actions self . receive_options = { SysActionType . Welcome : self . _receive_welcome , SysActionType . InformByName : self . _receive_informbyname , SysActionType . InformByAlternatives : self . _receive_informbyalternatives , SysActionType . Request : self . _receive_request , SysActionType . Confirm : self . _receive_confirm , SysActionType . Select : self . _receive_select , SysActionType . RequestMore : self . _receive_requestmore , SysActionType . Bad : self . _receive_bad , SysActionType . ConfirmRequest : self . _receive_confirmrequest } # parse config file self . logger = logger self . config = configparser . ConfigParser ( inline_comment_prefixes = ( '#' , ';' )) self . config . optionxform = str self . config . read ( os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), 'usermodel.cfg' )) self . parameters = {} # goal self . parameters [ 'goal' ] = {} for key in self . config [ \"goal\" ]: val = self . config . get ( \"goal\" , key ) self . parameters [ 'goal' ][ key ] = float ( val ) # usermodel self . parameters [ 'usermodel' ] = {} for key in self . config [ \"usermodel\" ]: val = self . config . get ( \"usermodel\" , key ) if key in [ 'patience' ]: # patience will be sampled on begin of each dialog self . parameters [ 'usermodel' ][ key ] = [ int ( x ) for x in ( val . replace ( ' ' , '' ) . strip ( '[]' ) . split ( ',' ))] else : if val . startswith ( \"[\" ) and val . endswith ( \"]\" ): # value is a list to sample the probability from self . parameters [ 'usermodel' ][ key ] = common . numpy . random . uniform ( * [ float ( x ) for x in val . replace ( ' ' , '' ) . strip ( '[]' ) . split ( ',' )]) else : # value is the probability self . parameters [ 'usermodel' ][ key ] = float ( val ) # member declarations self . turn = 0 self . domain = domain self . dialog_patience = None self . patience = None self . last_user_actions = None self . last_system_action = None self . excluded_venues = [] # member definitions self . goal = Goal ( domain , self . parameters [ 'goal' ]) self . agenda = Agenda () self . num_actions_next_turn = - 1 dialog_start ( self ) \u00b6 Resets the user model at the beginning of a dialog, e.g. draws a new goal and populates the agenda according to the goal. Source code in adviser/services/simulator/simulator.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def dialog_start ( self ): \"\"\"Resets the user model at the beginning of a dialog, e.g. draws a new goal and populates the agenda according to the goal.\"\"\" # self.goal = Goal(self.domain, self.parameters['goal']) self . goal . init () self . agenda . init ( self . goal ) if self . logger : self . logger . dialog_turn ( \"New goal has constraints {} and requests {} .\" . format ( self . goal . constraints , self . goal . requests )) self . logger . dialog_turn ( \"New agenda initialized: {} \" . format ( self . agenda )) # add hello action with some probability if common . random . random () < self . parameters [ 'usermodel' ][ 'Greeting' ]: self . agenda . push ( UserAct ( act_type = UserActionType . Hello , score = 1.0 )) # needed for possibility to reset patience if len ( self . parameters [ 'usermodel' ][ 'patience' ]) == 1 : self . dialog_patience = self . parameters [ 'usermodel' ][ 'patience' ][ 0 ] else : self . dialog_patience = common . random . randint ( * self . parameters [ 'usermodel' ][ 'patience' ]) self . patience = self . dialog_patience self . last_user_actions = None self . last_system_action = None self . excluded_venues = [] self . turn = 0 receive ( self , sys_act ) \u00b6 This function makes sure that the agenda reflects all changes needed for the received system action. Parameters: Name Type Description Default sys_act SysAct The action the system took required Source code in adviser/services/simulator/simulator.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def receive ( self , sys_act : SysAct ): \"\"\" This function makes sure that the agenda reflects all changes needed for the received system action. Args: sys_act (SysAct): The action the system took \"\"\" if self . last_system_action is not None : # check whether system action is the same as before if sys_act == self . last_system_action : self . patience -= 1 elif self . parameters [ 'usermodel' ][ 'resetPatience' ]: self . patience = self . dialog_patience self . last_system_action = sys_act if self . patience == 0 : self . logger . dialog_turn ( \"User patience run out, ending dialog.\" ) self . agenda . clear () self . _finish_dialog ( ungrateful = True ) else : ignored_requests , ignored_requests_alt = self . _check_system_ignored_request ( self . last_user_actions , sys_act ) # first stage: push operations on top of agenda if sys_act . type in self . receive_options : self . receive_options [ sys_act . type ]( sys_act ) # handle missing requests if ignored_requests : # repeat unanswered requests from user from last turn self . agenda . push ( ignored_requests ) if ignored_requests_alt : self . agenda . push ( ignored_requests_alt ) # make sure to pick only the requestalt actions (should be 1) self . num_actions_next_turn = len ( ignored_requests_alt ) # make sure that old request actions verifying an offer are removed self . agenda . remove_actions_of_type ( act_type = UserActionType . Request ) # second stage: clean agenda self . agenda . clean ( self . goal ) # agenda might be empty -> add requests again if self . agenda . is_empty (): if self . goal . is_fulfilled (): self . _finish_dialog () else : self . agenda . fill_with_requests ( self . goal , exclude_name = False ) else : self . logger . error ( \"System Action Type is {} , but I don't know how to handle it!\" . format ( sys_act . type )) respond ( self ) \u00b6 Gets n actions from the agenda, where n is drawn depending on the agenda or a pdf. Source code in adviser/services/simulator/simulator.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def respond ( self ): \"\"\" Gets n actions from the agenda, where n is drawn depending on the agenda or a pdf. \"\"\" # get some actions from the agenda assert len ( self . agenda ) > 0 , \"Agenda is empty, this must not happen at this point!\" if self . num_actions_next_turn > 0 : # use and reset self.num_actions_next_turn if set num_actions = self . num_actions_next_turn self . num_actions_next_turn = - 1 elif self . agenda . stack [ - 1 ] . type == UserActionType . Bye : # pop all actions from agenda since agenda can only contain thanks (optional) and # bye action num_actions = - 1 else : # draw amount of actions num_actions = min ( len ( self . agenda ), common . numpy . random . choice ( [ 1 , 2 , 3 ], p = [ . 6 , . 3 , . 1 ])) # hardcoded pdf # get actions from agenda user_actions = self . agenda . get_actions ( num_actions ) # copy needed for repeat action since they might be changed in other modules self . last_user_actions = copy . deepcopy ( user_actions ) for action in user_actions : if action . type == UserActionType . Inform : _constraint = Constraint ( action . slot , action . value ) # if _constraint in self.goal.constraints: if action in self . goal . missing_informs : self . goal . missing_informs . remove ( action ) return user_actions user_turn ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/simulator/simulator.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result stats special \u00b6 evaluation \u00b6 ObjectiveReachedEvaluator \u00b6 Evaluate single turns and complete dialog. This class assigns a negative reward to each turn . In case the user ' s goal could be satisfied ( meaning a matching database entry was found ), a large final reward is returned . Only needed when training against a simulator . __init__ ( self , domain , turn_reward =- 1 , success_reward = 20 , logger =< DiasysLogger adviser ( NOTSET ) > ) special \u00b6 Source code in adviser/services/stats/evaluation.py 38 39 40 41 42 43 44 def __init__ ( self , domain : Domain , turn_reward =- 1 , success_reward = 20 , logger : DiasysLogger = DiasysLogger ()): assert turn_reward <= 0.0 , 'the turn reward should be negative' self . domain = domain self . turn_reward = turn_reward self . success_reward = success_reward self . logger = logger get_final_reward ( self , sim_goal , logging = True ) \u00b6 Check whether the user's goal was completed. Parameters: Name Type Description Default sim_goal Goal the simulation's goal required logging bool whether or not the evaluation results should be logged True Returns: Type Description float Reward - the final reward (0 (unsuccessful) or 20 (successful)) bool: Success Source code in adviser/services/stats/evaluation.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_final_reward ( self , sim_goal : Goal , logging = True ): \"\"\" Check whether the user's goal was completed. Args: sim_goal (Goal): the simulation's goal logging (bool): whether or not the evaluation results should be logged Returns: float: Reward - the final reward (0 (unsuccessful) or 20 (successful)) bool: Success \"\"\" requests = sim_goal . requests constraints = sim_goal . constraints # list of constraints # self.logger.dialog_turn(\"User Goal > \" + str(sim_goal.constraints)) if None in requests . values () or requests [ 'name' ] == 'none' : if logging : self . logger . dialog_turn ( \"Fail with user requests \\n {} \" . format ( requests )) return 0.0 , False # TODO think about this more? if goals not satisfiable, # should system take the blame? not fair # print(requests['name']) db_matches = self . domain . find_info_about_entity ( entity_id = requests [ 'name' ], requested_slots = [ constraint . slot for constraint in constraints ]) if db_matches : match = db_matches [ 0 ] for const in constraints : if const . value != match [ const . slot ] and const . value != 'dontcare' : if logging : self . logger . dialog_turn ( \"Fail with user requests \\n {} \" . format ( requests )) return 0.0 , False if logging : self . logger . dialog_turn ( \"Success with user requests \\n {} \" . format ( requests )) return 20.0 , True if logging : self . logger . dialog_turn ( \"Fail with user requests \\n {} \" . format ( requests )) return 0.0 , False get_turn_reward ( self ) \u00b6 Get the reward for one turn Returns: Type Description (int) the reward for the given turn Source code in adviser/services/stats/evaluation.py 46 47 48 49 50 51 52 53 def get_turn_reward ( self ): \"\"\" Get the reward for one turn Returns: (int): the reward for the given turn \"\"\" return self . turn_reward PolicyEvaluator \u00b6 Policy evaluation module Plug this module into the dialog graph (somewhere after the policy), and policy metrics like success rate and reward will be recorded. __init__ ( self , domain , subgraph = None , use_tensorboard = False , experiment_name = '' , turn_reward =- 1 , success_reward = 20 , logger =< DiasysLogger adviser ( NOTSET ) > , summary_writer = None ) special \u00b6 Keyword Arguments: use_tensorboard {bool} -- [If true, metrics will be written to tensorboard in a runs directory] (default: {False}) experiment_name {str} -- [Name suffix for the log files] (default: {''}) turn_reward {float} -- [Reward for one turn - usually negative to penalize dialog length] (default: {-1}) success_reward {float} -- [Reward of the final transition if the dialog goal was reached] (default: {20}) Source code in adviser/services/stats/evaluation.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , domain : Domain , subgraph : dict = None , use_tensorboard = False , experiment_name : str = '' , turn_reward =- 1 , success_reward = 20 , logger : DiasysLogger = DiasysLogger (), summary_writer = None ): \"\"\" Keyword Arguments: use_tensorboard {bool} -- [If true, metrics will be written to tensorboard in a *runs* directory] (default: {False}) experiment_name {str} -- [Name suffix for the log files] (default: {''}) turn_reward {float} -- [Reward for one turn - usually negative to penalize dialog length] (default: {-1}) success_reward {float} -- [Reward of the final transition if the dialog goal was reached] (default: {20}) \"\"\" super ( PolicyEvaluator , self ) . __init__ ( domain ) self . logger = logger self . epoch = 0 self . evaluator = ObjectiveReachedEvaluator ( domain , turn_reward = turn_reward , success_reward = success_reward , logger = logger ) self . writer = summary_writer self . total_train_dialogs = 0 self . total_eval_dialogs = 0 self . epoch_train_dialogs = 0 self . epoch_eval_dialogs = 0 self . train_rewards = [] self . eval_rewards = [] self . train_success = [] self . eval_success = [] self . train_turns = [] self . eval_turns = [] self . is_training = False dialog_start ( self , dialog_start = False ) \u00b6 Clears the state of the evaluator in preparation to start a new dialog Source code in adviser/services/stats/evaluation.py 159 160 161 162 163 164 def dialog_start ( self , dialog_start = False ): \"\"\" Clears the state of the evaluator in preparation to start a new dialog \"\"\" self . dialog_reward = 0.0 self . dialog_turns = 0 end_dialog ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/stats/evaluation.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result end_epoch ( self ) \u00b6 Handles calculating statistics at the end of an epoch Source code in adviser/services/stats/evaluation.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def end_epoch ( self ): \"\"\" Handles calculating statistics at the end of an epoch \"\"\" if self . logger : if self . epoch_train_dialogs > 0 : self . logger . result ( \" ### Train ###\" ) self . logger . result ( \"# Num Dialogs \" + str ( self . epoch_train_dialogs )) self . logger . result ( \"# Avg Turns \" + str ( sum ( self . train_turns ) / self . epoch_train_dialogs )) self . logger . result ( \"# Avg Success \" + str ( sum ( self . train_success ) / self . epoch_train_dialogs )) self . logger . result ( \"# Avg Reward \" + str ( sum ( self . train_rewards ) / self . epoch_train_dialogs )) if self . epoch_eval_dialogs > 0 : self . logger . result ( \" ### Eval ###\" ) self . logger . result ( \"# Num Dialogs \" + str ( self . epoch_eval_dialogs )) self . logger . result ( \"# Avg Turns \" + str ( sum ( self . eval_turns ) / self . epoch_eval_dialogs )) self . logger . result ( \"# Avg Success \" + str ( sum ( self . eval_success ) / self . epoch_eval_dialogs )) self . logger . result ( \"# Avg Reward \" + str ( sum ( self . eval_rewards ) / self . epoch_eval_dialogs )) if self . is_training : return { 'num_dialogs' : self . epoch_train_dialogs , 'turns' : sum ( self . train_turns ) / self . epoch_train_dialogs , 'success' : float ( sum ( self . train_success )) / self . epoch_train_dialogs , 'reward' : float ( sum ( self . eval_rewards )) / self . epoch_train_dialogs } else : return { 'num_dialogs' : self . epoch_eval_dialogs , 'turns' : sum ( self . eval_turns ) / self . epoch_eval_dialogs , 'success' : float ( sum ( self . eval_success )) / self . epoch_eval_dialogs , 'reward' : float ( sum ( self . eval_rewards )) / self . epoch_eval_dialogs } eval ( self ) \u00b6 sets teh evaluator in eval mode Source code in adviser/services/stats/evaluation.py 172 173 174 175 176 def eval ( self ): \"\"\" sets teh evaluator in eval mode \"\"\" self . is_training = False evaluate_turn ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/stats/evaluation.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result start_epoch ( self ) \u00b6 Handles resetting variables between epochs Source code in adviser/services/stats/evaluation.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def start_epoch ( self ): \"\"\" Handles resetting variables between epochs \"\"\" # global statistics self . epoch_train_dialogs = 0 self . epoch_eval_dialogs = 0 self . train_rewards = [] self . eval_rewards = [] self . train_success = [] self . eval_success = [] self . train_turns = [] self . eval_turns = [] self . epoch += 1 self . logger . info ( \"### \\n ### EPOCH\" + str ( self . epoch ) + \" ### \\n ###\" ) train ( self ) \u00b6 sets the evaluator in train mode Source code in adviser/services/stats/evaluation.py 166 167 168 169 170 def train ( self ): \"\"\" sets the evaluator in train mode \"\"\" self . is_training = True ust special \u00b6 ust \u00b6 HandcraftedUST \u00b6 A rule-based approach on user state tracking. Currently very minimalist __init__ ( self , domain = None , logger = None ) special \u00b6 Source code in adviser/services/ust/ust.py 30 31 32 33 def __init__ ( self , domain = None , logger = None ): Service . __init__ ( self , domain = domain ) self . logger = logger self . us = UserState () dialog_start ( self ) \u00b6 Resets the user state so it is ready for a new dialog Source code in adviser/services/ust/ust.py 56 57 58 59 60 61 def dialog_start ( self ): \"\"\" Resets the user state so it is ready for a new dialog \"\"\" # initialize belief state self . us = UserState () update_emotion ( self , * args , ** kwargs ) \u00b6 Source code in adviser/services/ust/ust.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"Services"},{"location":"api/services/#services","text":"","title":"Services"},{"location":"api/services/#adviser.services","text":"","title":"adviser.services"},{"location":"api/services/#adviser.services.backchannel","text":"","title":"backchannel"},{"location":"api/services/#adviser.services.backchannel.acoustic_backchanneller","text":"","title":"acoustic_backchanneller"},{"location":"api/services/#adviser.services.backchannel.acoustic_backchanneller.AcousticBackchanneller","text":"AcousticBackchanneller predicts a backchannel given the last user utterance. The model can predict: No backchannel (0), Assessment (1), Continuer (2) The backchannel realization is added in the NLG module.","title":"AcousticBackchanneller"},{"location":"api/services/#adviser.services.backchannel.acoustic_backchanneller.AcousticBackchanneller.__init__","text":"Source code in adviser/services/backchannel/acoustic_backchanneller.py 42 43 44 45 46 def __init__ ( self ): Service . __init__ ( self ) self . speech_in_dir = os . path . dirname ( os . path . abspath ( __file__ )) + '/' self . trained_model_path = os . path . join ( 'resources' , 'models' , 'backchannel' ) + '/pytorch_acoustic_backchanneller.pt' self . load_model ()","title":"__init__()"},{"location":"api/services/#adviser.services.backchannel.acoustic_backchanneller.AcousticBackchanneller.backchannel_prediction","text":"Source code in adviser/services/backchannel/acoustic_backchanneller.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"backchannel_prediction()"},{"location":"api/services/#adviser.services.backchannel.acoustic_backchanneller.AcousticBackchanneller.load_model","text":"The PyTorch Backchannel model is instantiated and the pretrained parameters are loaded. Source code in adviser/services/backchannel/acoustic_backchanneller.py 48 49 50 51 52 53 54 55 56 def load_model ( self ): \"\"\" The PyTorch Backchannel model is instantiated and the pretrained parameters are loaded. Returns: \"\"\" self . model = PytorchAcousticBackchanneler () self . model . load_state_dict ( torch . load ( self . trained_model_path )) self . model . eval ()","title":"load_model()"},{"location":"api/services/#adviser.services.backchannel.acoustic_backchanneller.AcousticBackchanneller.split_input_data","text":"Preprocess and segmentation of MFCC features of the user's speech. Segmentation is done every 150ms without overlapping. Parameters: Name Type Description Default mfcc_features numpy.array mffcc features of users speech required Returns: Type Description new_data (list) segmented mfcc features Source code in adviser/services/backchannel/acoustic_backchanneller.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def split_input_data ( self , mfcc_features ): \"\"\" Preprocess and segmentation of MFCC features of the user's speech. Segmentation is done every 150ms without overlapping. Args: mfcc_features (numpy.array): mffcc features of users speech Returns: new_data (list): segmented mfcc features \"\"\" input_height = 150 # this stands for 150ms input_length = mfcc_features . shape [ 0 ] zero_shape = list ( mfcc_features . shape ) zero_shape [ 0 ] = input_height ranges = list ( reversed ([ idx for idx in range ( input_length - 1 , 0 , - input_height )])) new_data = [] for r in ranges : if r < input_height : zero_data = np . zeros ( zero_shape ) zero_data [ - r :, :] = mfcc_features [: r , :] new_data . append ( zero_data ) else : new_data . append ( mfcc_features [ r - input_height : r , :]) return ( new_data )","title":"split_input_data()"},{"location":"api/services/#adviser.services.backchannel.PytorchAcousticBackchanneler","text":"","title":"PytorchAcousticBackchanneler"},{"location":"api/services/#adviser.services.backchannel.PytorchAcousticBackchanneler.PytorchAcousticBackchanneler","text":"Class for defining the Deep Backchannel model in PyTorch","title":"PytorchAcousticBackchanneler"},{"location":"api/services/#adviser.services.backchannel.PytorchAcousticBackchanneler.PytorchAcousticBackchanneler.__init__","text":"Defines the elements/layers of the neural network as well as loads the pretrained parameters The model is constituted by two parallel CNNs followed by a concatenation, a FFN and a softmax layer. Parameters: Name Type Description Default parameters list list of pre-trained parameters to be used for prediction [] load_params bool Bool to signal if params should be loaded False Source code in adviser/services/backchannel/PytorchAcousticBackchanneler.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , parameters : list = [], load_params : bool = False ): \"\"\" Defines the elements/layers of the neural network as well as loads the pretrained parameters The model is constituted by two parallel CNNs followed by a concatenation, a FFN and a softmax layer. Args: parameters (list): list of pre-trained parameters to be used for prediction load_params (bool): Bool to signal if params should be loaded \"\"\" super ( PytorchAcousticBackchanneler , self ) . __init__ () # First CNN cnn = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = ( 11 , 13 ), stride = ( 3 , 1 )) if load_params : weights = np . transpose ( parameters [ 0 ][ 0 ], ( 3 , 2 , 0 , 1 )) cnn . weight = torch . nn . Parameter ( torch . tensor ( weights ) . float ()) cnn . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 0 ][ 1 ]) . float ()) self . cnn1 = nn . Sequential ( cnn , nn . ReLU (), nn . MaxPool2d (( 23 , 1 )) ) # Second CNN cnn = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = ( 12 , 13 ), stride = ( 3 , 1 )) if load_params : weights = np . transpose ( parameters [ 1 ][ 0 ], ( 3 , 2 , 0 , 1 )) cnn . weight = torch . nn . Parameter ( torch . tensor ( weights ) . float ()) cnn . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 1 ][ 1 ]) . float ()) self . cnn2 = nn . Sequential ( cnn , nn . ReLU (), nn . MaxPool2d (( 23 , 1 )) ) # Linear layer self . linear1 = nn . Linear ( in_features = 64 , out_features = 100 ) if load_params : self . linear1 . weight = torch . nn . Parameter ( torch . tensor ( parameters [ 2 ][ 0 ] . T ) . float ()) self . linear1 . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 2 ][ 1 ]) . float ()) self . relu = nn . ReLU () self . dropout = nn . Dropout ( 0.5 ) # Softmax self . linear2 = nn . Linear ( in_features = 100 , out_features = 3 ) if load_params : self . linear2 . weight = torch . nn . Parameter ( torch . tensor ( parameters [ 3 ][ 0 ] . T ) . float ()) self . linear2 . bias = torch . nn . Parameter ( torch . tensor ( parameters [ 3 ][ 1 ]) . float ()) self . softmax = nn . Softmax ( dim = 1 )","title":"__init__()"},{"location":"api/services/#adviser.services.backchannel.PytorchAcousticBackchanneler.PytorchAcousticBackchanneler.forward","text":"PyTorch forward method used for training and prediction. It defines the interaction between layers. Parameters: Name Type Description Default feat_inputs numpy array It contains the network's input. required Returns: Type Description out (torch.tensor) Network's output Source code in adviser/services/backchannel/PytorchAcousticBackchanneler.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , feat_inputs ): \"\"\" PyTorch forward method used for training and prediction. It defines the interaction between layers. Args: feat_inputs (numpy array): It contains the network's input. Returns: out (torch.tensor): Network's output \"\"\" feat_inputs = torch . tensor ( feat_inputs ) . float () feat_inputs = feat_inputs . unsqueeze ( 1 ) cnn_1 = self . cnn1 ( feat_inputs ) cnn_1 = cnn_1 . flatten ( 1 ) cnn_2 = self . cnn2 ( feat_inputs ) . flatten ( 1 ) out = torch . cat (( cnn_1 , cnn_2 ), 1 ) out = self . linear1 ( out ) out = self . relu ( out ) out = self . dropout ( out ) out = self . linear2 ( out ) out = self . softmax ( out ) return out","title":"forward()"},{"location":"api/services/#adviser.services.bst","text":"","title":"bst"},{"location":"api/services/#adviser.services.bst.bst","text":"","title":"bst"},{"location":"api/services/#adviser.services.bst.bst.HandcraftedBST","text":"A rule-based approach to belief state tracking.","title":"HandcraftedBST"},{"location":"api/services/#adviser.services.bst.bst.HandcraftedBST.__init__","text":"Source code in adviser/services/bst/bst.py 33 34 35 36 def __init__ ( self , domain = None , logger = None ): Service . __init__ ( self , domain = domain ) self . logger = logger self . bs = BeliefState ( domain )","title":"__init__()"},{"location":"api/services/#adviser.services.bst.bst.HandcraftedBST.dialog_start","text":"Restets the belief state so it is ready for a new dialog Returns: Type Description (dict) a dictionary with a single entry where the key is 'beliefstate'and the value is a new BeliefState object Source code in adviser/services/bst/bst.py 69 70 71 72 73 74 75 76 77 78 def dialog_start ( self ): \"\"\" Restets the belief state so it is ready for a new dialog Returns: (dict): a dictionary with a single entry where the key is 'beliefstate'and the value is a new BeliefState object \"\"\" # initialize belief state self . bs = BeliefState ( self . domain )","title":"dialog_start()"},{"location":"api/services/#adviser.services.bst.bst.HandcraftedBST.update_bst","text":"Source code in adviser/services/bst/bst.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"update_bst()"},{"location":"api/services/#adviser.services.domain_tracker","text":"","title":"domain_tracker"},{"location":"api/services/#adviser.services.domain_tracker.domain_tracker","text":"The console module provides ADVISER services for tracking current domain","title":"domain_tracker"},{"location":"api/services/#adviser.services.domain_tracker.domain_tracker.DomainTracker","text":"Responsible for selecting which domain should be active at a given time. Current implmentation uses keywords to switch domains.","title":"DomainTracker"},{"location":"api/services/#adviser.services.domain_tracker.domain_tracker.DomainTracker.__init__","text":"Source code in adviser/services/domain_tracker/domain_tracker.py 34 35 36 37 38 def __init__ ( self , domains : List [ Domain ], greet_on_first_turn : bool = False ): Service . __init__ ( self , domain = \"\" ) self . domains = domains self . current_domain = None self . greet_on_first_turn = greet_on_first_turn","title":"__init__()"},{"location":"api/services/#adviser.services.domain_tracker.domain_tracker.DomainTracker.dialog_start","text":"Resets the domain tracker for the start of a new dialog Source code in adviser/services/domain_tracker/domain_tracker.py 40 41 42 43 44 45 def dialog_start ( self ): \"\"\" Resets the domain tracker for the start of a new dialog \"\"\" self . turn = 0 self . current_domain = None","title":"dialog_start()"},{"location":"api/services/#adviser.services.domain_tracker.domain_tracker.DomainTracker.domains_to_str","text":"Method to create the greeting on the first turn, grammatically joins the names of possible domains into a string Returns: Type Description (str) String representing a list of all domain names the system can talk about Source code in adviser/services/domain_tracker/domain_tracker.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def domains_to_str ( self ): \"\"\" Method to create the greeting on the first turn, grammatically joins the names of possible domains into a string Returns: (str): String representing a list of all domain names the system can talk about \"\"\" if len ( self . domains ) == 1 : return self . domains [ 0 ] . get_display_name () elif len ( self . domains ) == 2 : return \" and \" . join ([ d . get_display_name () for d in self . domains ]) else : return \", \" . join ([ d . get_display_name () for d in self . domains ][: - 1 ]) + f \", and { self . domains [ - 1 ] . get_display_name () } \"","title":"domains_to_str()"},{"location":"api/services/#adviser.services.domain_tracker.domain_tracker.DomainTracker.select_domain","text":"Source code in adviser/services/domain_tracker/domain_tracker.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"select_domain()"},{"location":"api/services/#adviser.services.emotion","text":"","title":"emotion"},{"location":"api/services/#adviser.services.emotion.EmotionRecognition","text":"Emotion recognition module.","title":"EmotionRecognition"},{"location":"api/services/#adviser.services.emotion.EmotionRecognition.EmotionRecognition","text":"Emotion recognition module. This module receives acoustic features, loads pretrained models and outputs predictions of emotional states. It can easily be extended/adapted to use different models and facial features in addition.","title":"EmotionRecognition"},{"location":"api/services/#adviser.services.emotion.EmotionRecognition.EmotionRecognition.__init__","text":"Emotion recognition module. On initialization all necessary models are loaded. Source code in adviser/services/emotion/EmotionRecognition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self ): \"\"\" Emotion recognition module. On initialization all necessary models are loaded. \"\"\" Service . __init__ ( self ) self . emotion_dir = os . path . dirname ( os . path . abspath ( __file__ )) self . model_path = os . path . abspath ( os . path . join ( self . emotion_dir , \"..\" , \"..\" , \"resources\" , \"models\" , \"emotion\" ) ) def load_args ( emo_representation ): arg_dict = pickle . load ( open ( os . path . join ( self . model_path , f ' { emo_representation } _args.pkl' ), 'rb' ) ) return arg_dict def load_model ( emo_representation , arg_dict ): ARGS = arg_dict [ 'args' ] model = cnn ( kernel_size = ( ARGS . height , arg_dict [ 'D_in' ]), D_out = arg_dict [ 'D_out' ], args = ARGS ) model . load_state_dict ( torch . load ( os . path . join ( self . model_path , f ' { emo_representation } _model_params.pt' ), map_location = torch . device ( 'cpu' ) ) ) model . eval () return model self . emo_representations = [ 'category' , 'arousal' , 'valence' ] self . models = {} self . args = {} for emo_representation in self . emo_representations : self . args [ emo_representation ] = load_args ( emo_representation ) self . models [ emo_representation ] = load_model ( emo_representation , self . args [ emo_representation ] ) self . arousal_mapping = { 0 : 'low' , 1 : 'medium' , 2 : 'high' } self . valence_mapping = { 0 : 'negative' , 1 : 'neutral' , 2 : 'positive' } self . category_mapping = { 0 : EmotionType . Angry , 1 : EmotionType . Happy , 2 : EmotionType . Neutral , 3 : EmotionType . Sad }","title":"__init__()"},{"location":"api/services/#adviser.services.emotion.EmotionRecognition.EmotionRecognition.predict_from_audio","text":"Source code in adviser/services/emotion/EmotionRecognition.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"predict_from_audio()"},{"location":"api/services/#adviser.services.engagement","text":"","title":"engagement"},{"location":"api/services/#adviser.services.engagement.engagement_tracker","text":"","title":"engagement_tracker"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.EngagementTracker","text":"Start feature extraction with OpenFace. Requires OpenFace to be installed - instructions can be found in tool/openface.txt","title":"EngagementTracker"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.EngagementTracker.__init__","text":"Parameters: Name Type Description Default camera_id int index of the camera you want to use (if you only have one camera: 0) 0 Source code in adviser/services/engagement/engagement_tracker.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , domain = \"\" , camera_id : int = 0 , openface_port : int = 6004 , delay : int = 2 , identifier = None ): \"\"\" Args: camera_id: index of the camera you want to use (if you only have one camera: 0) \"\"\" Service . __init__ ( self , domain = \"\" , identifier = identifier ) self . camera_id = camera_id self . openface_port = openface_port self . openface_running = False self . threshold = delay # provide number of seconds as parameter, one second = 15 frames ctx = Context . instance () self . openface_endpoint = ctx . socket ( zmq . PAIR ) self . openface_endpoint . bind ( f \"tcp://127.0.0.1: { self . openface_port } \" ) startExtraction = f \" { os . path . join ( get_root_dir (), 'tools/OpenFace/build/bin/FaceLandmarkVidZMQ' ) } -device { self . camera_id } -port 6004\" # todo config open face port self . p_openface = subprocess . Popen ( startExtraction . split (), stdout = subprocess . PIPE ) # start OpenFace self . extracting = False self . extractor_thread = None","title":"__init__()"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.EngagementTracker.dialog_end","text":"This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. Source code in adviser/services/engagement/engagement_tracker.py 145 146 147 148 149 def dialog_end ( self ): # Set openface to non-publishing mode and wait until it is ready self . openface_endpoint . send ( bytes ( f \"OPENFACE_END\" , encoding = \"ascii\" )) if self . extractor_thread : self . extractor_thread . join ()","title":"dialog_end()"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.EngagementTracker.dialog_exit","text":"This function is called when the dialog system is shutting down. You should overwrite this function to stop your threads and cleanup any open resources. Source code in adviser/services/engagement/engagement_tracker.py 151 152 153 def dialog_exit ( self ): # close openface process self . p_openface . kill ()","title":"dialog_exit()"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.EngagementTracker.dialog_start","text":"This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/engagement/engagement_tracker.py 69 70 71 72 73 74 75 76 77 78 79 80 def dialog_start ( self ): # Set openface to publishing mode and wait until it is ready self . openface_endpoint . send ( bytes ( f \"OPENFACE_START\" , encoding = \"ascii\" )) self . extracting = False while not self . extracting : msg = self . openface_endpoint . recv () # receive started signal msg = msg . decode ( \"utf-8\" ) if msg == \"OPENFACE_STARTED\" : print ( \"START EXTRACTION\" ) self . extracting = True self . extractor_thread = Thread ( target = self . publish_gaze_directions ) self . extractor_thread . start ()","title":"dialog_start()"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.EngagementTracker.publish_gaze_directions","text":"Meant to be used in a thread. Runs an inifinte loop polling features from OpenFace library, parsing them and extracting engagement features. Calls yield_gaze_direction to publish the polled and processed engagement features. Source code in adviser/services/engagement/engagement_tracker.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def publish_gaze_directions ( self ): \"\"\" Meant to be used in a thread. Runs an inifinte loop polling features from OpenFace library, parsing them and extracting engagement features. Calls `yield_gaze_direction` to publish the polled and processed engagement features. \"\"\" x_coordinates = [] y_coordinates = [] norm = 0.0 # center point of screen; should be close(r) to 0 looking = True while self . extracting : req = self . openface_endpoint . send ( bytes ( f \"OPENFACE_PULL\" , encoding = \"ascii\" )) msg = self . openface_endpoint . recv () try : msg = msg . decode ( \"utf-8\" ) if msg == \"OPENFACE_ENDED\" : self . extracting = False msg_data = json . loads ( msg ) gaze_x = msg_data [ \"gaze\" ][ \"angle\" ][ \"x\" ] gaze_y = msg_data [ \"gaze\" ][ \"angle\" ][ \"y\" ] gaze_x = sqrt ( gaze_x ** 2 ) # gaze_angle_x (left-right movement), square + root is done to yield only positive values gaze_y = sqrt ( gaze_y ** 2 ) # gaze_angle_y (up-down movement) x_coordinates . append ( gaze_x ) y_coordinates . append ( gaze_y ) current = ( len ( x_coordinates )) - 1 if current > self . threshold : previous_x = mean ( x_coordinates [ current - ( self . threshold + 1 ): current ]) # obtain the average of previous frames previous_y = mean ( y_coordinates [ current - ( self . threshold + 1 ): current ]) difference_x = sqrt (( norm - previous_x ) ** 2 ) # compare current frame to average of previous frames difference_y = sqrt (( norm - previous_y ) ** 2 ) # print(difference_x, difference_y) if difference_x < 0.15 and difference_y < 0.15 : # check whether difference between current and previous frames exceeds certain threshold (regulates tolerance/strictness) if looking != True : looking = True self . yield_gaze_direction ( engagement = EngagementType . High , gaze_direction = ( gaze_x , gaze_y )) else : if looking != False : looking = False self . yield_gaze_direction ( engagement = EngagementType . Low , gaze_direction = ( gaze_x , gaze_y )) except : # import traceback # traceback.print_exc() pass","title":"publish_gaze_directions()"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.EngagementTracker.yield_gaze_direction","text":"Source code in adviser/services/engagement/engagement_tracker.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"yield_gaze_direction()"},{"location":"api/services/#adviser.services.engagement.engagement_tracker.get_root_dir","text":"Source code in adviser/services/engagement/engagement_tracker.py 39 40 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))","title":"get_root_dir()"},{"location":"api/services/#adviser.services.hci","text":"","title":"hci"},{"location":"api/services/#adviser.services.hci.console","text":"The console module provides ADVISER modules that access the console for input and output.","title":"console"},{"location":"api/services/#adviser.services.hci.console.ConsoleInput","text":"Gets the user utterance from the console. Waits for the built-in input function to return a non-empty text.","title":"ConsoleInput"},{"location":"api/services/#adviser.services.hci.console.ConsoleInput.__init__","text":"Source code in adviser/services/hci/console.py 40 41 42 43 44 45 def __init__ ( self , domain : Domain = None , conversation_log_dir : str = None , language : Language = None ): Service . __init__ ( self , domain = domain ) # self.language = language self . language = Language . ENGLISH self . conversation_log_dir = conversation_log_dir self . interaction_count = 0","title":"__init__()"},{"location":"api/services/#adviser.services.hci.console.ConsoleInput.dialog_start","text":"This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/hci/console.py 49 50 def dialog_start ( self ): self . interaction_count = 0","title":"dialog_start()"},{"location":"api/services/#adviser.services.hci.console.ConsoleInput.get_user_input","text":"Source code in adviser/services/hci/console.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"get_user_input()"},{"location":"api/services/#adviser.services.hci.console.ConsoleOutput","text":"Writes the system utterance to the console.","title":"ConsoleOutput"},{"location":"api/services/#adviser.services.hci.console.ConsoleOutput.__init__","text":"Source code in adviser/services/hci/console.py 113 114 def __init__ ( self , domain : Domain = None ): Service . __init__ ( self , domain = domain )","title":"__init__()"},{"location":"api/services/#adviser.services.hci.console.ConsoleOutput.print_sys_utterance","text":"Source code in adviser/services/hci/console.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"print_sys_utterance()"},{"location":"api/services/#adviser.services.hci.gui","text":"","title":"gui"},{"location":"api/services/#adviser.services.hci.gui.GUIServer","text":"Service for the React-based Web-UI. Run this as a remote service: * run this file seperately, will start the GUI Server * run the dialog system in another python instance, add a RemoteService with identifier GUIServer","title":"GUIServer"},{"location":"api/services/#adviser.services.hci.gui.GUIServer.__init__","text":"Source code in adviser/services/hci/gui.py 62 63 64 65 def __init__ ( self , socketio , identifier = \"GUIServer\" , logger : DiasysLogger = None ): super () . __init__ ( domain = '' , identifier = identifier ) self . socketio = socketio self . logger = logger","title":"__init__()"},{"location":"api/services/#adviser.services.hci.gui.GUIServer.forward_message_to_react","text":"Source code in adviser/services/hci/gui.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"forward_message_to_react()"},{"location":"api/services/#adviser.services.hci.gui.GUIServer.start_dialog","text":"Source code in adviser/services/hci/gui.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"start_dialog()"},{"location":"api/services/#adviser.services.hci.gui.GUIServer.user_utterance","text":"Source code in adviser/services/hci/gui.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"user_utterance()"},{"location":"api/services/#adviser.services.hci.gui.get_root_dir","text":"Source code in adviser/services/hci/gui.py 32 33 def get_root_dir (): return os . path . abspath ( os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"..\" , \"..\" ))","title":"get_root_dir()"},{"location":"api/services/#adviser.services.hci.speech","text":"","title":"speech"},{"location":"api/services/#adviser.services.hci.speech.cleaners","text":"This file is derived from https://github.com/keithito/tacotron.","title":"cleaners"},{"location":"api/services/#adviser.services.hci.speech.cleaners.basic_cleaners","text":"Basic pipeline that lowercases and collapses whitespace without transliteration. Source code in adviser/services/hci/speech/cleaners.py 208 209 210 211 212 def basic_cleaners ( text ): \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\" text = lowercase ( text ) text = collapse_whitespace ( text ) return text","title":"basic_cleaners()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.collapse_whitespace","text":"Source code in adviser/services/hci/speech/cleaners.py 200 201 def collapse_whitespace ( text ): return re . sub ( _whitespace_re , ' ' , text )","title":"collapse_whitespace()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.convert_to_ascii","text":"Source code in adviser/services/hci/speech/cleaners.py 204 205 def convert_to_ascii ( text ): return unidecode ( text )","title":"convert_to_ascii()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.custom_english_cleaners","text":"Custom pipeline for English text, including number and abbreviation expansion. Source code in adviser/services/hci/speech/cleaners.py 254 255 256 257 258 259 260 261 262 263 264 265 266 def custom_english_cleaners ( text ): \"\"\"Custom pipeline for English text, including number and abbreviation expansion.\"\"\" text = convert_to_ascii ( text ) text = expand_email ( text ) text = expand_acronym ( text ) text = lowercase ( text ) text = expand_numbers ( text ) text = expand_abbreviations ( text ) text = expand_symbols ( text ) text = remove_unnecessary_symbols ( text ) text = uppercase ( text ) text = collapse_whitespace ( text ) return text","title":"custom_english_cleaners()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.english_cleaners","text":"Pipeline for English text, including number and abbreviation expansion. Source code in adviser/services/hci/speech/cleaners.py 223 224 225 226 227 228 229 230 def english_cleaners ( text ): \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\" text = convert_to_ascii ( text ) text = lowercase ( text ) text = expand_numbers ( text ) text = expand_abbreviations ( text ) text = collapse_whitespace ( text ) return text","title":"english_cleaners()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.expand_abbreviations","text":"Preprocesses a text to turn abbreviations into forms that the TTS can pronounce properly text (string): Text to be preprocessed Source code in adviser/services/hci/speech/cleaners.py 137 138 139 140 141 142 143 144 145 def expand_abbreviations ( text ): \"\"\" Preprocesses a text to turn abbreviations into forms that the TTS can pronounce properly text (string): Text to be preprocessed \"\"\" for regex , replacement in _abbreviations : text = re . sub ( regex , replacement , text ) return text","title":"expand_abbreviations()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.expand_acronym","text":"Preprocesses a text to turn acronyms into forms that the TTS can pronounce properly text (string): Text to be preprocessed Source code in adviser/services/hci/speech/cleaners.py 171 172 173 174 175 176 177 178 179 def expand_acronym ( text ): \"\"\" Preprocesses a text to turn acronyms into forms that the TTS can pronounce properly text (string): Text to be preprocessed \"\"\" for word , replacement in _acronym : text = re . sub ( word , replacement , text ) return text","title":"expand_acronym()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.expand_email","text":"Source code in adviser/services/hci/speech/cleaners.py 188 189 190 def expand_email ( text ): text = re . sub ( _email_re , _expand_email , text ) return text","title":"expand_email()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.expand_numbers","text":"Source code in adviser/services/hci/speech/cleaners.py 192 193 def expand_numbers ( text ): return normalize_numbers ( text )","title":"expand_numbers()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.expand_symbols","text":"Source code in adviser/services/hci/speech/cleaners.py 240 241 242 243 244 245 246 def expand_symbols ( text ): # added text = re . sub ( \";\" , \",\" , text ) text = re . sub ( \":\" , \",\" , text ) text = re . sub ( \"-\" , \" \" , text ) text = re . sub ( \"&\" , \"and\" , text ) return text","title":"expand_symbols()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.lowercase","text":"Source code in adviser/services/hci/speech/cleaners.py 196 197 def lowercase ( text ): return text . lower ()","title":"lowercase()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.normalize_numbers","text":"Normalizes numbers in an utterance as preparation for TTS text (string): Text to be preprocessed Source code in adviser/services/hci/speech/cleaners.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def normalize_numbers ( text ): \"\"\" Normalizes numbers in an utterance as preparation for TTS text (string): Text to be preprocessed \"\"\" text = re . sub ( _comma_number_re , _remove_commas , text ) text = re . sub ( _pounds_re , r '\\1 pounds' , text ) text = re . sub ( _dollars_re , _expand_dollars , text ) text = re . sub ( _decimal_number_re , _expand_decimal_point , text ) text = re . sub ( _ordinal_re , _expand_ordinal , text ) text = re . sub ( _ID_number_re , _expand_ID_number , text ) text = re . sub ( _number_re , _expand_number , text ) return text","title":"normalize_numbers()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.remove_unnecessary_symbols","text":"Source code in adviser/services/hci/speech/cleaners.py 233 234 235 236 237 def remove_unnecessary_symbols ( text ): # added text = re . sub ( r '[()[]<>\"]+' , '' , text ) text = re . sub ( r '/' , ' ' , text ) return text","title":"remove_unnecessary_symbols()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.transliteration_cleaners","text":"Pipeline for non-English text that transliterates to ASCII. Source code in adviser/services/hci/speech/cleaners.py 215 216 217 218 219 220 def transliteration_cleaners ( text ): \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\" text = convert_to_ascii ( text ) text = lowercase ( text ) text = collapse_whitespace ( text ) return text","title":"transliteration_cleaners()"},{"location":"api/services/#adviser.services.hci.speech.cleaners.uppercase","text":"Source code in adviser/services/hci/speech/cleaners.py 249 250 251 def uppercase ( text ): # added return text . upper ()","title":"uppercase()"},{"location":"api/services/#adviser.services.hci.speech.FeatureExtractor","text":"Feature extraction with openSMILE. This module provides a feature extractor which uses the openSMILE toolkit to extract features from raw audio. The user utterance which is represented as a numpy array in-memory needs to be written to a temporary file first, so that openSMILE can read it.","title":"FeatureExtractor"},{"location":"api/services/#adviser.services.hci.speech.FeatureExtractor.SpeechFeatureExtractor","text":"SpeechFeatureExtractor calls openSMILE to extract features from audio. Note : openSMILE will be downloaded & compiled to tools / opensmile if not found there .","title":"SpeechFeatureExtractor"},{"location":"api/services/#adviser.services.hci.speech.FeatureExtractor.SpeechFeatureExtractor.__init__","text":"SpeechFeatureExtractor. The following things are setup on initialization: * directory for temporary audio files * path to openSMILE config files * path to openSMILE executable Source code in adviser/services/hci/speech/FeatureExtractor.py 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self ): \"\"\" SpeechFeatureExtractor. The following things are setup on initialization: * directory for temporary audio files * path to openSMILE config files * path to openSMILE executable \"\"\" Service . __init__ ( self ) self . speech_out_dir = os . path . join ( \"resources\" , \"tmp_audio_and_features\" ) self . cfg_dir = os . path . join ( \"resources\" , \"opensmile_config\" ) self . openSmile_path = get_opensmile_executable_path ()","title":"__init__()"},{"location":"api/services/#adviser.services.hci.speech.FeatureExtractor.SpeechFeatureExtractor.extract_wav_file_features","text":"Extracting acoustic features using openSMILE. Parameters: Name Type Description Default features str path to openSMILE's feature config required new_audio_file str path to audio file required Returns: Type Description numpy.ndarray extracted features for the audio file Source code in adviser/services/hci/speech/FeatureExtractor.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def extract_wav_file_features ( self , features , new_audio_file ): \"\"\"Extracting acoustic features using openSMILE. Args: features (str): path to openSMILE's feature config new_audio_file (str): path to audio file Returns: numpy.ndarray: extracted features for the audio file \"\"\" output_file = new_audio_file + '.csv' config_file = os . path . join ( self . cfg_dir , features + \".conf\" ) f = open ( os . devnull , 'w' ) try : # OpenSMILE command to extract features # SMILExtract -C <configfile> -I <input_file> \u2212O <output_file> command = ' ' . join ([ self . openSmile_path , '-C' , config_file , '-I' , new_audio_file , '-csvoutput' , output_file , '-headercsv' , '0' , '-timestampcsv' , '0' , '-instname' , '0' ]) subprocess . call ( command , stdout = f , stderr = f , shell = True ) return self . preprocess_csv ( output_file ) except OSError as err : print ( command ) print ( \"OS error: {0} \" . format ( err ))","title":"extract_wav_file_features()"},{"location":"api/services/#adviser.services.hci.speech.FeatureExtractor.SpeechFeatureExtractor.preprocess_csv","text":"Get features from csv file and normalize them if necessary. openSMILE feature are written to temporary csv file. This function reads them into a numpy array, removes instance names and could do a normalization step if needed. This is not implemented right now. Parameters: Name Type Description Default csv_file str path to csv file required Returns: Type Description numpy.ndarray raw feature values Source code in adviser/services/hci/speech/FeatureExtractor.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def preprocess_csv ( self , csv_file ): \"\"\"Get features from csv file and normalize them if necessary. openSMILE feature are written to temporary csv file. This function reads them into a numpy array, removes instance names and could do a normalization step if needed. This is not implemented right now. Args: csv_file (str): path to csv file Returns: numpy.ndarray: raw feature values \"\"\" feats = np . genfromtxt ( csv_file , delimiter = ';' ) if len ( feats . shape ) == 1 : # reshape one-dimensional features, e.g. gemaps feats = feats . reshape ( 1 , - 1 ) # take everything except first column which is the instance name feats = feats [:, 1 :] # StandardScaler normalizes feats to zero mean and unit variance # for frame-wise LLDs, it will standardize across frames # for gemaps (or functionals in general), it's not possible to scale # ideal setup: fit StandardScaler on training set across samples # and apply it with .transform() to new samples # scaler = preprocessing.StandardScaler() self . remove_file ( csv_file ) return feats","title":"preprocess_csv()"},{"location":"api/services/#adviser.services.hci.speech.FeatureExtractor.SpeechFeatureExtractor.remove_file","text":"Remove specified file. Parameters: Name Type Description Default file_name str full path of file which shall be removed required Source code in adviser/services/hci/speech/FeatureExtractor.py 94 95 96 97 98 99 100 101 102 103 104 def remove_file ( self , file_name ): \"\"\" Remove specified file. Args: file_name (str): full path of file which shall be removed \"\"\" try : os . remove ( file_name ) except FileNotFoundError as error : self . logger . error ( error ) raise ( error )","title":"remove_file()"},{"location":"api/services/#adviser.services.hci.speech.FeatureExtractor.SpeechFeatureExtractor.speech_to_features","text":"Source code in adviser/services/hci/speech/FeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"speech_to_features()"},{"location":"api/services/#adviser.services.hci.speech.speech_utility","text":"Utility for the emotion recognition script that needs the utterance a s file","title":"speech_utility"},{"location":"api/services/#adviser.services.hci.speech.speech_utility.delete_file","text":"Deletes the file at the given path to clean up the audio file once it's not needed anymore. This is why unique filenames are important. filepath (string): path to the file that is to be deleted Source code in adviser/services/hci/speech/speech_utility.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def delete_file ( filepath ): \"\"\" Deletes the file at the given path to clean up the audio file once it's not needed anymore. This is why unique filenames are important. filepath (string): path to the file that is to be deleted \"\"\" if os . path . exists ( filepath ): os . remove ( filepath ) else : print ( \"The file cannot be deleted, as it was not found. \" \"Please check the provided path for errors: \\n {} \" . format ( filepath ))","title":"delete_file()"},{"location":"api/services/#adviser.services.hci.speech.speech_utility.sound_array_to_file","text":"Saves the recording of the recorder to a file Turns the audio from the recorder service into a wav file for processing with opensmile c++ scripts filepath (string): full path, including filename and .wav suffix at an arbitrary location. Careful: python takes paths as relative to the main script. The name should be unique, to ensure files don't get mixed up if there are multiple calls in short time and one file might get overwriteen or deleted before it's done being processed. sampling_rate (int): the sampling rate of the audio, as published by the recorder sound_as_array (np.array): the audio in form of an array as published by the recorder Source code in adviser/services/hci/speech/speech_utility.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def sound_array_to_file ( filepath , sampling_rate , sound_as_array ): \"\"\" Saves the recording of the recorder to a file Turns the audio from the recorder service into a wav file for processing with opensmile c++ scripts filepath (string): full path, including filename and .wav suffix at an arbitrary location. Careful: python takes paths as relative to the main script. The name should be unique, to ensure files don't get mixed up if there are multiple calls in short time and one file might get overwriteen or deleted before it's done being processed. sampling_rate (int): the sampling rate of the audio, as published by the recorder sound_as_array (np.array): the audio in form of an array as published by the recorder \"\"\" librosa . output . write_wav ( filepath , sound_as_array , sampling_rate )","title":"sound_array_to_file()"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputDecoder","text":"","title":"SpeechInputDecoder"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputDecoder.SpeechInputDecoder","text":"","title":"SpeechInputDecoder"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputDecoder.SpeechInputDecoder.__init__","text":"Transforms spoken input from the user to text for further processing. Parameters: Name Type Description Default domain Domain Needed for Service, but has no meaning here '' identifier string Needed for Service None conversation_log_dir str If this is provided, logfiles will be placed by this Service into the specified directory. None use_cuda boolean Whether or not to run the computations on a GPU False Source code in adviser/services/hci/speech/SpeechInputDecoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , domain : Domain = \"\" , identifier = None , conversation_log_dir : str = None , use_cuda = False ): \"\"\" Transforms spoken input from the user to text for further processing. Args: domain (Domain): Needed for Service, but has no meaning here identifier (string): Needed for Service conversation_log_dir (string): If this is provided, logfiles will be placed by this Service into the specified directory. use_cuda (boolean): Whether or not to run the computations on a GPU \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) self . conversation_log_dir = conversation_log_dir # load model model_dir = os . path . join ( get_root_dir (), \"resources\" , \"models\" , \"speech\" , \"multi_en_20190916\" ) self . model , conf = load_trained_model ( os . path . join ( model_dir , \"model.bin\" )) self . vocab = conf . char_list # setup beam search self . bs = BeamSearch ( scorers = self . model . scorers (), weights = { \"decoder\" : 1.0 , \"ctc\" : 0.0 }, sos = self . model . sos , eos = self . model . eos , beam_size = 4 , vocab_size = len ( self . vocab ), pre_beam_score_key = \"decoder\" ) self . bs . __class__ = BatchBeamSearch # choose hardware to run on if use_cuda : self . device = \"cuda\" else : self . device = \"cpu\" self . model . to ( self . device ) self . bs . to ( self . device ) # change from training mode to eval mode self . model . eval () self . bs . eval () # scale and offset for feature normalization # follows https://github.com/kaldi-asr/kaldi/blob/33255ed224500f55c8387f1e4fa40e08b73ff48a/src/transform/cmvn.cc#L92-L111 norm = torch . load ( os . path . join ( model_dir , \"cmvn.bin\" )) count = norm [ 0 ][ - 1 ] mean = norm [ 0 ][: - 1 ] / count var = ( norm [ 1 ][: - 1 ] / count ) - mean * mean self . scale = 1.0 / torch . sqrt ( var ) self . offset = - ( mean * self . scale )","title":"__init__()"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputDecoder.SpeechInputDecoder.features_to_text","text":"Source code in adviser/services/hci/speech/SpeechInputDecoder.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"features_to_text()"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputDecoder.get_root_dir","text":"Source code in adviser/services/hci/speech/SpeechInputDecoder.py 34 35 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))))","title":"get_root_dir()"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputFeatureExtractor","text":"","title":"SpeechInputFeatureExtractor"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputFeatureExtractor.SpeechInputFeatureExtractor","text":"","title":"SpeechInputFeatureExtractor"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputFeatureExtractor.SpeechInputFeatureExtractor.__init__","text":"Given a sound, this service extracts features and passes them on to the decoder for ASR Parameters: Name Type Description Default domain Domain Needed for Service, no meaning here '' Source code in adviser/services/hci/speech/SpeechInputFeatureExtractor.py 32 33 34 35 36 37 38 39 def __init__ ( self , domain : Domain = \"\" ): \"\"\" Given a sound, this service extracts features and passes them on to the decoder for ASR Args: domain (Domain): Needed for Service, no meaning here \"\"\" Service . __init__ ( self , domain = domain )","title":"__init__()"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputFeatureExtractor.SpeechInputFeatureExtractor.speech_to_features","text":"Source code in adviser/services/hci/speech/SpeechInputFeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"speech_to_features()"},{"location":"api/services/#adviser.services.hci.speech.SpeechInputFeatureExtractor.SpeechInputFeatureExtractor.speech_to_mfcc","text":"Source code in adviser/services/hci/speech/SpeechInputFeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"speech_to_mfcc()"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputGenerator","text":"","title":"SpeechOutputGenerator"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputGenerator.SpeechOutputGenerator","text":"","title":"SpeechOutputGenerator"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputGenerator.SpeechOutputGenerator.__init__","text":"Text To Speech Module that reads out the system utterance. Parameters: Name Type Description Default domain Domain Needed for Service, no meaning here '' identifier str Needed for Service None use_cuda boolean Whether or not to perform computations on GPU. Highly recommended if available False sub_topic_domains Dict[str, str] see services.service.Service constructor for more details {} Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , domain : Domain = \"\" , identifier : str = None , use_cuda = False , sub_topic_domains : Dict [ str , str ] = {}): \"\"\" Text To Speech Module that reads out the system utterance. Args: domain (Domain): Needed for Service, no meaning here identifier (string): Needed for Service use_cuda (boolean): Whether or not to perform computations on GPU. Highly recommended if available sub_topic_domains: see `services.service.Service` constructor for more details \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier , sub_topic_domains = sub_topic_domains ) self . models_directory = os . path . join ( get_root_dir (), \"resources\" , \"models\" , \"speech\" ) # The following lines can be changed to incorporate different models. # This is the only thing that needs to be changed for that, everything else should be dynamic. self . transcription_type = \"phn\" self . dict_path = os . path . join ( self . models_directory , \"phn_train_no_dev_pytorch_train_fastspeech.v4\" , \"data\" , \"lang_1phn\" , \"train_no_dev_units.txt\" ) self . model_path = os . path . join ( self . models_directory , \"phn_train_no_dev_pytorch_train_fastspeech.v4\" , \"exp\" , \"phn_train_no_dev_pytorch_train_fastspeech.v4\" , \"results\" , \"model.last1.avg.best\" ) self . vocoder_path = os . path . join ( self . models_directory , \"ljspeech.parallel_wavegan.v1\" , \"checkpoint-400000steps.pkl\" ) self . vocoder_conf = os . path . join ( self . models_directory , \"ljspeech.parallel_wavegan.v1\" , \"config.yml\" ) # define device to run the synthesis on if use_cuda : self . device = torch . device ( \"cuda\" ) else : self . device = torch . device ( \"cpu\" ) # define end to end TTS model self . input_dimensions , self . output_dimensions , self . train_args = get_model_conf ( self . model_path ) model_class = dynamic_import . dynamic_import ( self . train_args . model_module ) model = model_class ( self . input_dimensions , self . output_dimensions , self . train_args ) torch_load ( self . model_path , model ) self . model = model . eval () . to ( self . device ) self . inference_args = Namespace ( ** { \"threshold\" : 0.5 , \"minlenratio\" : 0.0 , \"maxlenratio\" : 10.0 }) # define neural vocoder with open ( self . vocoder_conf ) as vocoder_config_file : self . config = yaml . load ( vocoder_config_file , Loader = yaml . Loader ) vocoder = ParallelWaveGANGenerator ( ** self . config [ \"generator_params\" ]) vocoder . load_state_dict ( torch . load ( self . vocoder_path , map_location = \"cpu\" )[ \"model\" ][ \"generator\" ]) vocoder . remove_weight_norm () self . vocoder = vocoder . eval () . to ( self . device ) with open ( self . dict_path ) as dictionary_file : lines = dictionary_file . readlines () lines = [ line . replace ( \" \\n \" , \"\" ) . split ( \" \" ) for line in lines ] self . char_to_id = { c : int ( i ) for c , i in lines } self . g2p = G2p () # download the pretrained Punkt tokenizer from NLTK. This is done only # the first time the code is executed on a machine, if it has been done # before, this line will be skipped and output a warning. We will probably # redirect warnings into a file rather than std_err in the future, since # there's also a lot of pytorch warnings going on etc. nltk . download ( 'punkt' , quiet = True )","title":"__init__()"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputGenerator.SpeechOutputGenerator.generate_speech","text":"Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"generate_speech()"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputGenerator.SpeechOutputGenerator.preprocess_text_input","text":"Clean the text and then convert it to id sequence. Parameters: Name Type Description Default text string The text to preprocess required Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def preprocess_text_input ( self , text ): \"\"\" Clean the text and then convert it to id sequence. Args: text (string): The text to preprocess \"\"\" text = custom_english_cleaners ( text ) # cleans the text if self . transcription_type == \"phn\" : # depending on the model type, different preprocessing is needed. text = filter ( lambda s : s != \" \" , self . g2p ( text )) text = \" \" . join ( text ) char_sequence = text . split ( \" \" ) else : char_sequence = list ( text ) id_sequence = [] for c in char_sequence : if c . isspace (): id_sequence += [ self . char_to_id [ \"<space>\" ]] elif c not in self . char_to_id . keys (): id_sequence += [ self . char_to_id [ \"<unk>\" ]] else : id_sequence += [ self . char_to_id [ c ]] id_sequence += [ self . input_dimensions - 1 ] # <eos> return torch . LongTensor ( id_sequence ) . view ( - 1 ) . to ( self . device )","title":"preprocess_text_input()"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputGenerator.get_root_dir","text":"Source code in adviser/services/hci/speech/SpeechOutputGenerator.py 39 40 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))))","title":"get_root_dir()"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputPlayer","text":"","title":"SpeechOutputPlayer"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputPlayer.SpeechOutputPlayer","text":"","title":"SpeechOutputPlayer"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputPlayer.SpeechOutputPlayer.__init__","text":"Service that plays the system utterance as sound Parameters: Name Type Description Default domain Domain Needed for Service, but has no meaning here '' conversation_log_dir str If this is provided it will create log files in the specified directory. None identifier str Needed for Service. None Source code in adviser/services/hci/speech/SpeechOutputPlayer.py 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , domain : Domain = \"\" , conversation_log_dir : str = None , identifier : str = None ): \"\"\" Service that plays the system utterance as sound Args: domain (Domain): Needed for Service, but has no meaning here conversation_log_dir (string): If this is provided it will create log files in the specified directory. identifier (string): Needed for Service. \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) self . conversation_log_dir = conversation_log_dir self . interaction_count = 0","title":"__init__()"},{"location":"api/services/#adviser.services.hci.speech.SpeechOutputPlayer.SpeechOutputPlayer.speak","text":"Source code in adviser/services/hci/speech/SpeechOutputPlayer.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"speak()"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder","text":"","title":"SpeechRecorder"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder.SpeechRecorder","text":"","title":"SpeechRecorder"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder.SpeechRecorder.__init__","text":"A service that can record a microphone upon a key pressing event and publish the result as an array. The end of the utterance is detected automatically, also the voice can be masked to alleviate privacy issues. Parameters: Name Type Description Default domain Union[str, utils.domain.domain.Domain] I don't know why this is here. Service needs it, but it means nothing in this context. '' conversation_log_dir str If this parameter is given, log files of the conversation will be created in this directory None enable_plotting bool If this is set to True, the recorder is no longer real time able and thus the recordings don't work properly. This is just to be used to tune the threshold for the end of utterance detection, not during deployment. False threshold int The threshold below which the assumption of the end of utterance detection is silence 8000 voice_privacy bool Whether or not to enable the masking of the users voice False identifier str I don't know why this is here. Service needs it. None Source code in adviser/services/hci/speech/SpeechRecorder.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , domain : Union [ str , Domain ] = \"\" , conversation_log_dir : str = None , enable_plotting : bool = False , threshold : int = 8000 , voice_privacy : bool = False , identifier : str = None ) -> None : \"\"\" A service that can record a microphone upon a key pressing event and publish the result as an array. The end of the utterance is detected automatically, also the voice can be masked to alleviate privacy issues. Args: domain (Domain): I don't know why this is here. Service needs it, but it means nothing in this context. conversation_log_dir (string): If this parameter is given, log files of the conversation will be created in this directory enable_plotting (boolean): If this is set to True, the recorder is no longer real time able and thus the recordings don't work properly. This is just to be used to tune the threshold for the end of utterance detection, not during deployment. threshold (int): The threshold below which the assumption of the end of utterance detection is silence voice_privacy (boolean): Whether or not to enable the masking of the users voice identifier (string): I don't know why this is here. Service needs it. \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) self . conversation_log_dir = conversation_log_dir self . recording_indicator = False self . audio_interface = pyaudio . PyAudio () self . push_to_talk_listener = keyboard . Listener ( on_press = self . start_recording ) self . threshold = threshold self . enable_plotting = enable_plotting self . voice_privacy = voice_privacy","title":"__init__()"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder.SpeechRecorder.record_user_utterance","text":"Source code in adviser/services/hci/speech/SpeechRecorder.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"record_user_utterance()"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder.SpeechRecorder.start_recorder","text":"Starts the listener and outputs that the speech recorder is ready for use Source code in adviser/services/hci/speech/SpeechRecorder.py 137 138 139 140 141 142 143 def start_recorder ( self ): \"\"\" Starts the listener and outputs that the speech recorder is ready for use \"\"\" self . push_to_talk_listener . start () print ( \"To speak to the system, tap your right [CTRL] or [CMD] key. \\n \" \"It will try to automatically detect when your utterance is over. \\n \" )","title":"start_recorder()"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder.SpeechRecorder.start_recording","text":"This method is a callback of the push to talk key listener. It calls the recorder, if it's not already recording. Parameters: Name Type Description Default key Key The pressed key required Source code in adviser/services/hci/speech/SpeechRecorder.py 126 127 128 129 130 131 132 133 134 135 def start_recording ( self , key ): \"\"\" This method is a callback of the push to talk key listener. It calls the recorder, if it's not already recording. Args: key (Key): The pressed key \"\"\" if ( key is keyboard . Key . cmd_r or key is keyboard . Key . ctrl_r ) and not self . recording_indicator : self . record_user_utterance ()","title":"start_recording()"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder.SpeechRecorder.threshold_plotter_generator","text":"Generates a plotter to visualize when the signal is above the set threshold Returns: Type Description function Plots the threshold with the current continuous waveform Source code in adviser/services/hci/speech/SpeechRecorder.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def threshold_plotter_generator ( self ): \"\"\" Generates a plotter to visualize when the signal is above the set threshold Returns: function: Plots the threshold with the current continuous waveform \"\"\" import matplotlib matplotlib . use ( 'TkAgg' ) plt . figure ( figsize = ( 10 , 2 )) plt . axhline ( y = self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . axhline ( y =- self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . pause ( 0.000000000001 ) def threshold_plotter ( data ): plt . clf () plt . tight_layout () plt . axis ([ 0 , len ( data ), - 20000 , 20000 ]) plt . plot ( data , color = 'b' ) plt . axhline ( y = self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . axhline ( y =- self . threshold , xmin = 0.0 , xmax = 1.0 , color = 'r' ) plt . pause ( 0.000000000001 ) return threshold_plotter","title":"threshold_plotter_generator()"},{"location":"api/services/#adviser.services.hci.speech.SpeechRecorder.voice_sanitizer","text":"While this is by no means a good voice sanitizer, it works as a proof of concept. It randomly shifts the spectrogram of a speakers utterance up or down, making automatic speaker identification much harder while keeping impact on asr performance as low as possible. The use should be turned off by default. Parameters: Name Type Description Default audio np.array The audio represented as array required Returns: Type Description np.array The mutated audio as array Source code in adviser/services/hci/speech/SpeechRecorder.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def voice_sanitizer ( audio ): \"\"\" While this is by no means a good voice sanitizer, it works as a proof of concept. It randomly shifts the spectrogram of a speakers utterance up or down, making automatic speaker identification much harder while keeping impact on asr performance as low as possible. The use should be turned off by default. Args: audio (np.array): The audio represented as array Returns: np.array: The mutated audio as array \"\"\" spectrogram = librosa . stft ( audio ) voice_shift = np . random . randint ( 3 , 6 ) if np . random . choice ([ True , False ]): for frequency_index , _ in enumerate ( spectrogram ): # mutate the voice to be higher try : spectrogram [ len ( spectrogram ) - ( frequency_index + 1 )] = spectrogram [ len ( spectrogram ) - ( frequency_index + 1 + voice_shift )] except IndexError : pass else : for frequency_index , _ in enumerate ( spectrogram ): # mutate the voice to be lower try : spectrogram [ frequency_index ] = spectrogram [ frequency_index + voice_shift ] except IndexError : pass return librosa . istft ( spectrogram )","title":"voice_sanitizer()"},{"location":"api/services/#adviser.services.hci.video","text":"","title":"video"},{"location":"api/services/#adviser.services.hci.video.FeatureExtractor","text":"Feature extraction with openSMILE","title":"FeatureExtractor"},{"location":"api/services/#adviser.services.hci.video.FeatureExtractor.VideoFeatureExtractor","text":"TODO","title":"VideoFeatureExtractor"},{"location":"api/services/#adviser.services.hci.video.FeatureExtractor.VideoFeatureExtractor.__init__","text":"Source code in adviser/services/hci/video/FeatureExtractor.py 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , domain : Domain = \"\" ): Service . __init__ ( self , domain = domain ) self . module_dir = os . path . dirname ( os . path . abspath ( __file__ )) # # CLAHE (Contrast Limited Adaptive Histogram Equalization) self . CLAHE = cv2 . createCLAHE ( clipLimit = 2.0 , tileGridSize = ( 8 , 8 )) # for detecting faces (returns coordinates of rectangle(s) of face area(s)) self . DETECTOR = dlib . get_frontal_face_detector () # facial landmark predictor predictor_file = os . path . abspath ( os . path . join ( self . module_dir , '..' , '..' , '..' , 'resources' , 'models' , 'video' , 'shape_predictor_68_face_landmarks.dat' )) self . PREDICTOR = dlib . shape_predictor ( predictor_file )","title":"__init__()"},{"location":"api/services/#adviser.services.hci.video.FeatureExtractor.VideoFeatureExtractor.extract_fl_features","text":"Source code in adviser/services/hci/video/FeatureExtractor.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"extract_fl_features()"},{"location":"api/services/#adviser.services.hci.video.VideoInput","text":"","title":"VideoInput"},{"location":"api/services/#adviser.services.hci.video.VideoInput.VideoInput","text":"Captures frames with a specified capture interval between two consecutive dialog turns and returns a list of frames.","title":"VideoInput"},{"location":"api/services/#adviser.services.hci.video.VideoInput.VideoInput.__init__","text":"Parameters: Name Type Description Default camera_id int device id (if only 1 camera device is connected, id is 0, if two are connected choose between 0 and 1, ...) 0 capture_interval int try to capture a frame every x microseconds - is a lower bound, no hard time guarantees (e.g. 5e5 -> every >= 0.5 seconds) 1000000.0 Source code in adviser/services/hci/video/VideoInput.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , domain = None , camera_id : int = 0 , capture_interval : int = 10e5 , identifier : str = None ): \"\"\" Args: camera_id (int): device id (if only 1 camera device is connected, id is 0, if two are connected choose between 0 and 1, ...) capture_interval (int): try to capture a frame every x microseconds - is a lower bound, no hard time guarantees (e.g. 5e5 -> every >= 0.5 seconds) \"\"\" Service . __init__ ( self , domain , identifier = identifier ) self . cap = cv2 . VideoCapture ( camera_id ) # get handle to camera device if not self . cap . isOpened (): self . cap . open () # open self . terminating = Event () self . terminating . clear () self . capture_thread = Thread ( target = self . capture ) # create thread object for capturing self . capture_interval = capture_interval","title":"__init__()"},{"location":"api/services/#adviser.services.hci.video.VideoInput.VideoInput.capture","text":"Continuous video capture, meant to be run in a loop. Calls publish_img once per interval tick to publish the captured image. Source code in adviser/services/hci/video/VideoInput.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def capture ( self ): \"\"\" Continuous video capture, meant to be run in a loop. Calls `publish_img` once per interval tick to publish the captured image. \"\"\" while self . cap . isOpened () and not self . terminating . isSet (): start_time = datetime . datetime . now () # Capture frame-by-frame # cap.read() returns a bool (true when frame was read correctly) ret , frame = self . cap . read () # Our operations on the frame come here if ret : # rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) self . publish_img ( rgb_img = frame ) end_time = datetime . datetime . now () time_diff = end_time - start_time wait_seconds = ( self . capture_interval - time_diff . microseconds ) * 1e-6 # note: time to wait for next capture to match specified sampling rate in seconds if wait_seconds > 0.0 : time . sleep ( wait_seconds ) if self . cap . isOpened (): self . cap . release ()","title":"capture()"},{"location":"api/services/#adviser.services.hci.video.VideoInput.VideoInput.dialog_end","text":"This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. Source code in adviser/services/hci/video/VideoInput.py 76 77 def dialog_end ( self ): self . terminating . set ()","title":"dialog_end()"},{"location":"api/services/#adviser.services.hci.video.VideoInput.VideoInput.dialog_start","text":"This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/hci/video/VideoInput.py 79 80 81 82 def dialog_start ( self ): if not self . capture_thread . is_alive (): print ( \"Starting video capture...\" ) self . capture_thread . start ()","title":"dialog_start()"},{"location":"api/services/#adviser.services.hci.video.VideoInput.VideoInput.publish_img","text":"Source code in adviser/services/hci/video/VideoInput.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"publish_img()"},{"location":"api/services/#adviser.services.nlg","text":"","title":"nlg"},{"location":"api/services/#adviser.services.nlg.affective_nlg","text":"Handcrafted (i.e. template-based) Natural Language Generation Module","title":"affective_nlg"},{"location":"api/services/#adviser.services.nlg.affective_nlg.HandcraftedEmotionNLG","text":"A child of the HandcraftedNLG, the HandcraftedEmotionNLG can choose between multiple affective response templates for each sys_act dependingon the current sys_emotion","title":"HandcraftedEmotionNLG"},{"location":"api/services/#adviser.services.nlg.affective_nlg.HandcraftedEmotionNLG.__init__","text":"Source code in adviser/services/nlg/affective_nlg.py 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , domain : Domain , sub_topic_domains = {}, template_file : str = None , logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , emotions : List [ str ] = [], debug_logger = None ): \"\"\"Constructor mainly extracts methods and rules from the template file\"\"\" Service . __init__ ( self , domain = domain , sub_topic_domains = sub_topic_domains , debug_logger = debug_logger ) self . domain = domain self . template_filename = template_file self . templates = {} self . logger = logger self . emotions = emotions self . _initialise_templates ()","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.affective_nlg.HandcraftedEmotionNLG.generate_system_utterance","text":"Source code in adviser/services/nlg/affective_nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"generate_system_utterance()"},{"location":"api/services/#adviser.services.nlg.bc_nlg","text":"Handcrafted (i.e. template-based) Natural Language Generation Module with backchannel","title":"bc_nlg"},{"location":"api/services/#adviser.services.nlg.bc_nlg.BackchannelHandcraftedNLG","text":"Handcrafted (i.e. template-based) Natural Language Generation Module A rule - based approach on natural language generation . The rules have to be specified within a template file using the ADVISER NLG syntax . Python methods that are called within a template file must be specified in the HandcraftedNLG class by using the prefix \"_template_\" . For example , the method \"_template_genitive_s\" can be accessed in the template file via calling { genitive_s ( name ) } !!! attributes domain ( Domain ) : the domain template_filename ( str ) : the NLG template filename templates ( TemplateFile ) : the parsed and ready - to - go NLG template file template_english ( str ) : the name of the English NLG template file template_german ( str ) : the name of the German NLG template file language ( Language ) : the language of the dialogue","title":"BackchannelHandcraftedNLG"},{"location":"api/services/#adviser.services.nlg.bc_nlg.BackchannelHandcraftedNLG.__init__","text":"Source code in adviser/services/nlg/bc_nlg.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , domain : Domain , sub_topic_domains : Dict [ str , str ] = {}, template_file : str = None , logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): \"\"\"Constructor mainly extracts methods and rules from the template file\"\"\" HandcraftedNLG . __init__ ( self , domain , template_file = None , logger = DiasysLogger (), template_file_german = None , language = None , sub_topic_domains = sub_topic_domains ) # class_int_mapping = {0: b'no_bc', 1: b'assessment', 2: b'continuer'} self . backchannels = { 0 : [ '' ], 1 : [ 'Okay. ' , 'Yeah. ' ], 2 : [ 'Um-hum. ' , 'Uh-huh. ' ] }","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.bc_nlg.BackchannelHandcraftedNLG.publish_system_utterance","text":"Source code in adviser/services/nlg/bc_nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"publish_system_utterance()"},{"location":"api/services/#adviser.services.nlg.nlg","text":"Handcrafted (i.e. template-based) Natural Language Generation Module","title":"nlg"},{"location":"api/services/#adviser.services.nlg.nlg.HandcraftedNLG","text":"Handcrafted (i.e. template-based) Natural Language Generation Module A rule - based approach on natural language generation . The rules have to be specified within a template file using the ADVISER NLG syntax . Python methods that are called within a template file must be specified in the HandcraftedNLG class by using the prefix \"_template_\" . For example , the method \"_template_genitive_s\" can be accessed in the template file via calling { genitive_s ( name ) } !!! attributes domain ( Domain ) : the domain template_filename ( str ) : the NLG template filename templates ( TemplateFile ) : the parsed and ready - to - go NLG template file template_english ( str ) : the name of the English NLG template file template_german ( str ) : the name of the German NLG template file language ( Language ) : the language of the dialogue","title":"HandcraftedNLG"},{"location":"api/services/#adviser.services.nlg.nlg.HandcraftedNLG.__init__","text":"Constructor mainly extracts methods and rules from the template file Source code in adviser/services/nlg/nlg.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , domain : Domain , template_file : str = None , sub_topic_domains : Dict [ str , str ] = {}, logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): \"\"\"Constructor mainly extracts methods and rules from the template file\"\"\" Service . __init__ ( self , domain = domain , sub_topic_domains = sub_topic_domains ) self . language = language if language else Language . ENGLISH self . template_english = template_file # TODO: at some point if we expand languages, maybe make kwargs? --LV self . template_german = template_file_german self . domain = domain self . template_filename = None self . templates = None self . logger = logger self . language = Language . ENGLISH self . _initialise_language ( self . language )","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.nlg.HandcraftedNLG.generate_system_utterance","text":"Main function of the NLG module Takes a system act, searches for a fitting rule, applies it and returns the message. Overwrite this function if you inherit from the NLG module. Parameters: Name Type Description Default sys_act SysAct The system act None Returns: Type Description str The utterance generated by applying a fitting template Source code in adviser/services/nlg/nlg.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def generate_system_utterance ( self , sys_act : SysAct = None ) -> str : \"\"\"Main function of the NLG module Takes a system act, searches for a fitting rule, applies it and returns the message. Overwrite this function if you inherit from the NLG module. Args: sys_act (SysAct): The system act Returns: The utterance generated by applying a fitting template \"\"\" rule_found = True message = \"\" try : message = self . templates . create_message ( sys_act ) except BaseException as error : rule_found = False self . logger . error ( error ) raise ( error ) # inform if no applicable rule could be found in the template file if not rule_found : self . logger . info ( 'Could not find a fitting rule for the given system act!' ) self . logger . info ( \"System Action: \" + str ( sys_act . type ) + \" - Slots: \" + str ( sys_act . slot_values )) # self.logger.dialog_turn(\"System Action: \" + message) return message","title":"generate_system_utterance()"},{"location":"api/services/#adviser.services.nlg.nlg.HandcraftedNLG.publish_system_utterance","text":"Source code in adviser/services/nlg/nlg.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"publish_system_utterance()"},{"location":"api/services/#adviser.services.nlg.templates","text":"","title":"templates"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions","text":"","title":"builtinfunctions"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryFunction","text":"","title":"ForEntryFunction"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryFunction.__init__","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 43 44 45 def __init__ ( self , global_memory ): Function . __init__ ( self , 'for_entry(slots, function, separator_first, separator_last)' ) self . global_memory = global_memory","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryFunction.apply","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def apply ( self , parameters : Memory = None ) -> str : function = parameters . get_function ( parameters . variables [ 1 ] . value ) extra_arguments = [ variable . value for variable in parameters . variables [ 4 :]] texts : List [ str ] = [] for slot_value_pair in parameters . variables [ 0 ] . value : memory = self . _build_memory ( slot_value_pair [ 0 ], slot_value_pair [ 1 ], extra_arguments ) if not function . is_applicable ( memory ): raise BaseException ( f 'The function { function . function_name } could not be called ' f 'from the for_entry function' ) texts . append ( function . apply ( memory )) return self . _create_text_from_elements ( texts , parameters . variables [ 2 ] . value , parameters . variables [ 3 ] . value )","title":"apply()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryFunction.is_applicable","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 47 48 def is_applicable ( self , parameters : Memory ) -> bool : return len ( parameters . variables ) >= 4","title":"is_applicable()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryListFunction","text":"","title":"ForEntryListFunction"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryListFunction.__init__","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 128 129 130 131 def __init__ ( self , global_memory : GlobalMemory ): Function . __init__ ( self , 'for_entry_list(slots, function, value_sep, value_sep_last, ' 'slot_sep, slot_sep_last)' ) self . global_memory = global_memory","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryListFunction.apply","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def apply ( self , parameters : Memory = None ) -> str : function = parameters . get_function ( parameters . variables [ 1 ] . value ) extra_arguments = [ variable . value for variable in parameters . variables [ 6 :]] texts_per_slot : List [ str ] = [] for slot_values_pair in parameters . variables [ 0 ] . value : slot_texts : List [ str ] = [] for value in slot_values_pair [ 1 ]: memory = self . _build_memory ( slot_values_pair [ 0 ], value , extra_arguments ) if not function . is_applicable ( memory ): raise BaseException ( f 'The function { function . function_name } could not be ' f 'called from the for_entry_list function' ) slot_texts . append ( function . apply ( memory )) text = self . _create_text_from_elements ( slot_texts , parameters . variables [ 2 ] . value , parameters . variables [ 3 ] . value ) texts_per_slot . append ( text ) return self . _create_text_from_elements ( texts_per_slot , parameters . variables [ 4 ] . value , parameters . variables [ 5 ] . value )","title":"apply()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForEntryListFunction.is_applicable","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 133 134 def is_applicable ( self , parameters : Memory ) -> bool : return len ( parameters . variables ) >= 6","title":"is_applicable()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForFunction","text":"","title":"ForFunction"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForFunction.__init__","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 86 87 88 def __init__ ( self , global_memory ): Function . __init__ ( self , 'for(values, function, separator_first, separator_last, *args)' ) self . global_memory = global_memory","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForFunction.apply","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def apply ( self , parameters : Memory = None ) -> str : function = parameters . get_function ( parameters . variables [ 1 ] . value ) extra_arguments = [ variable . value for variable in parameters . variables [ 4 :]] texts : List [ str ] = [] for value in parameters . variables [ 0 ] . value : memory = self . _build_memory ( value , extra_arguments ) if not function . is_applicable ( memory ): raise BaseException ( f 'The function { function . function_name } could not be called ' f 'from the for function' ) texts . append ( function . apply ( memory )) return self . _create_text_from_elements ( texts , parameters . variables [ 2 ] . value , parameters . variables [ 3 ] . value )","title":"apply()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.ForFunction.is_applicable","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 90 91 def is_applicable ( self , parameters : Memory ) -> bool : return len ( parameters . variables ) >= 4","title":"is_applicable()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.PythonFunction","text":"","title":"PythonFunction"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.PythonFunction.__init__","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 27 28 29 30 31 def __init__ ( self , function_name : str , function_to_call : Callable , obligatory_arguments : List [ object ] = []): Function . __init__ ( self , f ' { function_name } ()' ) self . function = function_to_call self . obligatory_arguments = obligatory_arguments","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.PythonFunction.apply","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 36 37 38 39 def apply ( self , parameters : Memory = None ) -> str : arguments = self . obligatory_arguments . copy () arguments . extend ([ variable . value for variable in parameters . variables ]) return self . function ( * arguments )","title":"apply()"},{"location":"api/services/#adviser.services.nlg.templates.builtinfunctions.PythonFunction.is_applicable","text":"Source code in adviser/services/nlg/templates/builtinfunctions.py 33 34 def is_applicable ( self , parameters : Memory ) -> bool : return True","title":"is_applicable()"},{"location":"api/services/#adviser.services.nlg.templates.parsing","text":"","title":"parsing"},{"location":"api/services/#adviser.services.nlg.templates.parsing.automaton","text":"","title":"automaton"},{"location":"api/services/#adviser.services.nlg.templates.parsing.automaton.ModifiedPushdownAutomaton","text":"__init__ ( self , start_state , accept_states , state_descriptions ) special Source code in adviser/services/nlg/templates/parsing/automaton.py 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , start_state : State , accept_states : List [ State ], state_descriptions : List [ StateDescription ]): self . start_state = start_state self . accept_states = accept_states self . state_descriptions = state_descriptions self . state_transition_mapping = self . _create_state_transition_mapping () self . state_default_transition_mapping = self . _create_state_default_transition_mapping () self . stack = AutomatonStack () parse ( self , input_tape ) Source code in adviser/services/nlg/templates/parsing/automaton.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def parse ( self , input_tape : str ) -> List [ object ]: self . stack . clear () current_state = self . start_state input_tape_index = 0 for input_char in input_tape : try : configuration = Configuration ( current_state , input_char ) transition = self . _find_transition ( configuration ) current_state = self . _apply_transition ( transition , configuration ) input_tape_index += 1 except ParsingError as error : print ( 'State:' , current_state . name ) print ( 'Index:' , input_tape_index ) print ( 'Original Input:' , input_tape ) raise error if current_state not in self . accept_states : print ( 'State:' , current_state . name ) raise ParsingError ( f 'Parser was not in a final state after the input tape was read.' ) return self . stack . data_stack [:]","title":"ModifiedPushdownAutomaton"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration","text":"","title":"configuration"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.Configuration","text":"__init__ ( self , state , character ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 37 38 39 def __init__ ( self , state : State , character : str ): self . state = state self . character = character","title":"Configuration"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.DefaultTransition","text":"__init__ ( self , state ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 54 55 def __init__ ( self , state : State ): Transition . __init__ ( self , Configuration ( state , None ))","title":"DefaultTransition"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.SimpleForwardDefaultTransition","text":"__init__ ( self , state ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 67 68 def __init__ ( self , state : State ): DefaultTransition . __init__ ( self , state ) get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 70 71 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : return Configuration ( input_configuration . state , input_configuration . character ) perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 73 74 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): pass","title":"SimpleForwardDefaultTransition"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.State","text":"__eq__ ( self , other ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 29 30 def __eq__ ( self , other ): return isinstance ( other , State ) and other . name == self . name __hash__ ( self ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 32 33 def __hash__ ( self ): return hash ( self . name ) __init__ ( self , name ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 26 27 def __init__ ( self , name ): self . name = name","title":"State"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.StateDescription","text":"__init__ ( self , default_state , default_transition , transitions ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 59 60 61 62 63 def __init__ ( self , default_state : State , default_transition : DefaultTransition , transitions : List [ Transition ]): self . default_state = default_state self . default_transition = default_transition self . transitions = transitions","title":"StateDescription"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.Transition","text":"__init__ ( self , input_configuration ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 43 44 def __init__ ( self , input_configuration : Configuration ): self . input_configuration = input_configuration get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 46 47 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : raise NotImplementedError () perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 49 50 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): raise NotImplementedError ()","title":"Transition"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.TransitionWithAction","text":"__init__ ( self , input_configuration , output_configuration , action ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 90 91 92 93 94 def __init__ ( self , input_configuration : Configuration , output_configuration : Configuration , action : Callable [[ AutomatonStack ], None ]): Transition . __init__ ( self , input_configuration ) self . output_configuration = output_configuration self . action = action get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 96 97 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : return self . output_configuration perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 99 100 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): self . action ( stack )","title":"TransitionWithAction"},{"location":"api/services/#adviser.services.nlg.templates.parsing.configuration.TransitionWithoutAction","text":"__init__ ( self , input_configuration , output_configuration ) special Source code in adviser/services/nlg/templates/parsing/configuration.py 78 79 80 def __init__ ( self , input_configuration : Configuration , output_configuration : Configuration ): Transition . __init__ ( self , input_configuration ) self . output_configuration = output_configuration get_output_configuration ( self , input_configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 82 83 def get_output_configuration ( self , input_configuration : Configuration ) -> Configuration : return self . output_configuration perform_stack_action ( self , stack , configuration ) Source code in adviser/services/nlg/templates/parsing/configuration.py 85 86 def perform_stack_action ( self , stack : AutomatonStack , configuration : Configuration ): pass","title":"TransitionWithoutAction"},{"location":"api/services/#adviser.services.nlg.templates.parsing.exceptions","text":"","title":"exceptions"},{"location":"api/services/#adviser.services.nlg.templates.parsing.exceptions.ParsingError","text":"__init__ ( self , message ) special Source code in adviser/services/nlg/templates/parsing/exceptions.py 21 22 def __init__ ( self , message ): Exception . __init__ ( self , message )","title":"ParsingError"},{"location":"api/services/#adviser.services.nlg.templates.parsing.parsers","text":"","title":"parsers"},{"location":"api/services/#adviser.services.nlg.templates.parsing.parsers.messageparser","text":"messageparser MessageParser __init__ ( self ) special Source code in adviser/services/nlg/templates/parsing/parsers/messageparser/messageparser.py 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self ): ModifiedPushdownAutomaton . __init__ ( self , StartState (), [ AcceptState ()], [ StartStateDescription (), AcceptStateDescription (), MessageStateDescription (), EscapeStateDescription (), CodeStateDescription (), AdviserStateDescription (), PythonStateDescription (), PythonClosingBraceStateDescription (), CodeStringStateDescription () ])","title":"messageparser"},{"location":"api/services/#adviser.services.nlg.templates.parsing.stack","text":"","title":"stack"},{"location":"api/services/#adviser.services.nlg.templates.parsing.stack.AutomatonStack","text":"__init__ ( self ) special Source code in adviser/services/nlg/templates/parsing/stack.py 24 25 26 27 def __init__ ( self ): # self.char_stack = [] # the automaton's stack self . data_stack = [] # the stack in which custom data structures can be stored self . levels = [[]] # multiple automaton stacks are possible here add_char ( self , stack_char ) Source code in adviser/services/nlg/templates/parsing/stack.py 29 30 31 32 def add_char ( self , stack_char : str ): if not self . levels : raise ParsingError ( 'No more levels left on the stack' ) self . levels [ - 1 ] . append ( stack_char ) add_data ( self , data ) Source code in adviser/services/nlg/templates/parsing/stack.py 34 35 def add_data ( self , data : object ): self . data_stack . append ( data ) add_level ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 43 44 def add_level ( self ): self . levels . append ([]) clear ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 56 57 58 def clear ( self ): self . data_stack = [] self . levels = [[]] fetch_data ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 40 41 def fetch_data ( self ) -> object : return self . data_stack [ - 1 ] get_current_content ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 46 47 48 49 def get_current_content ( self ) -> str : if not self . levels : raise ParsingError ( 'No more levels left on the stack' ) return '' . join ( self . levels [ - 1 ]) pop_data ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 37 38 def pop_data ( self ) -> object : return self . data_stack . pop ( - 1 ) remove_level ( self ) Source code in adviser/services/nlg/templates/parsing/stack.py 51 52 53 54 def remove_level ( self ): if not self . levels : raise ParsingError ( 'No more levels to remove from the stack' ) self . levels . pop ()","title":"AutomatonStack"},{"location":"api/services/#adviser.services.nlg.templates.preprocessing","text":"","title":"preprocessing"},{"location":"api/services/#adviser.services.nlg.templates.templatefile","text":"","title":"templatefile"},{"location":"api/services/#adviser.services.nlg.templates.templatefile.TemplateFile","text":"Interprets a template file !!! attributes global_memory {GlobalMemory} -- memory that can be accessed at all times in the tempaltes","title":"TemplateFile"},{"location":"api/services/#adviser.services.nlg.templates.templatefile.TemplateFile.__init__","text":"Source code in adviser/services/nlg/templates/templatefile.py 63 64 65 66 67 68 def __init__ ( self , filename : str , domain : JSONLookupDomain ): self . global_memory = GlobalMemory ( domain ) self . _add_built_in_functions () tfr = _TemplateFileReader ( filename ) self . _templates = self . _create_template_dict ( tfr . get_templates ()) self . _add_functions_to_global_memory ( tfr . get_functions ())","title":"__init__()"},{"location":"api/services/#adviser.services.nlg.templates.templatefile.TemplateFile.add_python_function","text":"Add a python function to the global memory of the template file interpreter Keyword Arguments: obligatory_arguments {List[object]} -- objects that are always passed as first arguments to the python function, e.g. \"self\" (default: {[]}) Source code in adviser/services/nlg/templates/templatefile.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def add_python_function ( self , function_name : str , python_function : Callable [[ object ], str ], obligatory_arguments : List [ object ] = []): \"\"\"Add a python function to the global memory of the template file interpreter Arguments: function_name {str} -- name under which the function can be accessed in template file python_function {Callable[[object], str]} -- python function which is called when being accessed in the template file Keyword Arguments: obligatory_arguments {List[object]} -- objects that are always passed as first arguments to the python function, e.g. \"self\" (default: {[]}) \"\"\" self . global_memory . add_function ( PythonFunction ( function_name , python_function , obligatory_arguments ))","title":"add_python_function()"},{"location":"api/services/#adviser.services.nlg.templates.templatefile.TemplateFile.create_message","text":"Iterates through all possible templates and applies the first one to fit the system act Exceptions: Type Description BaseException when no template could be applied Returns: Type Description str str -- the message returned by the template Source code in adviser/services/nlg/templates/templatefile.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def create_message ( self , sys_act : SysAct ) -> str : \"\"\"Iterates through all possible templates and applies the first one to fit the system act Arguments: sys_act {SysAct} -- the system act to find a template for Raises: BaseException: when no template could be applied Returns: str -- the message returned by the template \"\"\" slots = self . _create_memory_from_sys_act ( sys_act ) for template in self . _templates [ sys_act . type . value ]: if template . is_applicable ( slots ): return template . apply ( slots ) raise BaseException ( 'No template was found for the given system act.' )","title":"create_message()"},{"location":"api/services/#adviser.services.nlu","text":"","title":"nlu"},{"location":"api/services/#adviser.services.nlu.nlu","text":"","title":"nlu"},{"location":"api/services/#adviser.services.nlu.nlu.HandcraftedNLU","text":"Class for Handcrafted Natural Language Understanding Module (HDC-NLU). HDC-NLU is a rule-based approach to recognize the user acts as well as their respective slots and values from the user input (i.e. text) by means of regular expressions. HDC-NLU is domain-independet. The regular expressions of are read from JSON files. There exist a JSON file that stores general rules (GeneralRules.json), i.e. rules that apply to any domain, e.g. rules to detect salutation (Hello, Hi). There are two more files per domain that contain the domain-specific rules for request and inform user acts, e.g. ImsCoursesInformRules.json and ImsCoursesRequestRules.json. The output during dialog interaction of this module is a semantic representation of the user input. \"I am looking for pizza\" --> inform(slot=food,value=italian) See the regex_generator under tools, if the existing regular expressions need to be changed or a new domain should be added.","title":"HandcraftedNLU"},{"location":"api/services/#adviser.services.nlu.nlu.HandcraftedNLU.__init__","text":"Loads - domain key - informable slots - requestable slots - domain-independent regular expressions - domain-specific regualer espressions It sets the previous system act to None Source code in adviser/services/nlu/nlu.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , domain : JSONLookupDomain , logger : DiasysLogger = DiasysLogger (), language : Language = None ): \"\"\" Loads - domain key - informable slots - requestable slots - domain-independent regular expressions - domain-specific regualer espressions It sets the previous system act to None Args: domain {domain.jsonlookupdomain.JSONLookupDomain} -- Domain \"\"\" Service . __init__ ( self , domain = domain ) self . logger = logger self . language = language if language else Language . ENGLISH # Getting domain information self . domain_name = domain . get_domain_name () self . domain_key = domain . get_primary_key () # Getting lists of informable and requestable slots self . USER_INFORMABLE = domain . get_informable_slots () self . USER_REQUESTABLE = domain . get_requestable_slots () # Getting the relative path where regexes are stored self . base_folder = os . path . join ( get_root_dir (), 'resources' , 'nlu_regexes' ) # Setting previous system act to None to signal the first turn # self.prev_sys_act = None self . sys_act_info = { 'last_act' : None , 'lastInformedPrimKeyVal' : None , 'lastRequestSlot' : None } self . language = Language . ENGLISH self . _initialize ()","title":"__init__()"},{"location":"api/services/#adviser.services.nlu.nlu.HandcraftedNLU.extract_user_acts","text":"Source code in adviser/services/nlu/nlu.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"extract_user_acts()"},{"location":"api/services/#adviser.services.nlu.nlu.HandcraftedNLU.start_dialog","text":"Sets the previous system act as None. This function is called when the dialog starts Returns: Type Description dict Empty dictionary Source code in adviser/services/nlu/nlu.py 390 391 392 393 394 395 396 397 398 399 400 def start_dialog ( self ) -> dict : \"\"\" Sets the previous system act as None. This function is called when the dialog starts Returns: Empty dictionary \"\"\" self . sys_act_info = { 'last_act' : None , 'lastInformedPrimKeyVal' : None , 'lastRequestSlot' : None }","title":"start_dialog()"},{"location":"api/services/#adviser.services.nlu.nlu.get_root_dir","text":"Source code in adviser/services/nlu/nlu.py 35 36 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))","title":"get_root_dir()"},{"location":"api/services/#adviser.services.policy","text":"","title":"policy"},{"location":"api/services/#adviser.services.policy.affective_policy","text":"","title":"affective_policy"},{"location":"api/services/#adviser.services.policy.affective_policy.EmotionPolicy","text":"Module for deciding what type of emotional response/ engagement level of response, the system should give","title":"EmotionPolicy"},{"location":"api/services/#adviser.services.policy.affective_policy.EmotionPolicy.__init__","text":"Initializes the policy Parameters: Name Type Description Default domain JSONLookupDomain the domain that the affective policy should operate in None Source code in adviser/services/policy/affective_policy.py 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , domain : JSONLookupDomain = None , logger : DiasysLogger = DiasysLogger ()): \"\"\" Initializes the policy Arguments: domain (JSONLookupDomain): the domain that the affective policy should operate in \"\"\" self . first_turn = True Service . __init__ ( self , domain = domain ) self . logger = logger","title":"__init__()"},{"location":"api/services/#adviser.services.policy.affective_policy.EmotionPolicy.choose_sys_emotion","text":"Source code in adviser/services/policy/affective_policy.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"choose_sys_emotion()"},{"location":"api/services/#adviser.services.policy.affective_policy.EmotionPolicy.dialog_start","text":"This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/policy/affective_policy.py 46 47 def dialog_start ( self ): pass","title":"dialog_start()"},{"location":"api/services/#adviser.services.policy.policy_api","text":"","title":"policy_api"},{"location":"api/services/#adviser.services.policy.policy_api.HandcraftedPolicy","text":"Handcrafted policy for API domains Differs from the default HandcraftedPolicy class by taking into account mandatory slots, i.e. slots which have to be informed about before an API can even be called. The class is currently a copy of an older version of the HandcraftedPolicy class with the required changes for API usage. The classes will probably be merged in the future.","title":"HandcraftedPolicy"},{"location":"api/services/#adviser.services.policy.policy_api.HandcraftedPolicy.__init__","text":"Initializes the policy Source code in adviser/services/policy/policy_api.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , domain : LookupDomain , logger : DiasysLogger = DiasysLogger ()): \"\"\" Initializes the policy Arguments: domain {domain.lookupdomain.LookupDomain} -- Domain \"\"\" self . first_turn = True Service . __init__ ( self , domain = domain ) self . last_action = None self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation self . domain_key = domain . get_primary_key () self . logger = logger","title":"__init__()"},{"location":"api/services/#adviser.services.policy.policy_api.HandcraftedPolicy.choose_sys_act","text":"Source code in adviser/services/policy/policy_api.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"choose_sys_act()"},{"location":"api/services/#adviser.services.policy.policy_api.HandcraftedPolicy.dialog_start","text":"This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/policy/policy_api.py 124 125 126 127 128 def dialog_start ( self ): self . first_turn = True self . last_action = None self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation","title":"dialog_start()"},{"location":"api/services/#adviser.services.policy.policy_handcrafted","text":"","title":"policy_handcrafted"},{"location":"api/services/#adviser.services.policy.policy_handcrafted.HandcraftedPolicy","text":"Base class for handcrafted policies. Provides a simple rule-based policy. Can be used for any domain where a user is trying to find an entity (eg. a course from a module handbook) from a database by providing constraints (eg. semester the course is offered) or where a user is trying to find out additional information about a named entity. Output is a system action such as: * inform : provides information on an entity * request : request more information from the user * bye : issue parting message and end dialog In order to create your own policy, you can inherit from this class. Make sure to overwrite the choose_sys_act -method with whatever additionally rules/functionality required.","title":"HandcraftedPolicy"},{"location":"api/services/#adviser.services.policy.policy_handcrafted.HandcraftedPolicy.__init__","text":"Initializes the policy Source code in adviser/services/policy/policy_handcrafted.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , domain : JSONLookupDomain , logger : DiasysLogger = DiasysLogger (), max_turns : int = 25 ): \"\"\" Initializes the policy Arguments: domain {domain.jsonlookupdomain.JSONLookupDomain} -- Domain \"\"\" self . first_turn = True Service . __init__ ( self , domain = domain ) self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation self . domain_key = domain . get_primary_key () self . logger = logger self . max_turns = max_turns","title":"__init__()"},{"location":"api/services/#adviser.services.policy.policy_handcrafted.HandcraftedPolicy.choose_sys_act","text":"Source code in adviser/services/policy/policy_handcrafted.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"choose_sys_act()"},{"location":"api/services/#adviser.services.policy.policy_handcrafted.HandcraftedPolicy.dialog_start","text":"resets the policy after each dialog Source code in adviser/services/policy/policy_handcrafted.py 68 69 70 71 72 73 74 75 def dialog_start ( self ): \"\"\" resets the policy after each dialog \"\"\" self . turns = 0 self . first_turn = True self . current_suggestions = [] # list of current suggestions self . s_index = 0 # the index in current suggestions for the current system reccomendation","title":"dialog_start()"},{"location":"api/services/#adviser.services.policy.rl","text":"","title":"rl"},{"location":"api/services/#adviser.services.policy.rl.dqn","text":"","title":"dqn"},{"location":"api/services/#adviser.services.policy.rl.dqn.DQN","text":"Simple Deep Q-Network","title":"DQN"},{"location":"api/services/#adviser.services.policy.rl.dqn.DQN.__init__","text":"Initialize a DQN Network with an arbitrary amount of linear hidden layers Source code in adviser/services/policy/rl/dqn.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , state_dim : int , action_dim : int , hidden_layer_sizes : List [ int ] = [ 300 , 300 ], dropout_rate : float = 0.0 ): \"\"\" Initialize a DQN Network with an arbitrary amount of linear hidden layers \"\"\" super ( DQN , self ) . __init__ () print ( \"Architecture: DQN\" ) self . dropout_rate = dropout_rate # create layers self . layers = nn . ModuleList () current_input_dim = state_dim for layer_size in hidden_layer_sizes : self . layers . append ( nn . Linear ( current_input_dim , layer_size )) self . layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . layers . append ( nn . Dropout ( p = dropout_rate )) current_input_dim = layer_size # output layer self . layers . append ( nn . Linear ( current_input_dim , action_dim ))","title":"__init__()"},{"location":"api/services/#adviser.services.policy.rl.dqn.DQN.forward","text":"Forward pass: calculate Q(state) for all actions Parameters: Name Type Description Default state_batch FloatTensor tensor of size batch_size x state_dim required Returns: Type Description output tensor of size batch_size x action_dim Source code in adviser/services/policy/rl/dqn.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def forward ( self , state_batch : torch . FloatTensor ): \"\"\" Forward pass: calculate Q(state) for all actions Args: state_batch (torch.FloatTensor): tensor of size batch_size x state_dim Returns: output: tensor of size batch_size x action_dim \"\"\" output = state_batch for layer in self . layers : output = layer ( output ) return output","title":"forward()"},{"location":"api/services/#adviser.services.policy.rl.dqn.DuelingDQN","text":"Dueling DQN network architecture Splits network into value- and advantage stream (V(s) and A(s,a)), recombined in final layer to form Q-value again: Q(s,a) = V(s) + A(s,a).","title":"DuelingDQN"},{"location":"api/services/#adviser.services.policy.rl.dqn.DuelingDQN.__init__","text":"Source code in adviser/services/policy/rl/dqn.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def __init__ ( self , state_dim : int , action_dim : int , shared_layer_sizes : List [ int ] = [ 128 ], value_layer_sizes : List [ int ] = [ 128 ], advantage_layer_sizes : List [ int ] = [ 128 ], dropout_rate : float = 0.0 ): super ( DuelingDQN , self ) . __init__ () print ( \"ARCHITECTURE: Dueling\" ) self . dropout_rate = dropout_rate # configure layers self . shared_layers = nn . ModuleList () self . value_layers = nn . ModuleList () self . advantage_layers = nn . ModuleList () # shared layer: state_dim -> shared_layer_sizes[-1] shared_layer_dim = state_dim for layer_size in shared_layer_sizes : self . shared_layers . append ( nn . Linear ( shared_layer_dim , layer_size )) self . shared_layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . shared_layers . append ( nn . Dropout ( p = dropout_rate )) shared_layer_dim = layer_size # value layer: shared_layer_sizes[-1] -> 1 value_layer_dim = shared_layer_dim for layer_size in value_layer_sizes : self . value_layers . append ( nn . Linear ( value_layer_dim , layer_size )) self . value_layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . value_layers . append ( nn . Dropout ( p = dropout_rate )) value_layer_dim = layer_size self . value_layers . append ( nn . Linear ( value_layer_dim , 1 )) # advantage layer: shared_layer_sizes[-1] -> actions advantage_layer_dim = shared_layer_dim for layer_size in advantage_layer_sizes : self . advantage_layers . append ( nn . Linear ( advantage_layer_dim , layer_size )) self . advantage_layers . append ( nn . ReLU ()) if dropout_rate > 0.0 : self . advantage_layers . append ( nn . Dropout ( p = dropout_rate )) advantage_layer_dim = layer_size self . advantage_layers . append ( nn . Linear ( advantage_layer_dim , action_dim ))","title":"__init__()"},{"location":"api/services/#adviser.services.policy.rl.dqn.DuelingDQN.forward","text":"Forward pass: calculate Q(state) for all actions Parameters: Name Type Description Default input torch.FloatTensor tensor of size batch_size x state_dim required Returns: Type Description tensor of size batch_size x action_dim Source code in adviser/services/policy/rl/dqn.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def forward ( self , state_batch : torch . FloatTensor ): \"\"\" Forward pass: calculate Q(state) for all actions Args: input (torch.FloatTensor): tensor of size batch_size x state_dim Returns: tensor of size batch_size x action_dim \"\"\" shared_output = state_batch # shared layer representation for layer in self . shared_layers : shared_output = layer ( shared_output ) # value stream value_stream = shared_output for layer in self . value_layers : value_stream = layer ( value_stream ) # advantage stream advantage_stream = shared_output for layer in self . advantage_layers : advantage_stream = layer ( advantage_stream ) # combine value and advantage streams into Q values result = value_stream + advantage_stream - advantage_stream . mean () return result","title":"forward()"},{"location":"api/services/#adviser.services.policy.rl.dqn.NetArchitecture","text":"Network architecture for DQN vanilla: normal MLP dueling: splits network into value- and advantage stream, recombined in final layer","title":"NetArchitecture"},{"location":"api/services/#adviser.services.policy.rl.dqn.NetArchitecture.__new__","text":"Source code in adviser/services/policy/rl/dqn.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy","text":"","title":"dqnpolicy"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy","text":"","title":"DQNPolicy"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.__init__","text":"Parameters: Name Type Description Default target_update_rate int if 1, vanilla dqn update if > 1, double dqn with specified target update rate 3 Source code in adviser/services/policy/rl/dqnpolicy.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def __init__ ( self , domain : JSONLookupDomain , architecture : NetArchitecture = NetArchitecture . DUELING , hidden_layer_sizes : List [ int ] = [ 256 , 700 , 700 ], # vanilla architecture shared_layer_sizes : List [ int ] = [ 256 ], value_layer_sizes : List [ int ] = [ 300 , 300 ], advantage_layer_sizes : List [ int ] = [ 400 , 400 ], # dueling architecture lr : float = 0.0001 , discount_gamma : float = 0.99 , target_update_rate : int = 3 , replay_buffer_size : int = 8192 , batch_size : int = 64 , buffer_cls : Type [ Buffer ] = NaivePrioritizedBuffer , eps_start : float = 0.3 , eps_end : float = 0.0 , l2_regularisation : float = 0.0 , gradient_clipping : float = 5.0 , p_dropout : float = 0.0 , training_frequency : int = 2 , train_dialogs : int = 1000 , include_confreq : bool = False , logger : DiasysLogger = DiasysLogger (), max_turns : int = 25 , summary_writer : SummaryWriter = None , device = torch . device ( 'cpu' )): \"\"\" Args: target_update_rate: if 1, vanilla dqn update if > 1, double dqn with specified target update rate \"\"\" RLPolicy . __init__ ( self , domain , buffer_cls = buffer_cls , buffer_size = replay_buffer_size , batch_size = batch_size , discount_gamma = discount_gamma , include_confreq = include_confreq , logger = logger , max_turns = max_turns , device = device ) Service . __init__ ( self , domain = domain ) self . writer = summary_writer self . training_frequency = training_frequency self . train_dialogs = train_dialogs self . lr = lr self . gradient_clipping = gradient_clipping if gradient_clipping > 0.0 and self . logger : self . logger . info ( \"Gradient Clipping: \" + str ( gradient_clipping )) self . target_update_rate = target_update_rate self . epsilon_start = eps_start self . epsilon_end = eps_end # Select network architecture if architecture == NetArchitecture . VANILLA : if self . logger : self . logger . info ( \"Architecture: Vanilla\" ) self . model = DQN ( self . state_dim , self . action_dim , hidden_layer_sizes = hidden_layer_sizes , dropout_rate = p_dropout ) else : if self . logger : self . logger . info ( \"Architecture: Dueling\" ) self . model = DuelingDQN ( self . state_dim , self . action_dim , shared_layer_sizes = shared_layer_sizes , value_layer_sizes = value_layer_sizes , advantage_layer_sizes = advantage_layer_sizes , dropout_rate = p_dropout ) # Select network update self . target_model = None if target_update_rate > 1 : if self . logger : self . logger . info ( \"Update: Double\" ) if architecture == NetArchitecture . VANILLA : self . target_model = copy . deepcopy ( self . model ) elif self . logger : self . logger . info ( \"Update: Vanilla\" ) self . optim = optim . Adam ( self . model . parameters (), lr = lr , weight_decay = l2_regularisation ) self . loss_fun = nn . SmoothL1Loss ( reduction = 'none' ) # self.loss_fun = nn.MSELoss(reduction='none') self . train_call_count = 0 self . total_train_dialogs = 0 self . epsilon = self . epsilon_start self . turns = 0 self . cumulative_train_dialogs = - 1","title":"__init__()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.choose_sys_act","text":"Source code in adviser/services/policy/rl/dqnpolicy.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"choose_sys_act()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.dialog_end","text":"clean up needed at the end of a dialog Source code in adviser/services/policy/rl/dqnpolicy.py 163 164 165 166 167 168 169 170 def dialog_end ( self ): \"\"\" clean up needed at the end of a dialog \"\"\" self . end_dialog ( self . sim_goal ) if self . is_training : self . total_train_dialogs += 1 self . train_batch ()","title":"dialog_end()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.dialog_start","text":"This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/policy/rl/dqnpolicy.py 121 122 123 124 125 126 127 128 129 130 def dialog_start ( self , dialog_start = False ): self . turns = 0 self . last_sys_act = None if self . is_training : self . cumulative_train_dialogs += 1 self . sys_state = { \"lastInformedPrimKeyVal\" : None , \"lastActionInformNone\" : False , \"offerHappened\" : False , 'informedPrimKeyValsSinceNone' : []}","title":"dialog_start()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.end","text":"Source code in adviser/services/policy/rl/dqnpolicy.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"end()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.eps_scheduler","text":"Linear epsilon decay Source code in adviser/services/policy/rl/dqnpolicy.py 377 378 379 380 381 382 383 384 def eps_scheduler ( self ): \"\"\" Linear epsilon decay \"\"\" if self . is_training : self . epsilon = max ( 0 , self . epsilon_start - ( self . epsilon_start - self . epsilon_end ) * float ( self . num_dialogs ) / float ( self . train_dialogs )) if self . writer is not None : self . writer . add_scalar ( 'train/eps' , self . epsilon , self . total_train_dialogs )","title":"eps_scheduler()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.eval","text":"Sets module and its subgraph to eval mode Source code in adviser/services/policy/rl/dqnpolicy.py 425 426 427 428 429 430 431 def eval ( self , eval = True ): \"\"\" Sets module and its subgraph to eval mode \"\"\" super ( DQNPolicy , self ) . eval () self . is_training = False self . model . eval () if self . target_model is not None : self . target_model . eval ()","title":"eval()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.load","text":"Load model weights Parameters: Name Type Description Default path str path to model folder 'models/dqn' version str appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) '1.0' Source code in adviser/services/policy/rl/dqnpolicy.py 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def load ( self , path : str = os . path . join ( 'models' , 'dqn' ), version : str = \"1.0\" ): \"\"\" Load model weights Args: path (str): path to model folder version (str): appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) \"\"\" model_file = os . path . join ( path , \"rlpolicy_\" + self . domain . get_domain_name () + \"_\" + version + \".pt\" ) if not os . path . isfile ( model_file ): raise FileNotFoundError ( \"Could not find DQN policy weight file \" , model_file ) self . model = torch . load ( model_file ) self . logger . info ( \"Loaded DQN weights from file \" + model_file ) if self . target_model is not None : self . target_model . load_state_dict ( self . model . state_dict ())","title":"load()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.loss","text":"Calculate TD-loss for given experience tuples Parameters: Name Type Description Default s_batch FloatTensor states (dimension batch x state_dim) required a_batch LongTensor actions (dimension batch x 1) required s2_batch FloatTensor next states (dimension: batch x state_dim) required r_batch FloatTensor rewards (dimension batch x 1) required t_batch FloatTensor indicator {0,1} for terminal states (dimension: batch x 1) required gamma float discount factor required Returns: Type Description TD-loss Source code in adviser/services/policy/rl/dqnpolicy.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def loss ( self , s_batch : torch . FloatTensor , a_batch : torch . LongTensor , s2_batch : torch . FloatTensor , r_batch : torch . FloatTensor , t_batch : torch . FloatTensor , gamma : float ): \"\"\" Calculate TD-loss for given experience tuples Args: s_batch (torch.FloatTensor): states (dimension batch x state_dim) a_batch (torch.LongTensor): actions (dimension batch x 1) s2_batch (torch.FloatTensor): next states (dimension: batch x state_dim) r_batch (torch.FloatTensor): rewards (dimension batch x 1) t_batch (torch.FloatTensor): indicator {0,1} for terminal states (dimension: batch x 1) gamma (float): discount factor Returns: TD-loss \"\"\" # forward value torch . autograd . set_grad_enabled ( True ) q_val = self . _forward ( s_batch , a_batch ) # forward target torch . autograd . set_grad_enabled ( False ) if self . target_model is None : q_target = self . _forward_target ( s2_batch , r_batch , t_batch , gamma ) else : q_target = self . _forward_target_ddqn ( s2_batch , r_batch , t_batch , gamma ) torch . autograd . set_grad_enabled ( True ) # loss loss = self . loss_fun ( q_val , q_target ) return loss","title":"loss()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.save","text":"Save model weights Parameters: Name Type Description Default path str path to model folder 'models/dqn' version str appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) '1.0' Source code in adviser/services/policy/rl/dqnpolicy.py 386 387 388 389 390 391 392 393 394 395 396 397 398 def save ( self , path : str = os . path . join ( 'models' , 'dqn' ), version : str = \"1.0\" ): \"\"\" Save model weights Args: path (str): path to model folder version (str): appendix to filename, enables having multiple models for the same domain (or saving a model after each training epoch) \"\"\" if not os . path . exists ( path ): os . makedirs ( path , exist_ok = True ) model_file = os . path . join ( path , \"rlpolicy_\" + self . domain . get_domain_name () + \"_\" + version + \".pt\" ) torch . save ( self . model , model_file )","title":"save()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.select_action_eps_greedy","text":"Epsilon-greedy policy. Parameters: Name Type Description Default state_vector FloatTensor current state (dimension 1 x state_dim) required Returns: Type Description action index for action selected by the agent for the current state Source code in adviser/services/policy/rl/dqnpolicy.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def select_action_eps_greedy ( self , state_vector : torch . FloatTensor ): \"\"\" Epsilon-greedy policy. Args: state_vector (torch.FloatTensor): current state (dimension 1 x state_dim) Returns: action index for action selected by the agent for the current state \"\"\" self . eps_scheduler () # epsilon greedy exploration if self . is_training and common . random . random () < self . epsilon : next_action_idx = common . random . randint ( 0 , self . action_dim - 1 ) else : torch . autograd . set_grad_enabled ( False ) q_values = self . model ( state_vector ) next_action_idx = q_values . squeeze ( dim = 0 ) . max ( dim = 0 )[ 1 ] . item () torch . autograd . set_grad_enabled ( True ) return next_action_idx","title":"select_action_eps_greedy()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.train","text":"Sets module and its subgraph to training mode Source code in adviser/services/policy/rl/dqnpolicy.py 417 418 419 420 421 422 423 def train ( self , train = True ): \"\"\" Sets module and its subgraph to training mode \"\"\" super ( DQNPolicy , self ) . train () self . is_training = True self . model . train () if self . target_model is not None : self . target_model . train ()","title":"train()"},{"location":"api/services/#adviser.services.policy.rl.dqnpolicy.DQNPolicy.train_batch","text":"Train on a minibatch drawn from the experience buffer. Source code in adviser/services/policy/rl/dqnpolicy.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def train_batch ( self ): \"\"\" Train on a minibatch drawn from the experience buffer. \"\"\" if not self . is_training : return if len ( self . buffer ) >= self . batch_size * 10 and \\ self . total_train_dialogs % self . training_frequency == 0 : self . train_call_count += 1 s_batch , a_batch , r_batch , s2_batch , t_batch , indices , importance_weights = \\ self . buffer . sample () self . optim . zero_grad () torch . autograd . set_grad_enabled ( True ) s_batch . requires_grad_ () gamma = torch . tensor ([ self . discount_gamma ] * self . batch_size , dtype = torch . float , device = self . device ) . view ( self . batch_size , 1 ) # calculate loss loss = self . loss ( s_batch , a_batch , s2_batch , r_batch , t_batch , gamma ) if importance_weights is not None : loss = loss * importance_weights for i in range ( self . batch_size ): # importance weighting # update priorities self . buffer . update ( i , loss [ i ] . item ()) loss = loss . mean () loss . backward () # clip gradients if self . gradient_clipping > 0.0 : nn . utils . clip_grad_norm_ ( self . model . parameters (), self . gradient_clipping ) # update weights self . optim . step () current_loss = loss . item () torch . autograd . set_grad_enabled ( False ) if self . writer is not None : # plot loss self . writer . add_scalar ( 'train/loss' , current_loss , self . train_call_count ) # plot min/max gradients max_grad_norm = - 1.0 min_grad_norm = 1000000.0 for param in self . model . parameters (): if param . grad is not None : # TODO decide on norm current_grad_norm = torch . norm ( param . grad , 2 ) if current_grad_norm > max_grad_norm : max_grad_norm = current_grad_norm if current_grad_norm < min_grad_norm : min_grad_norm = current_grad_norm self . writer . add_scalar ( 'train/min_grad' , min_grad_norm , self . train_call_count ) self . writer . add_scalar ( 'train/max_grad' , max_grad_norm , self . train_call_count ) # update target net if self . target_model is not None and \\ self . train_call_count % self . target_update_rate == 0 : self . target_model . load_state_dict ( self . model . state_dict ())","title":"train_batch()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer","text":"","title":"experience_buffer"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.Buffer","text":"Base class for experience replay buffers Initializes the memory, provides a print function for the memory contents and a method to insert new items into the buffer. Sampling has to be implemented by child classes.","title":"Buffer"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.Buffer.__init__","text":"Source code in adviser/services/policy/rl/experience_buffer.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , buffer_size : int , batch_size : int , state_dim : int , discount_gamma : float = 0.99 , device = torch . device ( 'cpu' )): assert buffer_size >= batch_size , 'the buffer hast to be larger than the batch size' self . device = device self . buffer_size = buffer_size self . batch_size = batch_size self . discount_gamma = discount_gamma # construct memory self . mem_state = torch . empty ( buffer_size , state_dim , dtype = torch . float , device = device ) self . mem_action = torch . empty ( buffer_size , 1 , dtype = torch . long , device = device ) self . mem_reward = torch . empty ( buffer_size , 1 , dtype = torch . float , device = device ) self . mem_next_state = torch . empty ( buffer_size , state_dim , dtype = torch . float , device = device ) self . mem_terminal = torch . empty ( buffer_size , 1 , dtype = torch . float , device = device ) self . write_pos = 0 self . last_write_pos = 0 self . buffer_count = 0 self . _reset ()","title":"__init__()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.Buffer.__len__","text":"Returns the number of items currently inside the buffer Source code in adviser/services/policy/rl/experience_buffer.py 145 146 147 def __len__ ( self ): \"\"\" Returns the number of items currently inside the buffer \"\"\" return self . buffer_count","title":"__len__()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.Buffer.print_contents","text":"Print contents of the experience replay memory. Parameters: Name Type Description Default max_size int restrict the number of printed items to this number (if not None) None Source code in adviser/services/policy/rl/experience_buffer.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def print_contents ( self , max_size : int = None ): \"\"\" Print contents of the experience replay memory. Args: max_size (int): restrict the number of printed items to this number (if not None) \"\"\" # how many entries to print print_items = len ( self ) if max_size is not None : print_items = min ( print_items , max_size ) print ( \"# REPLAY BUFFER CAPACITY: \" , self . buffer_size ) print ( \"# CURRENT ITEM COUNT\" , len ( self )) for i in range ( print_items ): print ( \"entry \" , i ) print ( \" action\" , self . mem_action [ i ]) print ( \" reward\" , self . mem_reward [ i ]) print ( \" terminal\" , self . mem_terminal [ i ]) print ( '---------' )","title":"print_contents()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.Buffer.sample","text":"Sample from buffer, has to be implemented by subclasses Source code in adviser/services/policy/rl/experience_buffer.py 149 150 151 def sample ( self ): \"\"\" Sample from buffer, has to be implemented by subclasses \"\"\" raise NotImplementedError","title":"sample()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.Buffer.store","text":"Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Parameters: Name Type Description Default state FloatTensor this turn's state tensor, or None if terminal = True required action LongTensor this turn's action index (int), or None if terminal = True required reward float this turn's reward (float) required terminal bool indicates whether episode finished (boolean) False Source code in adviser/services/policy/rl/experience_buffer.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def store ( self , state : torch . FloatTensor , action : torch . LongTensor , reward : float , terminal : bool = False ): \"\"\" Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Args: state (torch.tensor): this turn's state tensor, or None if terminal = True action (torch.tensor): this turn's action index (int), or None if terminal = True reward (torch.tensor): this turn's reward (float) terminal (bool): indicates whether episode finished (boolean) \"\"\" reward /= 20.0 if isinstance ( self . last_state , type ( None )): # and terminal == False: # first turn of trajectory, don't record since s' is needed self . last_state = state self . last_action = action self . last_reward = reward return False else : if terminal == True : if self . episode_length > 0 : # update last state's reward and set it to terminal self . mem_terminal [ self . last_write_pos ] = float ( True ) self . mem_reward [ self . last_write_pos ] += reward self . _reset () return False else : # in-between turn of trajectory: record self . mem_state [ self . write_pos ] = \\ self . last_state . clone () . detach () self . mem_action [ self . write_pos ][ 0 ] = self . last_action self . mem_reward [ self . write_pos ][ 0 ] = self . last_reward self . mem_next_state [ self . write_pos ] = state . clone () . detach () self . mem_terminal [ self . write_pos ] = float ( False ) # update last encountered state self . last_state = state . clone () . detach () self . last_action = action self . last_reward = reward # update write index self . last_write_pos = self . write_pos self . write_pos = ( self . write_pos + 1 ) % self . buffer_size if self . buffer_count < self . buffer_size : self . buffer_count += 1 self . episode_length += 1 return True","title":"store()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.NaivePrioritizedBuffer","text":"Prioritized experience replay buffer. Assigns sampling probabilities dependent on TD-error of the transitions.","title":"NaivePrioritizedBuffer"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.NaivePrioritizedBuffer.__init__","text":"Source code in adviser/services/policy/rl/experience_buffer.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def __init__ ( self , buffer_size : int , batch_size : int , state_dim : int , sample_last_transition : bool = False , regularisation : float = 0.00001 , exponent : float = 0.6 , beta : float = 0.4 , discount_gamma : float = 0.99 , device = torch . device ( 'cpu' )): super ( NaivePrioritizedBuffer , self ) . __init__ ( buffer_size , batch_size , state_dim , discount_gamma = discount_gamma , device = device ) print ( \" REPLAY MEMORY: NAIVE Prioritized\" ) self . probs = [ 0.0 ] * buffer_size self . regularisation = regularisation self . exponent = exponent self . beta = beta self . max_p = 1.0 self . sample_last_transition = sample_last_transition","title":"__init__()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.NaivePrioritizedBuffer.sample","text":"Sample from buffer. Returns: Type Description states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, importance weights Source code in adviser/services/policy/rl/experience_buffer.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def sample ( self ): \"\"\" Sample from buffer. Returns: states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, importance weights \"\"\" batch_size = self . batch_size batch_write_pos = 0 data_indices = torch . empty ( self . batch_size , dtype = torch . long , device = self . device ) probabilities = torch . empty ( self . batch_size , dtype = torch . float , device = self . device ) indices = [] self . sample_last_transition = True p_normed = np . array ( self . probs [: self . buffer_count ]) / np . linalg . norm ( self . probs [: self . buffer_count ], ord = 1 ) indices = common . numpy . random . choice ( list ( range ( self . buffer_count )), size = self . batch_size , p = p_normed ) if self . sample_last_transition : # include last transition (was at tree.write - 1) # -> see Sutton: A deeper look at experience replay data_indices [ 0 ] = self . last_write_pos probabilities [ 0 ] = self . probs [ self . last_write_pos ] # correct size of batch batch_size = batch_size - 1 batch_write_pos += 1 # TODO add option to sample each segment uniformly for i in range ( batch_write_pos , self . batch_size ): data_indices [ i ] = int ( indices [ i ]) probabilities [ i ] = self . probs [ data_indices [ i ]] # assemble batch from data indices s_batch = self . mem_state . index_select ( 0 , data_indices ) a_batch = self . mem_action . index_select ( 0 , data_indices ) r_batch = self . mem_reward . index_select ( 0 , data_indices ) t_batch = self . mem_terminal . index_select ( 0 , data_indices ) s2_batch = self . mem_next_state . index_select ( 0 , data_indices ) # calculate importance sampling weights importance_weights = float ( len ( self )) * probabilities importance_weights = importance_weights . pow ( - self . beta ) importance_weights = importance_weights / importance_weights . max ( dim = 0 )[ 0 ] . item () return s_batch , a_batch , r_batch , s2_batch , t_batch , data_indices , \\ importance_weights . view ( - 1 , 1 )","title":"sample()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.NaivePrioritizedBuffer.store","text":"Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Newly added experience tuples will be assigned maximum priority. Parameters: Name Type Description Default state FloatTensor this turn's state tensor, or None if terminal = True required action LongTensor this turn's action index (int), or None if terminal = True required reward float this turn's reward (float) required terminal bool indicates whether episode finished (boolean) False Source code in adviser/services/policy/rl/experience_buffer.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def store ( self , state : torch . FloatTensor , action : torch . LongTensor , reward : float , terminal : bool = False ): \"\"\" Store an experience of the form (s,a,r,s',t). Only needs the current state s (will construct transition to s' automatically). Newly added experience tuples will be assigned maximum priority. Args: state: this turn's state tensor, or None if terminal = True action: this turn's action index (int), or None if terminal = True reward: this turn's reward (float) terminal: indicates whether episode finished (boolean) \"\"\" if super ( NaivePrioritizedBuffer , self ) . store ( state , action , reward , terminal = terminal ): # create new tree node only if something new was added to the buffers self . probs [ self . last_write_pos ] = self . _priority_to_probability ( self . max_p )","title":"store()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.NaivePrioritizedBuffer.update","text":"Update the priority of transition with index idx Source code in adviser/services/policy/rl/experience_buffer.py 252 253 254 255 256 257 def update ( self , idx : int , error : float ): \"\"\" Update the priority of transition with index idx \"\"\" p = self . _priority_to_probability ( error ) if p > self . max_p : self . max_p = p self . probs [ idx ] = p","title":"update()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.UniformBuffer","text":"Experience replay buffer with uniformly random sampling","title":"UniformBuffer"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.UniformBuffer.__init__","text":"Parameters: Name Type Description Default sample_last_transition bool if True, a batch will always include the most recent transition (see Sutton: A deeper look at experience replay) True Source code in adviser/services/policy/rl/experience_buffer.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def __init__ ( self , buffer_size : int , batch_size : int , state_dim : int , discount_gamma : float = 0.99 , sample_last_transition : bool = True , device = torch . device ( 'cpu' )): \"\"\" Args: sample_last_transition (bool): if True, a batch will always include the most recent transition (see Sutton: A deeper look at experience replay) \"\"\" super ( UniformBuffer , self ) . __init__ ( buffer_size , batch_size , state_dim , discount_gamma = discount_gamma , device = device ) print ( \" REPLAY MEMORY: Uniform\" ) self . sample_last_transition = sample_last_transition","title":"__init__()"},{"location":"api/services/#adviser.services.policy.rl.experience_buffer.UniformBuffer.sample","text":"Sample from buffer. Returns: Type Description states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, None Source code in adviser/services/policy/rl/experience_buffer.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def sample ( self ): \"\"\" Sample from buffer. Returns: states, actions, rewards, next states, terminal state indicator {0,1}, buffer indices, None \"\"\" # create random indices data_indices = [] if self . sample_last_transition : # include last transition (was at write - 1) # - see Sutton: A deeper look at experience replay if self . write_pos - 1 < 0 : # last transition filled the capacity of the buffer data_indices = [ self . buffer_size - 1 ] else : data_indices = [ self . write_pos - 1 ] data_indices . extend ([ common . random . randint ( 0 , self . buffer_count - 1 ) for i in range ( self . batch_size - int ( self . sample_last_transition ))]) data_indices = torch . tensor ( data_indices , dtype = torch . long , device = self . device ) state_batch = self . mem_state . index_select ( 0 , data_indices ) action_batch = self . mem_action . index_select ( 0 , data_indices ) reward_batch = self . mem_reward . index_select ( 0 , data_indices ) next_state_batch = self . mem_next_state . index_select ( 0 , data_indices ) terminal_batch = self . mem_terminal . index_select ( 0 , data_indices ) return state_batch , action_batch , reward_batch , next_state_batch , terminal_batch , \\ data_indices , None","title":"sample()"},{"location":"api/services/#adviser.services.policy.rl.policy_rl","text":"","title":"policy_rl"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy","text":"Base class for Reinforcement Learning based policies. Functionality provided includes the setup of state- and action spaces, conversion of BeliefState objects into pytorch tensors, updating the last performed system actions and informed entities, populating the experience replay buffer, extraction of most probable user hypothesis and candidate action expansion. Output of an agent is a candidate action like inform_food which is then populated with the most probable slot/value pair from the beliefstate and database candidates by the expand_system_action -function to become inform(slot=food,value=italian) . In order to create your own policy, you can inherit from this class. Make sure to call the turn_end -function after each system turn and the end_dialog -function after each completed dialog.","title":"RLPolicy"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy.__init__","text":"Creates state- and action spaces, initializes experience replay buffers. Keyword Arguments: subgraph {[type]} -- [see services.Module] (default: {None}) buffer_cls {services.policy.rl.experience_buffer.Buffer} -- [Experience replay buffer class , not an instance - will be initialized by this constructor!] (default: {UniformBuffer}) buffer_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {6000}) batch_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {64}) discount_gamma {float} -- [Discount factor] (default: {0.99}) include_confreq {bool} -- [Use confirm_request actions] (default: {False}) Source code in adviser/services/policy/rl/policy_rl.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , domain : JSONLookupDomain , buffer_cls = UniformBuffer , buffer_size = 6000 , batch_size = 64 , discount_gamma = 0.99 , max_turns : int = 25 , include_confreq = False , logger : DiasysLogger = DiasysLogger (), include_select : bool = False , device = torch . device ( 'cpu' )): \"\"\" Creates state- and action spaces, initializes experience replay buffers. Arguments: domain {domain.jsonlookupdomain.JSONLookupDomain} -- Domain Keyword Arguments: subgraph {[type]} -- [see services.Module] (default: {None}) buffer_cls {services.policy.rl.experience_buffer.Buffer} -- [Experience replay buffer *class*, **not** an instance - will be initialized by this constructor!] (default: {UniformBuffer}) buffer_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {6000}) batch_size {int} -- [see services.policy.rl.experience_buffer. Buffer] (default: {64}) discount_gamma {float} -- [Discount factor] (default: {0.99}) include_confreq {bool} -- [Use confirm_request actions] (default: {False}) \"\"\" self . device = device self . sys_state = { \"lastInformedPrimKeyVal\" : None , \"lastActionInformNone\" : False , \"offerHappened\" : False , 'informedPrimKeyValsSinceNone' : []} self . max_turns = max_turns self . logger = logger self . domain = domain # setup evaluator for training self . evaluator = ObjectiveReachedEvaluator ( domain , logger = logger ) self . buffer_size = buffer_size self . batch_size = batch_size self . discount_gamma = discount_gamma self . writer = None # get state size self . state_dim = self . beliefstate_dict_to_vector ( BeliefState ( domain ) . _init_beliefstate ()) . size ( 1 ) self . logger . info ( \"state space dim: \" + str ( self . state_dim )) # get system action list self . actions = [ \"inform_byname\" , # TODO rename to 'bykey' \"inform_alternatives\" , \"reqmore\" ] # TODO badaction for req_slot in self . domain . get_system_requestable_slots (): self . actions . append ( 'request#' + req_slot ) self . actions . append ( 'confirm#' + req_slot ) if include_select : self . actions . append ( 'select#' + req_slot ) if include_confreq : for conf_slot in self . domain . get_system_requestable_slots (): if not req_slot == conf_slot : # skip case where confirm slot = request slot self . actions . append ( 'confreq#' + conf_slot + '#' + req_slot ) self . action_dim = len ( self . actions ) # don't include closingmsg in learnable actions self . actions . append ( 'closingmsg' ) # self.actions.append(\"closingmsg\") self . logger . info ( \"action space dim: \" + str ( self . action_dim )) self . primary_key = self . domain . get_primary_key () # init replay memory self . buffer = buffer_cls ( buffer_size , batch_size , self . state_dim , discount_gamma = discount_gamma , device = device ) self . sys_state = {} self . last_sys_act = None","title":"__init__()"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy.action_idx","text":"Returns the action index for the specified action name Source code in adviser/services/policy/rl/policy_rl.py 140 141 142 def action_idx ( self , action_name : str ): \"\"\" Returns the action index for the specified action name \"\"\" return self . actions . index ( action_name )","title":"action_idx()"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy.action_name","text":"Returns the action name for the specified action index Source code in adviser/services/policy/rl/policy_rl.py 136 137 138 def action_name ( self , action_idx : int ): \"\"\" Returns the action name for the specified action index \"\"\" return self . actions [ action_idx ]","title":"action_name()"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy.beliefstate_dict_to_vector","text":"Converts the beliefstate dict to a torch tensor Parameters: Name Type Description Default beliefstate BeliefState dict of belief (with at least beliefs and system keys) required Returns: Type Description belief tensor with dimension 1 x state_dim Source code in adviser/services/policy/rl/policy_rl.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def beliefstate_dict_to_vector ( self , beliefstate : BeliefState ): \"\"\" Converts the beliefstate dict to a torch tensor Args: beliefstate: dict of belief (with at least beliefs and system keys) Returns: belief tensor with dimension 1 x state_dim \"\"\" belief_vec = [] # add user acts belief_vec += [ 1 if act in beliefstate [ 'user_acts' ] else 0 for act in UserActionType ] # handle none actions belief_vec . append ( 1 if sum ( belief_vec ) == 0 else 1 ) # add informs (including special flag if slot not mentioned) for slot in sorted ( self . domain . get_informable_slots ()): values = self . domain . get_possible_values ( slot ) + [ \"dontcare\" ] if slot not in beliefstate [ 'informs' ]: # add **NONE** value first, then 0.0 for all others belief_vec . append ( 1.0 ) # also add value for don't care belief_vec += [ 0 for i in range ( len ( values ))] else : # add **NONE** value first belief_vec . append ( 0.0 ) bs_slot = beliefstate [ 'informs' ][ slot ] belief_vec += [ bs_slot [ value ] if value in bs_slot else 0.0 for value in values ] # add requests for slot in sorted ( self . domain . get_requestable_slots ()): if slot in beliefstate [ 'requests' ]: belief_vec . append ( 1.0 ) else : belief_vec . append ( 0.0 ) # append system features belief_vec . append ( float ( self . sys_state [ 'lastActionInformNone' ])) belief_vec . append ( float ( self . sys_state [ 'offerHappened' ])) candidate_count = beliefstate [ 'num_matches' ] # buckets for match count: 0, 1, 2-4, >4 belief_vec . append ( float ( candidate_count == 0 )) belief_vec . append ( float ( candidate_count == 1 )) belief_vec . append ( float ( 2 <= candidate_count <= 4 )) belief_vec . append ( float ( candidate_count > 4 )) belief_vec . append ( float ( beliefstate [ \"discriminable\" ])) # convert to torch tensor return torch . tensor ([ belief_vec ], dtype = torch . float , device = self . device )","title":"beliefstate_dict_to_vector()"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy.end_dialog","text":"Call this function when a dialog ended Source code in adviser/services/policy/rl/policy_rl.py 544 545 546 547 548 549 550 551 552 553 554 def end_dialog ( self , sim_goal : Goal ): \"\"\" Call this function when a dialog ended \"\"\" if sim_goal is None : # real user interaction, no simulator - don't have to evaluate # anything, just reset counters return final_reward , success = self . evaluator . get_final_reward ( sim_goal , logging = False ) if self . is_training : self . buffer . store ( None , None , final_reward , terminal = True )","title":"end_dialog()"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy.expand_system_action","text":"Expands an action index to a real sytem act Source code in adviser/services/policy/rl/policy_rl.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def expand_system_action ( self , action_idx : int , beliefstate : BeliefState ): \"\"\" Expands an action index to a real sytem act \"\"\" action_name = self . action_name ( action_idx ) if 'request#' in action_name : return self . _expand_request ( action_name ) elif 'select#' in action_name : return self . _expand_select ( action_name , beliefstate ) elif 'confirm#' in action_name : return self . _expand_confirm ( action_name , beliefstate ) elif 'confreq#' in action_name : return self . _expand_confreq ( action_name , beliefstate ) elif action_name == 'inform_byname' : return self . _expand_informbyname ( beliefstate ) elif action_name == 'inform_alternatives' : return self . _expand_informbyalternatives ( beliefstate ) elif action_name == 'closingmsg' : return self . _expand_bye () elif action_name == 'repeat' : return self . last_sys_act elif action_name == 'reqmore' : return self . _expand_reqmore () elif self . logger : self . logger . warning ( \"RL POLICY: system action not supported: \" + action_name ) return None","title":"expand_system_action()"},{"location":"api/services/#adviser.services.policy.rl.policy_rl.RLPolicy.turn_end","text":"Call this function after a turn is done by the system Source code in adviser/services/policy/rl/policy_rl.py 520 521 522 523 524 525 526 527 528 529 530 531 532 def turn_end ( self , beliefstate : BeliefState , state_vector : torch . FloatTensor , sys_act_idx : int ): \"\"\" Call this function after a turn is done by the system \"\"\" self . last_sys_act = self . expand_system_action ( sys_act_idx , beliefstate ) if self . logger : self . logger . dialog_turn ( \"system action > \" + str ( self . last_sys_act )) self . _update_system_belief ( beliefstate , self . last_sys_act ) turn_reward = self . evaluator . get_turn_reward () if self . is_training : self . buffer . store ( state_vector , sys_act_idx , turn_reward , terminal = False )","title":"turn_end()"},{"location":"api/services/#adviser.services.policy.rl.train_dqnpolicy","text":"","title":"train_dqnpolicy"},{"location":"api/services/#this-script-can-be-executed-to-train-a-dqn-policy","text":"","title":"This script can be executed to train a DQN policy."},{"location":"api/services/#it-will-create-a-policy-model-file-ending-with-pt","text":"","title":"It will create a policy model (file ending with .pt)."},{"location":"api/services/#you-need-to-execute-this-script-before-you-can-interact-with-the-rl-agent","text":"","title":"You need to execute this script before you can interact with the RL agent."},{"location":"api/services/#_1","text":"","title":""},{"location":"api/services/#adviser.services.policy.rl.train_dqnpolicy.get_root_dir","text":"Source code in adviser/services/policy/rl/train_dqnpolicy.py 31 32 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))))","title":"get_root_dir()"},{"location":"api/services/#adviser.services.policy.rl.train_dqnpolicy.train","text":"Training loop for the RL policy, for information on the parameters, look at the descriptions of commandline arguments in the \"if main\" below Source code in adviser/services/policy/rl/train_dqnpolicy.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def train ( domain_name : str , log_to_file : bool , seed : int , train_epochs : int , train_dialogs : int , eval_dialogs : int , max_turns : int , train_error_rate : float , test_error_rate : float , lr : float , eps_start : float , grad_clipping : float , buffer_classname : str , buffer_size : int , use_tensorboard : bool ): \"\"\" Training loop for the RL policy, for information on the parameters, look at the descriptions of commandline arguments in the \"if main\" below \"\"\" common . init_random ( seed = seed ) file_log_lvl = LogLevel . DIALOGS if log_to_file else LogLevel . NONE logger = DiasysLogger ( console_log_lvl = LogLevel . RESULTS , file_log_lvl = file_log_lvl ) if buffer_classname == \"prioritized\" : buffer_cls = NaivePrioritizedBuffer elif buffer_classname == \"uniform\" : buffer_cls = UniformBuffer domain = JSONLookupDomain ( name = domain_name ) bst = HandcraftedBST ( domain = domain , logger = logger ) user = HandcraftedUserSimulator ( domain , logger = logger ) # noise = SimpleNoise(domain=domain, train_error_rate=train_error_rate, # test_error_rate=test_error_rate, logger=logger) policy = DQNPolicy ( domain = domain , lr = lr , eps_start = eps_start , gradient_clipping = grad_clipping , buffer_cls = buffer_cls , replay_buffer_size = buffer_size , train_dialogs = train_dialogs , logger = logger ) evaluator = PolicyEvaluator ( domain = domain , use_tensorboard = use_tensorboard , experiment_name = domain_name , logger = logger ) ds = DialogSystem ( services = [ user , bst , policy , evaluator ], protocol = 'tcp' ) # ds.draw_system_graph() error_free = ds . is_error_free_messaging_pipeline () if not error_free : ds . print_local_inconsistencies () for j in range ( train_epochs ): # START TRAIN EPOCH evaluator . train () policy . train () evaluator . start_epoch () for episode in range ( train_dialogs ): if episode % 100 == 0 : print ( \"DIALOG\" , episode ) logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () policy . save () # START EVAL EPOCH evaluator . eval () policy . eval () evaluator . start_epoch () for episode in range ( eval_dialogs ): logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () ds . shutdown ()","title":"train()"},{"location":"api/services/#adviser.services.service","text":"","title":"service"},{"location":"api/services/#adviser.services.service.DialogSystem","text":"This class will constrct a dialog system from the list of services provided to the constructor. It will also handle synchronization for initalization of services before dialog start / after dialog end / on system shutdown and lets you discover potential conflicts in you messaging pipeline. This class is also used to communicate / synchronize with services running on different nodes.","title":"DialogSystem"},{"location":"api/services/#adviser.services.service.DialogSystem.__init__","text":"Parameters: Name Type Description Default services List[Union[adviser.services.service.Service, adviser.services.service.RemoteService]] List of all (remote) services to connect to. Only once they're specified here will they start listening for messages. required sub_port(int) subscriber port required sub_addr(str) IP-address or domain name of proxy subscriber interface (e.g. 127.0.0.1 for your local machine) required pub_port(int) publisher port required pub_addr(str) IP-address or domain name of proxy publisher interface (e.g. 127.0.0.1 for your local machine) required reg_port int registration port for remote services 65535 protocol(str) communication protol, either 'inproc' or 'tcp' or ipc required debug_logger DiasysLogger If not None , all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the DialogSystem even if they are never forwarded (as expected) to your Service None Source code in adviser/services/service.py 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 def __init__ ( self , services : List [ Union [ Service , RemoteService ]], sub_port : int = 65533 , pub_port : int = 65534 , reg_port : int = 65535 , protocol : str = 'tcp' , debug_logger : DiasysLogger = None ): \"\"\" Args: services (List[Union[Service, RemoteService]]): List of all (remote) services to connect to. Only once they're specified here will they start listening for messages. sub_port(int): subscriber port sub_addr(str): IP-address or domain name of proxy subscriber interface (e.g. 127.0.0.1 for your local machine) pub_port(int): publisher port pub_addr(str): IP-address or domain name of proxy publisher interface (e.g. 127.0.0.1 for your local machine) reg_port (int): registration port for remote services protocol(str): communication protol, either 'inproc' or 'tcp' or `ipc` debug_logger (DiasysLogger): If not `None`, all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the `DialogSystem` even if they are never forwarded (as expected) to your `Service` \"\"\" # node-local topics self . debug_logger = debug_logger self . protocol = protocol self . _sub_topics = {} self . _pub_topics = {} self . _remote_identifiers = set () self . _services = [] # collects names and instances of local services self . _start_dialog_services = set () # collects names of local services that subscribe to dialog_start # node-local sockets self . _domains = set () # start proxy thread self . _proxy_dev = ProcessProxy ( in_type = zmq . XSUB , out_type = zmq . XPUB ) # , mon_type=zmq.XSUB) self . _proxy_dev . bind_in ( f \" { protocol } ://127.0.0.1: { pub_port } \" ) self . _proxy_dev . bind_out ( f \" { protocol } ://127.0.0.1: { sub_port } \" ) self . _proxy_dev . start () self . _sub_port = sub_port self . _pub_port = pub_port # thread control self . _start_topics = set () self . _end_topics = set () self . _terminate_topics = set () self . _stopEvent = threading . Event () # control channels ctx = Context . instance () self . _control_channel_pub = ctx . socket ( zmq . PUB ) self . _control_channel_pub . sndhwm = 1100000 self . _control_channel_pub . connect ( f \" { protocol } ://127.0.0.1: { pub_port } \" ) self . _control_channel_sub = ctx . socket ( zmq . SUB ) # register services (local and remote) remote_services = {} for service in services : if isinstance ( service , Service ): # register local service service_name = type ( service ) . __name__ if service . _identifier is None else service . _identifier service . _init_pubsub () self . _add_service_info ( service_name , service . _domain_name , service . _sub_topics , service . _pub_topics , service . _start_topic , service . _end_topic , service . _terminate_topic ) service . _register_with_dialogsystem () elif isinstance ( service , RemoteService ): remote_services [ getattr ( service , 'identifier' )] = service self . _register_remote_services ( remote_services , reg_port ) self . _control_channel_sub . connect ( f \" { protocol } ://127.0.0.1: { sub_port } \" ) self . _setup_dialog_end_listener () time . sleep ( 0.25 )","title":"__init__()"},{"location":"api/services/#adviser.services.service.DialogSystem.draw_system_graph","text":"Draws a graph of the system as a directed graph. Services are represented by nodes, messages by directed edges (from publisher to subscriber). Warnings are drawn as yellow edges (and the missing subscribers represented by an 'UNCONNECTED SERVICES' node), errors as red edges (and the missing publishers represented by the 'UNCONNECTED SERVICES' node as well). Will mark remote services with blue. Parameters: Name Type Description Default name str used to construct the name of your output file 'system' format str output file format (e.g. png, pdf, jpg, ...) 'png' show bool if True, the graph image will be opened in your default image viewer application True Requires graphviz library (pip install graphviz) Source code in adviser/services/service.py 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 def draw_system_graph ( self , name : str = 'system' , format : str = \"png\" , show : bool = True ): \"\"\" Draws a graph of the system as a directed graph. Services are represented by nodes, messages by directed edges (from publisher to subscriber). Warnings are drawn as yellow edges (and the missing subscribers represented by an 'UNCONNECTED SERVICES' node), errors as red edges (and the missing publishers represented by the 'UNCONNECTED SERVICES' node as well). Will mark remote services with blue. Args: name (str): used to construct the name of your output file format (str): output file format (e.g. png, pdf, jpg, ...) show (bool): if True, the graph image will be opened in your default image viewer application Requires: graphviz library (pip install graphviz) \"\"\" from graphviz import Digraph g = Digraph ( name = name , format = format ) # collect all services, errors and warnings services = set () for service_set in self . _pub_topics . values (): services = services . union ( service_set ) for service_set in self . _sub_topics . values (): services = services . union ( service_set ) errors , warnings = self . list_inconsistencies () # add services as nodes for service in services : if service in self . _remote_identifiers : g . node ( service , color = '#1f618d' , style = 'filled' , fontcolor = 'white' , shape = 'box' ) # remote service else : g . node ( service , color = '#1c2833' , shape = 'box' ) # local service if len ( errors ) > 0 or len ( warnings ) > 0 : g . node ( 'UNCONNECTED SERVICES' , style = 'filled' , color = '#922b21' , fontcolor = 'white' , shape = 'box' ) # draw connections from publisher to subscribers as edges for topic in self . _pub_topics : publishers = self . _pub_topics [ topic ] receivers = self . _sub_topics [ topic ] if topic in self . _sub_topics else [] for receiver in receivers : for publisher in publishers : g . edge ( publisher , receiver , label = topic ) # draw warnings and errors as edges to node 'UNCONNECTED SERVICES' for topic in errors : receivers = errors [ topic ] for receiver in receivers : g . edge ( 'UNCONNECTED SERVICES' , receiver , color = '#c34400' , fontcolor = '#c34400' , label = topic ) for topic in warnings : publishers = warnings [ topic ] for publisher in publishers : g . edge ( publisher , 'UNCONNECTED SERVICES' , color = '#e37c02' , fontcolor = '#e37c02' , label = topic ) # draw graph g . render ( view = show , cleanup = True )","title":"draw_system_graph()"},{"location":"api/services/#adviser.services.service.DialogSystem.is_error_free_messaging_pipeline","text":"Checks the current messaging pipeline for potential errors. (Potential) Errors are defined in this context as subscribed topics without publishers. Returns: Type Description bool True, if no potential errors could be found - else, False Notes Call this method after instantiating all services. Lists only node-local (or process-local) inconsistencies. Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. Source code in adviser/services/service.py 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 def is_error_free_messaging_pipeline ( self ) -> bool : \"\"\" Checks the current messaging pipeline for potential errors. (Potential) Errors are defined in this context as subscribed topics without publishers. Returns: True, if no potential errors could be found - else, False Notes: * Call this method after instantiating all services. * Lists only node-local (or process-local) inconsistencies. * Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. \"\"\" return len ( self . list_inconsistencies ()[ 0 ]) == 0","title":"is_error_free_messaging_pipeline()"},{"location":"api/services/#adviser.services.service.DialogSystem.list_inconsistencies","text":"Checks for potential errors in the current messaging pipleline: e.g. len(list_inconsistencies()[0]) == 0 -> error free pipeline (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Returns: Type Description A touple of dictionaries the first dictionary contains potential errors (with the mapping topics -> subsribing services) the second dictionary contains warnings (with the mapping topics -> publishing services). Notes Call this method after instantiating all services. Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. Source code in adviser/services/service.py 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 def list_inconsistencies ( self ): \"\"\" Checks for potential errors in the current messaging pipleline: e.g. len(list_inconsistencies()[0]) == 0 -> error free pipeline (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Returns: A touple of dictionaries: * the first dictionary contains potential errors (with the mapping topics -> subsribing services) * the second dictionary contains warnings (with the mapping topics -> publishing services). Notes: * Call this method after instantiating all services. * Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. \"\"\" # look for subscribers w/o publishers by checking topic prefixes errors = {} for sub_topic in self . _sub_topics : found_pub = False for pub_topic in self . _pub_topics : if pub_topic . startswith ( sub_topic ): found_pub = True break if not found_pub : errors [ sub_topic ] = self . _sub_topics [ sub_topic ] # look for publishers w/o subscribers by checking topic prefixes warnings = {} for pub_topic in self . _pub_topics : found_sub = False for sub_topic in self . _sub_topics : if pub_topic . startswith ( sub_topic ): found_sub = True break if not found_sub : warnings [ pub_topic ] = self . _pub_topics [ pub_topic ] return errors , warnings","title":"list_inconsistencies()"},{"location":"api/services/#adviser.services.service.DialogSystem.list_published_topics","text":"Get all declared publisher topics. Returns: Type Description A dictionary with mapping topic (str) -> publishing services (Set[str]). Note Call this method after instantiating all services. Even though a publishing topic might be listed here, there is no guarantee that its publisher(s) might ever publish to it. Source code in adviser/services/service.py 852 853 854 855 856 857 858 859 860 861 862 863 def list_published_topics ( self ): \"\"\" Get all declared publisher topics. Returns: A dictionary with mapping topic (str) -> publishing services (Set[str]). Note: * Call this method after instantiating all services. * Even though a publishing topic might be listed here, there is no guarantee that its publisher(s) might ever publish to it. \"\"\" return copy . deepcopy ( self . _pub_topics ) # copy s.t. no user changes this list","title":"list_published_topics()"},{"location":"api/services/#adviser.services.service.DialogSystem.list_subscribed_topics","text":"Get all declared subscribed topics. Returns: Type Description A dictionary with mapping topic (str) -> subscribing services (Set[str]). Notes Call this method after instantiating all services. Source code in adviser/services/service.py 865 866 867 868 869 870 871 872 873 874 def list_subscribed_topics ( self ): \"\"\" Get all declared subscribed topics. Returns: A dictionary with mapping topic (str) -> subscribing services (Set[str]). Notes: * Call this method after instantiating all services. \"\"\" return copy . deepcopy ( self . _sub_topics ) # copy s.t. no user changes this list","title":"list_subscribed_topics()"},{"location":"api/services/#adviser.services.service.DialogSystem.print_inconsistencies","text":"Checks for potential errors in the current messaging pipleline: e.g. len(list_local_inconsistencies()[0]) == 0 -> error free pipeline and prints them to the console. (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Notes Call this method after instantiating all services. Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. Source code in adviser/services/service.py 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 def print_inconsistencies ( self ): \"\"\" Checks for potential errors in the current messaging pipleline: e.g. len(list_local_inconsistencies()[0]) == 0 -> error free pipeline and prints them to the console. (Potential) Errors are defined in this context as subscribed topics without publishers. Warnings are defined in this context as published topics without subscribers. Notes: * Call this method after instantiating all services. * Even if there are no errors returned by this method, there is not guarantee that all publishers eventually publish to their respective topics. \"\"\" # console colors WARNING = ' \\033 [93m' ERROR = ' \\033 [91m' ENDC = ' \\033 [0m' errors , warnings = self . list_inconsistencies () print ( ERROR ) print ( \"(Potential) Errors (subscribed topics without publishers):\" ) for topic in errors : print ( f \" topic: ' { topic } ', subscribed to in services: { errors [ topic ] } \" ) print ( ENDC ) print ( WARNING ) print ( \"Warnings (published topics without subscribers):\" ) for topic in warnings : print ( f \" topic: ' { topic } ', published in services: { warnings [ topic ] } \" ) print ( ENDC )","title":"print_inconsistencies()"},{"location":"api/services/#adviser.services.service.DialogSystem.run_dialog","text":"Run a complete dialog (blocking). Dialog will be started via messages to the topics specified in start_signals . The dialog will end on receiving any Topic.DIALOG_END message with value 'True', so make sure at least one service in your dialog graph will publish this message eventually. Parameters: Name Type Description Default start_signals dict mapping from topic -> value Publishes the value given for each topic to the respective topic. Use this to trigger the start of your dialog system. {'dialog_end': False} Source code in adviser/services/service.py 838 839 840 841 842 843 844 845 846 847 848 849 850 def run_dialog ( self , start_signals : dict = { Topic . DIALOG_END : False }): \"\"\" Run a complete dialog (blocking). Dialog will be started via messages to the topics specified in `start_signals`. The dialog will end on receiving any `Topic.DIALOG_END` message with value 'True', so make sure at least one service in your dialog graph will publish this message eventually. Args: start_signals (Dict[str, Any]): mapping from topic -> value Publishes the value given for each topic to the respective topic. Use this to trigger the start of your dialog system. \"\"\" self . _start_dialog ( start_signals ) self . _end_dialog ()","title":"run_dialog()"},{"location":"api/services/#adviser.services.service.DialogSystem.shutdown","text":"Shutdown dialog system. This will trigger terminate messages to be sent to all registered services to stop their listener loops. Should be called in the end before exiting your program. Blocks until all services sent ACK's confirming they're stopped. Source code in adviser/services/service.py 780 781 782 783 784 785 786 787 788 789 def shutdown ( self ): \"\"\" Shutdown dialog system. This will trigger `terminate` messages to be sent to all registered services to stop their listener loops. Should be called in the end before exiting your program. Blocks until all services sent ACK's confirming they're stopped. \"\"\" self . _stopEvent . set () for terminate_topic in self . _terminate_topics : _send_msg ( self . _control_channel_pub , terminate_topic , True ) _recv_ack ( self . _control_channel_sub , terminate_topic )","title":"shutdown()"},{"location":"api/services/#adviser.services.service.DialogSystem.stop","text":"Set stop event (can be queried by services via the terminating() function) Source code in adviser/services/service.py 771 772 773 774 def stop ( self ): \"\"\" Set stop event (can be queried by services via the `terminating()` function) \"\"\" self . _stopEvent . set () pass","title":"stop()"},{"location":"api/services/#adviser.services.service.DialogSystem.terminating","text":"Returns True if the system is stopping, else False Source code in adviser/services/service.py 776 777 778 def terminating ( self ): \"\"\" Returns True if the system is stopping, else False \"\"\" return self . _stopEvent . is_set ()","title":"terminating()"},{"location":"api/services/#adviser.services.service.RemoteService","text":"This is a placeholder to be used in the service list argument when constructing a DialogSystem : * Run the real Service instance on a remote node, give it a *UNIQUE* identifier * call run_standalone() on this instance * Instantiate a remote service on the node about to run the DialogSystem , assign the *SAME* identifier to it * add it to the DialogSystem service list * Now, when calling the constructor of DialogSystem`, you should see messages informing you about the successfull connection, or if the system is still trying to connect, it will block until connected to the remote service.","title":"RemoteService"},{"location":"api/services/#adviser.services.service.RemoteService.__init__","text":"Parameters: Name Type Description Default identifier str the UNIQUE identifier to call the remote service instance required Source code in adviser/services/service.py 94 95 96 97 98 99 def __init__ ( self , identifier : str ): \"\"\" Args: identifier (str): the *UNIQUE* identifier to call the remote service instance \"\"\" self . identifier = identifier","title":"__init__()"},{"location":"api/services/#adviser.services.service.Service","text":"Service base class. Inherit from this class, if you want to publish / subscribe to topics (Don't forget to call the super constructor!) . You may decorate arbitrary functions in the child class with the services.service.PublishSubscribe decorator for this purpose. A Service will only start listening to messages once it is added to a DialogSystem (or calling run_standalone() in the remote case and adding a corresponding RemoteService to the DialogSystem ).","title":"Service"},{"location":"api/services/#adviser.services.service.Service.__init__","text":"Create a new service instance (call this super constructor from your inheriting classes!) . Parameters: Name Type Description Default domain Union[str, utils.domain.domain.Domain] The domain(-name) of your service (or empty string, if domain-agnostic). If a domain(-name) is set, it will automatically filter out all messages from other domains. If no domain(-name) is set, messages from all domains will be received. '' sub_topic_domains Dict[str, str] change subscribed to topics to listen to a specific domain (e.g. 'erase'/append a domain for a specific topic) {} pub_topic_domains Dict[str, str] change published topics to a specific domain (e.g. 'erase'/append a domain for a specific topic) {} ds_host_addr str IP-address of the parent DialogSystem (default: localhost) '127.0.0.1' sub_port int subscriber port following zmq's XSUB/XPUB pattern 65533 pub_port int publisher port following zmq's XSUB/XPUB pattern 65534 protocol str communication protocol with DialogSystem - has to match! Possible options: tcp , inproc , ipc 'tcp' debug_logger DiasysLogger If not None , all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the DialogSystem even if they are never forwarded (as expected) to your Service . None identifier str Set this to a UNIQUE identifier per service to be run remotely. See RemoteService for more details. None Source code in adviser/services/service.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self , domain : Union [ str , Domain ] = \"\" , sub_topic_domains : Dict [ str , str ] = {}, pub_topic_domains : Dict [ str , str ] = {}, ds_host_addr : str = \"127.0.0.1\" , sub_port : int = 65533 , pub_port : int = 65534 , protocol : str = \"tcp\" , debug_logger : DiasysLogger = None , identifier : str = None ): \"\"\" Create a new service instance *(call this super constructor from your inheriting classes!)*. Args: domain (Union[str, Domain]): The domain(-name) of your service (or empty string, if domain-agnostic). If a domain(-name) is set, it will automatically filter out all messages from other domains. If no domain(-name) is set, messages from all domains will be received. sub_topic_domains (Dict[str, str]): change subscribed to topics to listen to a specific domain (e.g. 'erase'/append a domain for a specific topic) pub_topic_domains (Dict[str, str]): change published topics to a specific domain (e.g. 'erase'/append a domain for a specific topic) ds_host_addr (str): IP-address of the parent `DialogSystem` (default: localhost) sub_port (int): subscriber port following zmq's XSUB/XPUB pattern pub_port (int): publisher port following zmq's XSUB/XPUB pattern protocol (string): communication protocol with `DialogSystem` - has to match! Possible options: `tcp`, `inproc`, `ipc` debug_logger (DiasysLogger): If not `None`, all messags are printed to the logger, including send/receive events. Can be useful for debugging because you can still see messages received by the `DialogSystem` even if they are never forwarded (as expected) to your `Service`. identifier (str): Set this to a *UNIQUE* identifier per service to be run remotely. See `RemoteService` for more details. \"\"\" self . is_training = False self . domain = domain # get domain name (gets appended to all sub/pub topics so that different domain topics don't get shared) if domain is not None : self . _domain_name = domain . get_domain_name () if isinstance ( domain , Domain ) else domain else : self . _domain_name = \"\" self . _sub_topic_domains = sub_topic_domains self . _pub_topic_domains = pub_topic_domains # socket information self . _host_addr = ds_host_addr self . _sub_port = sub_port self . _pub_port = pub_port self . _protocol = protocol self . _identifier = identifier self . debug_logger = debug_logger self . _sub_topics = set () self . _pub_topics = set () self . _publish_sockets = dict () self . _internal_start_topics = dict () self . _internal_end_topics = dict () self . _internal_terminate_topics = dict () # NOTE: class name + memory pointer make topic unique (required, e.g. for running mutliple instances of same module!) self . _start_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /START\" self . _end_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /END\" self . _terminate_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /TERMINATE\" self . _train_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /TRAIN\" self . _eval_topic = f \" { type ( self ) . __name__ } / { id ( self ) } /EVAL\"","title":"__init__()"},{"location":"api/services/#adviser.services.service.Service.dialog_end","text":"This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. Source code in adviser/services/service.py 331 332 333 334 def dialog_end ( self ): \"\"\" This function is called after a dialog ended (Topics.DIALOG_END message was received). You should overwrite this function to record dialog-level information. \"\"\" pass","title":"dialog_end()"},{"location":"api/services/#adviser.services.service.Service.dialog_exit","text":"This function is called when the dialog system is shutting down. You should overwrite this function to stop your threads and cleanup any open resources. Source code in adviser/services/service.py 336 337 338 339 def dialog_exit ( self ): \"\"\" This function is called when the dialog system is shutting down. You should overwrite this function to stop your threads and cleanup any open resources. \"\"\" pass","title":"dialog_exit()"},{"location":"api/services/#adviser.services.service.Service.dialog_start","text":"This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. Source code in adviser/services/service.py 326 327 328 329 def dialog_start ( self ): \"\"\" This function is called before the first message to a new dialog is published. You should overwrite this function to set/reset dialog-level variables. \"\"\" pass","title":"dialog_start()"},{"location":"api/services/#adviser.services.service.Service.eval","text":"Sets module to eval mode Source code in adviser/services/service.py 345 346 347 def eval ( self ): \"\"\" Sets module to eval mode \"\"\" self . is_training = False","title":"eval()"},{"location":"api/services/#adviser.services.service.Service.get_all_published_topics","text":"Returns: Type Description Set of all topics published to by this Service Source code in adviser/services/service.py 391 392 393 394 395 396 def get_all_published_topics ( self ): \"\"\" Returns: Set of all topics published to by this `Service` \"\"\" return copy . deepcopy ( self . _pub_topics )","title":"get_all_published_topics()"},{"location":"api/services/#adviser.services.service.Service.get_all_subscribed_topics","text":"Returns: Type Description Set of all topics subscribed to by this Service Source code in adviser/services/service.py 384 385 386 387 388 389 def get_all_subscribed_topics ( self ): \"\"\" Returns: Set of all topics subscribed to by this `Service` \"\"\" return copy . deepcopy ( self . _sub_topics )","title":"get_all_subscribed_topics()"},{"location":"api/services/#adviser.services.service.Service.run_standalone","text":"Run this service as a standalone serivce (without a DialogSystem ) on a remote node. Use a RemoteService with corresponding identifier on the DialogSystem node to connect both. Note: this call is blocking! Parameters: Name Type Description Default host_reg_port int The port on the DialogSystem node listening for Service register requests 65535 Source code in adviser/services/service.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def run_standalone ( self , host_reg_port : int = 65535 ): \"\"\" Run this service as a standalone serivce (without a `DialogSystem`) on a remote node. Use a `RemoteService` with *corresponding identifier* on the `DialogSystem` node to connect both. Note: this call is blocking! Args: host_reg_port (int): The port on the `DialogSystem` node listening for `Service` register requests \"\"\" assert self . _identifier is not None , \"running a service on a remote node requires a unique identifier\" print ( \"Waiting for dialog system host...\" ) # send service info to dialog system node self . _init_pubsub () ctx = Context . instance () sync_endpoint = ctx . socket ( zmq . REQ ) sync_endpoint . connect ( f \"tcp:// { self . _host_addr } : { host_reg_port } \" ) data = pickle . dumps (( self . _domain_name , self . _sub_topics , self . _pub_topics , self . _start_topic , self . _end_topic , self . _terminate_topic )) sync_endpoint . send_multipart (( bytes ( f \"REGISTER_ { self . _identifier } \" , encoding = \"ascii\" ), data )) # wait for registration confirmation registered = False while not registered : msg = sync_endpoint . recv () msg = msg . decode ( \"utf-8\" ) if msg . startswith ( \"ACK_REGISTER_\" ): remote_service_identifier = msg [ len ( \"ACK_REGISTER_\" ):] if remote_service_identifier == self . _identifier : self . _register_with_dialogsystem () sync_endpoint . send_multipart ( ( bytes ( f \"CONF_REGISTER_ { self . _identifier } \" , encoding = \"ascii\" ), pickle . dumps ( True ))) registered = True print ( f \"Done\" )","title":"run_standalone()"},{"location":"api/services/#adviser.services.service.Service.train","text":"Sets module to training mode Source code in adviser/services/service.py 341 342 343 def train ( self ): \"\"\" Sets module to training mode \"\"\" self . is_training = True","title":"train()"},{"location":"api/services/#adviser.services.service.PublishSubscribe","text":"Decorator function for services. To be able to publish / subscribe to / from topics, your class is required to inherit from services.service.Service. Then, decorate any function you like. Your function will be called as soon as: * at least one message is received for each topic in sub_topics (only latest message will be forwarded, others dropped) * at least one message is received for each topic in queued_sub_topics (all messages since the previous function call will be forwarded as a list) Parameters: Name Type Description Default sub_topics(List[str or utils.topics.Topic] The topics you want to get the latest messages from. If multiple messages are received until your function is called, you will only receive the value of the latest message, previously received values will be discarded. required pub_topics(List[str or utils.topics.Topic] The topics you want to publish messages to. required queued_sub_topics(List[str or utils.topics.Topic] The topics you want to get all messages from. If multiple messages are received until your function is called, you will receive all values since the previous function call as a list. required Notes Subscription topic names have to match your function keywords Your function should return a dictionary with the keys matching your publish topics names and the value being any arbitrary python object or primitive type you want to send sub_topics and queued_sub_topics have to be disjoint! If you need timestamps for your messages, specify a 'timestamps' argument in your subscribing function. It will be filled by a dictionary providing timestamps for each received value, indexed by name. Technical notes: * Data will be automatically pickled / unpickled during send / receive to reduce meassage size. However, some python objects are not serializable (e.g. database connections) for good reasons and will throw an error if you try to publish them. * The domain name of your service class will be appended to your publish topics. Subscription topics are prefix-matched, so you will receive all messages from 'topic/suffix' if you subscibe to 'topic'. Source code in adviser/services/service.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 def PublishSubscribe ( sub_topics : List [ str ] = [], pub_topics : List [ str ] = [], queued_sub_topics : List [ str ] = []): \"\"\" Decorator function for services. To be able to publish / subscribe to / from topics, your class is required to inherit from services.service.Service. Then, decorate any function you like. Your function will be called as soon as: * at least one message is received for each topic in sub_topics (only latest message will be forwarded, others dropped) * at least one message is received for each topic in queued_sub_topics (all messages since the previous function call will be forwarded as a list) Args: sub_topics(List[str or utils.topics.Topic]): The topics you want to get the latest messages from. If multiple messages are received until your function is called, you will only receive the value of the latest message, previously received values will be discarded. pub_topics(List[str or utils.topics.Topic]): The topics you want to publish messages to. queued_sub_topics(List[str or utils.topics.Topic]): The topics you want to get all messages from. If multiple messages are received until your function is called, you will receive all values since the previous function call as a list. Notes: * Subscription topic names have to match your function keywords * Your function should return a dictionary with the keys matching your publish topics names and the value being any arbitrary python object or primitive type you want to send * sub_topics and queued_sub_topics have to be disjoint! * If you need timestamps for your messages, specify a 'timestamps' argument in your subscribing function. It will be filled by a dictionary providing timestamps for each received value, indexed by name. Technical notes: * Data will be automatically pickled / unpickled during send / receive to reduce meassage size. However, some python objects are not serializable (e.g. database connections) for good reasons and will throw an error if you try to publish them. * The domain name of your service class will be appended to your publish topics. Subscription topics are prefix-matched, so you will receive all messages from 'topic/suffix' if you subscibe to 'topic'. \"\"\" def wrapper ( func ): def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result # declare function as publish / subscribe functions and attach the respective topics delegate . pubsub = True delegate . sub_topics = sub_topics delegate . queued_sub_topics = queued_sub_topics delegate . pub_topics = pub_topics # check arguments: is subsriber interested in timestamps? delegate . timestamp_enabled = 'timestamps' in inspect . getfullargspec ( func )[ 0 ] return delegate return wrapper","title":"PublishSubscribe()"},{"location":"api/services/#adviser.services.simulator","text":"This package contains the handcrafted user simulatod and related services.","title":"simulator"},{"location":"api/services/#adviser.services.simulator.emotion_simulator","text":"","title":"emotion_simulator"},{"location":"api/services/#adviser.services.simulator.emotion_simulator.EmotionSimulator","text":"Class which generates user emotion/engagements. Currently outputs either a user defined or random emotion/engagement level and was designed to test the affective services work correctly. However, in the future it could be extended to be more realistic.","title":"EmotionSimulator"},{"location":"api/services/#adviser.services.simulator.emotion_simulator.EmotionSimulator.__init__","text":"Source code in adviser/services/simulator/emotion_simulator.py 36 37 38 39 40 41 42 43 44 def __init__ ( self , domain : JSONLookupDomain = None , logger : DiasysLogger = None , random : bool = True , static_emotion : EmotionType = EmotionType . Neutral , static_engagement : EngagementType = EngagementType . High ): Service . __init__ ( self , domain = domain ) self . domain = domain self . logger = logger self . random = random self . engagement = static_engagement self . emotion = static_emotion","title":"__init__()"},{"location":"api/services/#adviser.services.simulator.emotion_simulator.EmotionSimulator.send_emotion","text":"Source code in adviser/services/simulator/emotion_simulator.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"send_emotion()"},{"location":"api/services/#adviser.services.simulator.goal","text":"This module provides the Goal class and related stuff.","title":"goal"},{"location":"api/services/#adviser.services.simulator.goal.Constraint","text":"","title":"Constraint"},{"location":"api/services/#adviser.services.simulator.goal.Constraint.__eq__","text":"Constraint should be equal if the slot and value is the same. Source code in adviser/services/simulator/goal.py 41 42 43 44 45 46 def __eq__ ( self , other ): \"\"\"Constraint should be equal if the slot and value is the same.\"\"\" if isinstance ( other , Constraint ): return ( self . slot == other . slot and self . value == other . value ) return False","title":"__eq__()"},{"location":"api/services/#adviser.services.simulator.goal.Constraint.__getitem__","text":"Source code in adviser/services/simulator/goal.py 48 49 50 51 52 53 54 55 56 def __getitem__ ( self , key ): if not isinstance ( key , int ): raise TypeError if key == 0 : return self . slot elif key == 1 : return self . value else : raise IndexError","title":"__getitem__()"},{"location":"api/services/#adviser.services.simulator.goal.Constraint.__hash__","text":"Source code in adviser/services/simulator/goal.py 61 62 def __hash__ ( self ): return hash ( self . slot ) * hash ( self . value )","title":"__hash__()"},{"location":"api/services/#adviser.services.simulator.goal.Constraint.__init__","text":"The class for a constraint as used in the goal. Parameters: Name Type Description Default slot str The slot. required value str The value. required Source code in adviser/services/simulator/goal.py 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , slot , value ): \"\"\" The class for a constraint as used in the goal. Args: slot (str): The slot. value (str): The value. \"\"\" self . slot = slot self . value = value","title":"__init__()"},{"location":"api/services/#adviser.services.simulator.goal.Constraint.__repr__","text":"Source code in adviser/services/simulator/goal.py 58 59 def __repr__ ( self ): return \"Constraint(slot= {} , value= {} )\" . format ( self . slot , self . value )","title":"__repr__()"},{"location":"api/services/#adviser.services.simulator.goal.Goal","text":"","title":"Goal"},{"location":"api/services/#adviser.services.simulator.goal.Goal.__init__","text":"The class representing a goal, therefore containing requests and constraints. Parameters: Name Type Description Default domain JSONLookupDomain The domain for which the goal will be instantiated. It will only work within this domain. required parameters dict The parameters for the goal defined by a key=value mapping: 'MinVenues' (int) allows to set a minimum number of venues which fulfill the constraints of the goal, 'MinConstraints' (int) and 'MaxConstraints' (int) set the minimum and maximum amount of constraints respectively, 'MinRequests' (int) and 'MaxRequests' (int) set the minimum and maximum amount of requests respectively and 'Reachable' (float) allows to specify how many (in percent) of all generated goals are definitely fulfillable (i.e. there exists a venue for the current goal) or not (doesn't have to be fulfillable). Although the parameter 'Reachable' equals 1.0 implicitly states that 'MinVenues' equals 1 or more, the implementation looks different, is more efficient and takes all goals into consideration (since 'Reachable' is a float (percentage of generated goals)). On the other hand, setting 'MinVenues' to any number bigger than 0 forces every goal to be fulfillable. None Source code in adviser/services/simulator/goal.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , domain : JSONLookupDomain , parameters = None ): \"\"\" The class representing a goal, therefore containing requests and constraints. Args: domain (JSONLookupDomain): The domain for which the goal will be instantiated. It will only work within this domain. parameters (dict): The parameters for the goal defined by a key=value mapping: 'MinVenues' (int) allows to set a minimum number of venues which fulfill the constraints of the goal, 'MinConstraints' (int) and 'MaxConstraints' (int) set the minimum and maximum amount of constraints respectively, 'MinRequests' (int) and 'MaxRequests' (int) set the minimum and maximum amount of requests respectively and 'Reachable' (float) allows to specify how many (in percent) of all generated goals are definitely fulfillable (i.e. there exists a venue for the current goal) or not (doesn't have to be fulfillable). Although the parameter 'Reachable' equals 1.0 implicitly states that 'MinVenues' equals 1 or more, the implementation looks different, is more efficient and takes all goals into consideration (since 'Reachable' is a float (percentage of generated goals)). On the other hand, setting 'MinVenues' to any number bigger than 0 forces every goal to be fulfillable. \"\"\" self . domain = domain self . parameters = parameters or {} # cache inform and request slots # make sure to copy the list (shallow is sufficient) self . inf_slots = sorted ( list ( domain . get_informable_slots ())[:]) # make sure that primary key is never a constraint if self . domain . get_primary_key () in self . inf_slots : self . inf_slots . remove ( self . domain . get_primary_key ()) # TODO sometimes ask for specific primary key with very small probability (instead of any other constraints?) # pylint: disable=line-too-long self . inf_slot_values = {} for slot in self . inf_slots : self . inf_slot_values [ slot ] = sorted ( domain . get_possible_values ( slot )[:]) self . req_slots = sorted ( domain . get_requestable_slots ()[:]) # self.req_slots_without_informables = sorted(list( # set(self.req_slots).difference(self.inf_slots))) # make sure that primary key is never a request as it is added anyway if self . domain . get_primary_key () in self . req_slots : self . req_slots . remove ( self . domain . get_primary_key ()) self . constraints = [] self . requests = {} self . excluded_inf_slot_values = {} self . missing_informs = []","title":"__init__()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.__repr__","text":"Source code in adviser/services/simulator/goal.py 257 258 def __repr__ ( self ): return \"Goal(constraints= {} , requests= {} )\" . format ( self . constraints , self . requests )","title":"__repr__()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.fulfill_request","text":"Fulfills a request, i.e. sets value for request slot . Parameters: Name Type Description Default slot str The request slot which will be filled. required value str The value the request slot will be filled with. required Source code in adviser/services/simulator/goal.py 280 281 282 283 284 285 286 287 288 289 290 def fulfill_request ( self , slot , value ): \"\"\" Fulfills a request, i.e. sets ``value`` for request ``slot``. Args: slot (str): The request slot which will be filled. value (str): The value the request slot will be filled with. \"\"\" if slot in self . requests : self . requests [ slot ] = value","title":"fulfill_request()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.get_constraint","text":"Gets the value for a given constraint slot . Parameters: Name Type Description Default slot str The constraint slot which will be looked up. required Returns: Type Description bool The constraint value . Source code in adviser/services/simulator/goal.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def get_constraint ( self , slot ): \"\"\" Gets the value for a given constraint ``slot``. Args: slot (str): The constraint ``slot`` which will be looked up. Returns: bool: The constraint ``value``. \"\"\" for _constraint in self . constraints : if _constraint . slot == slot : return _constraint . value return 'dontcare'","title":"get_constraint()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.init","text":"Initializes a goal randomly OR using the given constraints and requests. Parameters: Name Type Description Default random_goal bool If True, a goal will be drawn randomly from available constraints and requests (considering the parameters given in the constructor, if any). However if constraints and requests are given and both don't equal None, this parameter is considered as False. If False, the given constraints and requests are used. True constraints List[Constraint] The constraints which will be used for the goal. None requests Dict[str, Union[None,str]] The requests which will be used for the goal. None Source code in adviser/services/simulator/goal.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def init ( self , random_goal = True , constraints = None , requests = None ) -> None : \"\"\" Initializes a goal randomly OR using the given constraints and requests. Args: random_goal (bool): If True, a goal will be drawn randomly from available constraints and requests (considering the parameters given in the constructor, if any). However if constraints and requests are given and both don't equal None, this parameter is considered as False. If False, the given constraints and requests are used. constraints (List[Constraint]): The constraints which will be used for the goal. requests (Dict[str, Union[None,str]]): The requests which will be used for the goal. \"\"\" # reset goal self . constraints = [] self . requests = {} self . excluded_inf_slot_values = { key : set () for key in self . inf_slot_values } # TODO implement possibility to pass either constraints or requests as a parameter if random_goal and constraints is None and requests is None : self . _init_random_goal () else : self . _init_from_parameters ( constraints , requests ) # make sure that primary key is always requested self . requests [ self . domain . get_primary_key ()] = None self . missing_informs = [ UserAct ( act_type = UserActionType . Inform , slot = _constraint . slot , value = _constraint . value ) for _constraint in self . constraints ]","title":"init()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.is_fulfilled","text":"Checks whether all requests have been fulfilled. Returns: Type Description bool True if all requests have been fulfilled, False otherwise. .. note:: Does not check whether the venue (issued by the system) fulfills the constraints since it's the system's task to give an appropriate venue by requesting the user's constraints. Source code in adviser/services/simulator/goal.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def is_fulfilled ( self ): \"\"\" Checks whether all requests have been fulfilled. Returns: bool: ``True`` if all requests have been fulfilled, ``False`` otherwise. .. note:: Does not check whether the venue (issued by the system) fulfills the constraints since it's the system's task to give an appropriate venue by requesting the user's constraints. \"\"\" for slot , value in self . requests . items (): assert slot != self . domain . get_primary_key () or value != 'none' # TODO remove later if value is None : return False return True","title":"is_fulfilled()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.is_inconsistent_constraint","text":"Checks whether the given constraint is consistent with the goal. A constraint is also consistent if it's value is 'dontcare' in the current goal. Parameters: Name Type Description Default constraint Constraint The constraint which will be checked for consistency. required Returns: Type Description bool True if values match or value in goal is 'dontcare', False otherwise. Source code in adviser/services/simulator/goal.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def is_inconsistent_constraint ( self , constraint ): \"\"\" Checks whether the given constraint is consistent with the goal. A constraint is also consistent if it's value is 'dontcare' in the current goal. Args: constraint (Constraint): The constraint which will be checked for consistency. Returns: bool: True if values match or value in goal is 'dontcare', False otherwise. \"\"\" for _constraint in self . constraints : if _constraint . slot == constraint . slot and ( _constraint . value != constraint . value \\ and _constraint . value != 'dontcare' ): return True return False","title":"is_inconsistent_constraint()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.is_inconsistent_constraint_strict","text":"Checks whether the given constraint is strictly consistent with the goal, whereby 'dontcare' is treated as a different value (no match). Parameters: Name Type Description Default constraint Constraint The constraint which will be checked for consistency. required Returns: Type Description bool True if values match, False otherwise. See Also [ is_inconsistent_constraint ][adviser.services.simulator.goal.Goal.is_inconsistent_constraint] Source code in adviser/services/simulator/goal.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def is_inconsistent_constraint_strict ( self , constraint ): \"\"\" Checks whether the given constraint is strictly consistent with the goal, whereby 'dontcare' is treated as a different value (no match). Args: constraint (Constraint): The constraint which will be checked for consistency. Returns: bool: True if values match, False otherwise. !!! seealso \"See Also\" [`is_inconsistent_constraint`][adviser.services.simulator.goal.Goal.is_inconsistent_constraint] \"\"\" for _constraint in self . constraints : if _constraint . slot == constraint . slot and _constraint . value == constraint . value : return False # here there are only two possibilities: the constraint is implicitly 'dontcare' because # it is not explicitly listed and the given constraint is either 1) 'dontcare' or 2) not return constraint . value != 'dontcare'","title":"is_inconsistent_constraint_strict()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.reset","text":"Resets all requests of the goal. Source code in adviser/services/simulator/goal.py 250 251 252 253 def reset ( self ): \"\"\"Resets all requests of the goal.\"\"\" # reset goal -> empty all requests self . requests = dict . fromkeys ( self . requests )","title":"reset()"},{"location":"api/services/#adviser.services.simulator.goal.Goal.update_constraint","text":"Update a given constraint slot with value . Parameters: Name Type Description Default slot str The constraint slot which will be updated. required value str The value with which the constraint will be updated. required Returns: Type Description bool True if update was successful, i.e. the constraint slot is included in the goal, False otherwise. Source code in adviser/services/simulator/goal.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 def update_constraint ( self , slot , value ): \"\"\" Update a given constraint ``slot`` with ``value``. Args: slot (str): The constraint *slot* which will be updated. value (str): The *value* with which the constraint will be updated. Returns: bool: ``True`` if update was successful, i.e. the constraint ``slot`` is included in the goal, ``False`` otherwise. \"\"\" for _constraint in self . constraints : if _constraint . slot == slot : _constraint . value = value return True return False","title":"update_constraint()"},{"location":"api/services/#adviser.services.simulator.simulator","text":"This module provides the agenda-based user model for the handcrafted simulator.","title":"simulator"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda","text":"A stack-like object representing an agenda. Actions can be pushed on and popped off the agenda.","title":"Agenda"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.__bool__","text":"Source code in adviser/services/simulator/simulator.py 681 682 def __bool__ ( self ): return bool ( self . stack )","title":"__bool__()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.__contains__","text":"Source code in adviser/services/simulator/simulator.py 675 676 def __contains__ ( self , value ): return value in self . stack","title":"__contains__()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.__init__","text":"Source code in adviser/services/simulator/simulator.py 669 670 def __init__ ( self ): self . stack = []","title":"__init__()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.__iter__","text":"Source code in adviser/services/simulator/simulator.py 672 673 def __iter__ ( self ): return iter ( self . stack )","title":"__iter__()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.__len__","text":"Source code in adviser/services/simulator/simulator.py 678 679 def __len__ ( self ): return len ( self . stack )","title":"__len__()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.__repr__","text":"Source code in adviser/services/simulator/simulator.py 684 685 def __repr__ ( self ): return repr ( self . stack )","title":"__repr__()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.__str__","text":"Source code in adviser/services/simulator/simulator.py 687 688 def __str__ ( self ): return str ( self . stack )","title":"__str__()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.clean","text":"Cleans the agenda, i.e. makes sure that actions are consistent with goal and in the correct order. Parameters: Name Type Description Default goal Goal The goal which is needed to determine the consistent actions. required Source code in adviser/services/simulator/simulator.py 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 def clean ( self , goal : Goal ): \"\"\"Cleans the agenda, i.e. makes sure that actions are consistent with goal and in the correct order. Args: goal (Goal): The goal which is needed to determine the consistent actions. \"\"\" cleaned_stack = [] # reverse order since most recent actions are on top of agenda for action in self . stack [:: - 1 ]: if action not in cleaned_stack : # NOTE sufficient if there is only one slot per (request) action # remove accomplished requests if ( action . type is not UserActionType . Request or ( action . slot in goal . requests and goal . requests [ action . slot ] is None ) or action . slot not in goal . requests ): # make sure to remove \"old\" inform actions if action . type is UserActionType . Inform : if not goal . is_inconsistent_constraint ( Constraint ( action . slot , action . value )): cleaned_stack . insert ( 0 , action ) else : cleaned_stack . insert ( 0 , action ) self . stack = cleaned_stack","title":"clean()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.clear","text":"Empties the agenda. Source code in adviser/services/simulator/simulator.py 766 767 768 def clear ( self ): \"\"\"Empties the agenda.\"\"\" self . stack . clear ()","title":"clear()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.contains_action_of_type","text":"Checks whether agenda contains actions of a specific type. Parameters: Name Type Description Default act_type UserActionType The action type (intent) for which the agenda will be checked. required consider_dontcare bool If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. True Returns: Type Description (bool) True if agenda contains act_type , False otherwise. Source code in adviser/services/simulator/simulator.py 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 def contains_action_of_type ( self , act_type : UserActionType , consider_dontcare = True ): \"\"\"Checks whether agenda contains actions of a specific type. Args: act_type (UserActionType): The action type (intent) for which the agenda will be checked. consider_dontcare (bool): If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. Returns: (bool): True if agenda contains *act_type*, False otherwise. \"\"\" for _action in self . stack : if not consider_dontcare and _action . value == 'dontcare' : continue if _action . type == act_type : return True return False","title":"contains_action_of_type()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.fill_with_constraints","text":"Adds all inform actions to the agenda necessary to fulfill the goal . Generally there is no need to add all constraints from the goal to the agenda apart from the initialisation. Parameters: Name Type Description Default goal Goal The current goal of the (simulated) user for which actions will be pushed to the agenda. required Source code in adviser/services/simulator/simulator.py 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 def fill_with_constraints ( self , goal : Goal ): \"\"\" Adds all inform actions to the agenda necessary to fulfill the *goal*. Generally there is no need to add all constraints from the goal to the agenda apart from the initialisation. Args: goal (Goal): The current goal of the (simulated) user for which actions will be pushed to the agenda. \"\"\" # add informs from goal for constraint in goal . constraints : self . stack . append ( UserAct ( act_type = UserActionType . Inform , slot = constraint . slot , value = constraint . value , score = 1.0 ))","title":"fill_with_constraints()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.fill_with_requests","text":"Adds all request actions to the agenda necessary to fulfill the goal . Parameters: Name Type Description Default goal Goal The current goal of the (simulated) user for which actions will be pushed to the agenda. required exclude_name bool whehter or not to include an action to request an entities name. True Source code in adviser/services/simulator/simulator.py 838 839 840 841 842 843 844 845 846 847 848 849 850 851 def fill_with_requests ( self , goal : Goal , exclude_name : bool = True ): \"\"\"Adds all request actions to the agenda necessary to fulfill the *goal*. Args: goal (Goal): The current goal of the (simulated) user for which actions will be pushed to the agenda. exclude_name (bool): whehter or not to include an action to request an entities name. \"\"\" # add requests and make sure to add the name at the end (i.e. ask first for name) for key , value in goal . requests . items (): if (( key != 'name' and exclude_name ) or not exclude_name ) and value is None : self . stack . append ( UserAct ( act_type = UserActionType . Request , slot = key , value = value , score = 1.0 ))","title":"fill_with_requests()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.get_actions","text":"Retrieves num_actions actions from the agenda. Parameters: Name Type Description Default num_actions int Amount of actions which will be retrieved from the agenda. required Returns: Type Description (List[UserAct]) list of num_actions user actions. Source code in adviser/services/simulator/simulator.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 def get_actions ( self , num_actions : int ): \"\"\"Retrieves *num_actions* actions from the agenda. Args: num_actions (int): Amount of actions which will be retrieved from the agenda. Returns: (List[UserAct]): list of *num_actions* user actions. \"\"\" if num_actions < 0 or num_actions > len ( self . stack ): num_actions = len ( self . stack ) return [ self . stack . pop () for _ in range ( 0 , num_actions )]","title":"get_actions()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.get_actions_of_type","text":"Get actions of a specific type from the agenda. Parameters: Name Type Description Default act_type UserActionType The action type (intent) for which the agenda will be checked. <enum 'UserActionType'> consider_dontcare bool If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. True Returns: Type Description (Iterable[UserAct]) A list of user actions of the given type/intent. Source code in adviser/services/simulator/simulator.py 798 799 800 801 802 803 804 805 806 807 808 809 810 811 def get_actions_of_type ( self , act_type = UserActionType , consider_dontcare = True ): \"\"\"Get actions of a specific type from the agenda. Args: act_type (UserActionType): The action type (intent) for which the agenda will be checked. consider_dontcare (bool): If set to True also considers actions for which the value is 'dontcare', and ignores them otherwise. Returns: (Iterable[UserAct]): A list of user actions of the given type/intent. \"\"\" return filter ( lambda x : x . type == act_type and ( consider_dontcare or x . value != 'dontcare' ), self . stack )","title":"get_actions_of_type()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.init","text":"Initializes the agenda given a goal. For this purpose, inform actions for constraints in the goal and request actions for requests in the goal are added such that the informs are handled first followed by the requests. Parameters: Name Type Description Default goal Goal The goal for which the agenda will be initialized. required Source code in adviser/services/simulator/simulator.py 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 def init ( self , goal ): \"\"\" Initializes the agenda given a goal. For this purpose, inform actions for constraints in the goal and request actions for requests in the goal are added such that the informs are handled first followed by the requests. Args: goal (Goal): The goal for which the agenda will be initialized. \"\"\" self . stack . clear () # populate agenda according to goal # NOTE don't push bye action here since bye action could be poppped with another (missing) # request, but user should not end dialog before having the goal fulfilled # NOTE do not add requests to agenda since system can't handle inform and request action in # same turn currently! # self.fill_with_requests(goal) self . fill_with_constraints ( goal )","title":"init()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.is_empty","text":"Checks whether the agenda is empty. Returns: Type Description (bool) True if agenda is empty, False otherwise. Source code in adviser/services/simulator/simulator.py 770 771 772 773 774 775 776 777 def is_empty ( self ): \"\"\"Checks whether the agenda is empty. Returns: (bool): True if agenda is empty, False otherwise. \"\"\" return len ( self . stack ) == 0","title":"is_empty()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.push","text":"Pushes item onto the agenda. Parameters: Name Type Description Default item The goal for which the agenda will be initialized. required Source code in adviser/services/simulator/simulator.py 712 713 714 715 716 717 718 719 720 721 722 def push ( self , item ): \"\"\"Pushes *item* onto the agenda. Args: item: The goal for which the agenda will be initialized. \"\"\" if isinstance ( item , list ): self . stack += item else : self . stack . append ( item )","title":"push()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.remove_actions","text":"Removes actions of a specific type, slot and optionally value from the agenda. All arguments (value only if given) have to match in conjunction. Parameters: Name Type Description Default act_type UserActionType The action type (intent) which will be removed from the agenda. required slot str The action type (intent) which will be removed from the agenda. required value str The action type (intent) which will be removed from the agenda. None Source code in adviser/services/simulator/simulator.py 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 def remove_actions ( self , act_type : UserActionType , slot : str , value : str = None ): \"\"\"Removes actions of a specific type, slot and optionally value from the agenda. All arguments (value only if given) have to match in conjunction. Args: act_type (UserActionType): The action type (intent) which will be removed from the agenda. slot (str): The action type (intent) which will be removed from the agenda. value (str): The action type (intent) which will be removed from the agenda. \"\"\" if value is None : self . stack = list ( filter ( lambda x : x . type != act_type or x . slot != slot , self . stack )) else : self . stack = list ( filter ( lambda x : x . type != act_type or x . slot != slot or x . value != value , self . stack ))","title":"remove_actions()"},{"location":"api/services/#adviser.services.simulator.simulator.Agenda.remove_actions_of_type","text":"Removes actions of a specific type from the agenda. Parameters: Name Type Description Default act_type UserActionType The action type (intent) which will be removed from the agenda. required Source code in adviser/services/simulator/simulator.py 813 814 815 816 817 818 819 820 def remove_actions_of_type ( self , act_type : UserActionType ): \"\"\"Removes actions of a specific type from the agenda. Args: act_type (UserActionType): The action type (intent) which will be removed from the agenda. \"\"\" self . stack = list ( filter ( lambda x : x . type != act_type , self . stack ))","title":"remove_actions_of_type()"},{"location":"api/services/#adviser.services.simulator.simulator.HandcraftedUserSimulator","text":"The class for a handcrafted (agenda-based) user simulator. !!! args domain (Domain): The domain for which the user simulator will be instantiated. It will use this domain to generate the goals.","title":"HandcraftedUserSimulator"},{"location":"api/services/#adviser.services.simulator.simulator.HandcraftedUserSimulator.__init__","text":"Source code in adviser/services/simulator/simulator.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , domain : Domain , logger : DiasysLogger = DiasysLogger ()): super ( HandcraftedUserSimulator , self ) . __init__ ( domain ) # possible system actions self . receive_options = { SysActionType . Welcome : self . _receive_welcome , SysActionType . InformByName : self . _receive_informbyname , SysActionType . InformByAlternatives : self . _receive_informbyalternatives , SysActionType . Request : self . _receive_request , SysActionType . Confirm : self . _receive_confirm , SysActionType . Select : self . _receive_select , SysActionType . RequestMore : self . _receive_requestmore , SysActionType . Bad : self . _receive_bad , SysActionType . ConfirmRequest : self . _receive_confirmrequest } # parse config file self . logger = logger self . config = configparser . ConfigParser ( inline_comment_prefixes = ( '#' , ';' )) self . config . optionxform = str self . config . read ( os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), 'usermodel.cfg' )) self . parameters = {} # goal self . parameters [ 'goal' ] = {} for key in self . config [ \"goal\" ]: val = self . config . get ( \"goal\" , key ) self . parameters [ 'goal' ][ key ] = float ( val ) # usermodel self . parameters [ 'usermodel' ] = {} for key in self . config [ \"usermodel\" ]: val = self . config . get ( \"usermodel\" , key ) if key in [ 'patience' ]: # patience will be sampled on begin of each dialog self . parameters [ 'usermodel' ][ key ] = [ int ( x ) for x in ( val . replace ( ' ' , '' ) . strip ( '[]' ) . split ( ',' ))] else : if val . startswith ( \"[\" ) and val . endswith ( \"]\" ): # value is a list to sample the probability from self . parameters [ 'usermodel' ][ key ] = common . numpy . random . uniform ( * [ float ( x ) for x in val . replace ( ' ' , '' ) . strip ( '[]' ) . split ( ',' )]) else : # value is the probability self . parameters [ 'usermodel' ][ key ] = float ( val ) # member declarations self . turn = 0 self . domain = domain self . dialog_patience = None self . patience = None self . last_user_actions = None self . last_system_action = None self . excluded_venues = [] # member definitions self . goal = Goal ( domain , self . parameters [ 'goal' ]) self . agenda = Agenda () self . num_actions_next_turn = - 1","title":"__init__()"},{"location":"api/services/#adviser.services.simulator.simulator.HandcraftedUserSimulator.dialog_start","text":"Resets the user model at the beginning of a dialog, e.g. draws a new goal and populates the agenda according to the goal. Source code in adviser/services/simulator/simulator.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def dialog_start ( self ): \"\"\"Resets the user model at the beginning of a dialog, e.g. draws a new goal and populates the agenda according to the goal.\"\"\" # self.goal = Goal(self.domain, self.parameters['goal']) self . goal . init () self . agenda . init ( self . goal ) if self . logger : self . logger . dialog_turn ( \"New goal has constraints {} and requests {} .\" . format ( self . goal . constraints , self . goal . requests )) self . logger . dialog_turn ( \"New agenda initialized: {} \" . format ( self . agenda )) # add hello action with some probability if common . random . random () < self . parameters [ 'usermodel' ][ 'Greeting' ]: self . agenda . push ( UserAct ( act_type = UserActionType . Hello , score = 1.0 )) # needed for possibility to reset patience if len ( self . parameters [ 'usermodel' ][ 'patience' ]) == 1 : self . dialog_patience = self . parameters [ 'usermodel' ][ 'patience' ][ 0 ] else : self . dialog_patience = common . random . randint ( * self . parameters [ 'usermodel' ][ 'patience' ]) self . patience = self . dialog_patience self . last_user_actions = None self . last_system_action = None self . excluded_venues = [] self . turn = 0","title":"dialog_start()"},{"location":"api/services/#adviser.services.simulator.simulator.HandcraftedUserSimulator.receive","text":"This function makes sure that the agenda reflects all changes needed for the received system action. Parameters: Name Type Description Default sys_act SysAct The action the system took required Source code in adviser/services/simulator/simulator.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def receive ( self , sys_act : SysAct ): \"\"\" This function makes sure that the agenda reflects all changes needed for the received system action. Args: sys_act (SysAct): The action the system took \"\"\" if self . last_system_action is not None : # check whether system action is the same as before if sys_act == self . last_system_action : self . patience -= 1 elif self . parameters [ 'usermodel' ][ 'resetPatience' ]: self . patience = self . dialog_patience self . last_system_action = sys_act if self . patience == 0 : self . logger . dialog_turn ( \"User patience run out, ending dialog.\" ) self . agenda . clear () self . _finish_dialog ( ungrateful = True ) else : ignored_requests , ignored_requests_alt = self . _check_system_ignored_request ( self . last_user_actions , sys_act ) # first stage: push operations on top of agenda if sys_act . type in self . receive_options : self . receive_options [ sys_act . type ]( sys_act ) # handle missing requests if ignored_requests : # repeat unanswered requests from user from last turn self . agenda . push ( ignored_requests ) if ignored_requests_alt : self . agenda . push ( ignored_requests_alt ) # make sure to pick only the requestalt actions (should be 1) self . num_actions_next_turn = len ( ignored_requests_alt ) # make sure that old request actions verifying an offer are removed self . agenda . remove_actions_of_type ( act_type = UserActionType . Request ) # second stage: clean agenda self . agenda . clean ( self . goal ) # agenda might be empty -> add requests again if self . agenda . is_empty (): if self . goal . is_fulfilled (): self . _finish_dialog () else : self . agenda . fill_with_requests ( self . goal , exclude_name = False ) else : self . logger . error ( \"System Action Type is {} , but I don't know how to handle it!\" . format ( sys_act . type ))","title":"receive()"},{"location":"api/services/#adviser.services.simulator.simulator.HandcraftedUserSimulator.respond","text":"Gets n actions from the agenda, where n is drawn depending on the agenda or a pdf. Source code in adviser/services/simulator/simulator.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def respond ( self ): \"\"\" Gets n actions from the agenda, where n is drawn depending on the agenda or a pdf. \"\"\" # get some actions from the agenda assert len ( self . agenda ) > 0 , \"Agenda is empty, this must not happen at this point!\" if self . num_actions_next_turn > 0 : # use and reset self.num_actions_next_turn if set num_actions = self . num_actions_next_turn self . num_actions_next_turn = - 1 elif self . agenda . stack [ - 1 ] . type == UserActionType . Bye : # pop all actions from agenda since agenda can only contain thanks (optional) and # bye action num_actions = - 1 else : # draw amount of actions num_actions = min ( len ( self . agenda ), common . numpy . random . choice ( [ 1 , 2 , 3 ], p = [ . 6 , . 3 , . 1 ])) # hardcoded pdf # get actions from agenda user_actions = self . agenda . get_actions ( num_actions ) # copy needed for repeat action since they might be changed in other modules self . last_user_actions = copy . deepcopy ( user_actions ) for action in user_actions : if action . type == UserActionType . Inform : _constraint = Constraint ( action . slot , action . value ) # if _constraint in self.goal.constraints: if action in self . goal . missing_informs : self . goal . missing_informs . remove ( action ) return user_actions","title":"respond()"},{"location":"api/services/#adviser.services.simulator.simulator.HandcraftedUserSimulator.user_turn","text":"Source code in adviser/services/simulator/simulator.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"user_turn()"},{"location":"api/services/#adviser.services.stats","text":"","title":"stats"},{"location":"api/services/#adviser.services.stats.evaluation","text":"","title":"evaluation"},{"location":"api/services/#adviser.services.stats.evaluation.ObjectiveReachedEvaluator","text":"Evaluate single turns and complete dialog. This class assigns a negative reward to each turn . In case the user ' s goal could be satisfied ( meaning a matching database entry was found ), a large final reward is returned . Only needed when training against a simulator .","title":"ObjectiveReachedEvaluator"},{"location":"api/services/#adviser.services.stats.evaluation.ObjectiveReachedEvaluator.__init__","text":"Source code in adviser/services/stats/evaluation.py 38 39 40 41 42 43 44 def __init__ ( self , domain : Domain , turn_reward =- 1 , success_reward = 20 , logger : DiasysLogger = DiasysLogger ()): assert turn_reward <= 0.0 , 'the turn reward should be negative' self . domain = domain self . turn_reward = turn_reward self . success_reward = success_reward self . logger = logger","title":"__init__()"},{"location":"api/services/#adviser.services.stats.evaluation.ObjectiveReachedEvaluator.get_final_reward","text":"Check whether the user's goal was completed. Parameters: Name Type Description Default sim_goal Goal the simulation's goal required logging bool whether or not the evaluation results should be logged True Returns: Type Description float Reward - the final reward (0 (unsuccessful) or 20 (successful)) bool: Success Source code in adviser/services/stats/evaluation.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_final_reward ( self , sim_goal : Goal , logging = True ): \"\"\" Check whether the user's goal was completed. Args: sim_goal (Goal): the simulation's goal logging (bool): whether or not the evaluation results should be logged Returns: float: Reward - the final reward (0 (unsuccessful) or 20 (successful)) bool: Success \"\"\" requests = sim_goal . requests constraints = sim_goal . constraints # list of constraints # self.logger.dialog_turn(\"User Goal > \" + str(sim_goal.constraints)) if None in requests . values () or requests [ 'name' ] == 'none' : if logging : self . logger . dialog_turn ( \"Fail with user requests \\n {} \" . format ( requests )) return 0.0 , False # TODO think about this more? if goals not satisfiable, # should system take the blame? not fair # print(requests['name']) db_matches = self . domain . find_info_about_entity ( entity_id = requests [ 'name' ], requested_slots = [ constraint . slot for constraint in constraints ]) if db_matches : match = db_matches [ 0 ] for const in constraints : if const . value != match [ const . slot ] and const . value != 'dontcare' : if logging : self . logger . dialog_turn ( \"Fail with user requests \\n {} \" . format ( requests )) return 0.0 , False if logging : self . logger . dialog_turn ( \"Success with user requests \\n {} \" . format ( requests )) return 20.0 , True if logging : self . logger . dialog_turn ( \"Fail with user requests \\n {} \" . format ( requests )) return 0.0 , False","title":"get_final_reward()"},{"location":"api/services/#adviser.services.stats.evaluation.ObjectiveReachedEvaluator.get_turn_reward","text":"Get the reward for one turn Returns: Type Description (int) the reward for the given turn Source code in adviser/services/stats/evaluation.py 46 47 48 49 50 51 52 53 def get_turn_reward ( self ): \"\"\" Get the reward for one turn Returns: (int): the reward for the given turn \"\"\" return self . turn_reward","title":"get_turn_reward()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator","text":"Policy evaluation module Plug this module into the dialog graph (somewhere after the policy), and policy metrics like success rate and reward will be recorded.","title":"PolicyEvaluator"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.__init__","text":"Keyword Arguments: use_tensorboard {bool} -- [If true, metrics will be written to tensorboard in a runs directory] (default: {False}) experiment_name {str} -- [Name suffix for the log files] (default: {''}) turn_reward {float} -- [Reward for one turn - usually negative to penalize dialog length] (default: {-1}) success_reward {float} -- [Reward of the final transition if the dialog goal was reached] (default: {20}) Source code in adviser/services/stats/evaluation.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , domain : Domain , subgraph : dict = None , use_tensorboard = False , experiment_name : str = '' , turn_reward =- 1 , success_reward = 20 , logger : DiasysLogger = DiasysLogger (), summary_writer = None ): \"\"\" Keyword Arguments: use_tensorboard {bool} -- [If true, metrics will be written to tensorboard in a *runs* directory] (default: {False}) experiment_name {str} -- [Name suffix for the log files] (default: {''}) turn_reward {float} -- [Reward for one turn - usually negative to penalize dialog length] (default: {-1}) success_reward {float} -- [Reward of the final transition if the dialog goal was reached] (default: {20}) \"\"\" super ( PolicyEvaluator , self ) . __init__ ( domain ) self . logger = logger self . epoch = 0 self . evaluator = ObjectiveReachedEvaluator ( domain , turn_reward = turn_reward , success_reward = success_reward , logger = logger ) self . writer = summary_writer self . total_train_dialogs = 0 self . total_eval_dialogs = 0 self . epoch_train_dialogs = 0 self . epoch_eval_dialogs = 0 self . train_rewards = [] self . eval_rewards = [] self . train_success = [] self . eval_success = [] self . train_turns = [] self . eval_turns = [] self . is_training = False","title":"__init__()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.dialog_start","text":"Clears the state of the evaluator in preparation to start a new dialog Source code in adviser/services/stats/evaluation.py 159 160 161 162 163 164 def dialog_start ( self , dialog_start = False ): \"\"\" Clears the state of the evaluator in preparation to start a new dialog \"\"\" self . dialog_reward = 0.0 self . dialog_turns = 0","title":"dialog_start()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.end_dialog","text":"Source code in adviser/services/stats/evaluation.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"end_dialog()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.end_epoch","text":"Handles calculating statistics at the end of an epoch Source code in adviser/services/stats/evaluation.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def end_epoch ( self ): \"\"\" Handles calculating statistics at the end of an epoch \"\"\" if self . logger : if self . epoch_train_dialogs > 0 : self . logger . result ( \" ### Train ###\" ) self . logger . result ( \"# Num Dialogs \" + str ( self . epoch_train_dialogs )) self . logger . result ( \"# Avg Turns \" + str ( sum ( self . train_turns ) / self . epoch_train_dialogs )) self . logger . result ( \"# Avg Success \" + str ( sum ( self . train_success ) / self . epoch_train_dialogs )) self . logger . result ( \"# Avg Reward \" + str ( sum ( self . train_rewards ) / self . epoch_train_dialogs )) if self . epoch_eval_dialogs > 0 : self . logger . result ( \" ### Eval ###\" ) self . logger . result ( \"# Num Dialogs \" + str ( self . epoch_eval_dialogs )) self . logger . result ( \"# Avg Turns \" + str ( sum ( self . eval_turns ) / self . epoch_eval_dialogs )) self . logger . result ( \"# Avg Success \" + str ( sum ( self . eval_success ) / self . epoch_eval_dialogs )) self . logger . result ( \"# Avg Reward \" + str ( sum ( self . eval_rewards ) / self . epoch_eval_dialogs )) if self . is_training : return { 'num_dialogs' : self . epoch_train_dialogs , 'turns' : sum ( self . train_turns ) / self . epoch_train_dialogs , 'success' : float ( sum ( self . train_success )) / self . epoch_train_dialogs , 'reward' : float ( sum ( self . eval_rewards )) / self . epoch_train_dialogs } else : return { 'num_dialogs' : self . epoch_eval_dialogs , 'turns' : sum ( self . eval_turns ) / self . epoch_eval_dialogs , 'success' : float ( sum ( self . eval_success )) / self . epoch_eval_dialogs , 'reward' : float ( sum ( self . eval_rewards )) / self . epoch_eval_dialogs }","title":"end_epoch()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.eval","text":"sets teh evaluator in eval mode Source code in adviser/services/stats/evaluation.py 172 173 174 175 176 def eval ( self ): \"\"\" sets teh evaluator in eval mode \"\"\" self . is_training = False","title":"eval()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.evaluate_turn","text":"Source code in adviser/services/stats/evaluation.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"evaluate_turn()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.start_epoch","text":"Handles resetting variables between epochs Source code in adviser/services/stats/evaluation.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def start_epoch ( self ): \"\"\" Handles resetting variables between epochs \"\"\" # global statistics self . epoch_train_dialogs = 0 self . epoch_eval_dialogs = 0 self . train_rewards = [] self . eval_rewards = [] self . train_success = [] self . eval_success = [] self . train_turns = [] self . eval_turns = [] self . epoch += 1 self . logger . info ( \"### \\n ### EPOCH\" + str ( self . epoch ) + \" ### \\n ###\" )","title":"start_epoch()"},{"location":"api/services/#adviser.services.stats.evaluation.PolicyEvaluator.train","text":"sets the evaluator in train mode Source code in adviser/services/stats/evaluation.py 166 167 168 169 170 def train ( self ): \"\"\" sets the evaluator in train mode \"\"\" self . is_training = True","title":"train()"},{"location":"api/services/#adviser.services.ust","text":"","title":"ust"},{"location":"api/services/#adviser.services.ust.ust","text":"","title":"ust"},{"location":"api/services/#adviser.services.ust.ust.HandcraftedUST","text":"A rule-based approach on user state tracking. Currently very minimalist","title":"HandcraftedUST"},{"location":"api/services/#adviser.services.ust.ust.HandcraftedUST.__init__","text":"Source code in adviser/services/ust/ust.py 30 31 32 33 def __init__ ( self , domain = None , logger = None ): Service . __init__ ( self , domain = domain ) self . logger = logger self . us = UserState ()","title":"__init__()"},{"location":"api/services/#adviser.services.ust.ust.HandcraftedUST.dialog_start","text":"Resets the user state so it is ready for a new dialog Source code in adviser/services/ust/ust.py 56 57 58 59 60 61 def dialog_start ( self ): \"\"\" Resets the user state so it is ready for a new dialog \"\"\" # initialize belief state self . us = UserState ()","title":"dialog_start()"},{"location":"api/services/#adviser.services.ust.ust.HandcraftedUST.update_emotion","text":"Source code in adviser/services/ust/ust.py 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def delegate ( self , * args , ** kwargs ): func_inst = getattr ( self , func . __name__ ) callargs = list ( args ) if self in callargs : # remove self when in *args, because already known to function callargs . remove ( self ) result = func ( self , * callargs , ** kwargs ) if result : # fix! (user could have multiple \"/\" characters in topic - only use last one ) domains = { res . split ( \"/\" )[ 0 ]: res . split ( \"/\" )[ 1 ] if \"/\" in res else \"\" for res in result } result = { key . split ( \"/\" )[ 0 ]: result [ key ] for key in result } if func_inst not in self . _publish_sockets : # not a publisher, just normal function return result socket = self . _publish_sockets [ func_inst ] domain = self . _domain_name if socket and result : # publish messages for topic in pub_topics : # for topic in result: # NOTE publish any returned value in dict with it's key as topic if topic in result : domain = domain if domain else domains [ topic ] topic_domain_str = f \" { topic } / { domain } \" if domain else topic if topic in self . _pub_topic_domains : topic_domain_str = f \" { topic } / { self . _pub_topic_domains [ topic ] } \" if self . _pub_topic_domains [ topic ] else topic _send_msg ( socket , topic_domain_str , result [ topic ]) if self . debug_logger : self . debug_logger . info ( f \"- (DS): sent message from { func } to topic { topic_domain_str } : \\n { result [ topic ] } \" ) return result","title":"update_emotion()"},{"location":"api/tools/","text":"Tools \u00b6 \u00b6 create_ontology \u00b6 Database \u00b6 __init__ ( self , path ) special \u00b6 Source code in adviser/tools/create_ontology.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , path ): conn = sqlite3 . connect ( path ) cursor = conn . cursor () self . tables = {} # result will be (type, name, tbl_name, rootpage, sql) cursor . execute ( \"SELECT * FROM sqlite_master where type='table'\" ) for _ , _ , table , _ , _ in cursor . fetchall (): self . tables [ table ] = DatabaseTable ( table ) for table in self . tables . keys (): # get fields/slots # result will be (id, name, type, not null, default, primary key) cursor . execute ( f \"PRAGMA table_info( { table } );\" ) self . tables [ table ] . fields = cursor . fetchall () # make sure that fields are sorted according to field index (should be already anyway) self . tables [ table ] . fields = sorted ( self . tables [ table ] . fields , key = lambda field : field [ 0 ]) # get entries (especially for possible values) cursor . execute ( f \"SELECT * FROM { table } \" ) self . tables [ table ] . entries = cursor . fetchall () get_slot_values ( self , table , slot ) \u00b6 Source code in adviser/tools/create_ontology.py 102 103 def get_slot_values ( self , table , slot ): return self . tables [ table ] . get_slot_values ( slot ) get_slots ( self , table ) \u00b6 Source code in adviser/tools/create_ontology.py 99 100 def get_slots ( self , table ): return self . tables [ table ] . get_slots () get_tables ( self ) \u00b6 Source code in adviser/tools/create_ontology.py 96 97 def get_tables ( self ): return list ( self . tables . keys ()) DatabaseTable \u00b6 __init__ ( self , name ) special \u00b6 Source code in adviser/tools/create_ontology.py 47 48 49 50 def __init__ ( self , name ): self . name = name self . fields = [] self . entries = [] get_slot_values ( self , slot , dontcare = False ) \u00b6 Source code in adviser/tools/create_ontology.py 61 62 63 64 65 66 67 68 def get_slot_values ( self , slot , dontcare = False ): # get slot id id = self . _get_slot_id ( slot ) assert id >= 0 , f \"Slot ' { slot } ' is not part of the database table ' { self . name } '\" values = sorted ( list ( set ([ entry [ id ] for entry in self . entries ]))) if dontcare and not ( 'dontcare' in values or \"do n't care\" in values ): values . append ( 'dontcare' ) return values get_slots ( self ) \u00b6 Source code in adviser/tools/create_ontology.py 58 59 def get_slots ( self ): return [ field [ 1 ] for field in self . fields ] get_defaults () \u00b6 Source code in adviser/tools/create_ontology.py 105 106 107 108 def get_defaults (): return { 'discourseAct' : [ \"ack\" , \"hello\" , \"none\" , \"silence\" , \"thanks\" , \"bad\" ], 'method' : [ \"none\" , \"byconstraints\" , \"byprimarykey\" , \"finished\" , \"byalternatives\" , \"restart\" ], 'key' : 'name' } run_questions ( db ) \u00b6 Source code in adviser/tools/create_ontology.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def run_questions ( db : Database ): # initialize with default values answers = get_defaults () questions = [ { 'type' : 'list' , 'qmark' : '>>>' , 'message' : 'Select table to create ontology for' , 'name' : 'table' , 'choices' : [{ 'key' : str ( id ), 'name' : table , 'value' : table } for id , table in enumerate ( db . get_tables ())], 'validate' : lambda answer : 'You must choose at least one table.' \\ if len ( answer ) == 0 else True }, { 'type' : 'input' , 'qmark' : '>>>' , 'message' : 'Enter the name of the domain:' , 'name' : 'domain' , 'default' : lambda answers : answers [ 'table' ] }, { 'type' : 'list' , 'qmark' : '>>>' , 'name' : 'key' , 'message' : 'Which slot will be used as key? (The key uniquely identifies an entity in the database, e.g. the name in case of restaurants)' , 'choices' : lambda answers : [{ 'name' : slot } for slot in db . get_slots ( answers [ 'table' ])] }, { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'requestable' , 'message' : 'Select user requestables' , 'choices' : lambda answers : [{ 'name' : slot , 'checked' : slot != 'id' } for slot in db . get_slots ( answers [ 'table' ])] }, { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'system_requestable' , 'message' : 'Select system requestables' , 'choices' : lambda answers : [{ 'name' : slot } for slot in db . get_slots ( answers [ 'table' ])] }, { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'informable' , 'message' : 'Select informable slots' , 'choices' : lambda answers : [{ 'name' : slot } for slot in db . get_slots ( answers [ 'table' ])] }] answers_ = prompt ( questions , style = custom_style_2 ) # check whether there are answers (e.g. if the user cancels the prompt using Ctrl+c) if not answers_ : exit () answers . update ( answers_ ) # get values for informable slots questions = [ { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : slot , 'message' : f 'Select values for informable slot { slot } ' , 'choices' : [{ 'name' : value , 'checked' : value != 'dontcare' } for value in db . get_slot_values ( answers [ 'table' ], slot )] } for slot in answers [ 'informable' ] ] values = prompt ( questions , style = custom_style_2 ) # merge informable slot values with informable slots answers [ 'informable' ] = { slot : values [ slot ] for slot in answers [ 'informable' ] if slot in values } # get binary slots questions = [ { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'binary' , 'message' : 'Select binary slots' , 'choices' : [{ 'name' : slot , 'checked' : set ( db . get_slot_values ( answers [ 'table' ], slot )) == { 'true' , 'false' }} for slot in list ( answers [ 'informable' ] . keys ()) + answers [ 'requestable' ] + answers [ 'system_requestable' ]] } ] answers_ = prompt ( questions , style = custom_style_2 ) # check whether there are answers (e.g. if the user cancels the prompt using Ctrl+c) if not answers_ : exit () answers . update ( answers_ ) return answers espnet_minimal special \u00b6 asr special \u00b6 asr_utils \u00b6 add_gradient_noise ( model , iteration , duration = 100 , eta = 1.0 , scale_factor = 0.55 ) \u00b6 Adds noise from a standard normal distribution to the gradients. The standard deviation ( sigma ) is controlled by the three hyper-parameters below. sigma goes to zero (no noise) with more iterations. Parameters: Name Type Description Default model torch.nn.model Model. required iteration int Number of iterations. required duration int) {100, 1000} Number of durations to control the interval of the sigma change. 100 eta float) {0.01, 0.3, 1.0} The magnitude of sigma . 1.0 scale_factor float) {0.55} The scale of sigma . 0.55 Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def add_gradient_noise ( model , iteration , duration = 100 , eta = 1.0 , scale_factor = 0.55 ): \"\"\"Adds noise from a standard normal distribution to the gradients. The standard deviation (`sigma`) is controlled by the three hyper-parameters below. `sigma` goes to zero (no noise) with more iterations. Args: model (torch.nn.model): Model. iteration (int): Number of iterations. duration (int) {100, 1000}: Number of durations to control the interval of the `sigma` change. eta (float) {0.01, 0.3, 1.0}: The magnitude of `sigma`. scale_factor (float) {0.55}: The scale of `sigma`. \"\"\" interval = ( iteration // duration ) + 1 sigma = eta / interval ** scale_factor for param in model . parameters (): if param . grad is not None : _shape = param . grad . size () noise = sigma * torch . randn ( _shape ) . to ( param . device ) param . grad += noise get_model_conf ( model_path , conf_path = None ) \u00b6 Get model config information by reading a model config file (model.json). Parameters: Name Type Description Default model_path str Model path. required conf_path str Optional model config path. None Returns: Type Description list[int, int, dict[str, Any]] Config information loaded from json file. Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_model_conf ( model_path , conf_path = None ): \"\"\"Get model config information by reading a model config file (model.json). Args: model_path (str): Model path. conf_path (str): Optional model config path. Returns: list[int, int, dict[str, Any]]: Config information loaded from json file. \"\"\" if conf_path is None : model_conf = os . path . dirname ( model_path ) + '/model.json' else : model_conf = conf_path with open ( model_conf , \"rb\" ) as f : logging . info ( 'reading a config file from ' + model_conf ) confs = json . load ( f ) if isinstance ( confs , dict ): # for lm args = confs return argparse . Namespace ( ** args ) else : # for asr, tts, mt idim , odim , args = confs return idim , odim , argparse . Namespace ( ** args ) torch_load ( path , model ) \u00b6 Load torch model states. Parameters: Name Type Description Default path str Model path or snapshot file path to be loaded. required model torch.nn.Module Torch model. required Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def torch_load ( path , model ): \"\"\"Load torch model states. Args: path (str): Model path or snapshot file path to be loaded. model (torch.nn.Module): Torch model. \"\"\" if 'snapshot' in path : model_state_dict = torch . load ( path , map_location = lambda storage , loc : storage )[ 'model' ] else : model_state_dict = torch . load ( path , map_location = lambda storage , loc : storage ) # debugging: # print(model_state_dict) if hasattr ( model , 'module' ): model . module . load_state_dict ( model_state_dict ) else : model . load_state_dict ( model_state_dict ) del model_state_dict torch_save ( path , model ) \u00b6 Save torch model states. Parameters: Name Type Description Default path str Model path to be saved. required model torch.nn.Module Torch model. required Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 70 71 72 73 74 75 76 77 78 79 80 81 def torch_save ( path , model ): \"\"\"Save torch model states. Args: path (str): Model path to be saved. model (torch.nn.Module): Torch model. \"\"\" if hasattr ( model , 'module' ): torch . save ( model . module . state_dict (), path ) else : torch . save ( model . state_dict (), path ) pytorch_backend special \u00b6 asr_init \u00b6 Finetuning methods. filter_modules ( model_state_dict , modules ) \u00b6 Filter non-matched modules in module_state_dict. Parameters: Name Type Description Default model_state_dict OrderedDict trained model state_dict required modules list specified module list for transfer required Returns: Type Description new_mods (list) the update module list Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def filter_modules ( model_state_dict , modules ): \"\"\"Filter non-matched modules in module_state_dict. Args: model_state_dict (OrderedDict): trained model state_dict modules (list): specified module list for transfer Return: new_mods (list): the update module list \"\"\" new_mods = [] incorrect_mods = [] mods_model = list ( model_state_dict . keys ()) for mod in modules : if any ( key . startswith ( mod ) for key in mods_model ): new_mods += [ mod ] else : incorrect_mods += [ mod ] if incorrect_mods : logging . warning ( \"module(s) %s don \\' t match or (partially match) \" \"available modules in model.\" , incorrect_mods ) logging . warning ( 'for information, the existing modules in model are:' ) logging . warning ( ' %s ' , mods_model ) return new_mods get_partial_asr_mt_state_dict ( model_state_dict , modules ) \u00b6 Create state_dict with specified modules matching input model modules. Parameters: Name Type Description Default model_state_dict OrderedDict trained model state_dict required modules list specified module list for transfer required Returns: Type Description new_state_dict (OrderedDict) the updated state_dict Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def get_partial_asr_mt_state_dict ( model_state_dict , modules ): \"\"\"Create state_dict with specified modules matching input model modules. Args: model_state_dict (OrderedDict): trained model state_dict modules (list): specified module list for transfer Return: new_state_dict (OrderedDict): the updated state_dict \"\"\" new_state_dict = OrderedDict () for key , value in model_state_dict . items (): if any ( key . startswith ( m ) for m in modules ): new_state_dict [ key ] = value return new_state_dict get_partial_lm_state_dict ( model_state_dict , modules ) \u00b6 Create compatible ASR state_dict from model_state_dict (LM). The keys for specified modules are modified to match ASR decoder modules keys. Parameters: Name Type Description Default model_state_dict OrderedDict trained model state_dict required modules list specified module list for transfer required Returns: Type Description new_state_dict (OrderedDict) the updated state_dict new_mods (list): the updated module list Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def get_partial_lm_state_dict ( model_state_dict , modules ): \"\"\"Create compatible ASR state_dict from model_state_dict (LM). The keys for specified modules are modified to match ASR decoder modules keys. Args: model_state_dict (OrderedDict): trained model state_dict modules (list): specified module list for transfer Return: new_state_dict (OrderedDict): the updated state_dict new_mods (list): the updated module list \"\"\" new_state_dict = OrderedDict () new_modules = [] for key , value in list ( model_state_dict . items ()): if key == \"predictor.embed.weight\" and \"predictor.embed.\" in modules : new_key = \"dec.embed.weight\" new_state_dict [ new_key ] = value new_modules += [ new_key ] elif \"predictor.rnn.\" in key and \"predictor.rnn.\" in modules : new_key = \"dec.decoder.\" + key . split ( \"predictor.rnn.\" , 1 )[ 1 ] new_state_dict [ new_key ] = value new_modules += [ new_key ] return new_state_dict , new_modules get_root_dir () \u00b6 Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 16 17 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))))) get_trained_model_state_dict ( model_path ) \u00b6 Extract the trained model state dict for pre-initialization. Parameters: Name Type Description Default model_path str Path to model.***.best required Returns: Type Description model.state_dict() (OrderedDict) the loaded model state_dict (str): Type of model. Either ASR/MT or LM. Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def get_trained_model_state_dict ( model_path ): \"\"\"Extract the trained model state dict for pre-initialization. Args: model_path (str): Path to model.***.best Return: model.state_dict() (OrderedDict): the loaded model state_dict (str): Type of model. Either ASR/MT or LM. \"\"\" conf_path = os . path . join ( os . path . dirname ( model_path ), 'model.json' ) if 'rnnlm' in model_path : logging . warning ( 'reading model parameters from %s ' , model_path ) return torch . load ( model_path ), 'lm' idim , odim , args = get_model_conf ( model_path , conf_path ) logging . warning ( 'reading model parameters from ' + model_path ) if hasattr ( args , \"model_module\" ): model_module = args . model_module else : model_module = \"services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_asr:E2E\" model_class = dynamic_import ( model_module ) model = model_class ( idim , odim , args ) torch_load ( model_path , model ) assert isinstance ( model , MTInterface ) or isinstance ( model , ASRInterface ) return model . state_dict (), 'asr-mt' load_trained_model ( model_path ) \u00b6 Load the trained model for recognition. Parameters: Name Type Description Default model_path str Path to model.***.best required Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def load_trained_model ( model_path ): \"\"\"Load the trained model for recognition. Args: model_path (str): Path to model.***.best \"\"\" idim , odim , train_args = get_model_conf ( model_path , os . path . join ( get_root_dir (), os . path . dirname ( model_path ), 'model.json' )) # logging.warning('reading model parameters from ' + model_path) if hasattr ( train_args , \"model_module\" ): model_module = train_args . model_module else : model_module = \"services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_asr:E2E\" model_class = dynamic_import ( model_module ) model = model_class ( idim , odim , train_args ) torch_load ( model_path , model ) return model , train_args load_trained_modules ( idim , odim , args , interface =< class ' tools . espnet_minimal . nets . asr_interface . ASRInterface '>) \u00b6 Load model encoder or/and decoder modules with ESPNET pre-trained model(s). Parameters: Name Type Description Default idim int initial input dimension. required odim int initial output dimension. required args Namespace The initial model arguments. required interface Interface ASRInterface or STInterface <class 'tools.espnet_minimal.nets.asr_interface.ASRInterface'> Returns: Type Description model (torch.nn.Module) The model with pretrained modules. Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def load_trained_modules ( idim , odim , args , interface = ASRInterface ): \"\"\"Load model encoder or/and decoder modules with ESPNET pre-trained model(s). Args: idim (int): initial input dimension. odim (int): initial output dimension. args (Namespace): The initial model arguments. interface (Interface): ASRInterface or STInterface Return: model (torch.nn.Module): The model with pretrained modules. \"\"\" enc_model_path = args . enc_init dec_model_path = args . dec_init enc_modules = args . enc_init_mods dec_modules = args . dec_init_mods model_class = dynamic_import ( args . model_module ) main_model = model_class ( idim , odim , args ) assert isinstance ( main_model , interface ) main_state_dict = main_model . state_dict () logging . warning ( 'model(s) found for pre-initialization' ) for model_path , modules in [( enc_model_path , enc_modules ), ( dec_model_path , dec_modules )]: if model_path is not None : if os . path . isfile ( model_path ): model_state_dict , mode = get_trained_model_state_dict ( model_path ) modules = filter_modules ( model_state_dict , modules ) if mode == 'lm' : partial_state_dict , modules = get_partial_lm_state_dict ( model_state_dict , modules ) else : partial_state_dict = get_partial_asr_mt_state_dict ( model_state_dict , modules ) if partial_state_dict : if transfer_verification ( main_state_dict , partial_state_dict , modules ): logging . warning ( 'loading %s from model: %s ' , modules , model_path ) for k in partial_state_dict . keys (): logging . warning ( 'override %s ' % k ) main_state_dict . update ( partial_state_dict ) else : logging . warning ( 'modules %s in model %s don \\' t match your training config' , modules , model_path ) else : logging . warning ( 'model was not found : %s ' , model_path ) main_model . load_state_dict ( main_state_dict ) return main_model transfer_verification ( model_state_dict , partial_state_dict , modules ) \u00b6 Verify tuples (key, shape) for input model modules match specified modules. Parameters: Name Type Description Default model_state_dict OrderedDict the initial model state_dict required partial_state_dict OrderedDict the trained model state_dict required modules list specified module list for transfer required Returns: Type Description (boolean) allow transfer Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def transfer_verification ( model_state_dict , partial_state_dict , modules ): \"\"\"Verify tuples (key, shape) for input model modules match specified modules. Args: model_state_dict (OrderedDict): the initial model state_dict partial_state_dict (OrderedDict): the trained model state_dict modules (list): specified module list for transfer Return: (boolean): allow transfer \"\"\" partial_modules = [] for key_p , value_p in partial_state_dict . items (): if any ( key_p . startswith ( m ) for m in modules ): if value_p . shape == model_state_dict [ key_p ] . shape : partial_modules += [( key_p , value_p . shape )] return len ( partial_modules ) > 0 bin special \u00b6 asr_recog \u00b6 End-to-end speech recognition model decoding script. get_parser () \u00b6 Get default arguments. Source code in adviser/tools/espnet_minimal/bin/asr_recog.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def get_parser (): \"\"\"Get default arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Transcribe text from speech using a speech recognition model on one CPU or GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'Config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'Second config file path that overwrites the settings in `--config`' ) parser . add ( '--config3' , is_config_file = True , help = 'Third config file path that overwrites the settings in `--config` and `--config2`' ) parser . add_argument ( '--ngpu' , type = int , default = 0 , help = 'Number of GPUs' ) parser . add_argument ( '--dtype' , choices = ( \"float16\" , \"float32\" , \"float64\" ), default = \"float32\" , help = 'Float precision (only available in --api v2)' ) parser . add_argument ( '--backend' , type = str , default = 'chainer' , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , type = int , default = 1 , help = 'Debugmode' ) parser . add_argument ( '--seed' , type = int , default = 1 , help = 'Random seed' ) parser . add_argument ( '--verbose' , '-V' , type = int , default = 1 , help = 'Verbose option' ) parser . add_argument ( '--batchsize' , type = int , default = 1 , help = 'Batch size for beam search (0: means no batch processing)' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) parser . add_argument ( '--api' , default = \"v1\" , choices = [ \"v1\" , \"v2\" ], help = '''Beam search APIs v1: Default API. It only supports the ASRInterface.recognize method and DefaultRNNLM. v2: Experimental API. It supports any models that implements ScorerInterface.''' ) # task related parser . add_argument ( '--recog-json' , type = str , help = 'Filename of recognition data (json)' ) parser . add_argument ( '--result-label' , type = str , required = True , help = 'Filename of result label data (json)' ) # model (parameter) related parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) parser . add_argument ( '--model-conf' , type = str , default = None , help = 'Model config file' ) parser . add_argument ( '--num-spkrs' , type = int , default = 1 , choices = [ 1 , 2 ], help = 'Number of speakers in the speech' ) parser . add_argument ( '--num-encs' , default = 1 , type = int , help = 'Number of encoders in the model.' ) # search related parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 1 , help = 'Beam size' ) parser . add_argument ( '--penalty' , type = float , default = 0.0 , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , type = float , default = 0.0 , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , type = float , default = 0.0 , help = 'Input length ratio to obtain min output length' ) parser . add_argument ( '--ctc-weight' , type = float , default = 0.0 , help = 'CTC weight in joint decoding' ) parser . add_argument ( '--weights-ctc-dec' , type = float , action = 'append' , help = 'ctc weight assigned to each encoder during decoding.[in multi-encoder mode only]' ) parser . add_argument ( '--ctc-window-margin' , type = int , default = 0 , help = \"\"\"Use CTC window with margin parameter to accelerate CTC/attention decoding especially on GPU. Smaller magin makes decoding faster, but may increase search errors. If margin=0 (default), this function is disabled\"\"\" ) # transducer related parser . add_argument ( '--score-norm-transducer' , type = strtobool , nargs = '?' , default = True , help = 'Normalize transducer scores by length' ) # rnnlm related parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--word-rnnlm' , type = str , default = None , help = 'Word RNNLM model file to read' ) parser . add_argument ( '--word-rnnlm-conf' , type = str , default = None , help = 'Word RNNLM model config file to read' ) parser . add_argument ( '--word-dict' , type = str , default = None , help = 'Word list to read' ) parser . add_argument ( '--lm-weight' , type = float , default = 0.1 , help = 'RNNLM weight' ) # streaming related parser . add_argument ( '--streaming-mode' , type = str , default = None , choices = [ 'window' , 'segment' ], help = \"\"\"Use streaming recognizer for inference. `--batchsize` must be set to 0 to enable this mode\"\"\" ) parser . add_argument ( '--streaming-window' , type = int , default = 10 , help = 'Window size' ) parser . add_argument ( '--streaming-min-blank-dur' , type = int , default = 10 , help = 'Minimum blank duration threshold' ) parser . add_argument ( '--streaming-onset-margin' , type = int , default = 1 , help = 'Onset margin' ) parser . add_argument ( '--streaming-offset-margin' , type = int , default = 1 , help = 'Offset margin' ) return parser main ( args ) \u00b6 Run the main decoding function. Source code in adviser/tools/espnet_minimal/bin/asr_recog.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def main ( args ): \"\"\"Run the main decoding function.\"\"\" parser = get_parser () args = parser . parse_args ( args ) if args . ngpu == 0 and args . dtype == \"float16\" : raise ValueError ( f \"--dtype { args . dtype } does not support the CPU backend.\" ) # logging info if args . verbose == 1 : logging . basicConfig ( level = logging . INFO , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) elif args . verbose == 2 : logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) else : logging . basicConfig ( level = logging . WARN , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) logging . warning ( \"Skip DEBUG/INFO messages\" ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # TODO(mn5k): support of multiple GPUs if args . ngpu > 1 : logging . error ( \"The program only supports ngpu=1.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting random . seed ( args . seed ) np . random . seed ( args . seed ) logging . info ( 'set random seed = %d ' % args . seed ) # validate rnn options if args . rnnlm is not None and args . word_rnnlm is not None : logging . error ( \"It seems that both --rnnlm and --word-rnnlm are specified. Please use either option.\" ) sys . exit ( 1 ) # recog logging . info ( 'backend = ' + args . backend ) if args . num_spkrs == 1 : if args . backend == \"chainer\" : from tools.espnet_minimal.asr.chainer_backend.asr import recog recog ( args ) elif args . backend == \"pytorch\" : if args . num_encs == 1 : # Experimental API that supports custom LMs if args . api == \"v2\" : from tools.espnet_minimal.asr.pytorch_backend.recog import recog_v2 recog_v2 ( args ) else : from tools.espnet_minimal.asr.pytorch_backend.asr import recog if args . dtype != \"float32\" : raise NotImplementedError ( f \"`--dtype { args . dtype } ` is only available with `--api v2`\" ) recog ( args ) else : if args . api == \"v2\" : raise NotImplementedError ( f \"--num-encs { args . num_encs } > 1 is not supported in --api v2\" ) else : from tools.espnet_minimal.asr.pytorch_backend.asr import recog recog ( args ) else : raise ValueError ( \"Only chainer and pytorch are supported.\" ) elif args . num_spkrs == 2 : if args . backend == \"pytorch\" : from tools.espnet_minimal.asr.pytorch_backend.asr_mix import recog recog ( args ) else : raise ValueError ( \"Only pytorch is supported.\" ) asr_train \u00b6 Automatic speech recognition model training script. get_parser ( parser = None , required = True ) \u00b6 Get default arguments. Source code in adviser/tools/espnet_minimal/bin/asr_train.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def get_parser ( parser = None , required = True ): \"\"\"Get default arguments.\"\"\" if parser is None : parser = configargparse . ArgumentParser ( description = \"Train an automatic speech recognition (ASR) model on one CPU, one or multiple GPUs\" , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'second config file path that overwrites the settings in `--config`.' ) parser . add ( '--config3' , is_config_file = True , help = 'third config file path that overwrites the settings in `--config` and `--config2`.' ) parser . add_argument ( '--ngpu' , default = None , type = int , help = 'Number of GPUs. If not given, use all visible devices' ) parser . add_argument ( '--train-dtype' , default = \"float32\" , choices = [ \"float16\" , \"float32\" , \"float64\" , \"O0\" , \"O1\" , \"O2\" , \"O3\" ], help = 'Data type for training (only pytorch backend). ' 'O0,O1,.. flags require apex. See https://nvidia.github.io/apex/amp.html#opt-levels' ) parser . add_argument ( '--backend' , default = 'chainer' , type = str , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--outdir' , type = str , required = required , help = 'Output directory' ) parser . add_argument ( '--debugmode' , default = 1 , type = int , help = 'Debugmode' ) parser . add_argument ( '--dict' , required = required , help = 'Dictionary' ) parser . add_argument ( '--seed' , default = 1 , type = int , help = 'Random seed' ) parser . add_argument ( '--debugdir' , type = str , help = 'Output directory for debugging' ) parser . add_argument ( '--resume' , '-r' , default = '' , nargs = '?' , help = 'Resume the training from snapshot' ) parser . add_argument ( '--minibatches' , '-N' , type = int , default = '-1' , help = 'Process only N minibatches (for debug)' ) parser . add_argument ( '--verbose' , '-V' , default = 0 , type = int , help = 'Verbose option' ) parser . add_argument ( '--tensorboard-dir' , default = None , type = str , nargs = '?' , help = \"Tensorboard log dir path\" ) parser . add_argument ( '--report-interval-iters' , default = 100 , type = int , help = \"Report interval iterations\" ) parser . add_argument ( '--save-interval-iters' , default = 0 , type = int , help = \"Save snapshot interval iterations\" ) # task related parser . add_argument ( '--train-json' , type = str , default = None , help = 'Filename of train label data (json)' ) parser . add_argument ( '--valid-json' , type = str , default = None , help = 'Filename of validation label data (json)' ) # network architecture parser . add_argument ( '--model-module' , type = str , default = None , help = 'model defined module (default: services.hci.speech.espnet_minimal.nets.xxx_backend.e2e_asr:E2E)' ) # encoder parser . add_argument ( '--num-encs' , default = 1 , type = int , help = 'Number of encoders in the model.' ) # loss related parser . add_argument ( '--ctc_type' , default = 'warpctc' , type = str , choices = [ 'builtin' , 'warpctc' ], help = 'Type of CTC implementation to calculate loss.' ) parser . add_argument ( '--mtlalpha' , default = 0.5 , type = float , help = 'Multitask learning coefficient, alpha: alpha*ctc_loss + (1-alpha)*att_loss ' ) parser . add_argument ( '--lsm-weight' , default = 0.0 , type = float , help = 'Label smoothing weight' ) # recognition options to compute CER/WER parser . add_argument ( '--report-cer' , default = False , action = 'store_true' , help = 'Compute CER on development set' ) parser . add_argument ( '--report-wer' , default = False , action = 'store_true' , help = 'Compute WER on development set' ) parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 4 , help = 'Beam size' ) parser . add_argument ( '--penalty' , default = 0.0 , type = float , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , default = 0.0 , type = float , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , default = 0.0 , type = float , help = 'Input length ratio to obtain min output length' ) parser . add_argument ( '--ctc-weight' , default = 0.3 , type = float , help = 'CTC weight in joint decoding' ) parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--lm-weight' , default = 0.1 , type = float , help = 'RNNLM weight.' ) parser . add_argument ( '--sym-space' , default = '<space>' , type = str , help = 'Space symbol' ) parser . add_argument ( '--sym-blank' , default = '<blank>' , type = str , help = 'Blank symbol' ) # minibatch related parser . add_argument ( '--sortagrad' , default = 0 , type = int , nargs = '?' , help = \"How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs\" ) parser . add_argument ( '--batch-count' , default = 'auto' , choices = BATCH_COUNT_CHOICES , help = 'How to count batch_size. The default (auto) will find how to count by args.' ) parser . add_argument ( '--batch-size' , '--batch-seqs' , '-b' , default = 0 , type = int , help = 'Maximum seqs in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-bins' , default = 0 , type = int , help = 'Maximum bins in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-frames-in' , default = 0 , type = int , help = 'Maximum input frames in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-frames-out' , default = 0 , type = int , help = 'Maximum output frames in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-frames-inout' , default = 0 , type = int , help = 'Maximum input+output frames in a minibatch (0 to disable)' ) parser . add_argument ( '--maxlen-in' , '--batch-seq-maxlen-in' , default = 800 , type = int , metavar = 'ML' , help = 'When --batch-count=seq, batch size is reduced if the input sequence length > ML.' ) parser . add_argument ( '--maxlen-out' , '--batch-seq-maxlen-out' , default = 150 , type = int , metavar = 'ML' , help = 'When --batch-count=seq, batch size is reduced if the output sequence length > ML' ) parser . add_argument ( '--n-iter-processes' , default = 0 , type = int , help = 'Number of processes of iterator' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , nargs = '?' , help = 'The configuration file for the pre-processing' ) # optimization related parser . add_argument ( '--opt' , default = 'adadelta' , type = str , choices = [ 'adadelta' , 'adam' , 'noam' ], help = 'Optimizer' ) parser . add_argument ( '--accum-grad' , default = 1 , type = int , help = 'Number of gradient accumuration' ) parser . add_argument ( '--eps' , default = 1e-8 , type = float , help = 'Epsilon constant for optimizer' ) parser . add_argument ( '--eps-decay' , default = 0.01 , type = float , help = 'Decaying ratio of epsilon' ) parser . add_argument ( '--weight-decay' , default = 0.0 , type = float , help = 'Weight decay ratio' ) parser . add_argument ( '--criterion' , default = 'acc' , type = str , choices = [ 'loss' , 'acc' ], help = 'Criterion to perform epsilon decay' ) parser . add_argument ( '--threshold' , default = 1e-4 , type = float , help = 'Threshold to stop iteration' ) parser . add_argument ( '--epochs' , '-e' , default = 30 , type = int , help = 'Maximum number of epochs' ) parser . add_argument ( '--early-stop-criterion' , default = 'validation/main/acc' , type = str , nargs = '?' , help = \"Value to monitor to trigger an early stopping of the training\" ) parser . add_argument ( '--patience' , default = 3 , type = int , nargs = '?' , help = \"Number of epochs to wait without improvement before stopping the training\" ) parser . add_argument ( '--grad-clip' , default = 5 , type = float , help = 'Gradient norm threshold to clip' ) parser . add_argument ( '--num-save-attention' , default = 3 , type = int , help = 'Number of samples of attention to be saved' ) parser . add_argument ( '--grad-noise' , type = strtobool , default = False , help = 'The flag to switch to use noise injection to gradients during training' ) # asr_mix related parser . add_argument ( '--num-spkrs' , default = 1 , type = int , choices = [ 1 , 2 ], help = 'Number of speakers in the speech.' ) # decoder related parser . add_argument ( '--context-residual' , default = False , type = strtobool , nargs = '?' , help = 'The flag to switch to use context vector residual in the decoder network' ) # finetuning related parser . add_argument ( '--enc-init' , default = None , type = str , help = 'Pre-trained ASR model to initialize encoder.' ) parser . add_argument ( '--enc-init-mods' , default = 'enc.enc.' , type = lambda s : [ str ( mod ) for mod in s . split ( ',' ) if s != '' ], help = 'List of encoder modules to initialize, separated by a comma.' ) parser . add_argument ( '--dec-init' , default = None , type = str , help = 'Pre-trained ASR, MT or LM model to initialize decoder.' ) parser . add_argument ( '--dec-init-mods' , default = 'att., dec.' , type = lambda s : [ str ( mod ) for mod in s . split ( ',' ) if s != '' ], help = 'List of decoder modules to initialize, separated by a comma.' ) # front end related parser . add_argument ( '--use-frontend' , type = strtobool , default = False , help = 'The flag to switch to use frontend system.' ) # WPE related parser . add_argument ( '--use-wpe' , type = strtobool , default = False , help = 'Apply Weighted Prediction Error' ) parser . add_argument ( '--wtype' , default = 'blstmp' , type = str , choices = [ 'lstm' , 'blstm' , 'lstmp' , 'blstmp' , 'vgglstmp' , 'vggblstmp' , 'vgglstm' , 'vggblstm' , 'gru' , 'bgru' , 'grup' , 'bgrup' , 'vgggrup' , 'vggbgrup' , 'vgggru' , 'vggbgru' ], help = 'Type of encoder network architecture ' 'of the mask estimator for WPE. ' '' ) parser . add_argument ( '--wlayers' , type = int , default = 2 , help = '' ) parser . add_argument ( '--wunits' , type = int , default = 300 , help = '' ) parser . add_argument ( '--wprojs' , type = int , default = 300 , help = '' ) parser . add_argument ( '--wdropout-rate' , type = float , default = 0.0 , help = '' ) parser . add_argument ( '--wpe-taps' , type = int , default = 5 , help = '' ) parser . add_argument ( '--wpe-delay' , type = int , default = 3 , help = '' ) parser . add_argument ( '--use-dnn-mask-for-wpe' , type = strtobool , default = False , help = 'Use DNN to estimate the power spectrogram. ' 'This option is experimental.' ) # Beamformer related parser . add_argument ( '--use-beamformer' , type = strtobool , default = True , help = '' ) parser . add_argument ( '--btype' , default = 'blstmp' , type = str , choices = [ 'lstm' , 'blstm' , 'lstmp' , 'blstmp' , 'vgglstmp' , 'vggblstmp' , 'vgglstm' , 'vggblstm' , 'gru' , 'bgru' , 'grup' , 'bgrup' , 'vgggrup' , 'vggbgrup' , 'vgggru' , 'vggbgru' ], help = 'Type of encoder network architecture ' 'of the mask estimator for Beamformer.' ) parser . add_argument ( '--blayers' , type = int , default = 2 , help = '' ) parser . add_argument ( '--bunits' , type = int , default = 300 , help = '' ) parser . add_argument ( '--bprojs' , type = int , default = 300 , help = '' ) parser . add_argument ( '--badim' , type = int , default = 320 , help = '' ) parser . add_argument ( '--bnmask' , type = int , default = 2 , help = 'Number of beamforming masks, ' 'default is 2 for [speech, noise].' ) parser . add_argument ( '--ref-channel' , type = int , default =- 1 , help = 'The reference channel used for beamformer. ' 'By default, the channel is estimated by DNN.' ) parser . add_argument ( '--bdropout-rate' , type = float , default = 0.0 , help = '' ) # Feature transform: Normalization parser . add_argument ( '--stats-file' , type = str , default = None , help = 'The stats file for the feature normalization' ) parser . add_argument ( '--apply-uttmvn' , type = strtobool , default = True , help = 'Apply utterance level mean ' 'variance normalization.' ) parser . add_argument ( '--uttmvn-norm-means' , type = strtobool , default = True , help = '' ) parser . add_argument ( '--uttmvn-norm-vars' , type = strtobool , default = False , help = '' ) # Feature transform: Fbank parser . add_argument ( '--fbank-fs' , type = int , default = 16000 , help = 'The sample frequency used for ' 'the mel-fbank creation.' ) parser . add_argument ( '--n-mels' , type = int , default = 80 , help = 'The number of mel-frequency bins.' ) parser . add_argument ( '--fbank-fmin' , type = float , default = 0. , help = '' ) parser . add_argument ( '--fbank-fmax' , type = float , default = None , help = '' ) return parser main ( cmd_args ) \u00b6 Run the main training function. Source code in adviser/tools/espnet_minimal/bin/asr_train.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def main ( cmd_args ): \"\"\"Run the main training function.\"\"\" parser = get_parser () args , _ = parser . parse_known_args ( cmd_args ) if args . backend == \"chainer\" and args . train_dtype != \"float32\" : raise NotImplementedError ( f \"chainer backend does not support --train-dtype { args . train_dtype } .\" \"Use --dtype float32.\" ) if args . ngpu == 0 and args . train_dtype in ( \"O0\" , \"O1\" , \"O2\" , \"O3\" , \"float16\" ): raise ValueError ( f \"--train-dtype { args . train_dtype } does not support the CPU backend.\" ) from tools.espnet_minimal import dynamic_import if args . model_module is None : model_module = \"services.hci.speech.espnet_minimal.nets.\" + args . backend + \"_backend.e2e_asr:E2E\" else : model_module = args . model_module model_class = dynamic_import ( model_module ) model_class . add_arguments ( parser ) args = parser . parse_args ( cmd_args ) args . model_module = model_module if 'chainer_backend' in args . model_module : args . backend = 'chainer' if 'pytorch_backend' in args . model_module : args . backend = 'pytorch' # logging info if args . verbose > 0 : logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) else : logging . basicConfig ( level = logging . WARN , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) logging . warning ( 'Skip DEBUG/INFO messages' ) # If --ngpu is not given, # 1. if CUDA_VISIBLE_DEVICES is set, all visible devices # 2. if nvidia-smi exists, use all devices # 3. else ngpu=0 if args . ngpu is None : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is not None : ngpu = len ( cvd . split ( ',' )) else : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) try : p = subprocess . run ([ 'nvidia-smi' , '-L' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except ( subprocess . CalledProcessError , FileNotFoundError ): ngpu = 0 else : ngpu = len ( p . stderr . decode () . split ( ' \\n ' )) - 1 else : if is_torch_1_2_plus : assert args . ngpu == 1 , \"There are some bugs with multi-GPU processing in PyTorch 1.2+\" \\ \" (see https://github.com/pytorch/pytorch/issues/21108)\" ngpu = args . ngpu logging . info ( f \"ngpu: { ngpu } \" ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # set random seed logging . info ( 'random seed = %d ' % args . seed ) random . seed ( args . seed ) np . random . seed ( args . seed ) # load dictionary for debug log if args . dict is not None : with open ( args . dict , 'rb' ) as f : dictionary = f . readlines () char_list = [ entry . decode ( 'utf-8' ) . split ( ' ' )[ 0 ] for entry in dictionary ] char_list . insert ( 0 , '<blank>' ) char_list . append ( '<eos>' ) args . char_list = char_list else : args . char_list = None # train logging . info ( 'backend = ' + args . backend ) if args . num_spkrs == 1 : if args . backend == \"chainer\" : from tools.espnet_minimal.asr.chainer_backend.asr import train train ( args ) elif args . backend == \"pytorch\" : from tools.espnet_minimal.asr.pytorch_backend.asr import train train ( args ) else : raise ValueError ( \"Only chainer and pytorch are supported.\" ) else : # FIXME(kamo): Support --model-module if args . backend == \"pytorch\" : from tools.espnet_minimal.asr.pytorch_backend.asr_mix import train train ( args ) else : raise ValueError ( \"Only pytorch is supported.\" ) lm_train \u00b6 Language model training script. get_parser () \u00b6 Get parser. Source code in adviser/tools/espnet_minimal/bin/lm_train.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def get_parser (): \"\"\"Get parser.\"\"\" parser = configargparse . ArgumentParser ( description = 'Train a new language model on one CPU or one GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'second config file path that overwrites the settings in `--config`.' ) parser . add ( '--config3' , is_config_file = True , help = 'third config file path that overwrites the settings in `--config` and `--config2`.' ) parser . add_argument ( '--ngpu' , default = None , type = int , help = 'Number of GPUs. If not given, use all visible devices' ) parser . add_argument ( '--train-dtype' , default = \"float32\" , choices = [ \"float16\" , \"float32\" , \"float64\" , \"O0\" , \"O1\" , \"O2\" , \"O3\" ], help = 'Data type for training (only pytorch backend). ' 'O0,O1,.. flags require apex. See https://nvidia.github.io/apex/amp.html#opt-levels' ) parser . add_argument ( '--backend' , default = 'chainer' , type = str , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--outdir' , type = str , required = True , help = 'Output directory' ) parser . add_argument ( '--debugmode' , default = 1 , type = int , help = 'Debugmode' ) parser . add_argument ( '--dict' , type = str , required = True , help = 'Dictionary' ) parser . add_argument ( '--seed' , default = 1 , type = int , help = 'Random seed' ) parser . add_argument ( '--resume' , '-r' , default = '' , nargs = '?' , help = 'Resume the training from snapshot' ) parser . add_argument ( '--verbose' , '-V' , default = 0 , type = int , help = 'Verbose option' ) parser . add_argument ( '--tensorboard-dir' , default = None , type = str , nargs = '?' , help = \"Tensorboard log dir path\" ) parser . add_argument ( '--report-interval-iters' , default = 100 , type = int , help = \"Report interval iterations\" ) # task related parser . add_argument ( '--train-label' , type = str , required = True , help = 'Filename of train label data' ) parser . add_argument ( '--valid-label' , type = str , required = True , help = 'Filename of validation label data' ) parser . add_argument ( '--test-label' , type = str , help = 'Filename of test label data' ) parser . add_argument ( '--dump-hdf5-path' , type = str , default = None , help = 'Path to dump a preprocessed dataset as hdf5' ) # training configuration parser . add_argument ( '--opt' , default = 'sgd' , type = str , choices = [ 'sgd' , 'adam' ], help = 'Optimizer' ) parser . add_argument ( '--sortagrad' , default = 0 , type = int , nargs = '?' , help = \"How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs\" ) parser . add_argument ( '--batchsize' , '-b' , type = int , default = 300 , help = 'Number of examples in each mini-batch' ) parser . add_argument ( '--epoch' , '-e' , type = int , default = 20 , help = 'Number of sweeps over the dataset to train' ) parser . add_argument ( '--early-stop-criterion' , default = 'validation/main/loss' , type = str , nargs = '?' , help = \"Value to monitor to trigger an early stopping of the training\" ) parser . add_argument ( '--patience' , default = 3 , type = int , nargs = '?' , help = \"Number of epochs to wait without improvement before stopping the training\" ) parser . add_argument ( '--gradclip' , '-c' , type = float , default = 5 , help = 'Gradient norm threshold to clip' ) parser . add_argument ( '--maxlen' , type = int , default = 40 , help = 'Batch size is reduced if the input sequence > ML' ) parser . add_argument ( '--model-module' , type = str , default = 'default' , help = 'model defined module (default: services.hci.speech.espnet_minimal.nets.xxx_backend.lm.default:DefaultRNNLM)' ) return parser main ( cmd_args ) \u00b6 Train LM. Source code in adviser/tools/espnet_minimal/bin/lm_train.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def main ( cmd_args ): \"\"\"Train LM.\"\"\" parser = get_parser () args , _ = parser . parse_known_args ( cmd_args ) if args . backend == \"chainer\" and args . train_dtype != \"float32\" : raise NotImplementedError ( f \"chainer backend does not support --train-dtype { args . train_dtype } .\" \"Use --dtype float32.\" ) if args . ngpu == 0 and args . train_dtype in ( \"O0\" , \"O1\" , \"O2\" , \"O3\" , \"float16\" ): raise ValueError ( f \"--train-dtype { args . train_dtype } does not support the CPU backend.\" ) # parse model-specific arguments dynamically model_class = dynamic_import_lm ( args . model_module , args . backend ) model_class . add_arguments ( parser ) args = parser . parse_args ( cmd_args ) # logging info if args . verbose > 0 : logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) else : logging . basicConfig ( level = logging . WARN , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) logging . warning ( 'Skip DEBUG/INFO messages' ) # If --ngpu is not given, # 1. if CUDA_VISIBLE_DEVICES is set, all visible devices # 2. if nvidia-smi exists, use all devices # 3. else ngpu=0 if args . ngpu is None : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is not None : ngpu = len ( cvd . split ( ',' )) else : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) try : p = subprocess . run ([ 'nvidia-smi' , '-L' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except ( subprocess . CalledProcessError , FileNotFoundError ): ngpu = 0 else : ngpu = len ( p . stderr . decode () . split ( ' \\n ' )) - 1 else : ngpu = args . ngpu logging . info ( f \"ngpu: { ngpu } \" ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting nseed = args . seed random . seed ( nseed ) np . random . seed ( nseed ) # load dictionary with open ( args . dict , 'rb' ) as f : dictionary = f . readlines () char_list = [ entry . decode ( 'utf-8' ) . split ( ' ' )[ 0 ] for entry in dictionary ] char_list . insert ( 0 , '<blank>' ) char_list . append ( '<eos>' ) args . char_list_dict = { x : i for i , x in enumerate ( char_list )} args . n_vocab = len ( char_list ) # train logging . info ( 'backend = ' + args . backend ) if args . backend == \"chainer\" : from tools.espnet_minimal import train train ( args ) elif args . backend == \"pytorch\" : from tools.espnet_minimal.lm.pytorch_backend.lm import train train ( args ) else : raise ValueError ( \"Only chainer and pytorch are supported.\" ) mt_trans \u00b6 Neural machine translation model decoding script. get_parser () \u00b6 Get default arguments. Source code in adviser/tools/espnet_minimal/bin/mt_trans.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def get_parser (): \"\"\"Get default arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Translate text from speech using a speech translation model on one CPU or GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'Config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'Second config file path that overwrites the settings in `--config`' ) parser . add ( '--config3' , is_config_file = True , help = 'Third config file path that overwrites the settings in `--config` and `--config2`' ) parser . add_argument ( '--ngpu' , type = int , default = 0 , help = 'Number of GPUs' ) parser . add_argument ( '--dtype' , choices = ( \"float16\" , \"float32\" , \"float64\" ), default = \"float32\" , help = 'Float precision (only available in --api v2)' ) parser . add_argument ( '--backend' , type = str , default = 'chainer' , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , type = int , default = 1 , help = 'Debugmode' ) parser . add_argument ( '--seed' , type = int , default = 1 , help = 'Random seed' ) parser . add_argument ( '--verbose' , '-V' , type = int , default = 1 , help = 'Verbose option' ) parser . add_argument ( '--batchsize' , type = int , default = 1 , help = 'Batch size for beam search (0: means no batch processing)' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) parser . add_argument ( '--api' , default = \"v1\" , choices = [ \"v1\" , \"v2\" ], help = '''Beam search APIs v1: Default API. It only supports the ASRInterface.recognize method and DefaultRNNLM. v2: Experimental API. It supports any models that implements ScorerInterface.''' ) # task related parser . add_argument ( '--trans-json' , type = str , help = 'Filename of translation data (json)' ) parser . add_argument ( '--result-label' , type = str , required = True , help = 'Filename of result label data (json)' ) # model (parameter) related parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) parser . add_argument ( '--model-conf' , type = str , default = None , help = 'Model config file' ) # search related parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 1 , help = 'Beam size' ) parser . add_argument ( '--penalty' , type = float , default = 0.1 , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , type = float , default = 3.0 , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , type = float , default = 0.0 , help = 'Input length ratio to obtain min output length' ) # rnnlm related parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--lm-weight' , type = float , default = 0.0 , help = 'RNNLM weight' ) # multilingual related parser . add_argument ( '--tgt-lang' , default = False , type = str , help = 'target language ID (e.g., <en>, <de>, and <fr> etc.)' ) return parser main ( args ) \u00b6 Run the main decoding function. Source code in adviser/tools/espnet_minimal/bin/mt_trans.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def main ( args ): \"\"\"Run the main decoding function.\"\"\" parser = get_parser () args = parser . parse_args ( args ) # logging info if args . verbose == 1 : logging . basicConfig ( level = logging . INFO , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) elif args . verbose == 2 : logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) else : logging . basicConfig ( level = logging . WARN , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) logging . warning ( \"Skip DEBUG/INFO messages\" ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # TODO(mn5k): support of multiple GPUs if args . ngpu > 1 : logging . error ( \"The program only supports ngpu=1.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting random . seed ( args . seed ) np . random . seed ( args . seed ) logging . info ( 'set random seed = %d ' % args . seed ) # trans logging . info ( 'backend = ' + args . backend ) if args . backend == \"pytorch\" : # Experimental API that supports custom LMs from tools.espnet_minimal.mt.pytorch_backend.mt import trans if args . dtype != \"float32\" : raise NotImplementedError ( f \"`--dtype { args . dtype } ` is only available with `--api v2`\" ) trans ( args ) else : raise ValueError ( \"Only pytorch are supported.\" ) st_trans \u00b6 End-to-end speech translation model decoding script. get_parser () \u00b6 Get default arguments. Source code in adviser/tools/espnet_minimal/bin/st_trans.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def get_parser (): \"\"\"Get default arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Translate text from speech using a speech translation model on one CPU or GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'Config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'Second config file path that overwrites the settings in `--config`' ) parser . add ( '--config3' , is_config_file = True , help = 'Third config file path that overwrites the settings in `--config` and `--config2`' ) parser . add_argument ( '--ngpu' , type = int , default = 0 , help = 'Number of GPUs' ) parser . add_argument ( '--dtype' , choices = ( \"float16\" , \"float32\" , \"float64\" ), default = \"float32\" , help = 'Float precision (only available in --api v2)' ) parser . add_argument ( '--backend' , type = str , default = 'chainer' , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , type = int , default = 1 , help = 'Debugmode' ) parser . add_argument ( '--seed' , type = int , default = 1 , help = 'Random seed' ) parser . add_argument ( '--verbose' , '-V' , type = int , default = 1 , help = 'Verbose option' ) parser . add_argument ( '--batchsize' , type = int , default = 1 , help = 'Batch size for beam search (0: means no batch processing)' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) parser . add_argument ( '--api' , default = \"v1\" , choices = [ \"v1\" , \"v2\" ], help = '''Beam search APIs v1: Default API. It only supports the ASRInterface.recognize method and DefaultRNNLM. v2: Experimental API. It supports any models that implements ScorerInterface.''' ) # task related parser . add_argument ( '--trans-json' , type = str , help = 'Filename of translation data (json)' ) parser . add_argument ( '--result-label' , type = str , required = True , help = 'Filename of result label data (json)' ) # model (parameter) related parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) # search related parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 1 , help = 'Beam size' ) parser . add_argument ( '--penalty' , type = float , default = 0.0 , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , type = float , default = 0.0 , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , type = float , default = 0.0 , help = 'Input length ratio to obtain min output length' ) # rnnlm related parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--word-rnnlm' , type = str , default = None , help = 'Word RNNLM model file to read' ) parser . add_argument ( '--word-rnnlm-conf' , type = str , default = None , help = 'Word RNNLM model config file to read' ) parser . add_argument ( '--word-dict' , type = str , default = None , help = 'Word list to read' ) parser . add_argument ( '--lm-weight' , type = float , default = 0.1 , help = 'RNNLM weight' ) # multilingual related parser . add_argument ( '--tgt-lang' , default = False , type = str , help = 'target language ID (e.g., <en>, <de>, and <fr> etc.)' ) return parser main ( args ) \u00b6 Run the main decoding function. Source code in adviser/tools/espnet_minimal/bin/st_trans.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def main ( args ): \"\"\"Run the main decoding function.\"\"\" parser = get_parser () args = parser . parse_args ( args ) # logging info if args . verbose == 1 : logging . basicConfig ( level = logging . INFO , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) elif args . verbose == 2 : logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) else : logging . basicConfig ( level = logging . WARN , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) logging . warning ( \"Skip DEBUG/INFO messages\" ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # TODO(mn5k): support of multiple GPUs if args . ngpu > 1 : logging . error ( \"The program only supports ngpu=1.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting random . seed ( args . seed ) np . random . seed ( args . seed ) logging . info ( 'set random seed = %d ' % args . seed ) # validate rnn options if args . rnnlm is not None and args . word_rnnlm is not None : logging . error ( \"It seems that both --rnnlm and --word-rnnlm are specified. Please use either option.\" ) sys . exit ( 1 ) # trans logging . info ( 'backend = ' + args . backend ) if args . backend == \"pytorch\" : # Experimental API that supports custom LMs from tools.espnet_minimal import trans if args . dtype != \"float32\" : raise NotImplementedError ( f \"`--dtype { args . dtype } ` is only available with `--api v2`\" ) trans ( args ) else : raise ValueError ( \"Only pytorch are supported.\" ) tts_decode \u00b6 TTS decoding script. get_parser () \u00b6 Get parser of decoding arguments. Source code in adviser/tools/espnet_minimal/bin/tts_decode.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_parser (): \"\"\"Get parser of decoding arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Synthesize speech from text using a TTS model on one CPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'second config file path that overwrites the settings in `--config`.' ) parser . add ( '--config3' , is_config_file = True , help = 'third config file path that overwrites the settings in `--config` and `--config2`.' ) parser . add_argument ( '--ngpu' , default = 0 , type = int , help = 'Number of GPUs' ) parser . add_argument ( '--backend' , default = 'pytorch' , type = str , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , default = 1 , type = int , help = 'Debugmode' ) parser . add_argument ( '--seed' , default = 1 , type = int , help = 'Random seed' ) parser . add_argument ( '--out' , type = str , required = True , help = 'Output filename' ) parser . add_argument ( '--verbose' , '-V' , default = 0 , type = int , help = 'Verbose option' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) # task related parser . add_argument ( '--json' , type = str , required = True , help = 'Filename of train label data (json)' ) parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) parser . add_argument ( '--model-conf' , type = str , default = None , help = 'Model config file' ) # decoding related parser . add_argument ( '--maxlenratio' , type = float , default = 5 , help = 'Maximum length ratio in decoding' ) parser . add_argument ( '--minlenratio' , type = float , default = 0 , help = 'Minimum length ratio in decoding' ) parser . add_argument ( '--threshold' , type = float , default = 0.5 , help = 'Threshold value in decoding' ) parser . add_argument ( '--use-att-constraint' , type = strtobool , default = False , help = 'Whether to use the attention constraint' ) parser . add_argument ( '--backward-window' , type = int , default = 1 , help = 'Backward window size in the attention constraint' ) parser . add_argument ( '--forward-window' , type = int , default = 3 , help = 'Forward window size in the attention constraint' ) # save related parser . add_argument ( '--save-durations' , default = False , type = strtobool , help = 'Whether to save durations converted from attentions' ) parser . add_argument ( '--save-focus-rates' , default = False , type = strtobool , help = 'Whether to save focus rates of attentions' ) return parser main ( args ) \u00b6 Run deocding. Source code in adviser/tools/espnet_minimal/bin/tts_decode.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def main ( args ): \"\"\"Run deocding.\"\"\" parser = get_parser () args = parser . parse_args ( args ) # logging info if args . verbose > 0 : logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) else : logging . basicConfig ( level = logging . WARN , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) logging . warning ( 'Skip DEBUG/INFO messages' ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : # python 2 case if platform . python_version_tuple ()[ 0 ] == '2' : if \"clsp.jhu.edu\" in subprocess . check_output ([ \"hostname\" , \"-f\" ]): cvd = subprocess . check_output ([ \"/usr/local/bin/free-gpu\" , \"-n\" , str ( args . ngpu )]) . strip () logging . info ( 'CLSP: use gpu' + cvd ) os . environ [ 'CUDA_VISIBLE_DEVICES' ] = cvd # python 3 case else : if \"clsp.jhu.edu\" in subprocess . check_output ([ \"hostname\" , \"-f\" ]) . decode (): cvd = subprocess . check_output ([ \"/usr/local/bin/free-gpu\" , \"-n\" , str ( args . ngpu )]) . decode () . strip () logging . info ( 'CLSP: use gpu' + cvd ) os . environ [ 'CUDA_VISIBLE_DEVICES' ] = cvd cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # extract logging . info ( 'backend = ' + args . backend ) if args . backend == \"pytorch\" : from tools.espnet_minimal.tts.pytorch_backend.tts import decode decode ( args ) else : raise NotImplementedError ( \"Only pytorch is supported.\" ) nets special \u00b6 asr_interface \u00b6 ASR Interface module. ASRInterface \u00b6 ASR Interface for ESPnet model implementation. attention_plot_class property readonly \u00b6 Get attention plot class. add_arguments ( parser ) staticmethod \u00b6 Add arguments to parser. Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 12 13 14 15 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments to parser.\"\"\" return parser build ( idim , odim , ** kwargs ) classmethod \u00b6 Initialize this class with python-level args. Parameters: Name Type Description Default idim int The number of an input feature dim. required odim int The number of output vocab. required Returns: Type Description ASRinterface A new instance of ASRInterface. Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @classmethod def build ( cls , idim : int , odim : int , ** kwargs ): \"\"\"Initialize this class with python-level args. Args: idim (int): The number of an input feature dim. odim (int): The number of output vocab. Returns: ASRinterface: A new instance of ASRInterface. \"\"\" def wrap ( parser ): return get_parser ( parser , required = False ) args = argparse . Namespace ( ** kwargs ) args = fill_missing_args ( args , wrap ) args = fill_missing_args ( args , cls . add_arguments ) return cls ( idim , odim , args ) calculate_all_attentions ( self , xs , ilens , ys ) \u00b6 Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 79 80 81 82 83 84 85 86 87 88 def calculate_all_attentions ( self , xs , ilens , ys ): \"\"\"Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray \"\"\" raise NotImplementedError ( \"calculate_all_attentions method is not implemented\" ) encode ( self , feat ) \u00b6 Encode feature in beam_search (optional). Parameters: Name Type Description Default x numpy.ndarray input feature (T, D) required Returns: Type Description torch.Tensor for pytorch, chainer.Variable for chainer encoded feature (T, D) Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 96 97 98 99 100 101 102 103 104 105 106 def encode ( self , feat ): \"\"\"Encode feature in `beam_search` (optional). Args: x (numpy.ndarray): input feature (T, D) Returns: torch.Tensor for pytorch, chainer.Variable for chainer: encoded feature (T, D) \"\"\" raise NotImplementedError ( \"encode method is not implemented\" ) forward ( self , xs , ilens , ys ) \u00b6 Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , xs , ilens , ys ): \"\"\"Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer \"\"\" raise NotImplementedError ( \"forward method is not implemented\" ) recognize ( self , x , recog_args , char_list = None , rnnlm = None ) \u00b6 Recognize x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace recog_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 55 56 57 58 59 60 61 62 63 64 65 def recognize ( self , x , recog_args , char_list = None , rnnlm = None ): \"\"\"Recognize x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace recog_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"recognize method is not implemented\" ) recognize_batch ( self , x , recog_args , char_list = None , rnnlm = None ) \u00b6 Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace recog_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 67 68 69 70 71 72 73 74 75 76 77 def recognize_batch ( self , x , recog_args , char_list = None , rnnlm = None ): \"\"\"Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace recog_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"Batch decoding is not supported yet.\" ) scorers ( self ) \u00b6 Get scorers for beam_search (optional). Returns: Type Description dict[str, ScorerInterface] dict of ScorerInterface objects Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 108 109 110 111 112 113 114 115 def scorers ( self ): \"\"\"Get scorers for `beam_search` (optional). Returns: dict[str, ScorerInterface]: dict of `ScorerInterface` objects \"\"\" raise NotImplementedError ( \"decoders method is not implemented\" ) dynamic_import_asr ( module , backend ) \u00b6 Import ASR models dynamically. Parameters: Name Type Description Default module str module_name:class_name or alias in predefined_asr required backend str NN backend. e.g., pytorch, chainer required Returns: Type Description type ASR class Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def dynamic_import_asr ( module , backend ): \"\"\"Import ASR models dynamically. Args: module (str): module_name:class_name or alias in `predefined_asr` backend (str): NN backend. e.g., pytorch, chainer Returns: type: ASR class \"\"\" model_class = dynamic_import ( module , predefined_asr . get ( backend , dict ())) assert issubclass ( model_class , ASRInterface ), f \" { module } does not implement ASRInterface\" return model_class batch_beam_search \u00b6 Parallel beam search module. BatchBeamSearch \u00b6 Batch beam search implementation. batch_beam ( self , weighted_scores , ids ) \u00b6 Batch-compute topk full token ids and partial token ids. Parameters: Name Type Description Default weighted_scores Tensor The weighted sum scores for each tokens. Its shape is (n_beam, self.vocab_size) . required ids Tensor The partial token ids to compute topk. Its shape is (n_beam, self.pre_beam_size) . required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all (self.beam_size,) Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def batch_beam ( self , weighted_scores : torch . Tensor , ids : torch . Tensor ) \\ -> Tuple [ torch . Tensor , torch . Tensor , torch . Tensor , torch . Tensor ]: \"\"\"Batch-compute topk full token ids and partial token ids. Args: weighted_scores (torch.Tensor): The weighted sum scores for each tokens. Its shape is `(n_beam, self.vocab_size)`. ids (torch.Tensor): The partial token ids to compute topk. Its shape is `(n_beam, self.pre_beam_size)`. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all `(self.beam_size,)` \"\"\" if not self . do_pre_beam : top_ids = weighted_scores . view ( - 1 ) . topk ( self . beam_size )[ 1 ] # Because of the flatten above, `top_ids` is organized as: # [hyp1 * V + token1, hyp2 * V + token2, ..., hypK * V + tokenK], # where V is `self.n_vocab` and K is `self.beam_size` prev_hyp_ids = top_ids // self . n_vocab new_token_ids = top_ids % self . n_vocab return prev_hyp_ids , new_token_ids , prev_hyp_ids , new_token_ids raise NotImplementedError ( \"batch decoding with PartialScorer is not supported yet.\" ) batchfy ( self , hyps ) \u00b6 Convert list to batch. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 33 34 35 36 37 38 39 40 41 42 43 def batchfy ( self , hyps : List [ Hypothesis ]) -> BatchHypothesis : \"\"\"Convert list to batch.\"\"\" if len ( hyps ) == 0 : return BatchHypothesis () return BatchHypothesis ( yseq = pad_sequence ([ h . yseq for h in hyps ], batch_first = True , padding_value = self . eos ), length = torch . tensor ([ len ( h . yseq ) for h in hyps ], dtype = torch . int64 ), score = torch . tensor ([ h . score for h in hyps ]), scores = { k : torch . tensor ([ h . scores [ k ] for h in hyps ]) for k in self . scorers }, states = { k : [ h . states [ k ] for h in hyps ] for k in self . scorers } ) init_hyp ( self , x ) \u00b6 Get an initial hypothesis data. Parameters: Name Type Description Default x Tensor The encoder output feature required Returns: Type Description BatchHypothesis Hypothesis: The initial hypothesis. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def init_hyp ( self , x : torch . Tensor ) -> BatchHypothesis : \"\"\"Get an initial hypothesis data. Args: x (torch.Tensor): The encoder output feature Returns: Hypothesis: The initial hypothesis. \"\"\" init_states = dict () init_scores = dict () for k , d in self . scorers . items (): init_states [ k ] = d . init_state ( x ) init_scores [ k ] = 0.0 return self . batchfy ( super () . init_hyp ( x )) post_process ( self , i , maxlen , maxlenratio , running_hyps , ended_hyps ) \u00b6 Perform post-processing of beam search iterations. Parameters: Name Type Description Default i int The length of hypothesis tokens. required maxlen int The maximum length of tokens in beam search. required maxlenratio float The maximum length ratio in beam search. required running_hyps BatchHypothesis The running hypotheses in beam search. required ended_hyps List[tools.espnet_minimal.nets.beam_search.Hypothesis] The ended hypotheses in beam search. required Returns: Type Description BatchHypothesis BatchHypothesis: The new running hypotheses. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def post_process ( self , i : int , maxlen : int , maxlenratio : float , running_hyps : BatchHypothesis , ended_hyps : List [ Hypothesis ]) -> BatchHypothesis : \"\"\"Perform post-processing of beam search iterations. Args: i (int): The length of hypothesis tokens. maxlen (int): The maximum length of tokens in beam search. maxlenratio (int): The maximum length ratio in beam search. running_hyps (BatchHypothesis): The running hypotheses in beam search. ended_hyps (List[Hypothesis]): The ended hypotheses in beam search. Returns: BatchHypothesis: The new running hypotheses. \"\"\" n_batch , maxlen = running_hyps . yseq . shape logging . debug ( f 'the number of running hypothes: { n_batch } ' ) if self . token_list is not None : logging . debug ( \"best hypo: \" + \"\" . join ( [ self . token_list [ x ] for x in running_hyps . yseq [ 0 , 1 : running_hyps . length [ 0 ]]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( \"adding <eos> in the last position in the loop\" ) running_hyps . yseq . resize_ ( n_batch , maxlen + 1 ) running_hyps . yseq [:, - 1 ] = self . eos running_hyps . yseq . index_fill_ ( 1 , running_hyps . length , self . eos ) # add ended hypotheses to a final list, and removed them from current hypotheses # (this will be a probmlem, number of hyps < beam) is_eos = running_hyps . yseq [ torch . arange ( n_batch ), running_hyps . length - 1 ] == self . eos for b in torch . nonzero ( is_eos ) . view ( - 1 ): hyp = self . _select ( running_hyps , b ) ended_hyps . append ( hyp ) remained_ids = torch . nonzero ( is_eos == 0 ) . view ( - 1 ) return self . _batch_select ( running_hyps , remained_ids ) search ( self , running_hyps , x ) \u00b6 Search new tokens for running hypotheses and encoded speech x. Parameters: Name Type Description Default running_hyps BatchHypothesis Running hypotheses on beam required x Tensor Encoded speech feature (T, D) required Returns: Type Description BatchHypothesis BatchHypothesis: Best sorted hypotheses Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def search ( self , running_hyps : BatchHypothesis , x : torch . Tensor ) -> BatchHypothesis : \"\"\"Search new tokens for running hypotheses and encoded speech x. Args: running_hyps (BatchHypothesis): Running hypotheses on beam x (torch.Tensor): Encoded speech feature (T, D) Returns: BatchHypothesis: Best sorted hypotheses \"\"\" n_batch = len ( running_hyps ) # batch scoring scores , states = self . score_full ( running_hyps , x . expand ( n_batch , * x . shape )) if self . do_pre_beam : part_ids = torch . topk ( scores [ self . pre_beam_score_key ], self . pre_beam_size , dim =- 1 )[ 1 ] else : part_ids = torch . arange ( self . n_vocab , device = x . device ) . expand ( n_batch , self . n_vocab ) part_scores , part_states = self . score_partial ( running_hyps , part_ids , x ) # weighted sum scores weighted_scores = torch . zeros ( n_batch , self . n_vocab , dtype = x . dtype , device = x . device ) for k in self . full_scorers : weighted_scores += self . weights [ k ] * scores [ k ] for k in self . part_scorers : weighted_scores [ part_ids ] += self . weights [ k ] * part_scores [ k ] weighted_scores += running_hyps . score . unsqueeze ( 1 ) # TODO(karita): do not use list. use batch instead # update hyps best_hyps = [] prev_hyps = self . unbatchfy ( running_hyps ) for full_prev_hyp_id , full_new_token_id , part_prev_hyp_id , part_new_token_id in zip ( * self . batch_beam ( weighted_scores , part_ids )): prev_hyp = prev_hyps [ full_prev_hyp_id ] best_hyps . append ( Hypothesis ( score = weighted_scores [ full_prev_hyp_id , full_new_token_id ], yseq = self . append_token ( prev_hyp . yseq , full_new_token_id ), scores = self . merge_scores ( prev_hyp . scores , { k : v [ full_prev_hyp_id ] for k , v in scores . items ()}, full_new_token_id , { k : v [ part_prev_hyp_id ] for k , v in part_scores . items ()}, part_new_token_id ), states = self . merge_states ( { k : self . full_scorers [ k ] . select_state ( v , full_prev_hyp_id ) for k , v in states . items ()}, { k : self . part_scorers [ k ] . select_state ( v , part_prev_hyp_id ) for k , v in part_states . items ()}, part_new_token_id ) )) return self . batchfy ( best_hyps ) unbatchfy ( self , batch_hyps ) \u00b6 Revert batch to list. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 63 64 65 66 67 68 69 70 71 72 def unbatchfy ( self , batch_hyps : BatchHypothesis ) -> List [ Hypothesis ]: \"\"\"Revert batch to list.\"\"\" return [ Hypothesis ( yseq = batch_hyps . yseq [ i ][: batch_hyps . length [ i ]], score = batch_hyps . score [ i ], scores = { k : batch_hyps . scores [ k ][ i ] for k in self . scorers }, states = { k : v . select_state ( batch_hyps . states [ k ], i ) for k , v in self . scorers . items ()} ) for i in range ( len ( batch_hyps . length ))] BatchHypothesis \u00b6 Batchfied/Vectorized hypothesis data type. __getnewargs__ ( self ) special \u00b6 Return self as a plain tuple. Used by copy and pickle. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 427 428 429 def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __len__ ( self ) special \u00b6 Return a batch size. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 25 26 27 def __len__ ( self ) -> int : \"\"\"Return a batch size.\"\"\" return len ( self . length ) __new__ ( _cls , yseq = tensor ([]), score = tensor ([]), length = tensor ([]), scores = {}, states = {}) special staticmethod \u00b6 Create new instance of BatchHypothesis(yseq, score, length, scores, states) __repr__ ( self ) special \u00b6 Return a nicely formatted representation string Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 419 420 421 def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self beam_search \u00b6 Beam search module. BeamSearch \u00b6 Beam search implementation. __init__ ( self , scorers , weights , beam_size , vocab_size , sos , eos , token_list = None , pre_beam_ratio = 1.5 , pre_beam_score_key = None ) special \u00b6 Initialize beam search. Parameters: Name Type Description Default scorers Dict[str, tools.espnet_minimal.nets.scorer_interface.ScorerInterface] Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None required weights Dict[str, float] Dict of weights for each scorers The scorer will be ignored if its weight is 0 required beam_size int The number of hypotheses kept during search required vocab_size int The number of vocabulary required sos int Start of sequence id required eos int End of sequence id required token_list List[str] List of tokens for debug log None pre_beam_score_key str key of scores to perform pre-beam search None pre_beam_ratio float beam size in the pre-beam search will be int(pre_beam_ratio * beam_size) 1.5 Source code in adviser/tools/espnet_minimal/nets/beam_search.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , scorers : Dict [ str , ScorerInterface ], weights : Dict [ str , float ], beam_size : int , vocab_size : int , sos : int , eos : int , token_list : List [ str ] = None , pre_beam_ratio : float = 1.5 , pre_beam_score_key : str = None ): \"\"\"Initialize beam search. Args: scorers (dict[str, ScorerInterface]): Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is `None` weights (dict[str, float]): Dict of weights for each scorers The scorer will be ignored if its weight is 0 beam_size (int): The number of hypotheses kept during search vocab_size (int): The number of vocabulary sos (int): Start of sequence id eos (int): End of sequence id token_list (list[str]): List of tokens for debug log pre_beam_score_key (str): key of scores to perform pre-beam search pre_beam_ratio (float): beam size in the pre-beam search will be `int(pre_beam_ratio * beam_size)` \"\"\" super () . __init__ () # set scorers self . weights = weights self . scorers = dict () self . full_scorers = dict () self . part_scorers = dict () # this module dict is required for recursive cast `self.to(device, dtype)` in `recog.py` self . nn_dict = torch . nn . ModuleDict () for k , v in scorers . items (): w = weights . get ( k , 0 ) if w == 0 or v is None : continue assert isinstance ( v , ScorerInterface ), f \" { k } ( { type ( v ) } ) does not implement ScorerInterface\" self . scorers [ k ] = v if isinstance ( v , PartialScorerInterface ): self . part_scorers [ k ] = v else : self . full_scorers [ k ] = v if isinstance ( v , torch . nn . Module ): self . nn_dict [ k ] = v # set configurations self . sos = sos self . eos = eos self . token_list = token_list self . pre_beam_size = int ( pre_beam_ratio * beam_size ) self . beam_size = beam_size self . n_vocab = vocab_size if pre_beam_score_key is not None and pre_beam_score_key not in self . full_scorers : raise KeyError ( f \" { pre_beam_score_key } is not found in { self . full_scorers } \" ) self . pre_beam_score_key = pre_beam_score_key self . do_pre_beam = self . pre_beam_score_key is not None and \\ self . pre_beam_size < self . n_vocab and len ( self . part_scorers ) > 0 append_token ( xs , x ) staticmethod \u00b6 Append new token to prefix tokens. Parameters: Name Type Description Default xs Tensor The prefix token required x int The new token to append required Returns: Type Description Tensor torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device Source code in adviser/tools/espnet_minimal/nets/beam_search.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @staticmethod def append_token ( xs : torch . Tensor , x : int ) -> torch . Tensor : \"\"\"Append new token to prefix tokens. Args: xs (torch.Tensor): The prefix token x (int): The new token to append Returns: torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device \"\"\" x = torch . tensor ([ x ], dtype = xs . dtype , device = xs . device ) return torch . cat (( xs , x )) beam ( self , weighted_scores , ids ) \u00b6 Compute topk full token ids and partial token ids. Parameters: Name Type Description Default weighted_scores Tensor The weighted sum scores for each tokens. Its shape is (self.n_vocab,) . required ids Tensor The partial token ids to compute topk required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: The topk full token ids and partial token ids. Their shapes are (self.beam_size,) Source code in adviser/tools/espnet_minimal/nets/beam_search.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def beam ( self , weighted_scores : torch . Tensor , ids : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Compute topk full token ids and partial token ids. Args: weighted_scores (torch.Tensor): The weighted sum scores for each tokens. Its shape is `(self.n_vocab,)`. ids (torch.Tensor): The partial token ids to compute topk Returns: Tuple[torch.Tensor, torch.Tensor]: The topk full token ids and partial token ids. Their shapes are `(self.beam_size,)` \"\"\" # no pre beam performed if weighted_scores . size ( 0 ) == ids . size ( 0 ): top_ids = weighted_scores . topk ( self . beam_size )[ 1 ] return top_ids , top_ids # mask pruned in pre-beam not to select in topk tmp = weighted_scores [ ids ] weighted_scores [:] = - float ( \"inf\" ) weighted_scores [ ids ] = tmp top_ids = weighted_scores . topk ( self . beam_size )[ 1 ] local_ids = weighted_scores [ ids ] . topk ( self . beam_size )[ 1 ] return top_ids , local_ids forward ( self , x , maxlenratio = 0.0 , minlenratio = 0.0 ) \u00b6 Perform beam search. Parameters: Name Type Description Default x Tensor Encoded speech feature (T, D) required maxlenratio float Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths 0.0 minlenratio float Input length ratio to obtain min output length. 0.0 Returns: Type Description List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] list[Hypothesis]: N-best decoding results Source code in adviser/tools/espnet_minimal/nets/beam_search.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def forward ( self , x : torch . Tensor , maxlenratio : float = 0.0 , minlenratio : float = 0.0 ) -> List [ Hypothesis ]: \"\"\"Perform beam search. Args: x (torch.Tensor): Encoded speech feature (T, D) maxlenratio (float): Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths minlenratio (float): Input length ratio to obtain min output length. Returns: list[Hypothesis]: N-best decoding results \"\"\" # set length bounds if maxlenratio == 0 : maxlen = x . shape [ 0 ] else : maxlen = max ( 1 , int ( maxlenratio * x . size ( 0 ))) minlen = int ( minlenratio * x . size ( 0 )) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # main loop of prefix search running_hyps = self . init_hyp ( x ) ended_hyps = [] for i in range ( maxlen ): logging . debug ( 'position ' + str ( i )) best = self . search ( running_hyps , x ) # post process of one iteration running_hyps = self . post_process ( i , maxlen , maxlenratio , best , ended_hyps ) # end detection if maxlenratio == 0.0 and end_detect ([ h . asdict () for h in ended_hyps ], i ): logging . info ( f 'end detected at { i } ' ) break if len ( running_hyps ) == 0 : logging . info ( 'no hypothesis. Finish decoding.' ) break else : logging . debug ( f 'remeined hypothes: { len ( running_hyps ) } ' ) nbest_hyps = sorted ( ended_hyps , key = lambda x : x . score , reverse = True ) # check number of hypotheis if len ( nbest_hyps ) == 0 : logging . warning ( 'there is no N-best results, perform recognition again with smaller minlenratio.' ) return [] if minlenratio < 0.1 else self . forward ( x , maxlenratio , max ( 0.0 , minlenratio - 0.1 )) # report the best result best = nbest_hyps [ 0 ] logging . info ( f 'total log probability: { best . score } ' ) logging . info ( f 'normalized log probability: { best . score / len ( best . yseq ) } ' ) return nbest_hyps init_hyp ( self , x ) \u00b6 Get an initial hypothesis data. Parameters: Name Type Description Default x Tensor The encoder output feature required Returns: Type Description Hypothesis Hypothesis: The initial hypothesis. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def init_hyp ( self , x : torch . Tensor ) -> Hypothesis : \"\"\"Get an initial hypothesis data. Args: x (torch.Tensor): The encoder output feature Returns: Hypothesis: The initial hypothesis. \"\"\" init_states = dict () init_scores = dict () for k , d in self . scorers . items (): init_states [ k ] = d . init_state ( x ) init_scores [ k ] = 0.0 return [ Hypothesis ( score = 0.0 , scores = init_scores , states = init_states , yseq = torch . tensor ([ self . sos ], device = x . device ))] merge_scores ( prev_scores , next_full_scores , full_idx , next_part_scores , part_idx ) staticmethod \u00b6 Merge scores for new hypothesis. Parameters: Name Type Description Default prev_scores Dict[str, float] The previous hypothesis scores by self.scorers required next_full_scores Dict[str, torch.Tensor] scores by self.full_scorers required full_idx int The next token id for next_full_scores required next_part_scores Dict[str, torch.Tensor] scores of partial tokens by self.part_scorers required part_idx int The new token id for next_part_scores required Returns: Type Description Dict[str, torch.Tensor] Dict[str, torch.Tensor]: The new score dict. Its keys are names of self.full_scorers and self.part_scorers . Its values are scalar tensors by the scorers. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 @staticmethod def merge_scores ( prev_scores : Dict [ str , float ], next_full_scores : Dict [ str , torch . Tensor ], full_idx : int , next_part_scores : Dict [ str , torch . Tensor ], part_idx : int ) -> Dict [ str , torch . Tensor ]: \"\"\"Merge scores for new hypothesis. Args: prev_scores (Dict[str, float]): The previous hypothesis scores by `self.scorers` next_full_scores (Dict[str, torch.Tensor]): scores by `self.full_scorers` full_idx (int): The next token id for `next_full_scores` next_part_scores (Dict[str, torch.Tensor]): scores of partial tokens by `self.part_scorers` part_idx (int): The new token id for `next_part_scores` Returns: Dict[str, torch.Tensor]: The new score dict. Its keys are names of `self.full_scorers` and `self.part_scorers`. Its values are scalar tensors by the scorers. \"\"\" new_scores = dict () for k , v in next_full_scores . items (): new_scores [ k ] = prev_scores [ k ] + v [ full_idx ] for k , v in next_part_scores . items (): new_scores [ k ] = v [ part_idx ] return new_scores merge_states ( self , states , part_states , part_idx ) \u00b6 Merge states for new hypothesis. Parameters: Name Type Description Default states Any states of self.full_scorers required part_states Any states of self.part_scorers required part_idx int The new token id for part_scores required Returns: Type Description Any Dict[str, torch.Tensor]: The new score dict. Its keys are names of self.full_scorers and self.part_scorers . Its values are states of the scorers. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def merge_states ( self , states : Any , part_states : Any , part_idx : int ) -> Any : \"\"\"Merge states for new hypothesis. Args: states: states of `self.full_scorers` part_states: states of `self.part_scorers` part_idx (int): The new token id for `part_scores` Returns: Dict[str, torch.Tensor]: The new score dict. Its keys are names of `self.full_scorers` and `self.part_scorers`. Its values are states of the scorers. \"\"\" new_states = dict () for k , v in states . items (): new_states [ k ] = v for k , d in self . part_scorers . items (): new_states [ k ] = d . select_state ( part_states [ k ], part_idx ) return new_states post_process ( self , i , maxlen , maxlenratio , running_hyps , ended_hyps ) \u00b6 Perform post-processing of beam search iterations. Parameters: Name Type Description Default i int The length of hypothesis tokens. required maxlen int The maximum length of tokens in beam search. required maxlenratio float The maximum length ratio in beam search. required running_hyps List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] The running hypotheses in beam search. required ended_hyps List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] The ended hypotheses in beam search. required Returns: Type Description List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] List[Hypothesis]: The new running hypotheses. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def post_process ( self , i : int , maxlen : int , maxlenratio : float , running_hyps : List [ Hypothesis ], ended_hyps : List [ Hypothesis ]) -> List [ Hypothesis ]: \"\"\"Perform post-processing of beam search iterations. Args: i (int): The length of hypothesis tokens. maxlen (int): The maximum length of tokens in beam search. maxlenratio (int): The maximum length ratio in beam search. running_hyps (List[Hypothesis]): The running hypotheses in beam search. ended_hyps (List[Hypothesis]): The ended hypotheses in beam search. Returns: List[Hypothesis]: The new running hypotheses. \"\"\" logging . debug ( f 'the number of running hypothes: { len ( running_hyps ) } ' ) if self . token_list is not None : logging . debug ( \"best hypo: \" + \"\" . join ([ self . token_list [ x ] for x in running_hyps [ 0 ] . yseq [ 1 :]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( \"adding <eos> in the last position in the loop\" ) running_hyps = [ h . _replace ( yseq = self . append_token ( h . yseq , self . eos )) for h in running_hyps ] # add ended hypotheses to a final list, and removed them from current hypotheses # (this will be a probmlem, number of hyps < beam) remained_hyps = [] for hyp in running_hyps : if hyp . yseq [ - 1 ] == self . eos : # e.g., Word LM needs to add final <eos> score for k , d in chain ( self . full_scorers . items (), self . part_scorers . items ()): s = d . final_score ( hyp . states [ k ]) hyp . scores [ k ] += s hyp = hyp . _replace ( score = hyp . score + self . weights [ k ] * s ) ended_hyps . append ( hyp ) else : remained_hyps . append ( hyp ) return remained_hyps score_full ( self , hyp , x ) \u00b6 Score new hypothesis by self.full_scorers . Parameters: Name Type Description Default hyp Hypothesis Hypothesis with prefix tokens to score required x Tensor Corresponding input feature required Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, Any]] Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,) , and state dict that has string keys and state values of self.full_scorers Source code in adviser/tools/espnet_minimal/nets/beam_search.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def score_full ( self , hyp : Hypothesis , x : torch . Tensor ) -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , Any ]]: \"\"\"Score new hypothesis by `self.full_scorers`. Args: hyp (Hypothesis): Hypothesis with prefix tokens to score x (torch.Tensor): Corresponding input feature Returns: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of `hyp` that has string keys of `self.full_scorers` and tensor score values of shape: `(self.n_vocab,)`, and state dict that has string keys and state values of `self.full_scorers` \"\"\" scores = dict () states = dict () for k , d in self . full_scorers . items (): scores [ k ], states [ k ] = d . score ( hyp . yseq , hyp . states [ k ], x ) return scores , states score_partial ( self , hyp , ids , x ) \u00b6 Score new hypothesis by self.part_scorers . Parameters: Name Type Description Default hyp Hypothesis Hypothesis with prefix tokens to score required ids Tensor 1D tensor of new partial tokens to score required x Tensor Corresponding input feature required Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, Any]] Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of hyp that has string keys of self.part_scorers and tensor score values of shape: (len(ids),) , and state dict that has string keys and state values of self.part_scorers Source code in adviser/tools/espnet_minimal/nets/beam_search.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def score_partial ( self , hyp : Hypothesis , ids : torch . Tensor , x : torch . Tensor ) \\ -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , Any ]]: \"\"\"Score new hypothesis by `self.part_scorers`. Args: hyp (Hypothesis): Hypothesis with prefix tokens to score ids (torch.Tensor): 1D tensor of new partial tokens to score x (torch.Tensor): Corresponding input feature Returns: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of `hyp` that has string keys of `self.part_scorers` and tensor score values of shape: `(len(ids),)`, and state dict that has string keys and state values of `self.part_scorers` \"\"\" scores = dict () states = dict () for k , d in self . part_scorers . items (): scores [ k ], states [ k ] = d . score_partial ( hyp . yseq , ids , hyp . states [ k ], x ) return scores , states search ( self , running_hyps , x ) \u00b6 Search new tokens for running hypotheses and encoded speech x. Parameters: Name Type Description Default running_hyps List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] Running hypotheses on beam required x Tensor Encoded speech feature (T, D) required Returns: Type Description List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] List[Hypotheses]: Best sorted hypotheses Source code in adviser/tools/espnet_minimal/nets/beam_search.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def search ( self , running_hyps : List [ Hypothesis ], x : torch . Tensor ) -> List [ Hypothesis ]: \"\"\"Search new tokens for running hypotheses and encoded speech x. Args: running_hyps (List[Hypothesis]): Running hypotheses on beam x (torch.Tensor): Encoded speech feature (T, D) Returns: List[Hypotheses]: Best sorted hypotheses \"\"\" best_hyps = [] part_ids = torch . arange ( self . n_vocab , device = x . device ) # no pre-beam for hyp in running_hyps : # scoring scores , states = self . score_full ( hyp , x ) if self . do_pre_beam : part_ids = torch . topk ( scores [ self . pre_beam_score_key ], self . pre_beam_size )[ 1 ] part_scores , part_states = self . score_partial ( hyp , part_ids , x ) # weighted sum scores weighted_scores = torch . zeros ( self . n_vocab , dtype = x . dtype , device = x . device ) for k in self . full_scorers : weighted_scores += self . weights [ k ] * scores [ k ] for k in self . part_scorers : weighted_scores [ part_ids ] += self . weights [ k ] * part_scores [ k ] weighted_scores += hyp . score # update hyps for j , part_j in zip ( * self . beam ( weighted_scores , part_ids )): # will be (2 x beam at most) best_hyps . append ( Hypothesis ( score = weighted_scores [ j ], yseq = self . append_token ( hyp . yseq , j ), scores = self . merge_scores ( hyp . scores , scores , j , part_scores , part_j ), states = self . merge_states ( states , part_states , part_j ))) # sort and prune 2 x beam -> beam best_hyps = sorted ( best_hyps , key = lambda x : x . score , reverse = True )[: min ( len ( best_hyps ), self . beam_size )] return best_hyps Hypothesis \u00b6 Hypothesis data type. __getnewargs__ ( self ) special \u00b6 Return self as a plain tuple. Used by copy and pickle. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 427 428 429 def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , yseq , score = 0 , scores = {}, states = {}) special staticmethod \u00b6 Create new instance of Hypothesis(yseq, score, scores, states) __repr__ ( self ) special \u00b6 Return a nicely formatted representation string Source code in adviser/tools/espnet_minimal/nets/beam_search.py 419 420 421 def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self asdict ( self ) \u00b6 Convert data to JSON-friendly dict. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 26 27 28 29 30 31 32 def asdict ( self ) -> dict : \"\"\"Convert data to JSON-friendly dict.\"\"\" return self . _replace ( yseq = self . yseq . tolist (), score = float ( self . score ), scores = { k : float ( v ) for k , v in self . scores . items ()} ) . _asdict () beam_search ( x , sos , eos , beam_size , vocab_size , scorers , weights , token_list = None , maxlenratio = 0.0 , minlenratio = 0.0 , pre_beam_ratio = 1.5 , pre_beam_score_key = 'decoder' ) \u00b6 Perform beam search with scorers. Parameters: Name Type Description Default x Tensor Encoded speech feature (T, D) required sos int Start of sequence id required eos int End of sequence id required beam_size int The number of hypotheses kept during search required vocab_size int The number of vocabulary required scorers Dict[str, tools.espnet_minimal.nets.scorer_interface.ScorerInterface] Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None required weights Dict[str, float] Dict of weights for each scorers The scorer will be ignored if its weight is 0 required token_list List[str] List of tokens for debug log None maxlenratio float Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths 0.0 minlenratio float Input length ratio to obtain min output length. 0.0 pre_beam_score_key str key of scores to perform pre-beam search 'decoder' pre_beam_ratio float beam size in the pre-beam search will be int(pre_beam_ratio * beam_size) 1.5 Returns: Type Description list list: N-best decoding results Source code in adviser/tools/espnet_minimal/nets/beam_search.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def beam_search ( x : torch . Tensor , sos : int , eos : int , beam_size : int , vocab_size : int , scorers : Dict [ str , ScorerInterface ], weights : Dict [ str , float ], token_list : List [ str ] = None , maxlenratio : float = 0.0 , minlenratio : float = 0.0 , pre_beam_ratio : float = 1.5 , pre_beam_score_key : str = \"decoder\" ) -> list : \"\"\"Perform beam search with scorers. Args: x (torch.Tensor): Encoded speech feature (T, D) sos (int): Start of sequence id eos (int): End of sequence id beam_size (int): The number of hypotheses kept during search vocab_size (int): The number of vocabulary scorers (dict[str, ScorerInterface]): Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is `None` weights (dict[str, float]): Dict of weights for each scorers The scorer will be ignored if its weight is 0 token_list (list[str]): List of tokens for debug log maxlenratio (float): Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths minlenratio (float): Input length ratio to obtain min output length. pre_beam_score_key (str): key of scores to perform pre-beam search pre_beam_ratio (float): beam size in the pre-beam search will be `int(pre_beam_ratio * beam_size)` Returns: list: N-best decoding results \"\"\" ret = BeamSearch ( scorers , weights , beam_size = beam_size , vocab_size = vocab_size , pre_beam_ratio = pre_beam_ratio , pre_beam_score_key = pre_beam_score_key , sos = sos , eos = eos , token_list = token_list , ) . forward ( x = x , maxlenratio = maxlenratio , minlenratio = minlenratio ) return [ h . asdict () for h in ret ] ctc_prefix_score \u00b6 CTCPrefixScore \u00b6 Compute CTC label sequence scores which is based on Algorithm 2 in WATANABE et al. \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\" but extended to efficiently compute the probablities of multiple labels simultaneously __call__ ( self , y , cs , r_prev ) special \u00b6 Compute CTC prefix scores for next labels :param y : prefix label sequence :param cs : array of next labels :param r_prev: previous CTC state :return ctc_scores, ctc_states Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __call__ ( self , y , cs , r_prev ): \"\"\"Compute CTC prefix scores for next labels :param y : prefix label sequence :param cs : array of next labels :param r_prev: previous CTC state :return ctc_scores, ctc_states \"\"\" # initialize CTC states output_length = len ( y ) - 1 # ignore sos # new CTC states are prepared as a frame x (n or b) x n_labels tensor # that corresponds to r_t^n(h) and r_t^b(h). r = self . xp . ndarray (( self . input_length , 2 , len ( cs )), dtype = np . float32 ) xs = self . x [:, cs ] if output_length == 0 : r [ 0 , 0 ] = xs [ 0 ] r [ 0 , 1 ] = self . logzero else : r [ output_length - 1 ] = self . logzero # prepare forward probabilities for the last label r_sum = self . xp . logaddexp ( r_prev [:, 0 ], r_prev [:, 1 ]) # log(r_t^n(g) + r_t^b(g)) last = y [ - 1 ] if output_length > 0 and last in cs : log_phi = self . xp . ndarray (( self . input_length , len ( cs )), dtype = np . float32 ) for i in six . moves . range ( len ( cs )): log_phi [:, i ] = r_sum if cs [ i ] != last else r_prev [:, 1 ] else : log_phi = r_sum # compute forward probabilities log(r_t^n(h)), log(r_t^b(h)), # and log prefix probabilites log(psi) start = max ( output_length , 1 ) log_psi = r [ start - 1 , 0 ] for t in six . moves . range ( start , self . input_length ): r [ t , 0 ] = self . xp . logaddexp ( r [ t - 1 , 0 ], log_phi [ t - 1 ]) + xs [ t ] r [ t , 1 ] = self . xp . logaddexp ( r [ t - 1 , 0 ], r [ t - 1 , 1 ]) + self . x [ t , self . blank ] log_psi = self . xp . logaddexp ( log_psi , log_phi [ t - 1 ] + xs [ t ]) # get P(...eos|X) that ends with the prefix itself eos_pos = self . xp . where ( cs == self . eos )[ 0 ] if len ( eos_pos ) > 0 : log_psi [ eos_pos ] = r_sum [ - 1 ] # log(r_T^n(g) + r_T^b(g)) # return the log prefix probability and CTC states, where the label axis # of the CTC states is moved to the first axis to slice it easily return log_psi , self . xp . rollaxis ( r , 2 ) __init__ ( self , x , blank , eos , xp ) special \u00b6 Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 202 203 204 205 206 207 208 def __init__ ( self , x , blank , eos , xp ): self . xp = xp self . logzero = - 10000000000.0 self . blank = blank self . eos = eos self . input_length = len ( x ) self . x = x initial_state ( self ) \u00b6 Obtain an initial CTC state :return: CTC state Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 210 211 212 213 214 215 216 217 218 219 220 221 222 def initial_state ( self ): \"\"\"Obtain an initial CTC state :return: CTC state \"\"\" # initial CTC state is made of a frame x 2 tensor that corresponds to # r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent # superscripts n and b (non-blank and blank), respectively. r = self . xp . full (( self . input_length , 2 ), self . logzero , dtype = np . float32 ) r [ 0 , 1 ] = self . x [ 0 , self . blank ] for i in six . moves . range ( 1 , self . input_length ): r [ i , 1 ] = r [ i - 1 , 1 ] + self . x [ i , self . blank ] return r CTCPrefixScoreTH \u00b6 Batch processing of CTCPrefixScore which is based on Algorithm 2 in WATANABE et al. \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\" but extended to efficiently compute the probablities of multiple labels simultaneously __call__ ( self , y , state , pre_scores = None , att_w = None ) special \u00b6 Compute CTC prefix scores for next labels :param list y: prefix label sequences :param tuple state: previous CTC state :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O) :param torch.Tensor att_w: attention weights to decide CTC window :return new_state, ctc_local_scores (BW, O) Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __call__ ( self , y , state , pre_scores = None , att_w = None ): \"\"\"Compute CTC prefix scores for next labels :param list y: prefix label sequences :param tuple state: previous CTC state :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O) :param torch.Tensor att_w: attention weights to decide CTC window :return new_state, ctc_local_scores (BW, O) \"\"\" output_length = len ( y [ 0 ]) - 1 # ignore sos last_ids = [ yi [ - 1 ] for yi in y ] # last output label ids # prepare state info if state is None : if self . scoring_num > 0 : r_prev = torch . full (( self . input_length , 2 , self . batch , self . beam ), self . logzero , dtype = torch . float32 , device = self . device ) r_prev [:, 1 ] = torch . cumsum ( self . x [ 0 , :, :, self . blank ], 0 ) . unsqueeze ( 2 ) r_prev = r_prev . view ( - 1 , 2 , self . n_bb ) else : r_prev = torch . full (( self . input_length , 2 , self . n_bb ), self . logzero , dtype = torch . float32 , device = self . device ) r_prev [:, 1 ] = torch . cumsum ( self . x [ 0 , :, :, self . blank ], 0 ) s_prev = 0.0 f_min_prev = 0 f_max_prev = 1 else : r_prev , s_prev , f_min_prev , f_max_prev = state # select input dimensions for scoring if self . scoring_num > 0 and pre_scores is not None : pre_scores [:, self . blank ] = self . logzero # ignore blank from pre-selection scoring_ids = torch . topk ( pre_scores , self . scoring_num , 1 )[ 1 ] scoring_idmap = torch . full (( self . n_bb , self . odim ), - 1 , dtype = torch . long , device = self . device ) snum = scoring_ids . size ( 1 ) scoring_idmap [ self . bb_idx , scoring_ids ] = torch . arange ( snum , device = self . device ) scoring_idx = ( scoring_ids + self . pad_o ) . view ( - 1 ) x_ = torch . index_select ( self . x . view ( 2 , - 1 , self . batch * self . odim ), 2 , scoring_idx ) . view ( 2 , - 1 , self . n_bb , snum ) else : scoring_ids = None scoring_idmap = None snum = self . odim x_ = self . x # new CTC forward probs are prepared as a (T x 2 x BW x S) tensor # that corresponds to r_t^n(h) and r_t^b(h) in a batch. r = torch . full (( self . input_length , 2 , self . n_bb , snum ), self . logzero , dtype = torch . float32 , device = self . device ) if output_length == 0 : r [ 0 , 0 ] = x_ [ 0 , 0 ] r_sum = torch . logsumexp ( r_prev , 1 ) log_phi = r_sum . unsqueeze ( 2 ) . repeat ( 1 , 1 , snum ) if scoring_ids is not None : for idx in range ( self . n_bb ): pos = scoring_idmap [ idx , last_ids [ idx ]] if pos >= 0 : log_phi [:, idx , pos ] = r_prev [:, 1 , idx ] else : for idx in range ( self . n_bb ): log_phi [:, idx , last_ids [ idx ]] = r_prev [:, 1 , idx ] # decide start and end frames based on attention weights if att_w is not None and self . margin > 0 : f_arg = torch . matmul ( att_w , self . frame_ids ) f_min = max ( int ( f_arg . min () . cpu ()), f_min_prev ) f_max = max ( int ( f_arg . max () . cpu ()), f_max_prev ) start = min ( f_max_prev , max ( f_min - self . margin , output_length , 1 )) end = min ( f_max + self . margin , self . input_length ) else : f_min = f_max = 0 start = max ( output_length , 1 ) end = self . input_length # compute forward probabilities log(r_t^n(h)) and log(r_t^b(h)) for t in range ( start , end ): rp = r [ t - 1 ] rr = torch . stack ([ rp [ 0 ], log_phi [ t - 1 ], rp [ 0 ], rp [ 1 ]]) . view ( 2 , 2 , self . n_bb , snum ) r [ t ] = torch . logsumexp ( rr , 1 ) + x_ [:, t ] # compute log prefix probabilites log(psi) log_phi_x = torch . cat (( log_phi [ 0 ] . unsqueeze ( 0 ), log_phi [: - 1 ]), dim = 0 ) + x_ [ 0 ] if scoring_ids is not None : log_psi = torch . full (( self . n_bb , self . odim ), self . logzero , device = self . device ) log_psi_ = torch . logsumexp ( torch . cat (( log_phi_x [ start : end ], r [ start - 1 , 0 ] . unsqueeze ( 0 )), dim = 0 ), dim = 0 ) for si in range ( self . n_bb ): log_psi [ si , scoring_ids [ si ]] = log_psi_ [ si ] else : log_psi = torch . logsumexp ( torch . cat (( log_phi_x [ start : end ], r [ start - 1 , 0 ] . unsqueeze ( 0 )), dim = 0 ), dim = 0 ) for si in range ( self . n_bb ): log_psi [ si , self . eos ] = r_sum [ self . end_frames [ si ], si ] return ( r , log_psi , f_min , f_max , scoring_idmap ), log_psi - s_prev __init__ ( self , x , xlens , blank , eos , beam , scoring_ratio = 1.5 , margin = 0 ) special \u00b6 Construct CTC prefix scorer :param torch.Tensor x: input label posterior sequences (B, T, O) :param torch.Tensor xlens: input lengths (B,) :param int blank: blank label id :param int eos: end-of-sequence id :param int beam: beam size :param float scoring_ratio: ratio of #scored hypos to beam size :param int margin: margin parameter for windowing (0 means no windowing) Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , x , xlens , blank , eos , beam , scoring_ratio = 1.5 , margin = 0 ): \"\"\"Construct CTC prefix scorer :param torch.Tensor x: input label posterior sequences (B, T, O) :param torch.Tensor xlens: input lengths (B,) :param int blank: blank label id :param int eos: end-of-sequence id :param int beam: beam size :param float scoring_ratio: ratio of #scored hypos to beam size :param int margin: margin parameter for windowing (0 means no windowing) \"\"\" # In the comment lines, we assume T: input_length, B: batch size, W: beam width, O: output dim. self . logzero = - 10000000000.0 self . blank = blank self . eos = eos self . batch = x . size ( 0 ) self . input_length = x . size ( 1 ) self . odim = x . size ( 2 ) self . beam = beam self . n_bb = self . batch * beam self . device = torch . device ( 'cuda: %d ' % x . get_device ()) if x . is_cuda else torch . device ( 'cpu' ) # Pad the rest of posteriors in the batch # TODO(takaaki-hori): need a better way without for-loops for i , l in enumerate ( xlens ): if l < self . input_length : x [ i , l :, :] = self . logzero x [ i , l :, blank ] = 0 # Set the number of scoring hypotheses (scoring_num=0 means all) self . scoring_num = int ( beam * scoring_ratio ) if self . scoring_num >= self . odim : self . scoring_num = 0 # Expand input posteriors for fast computation if self . scoring_num == 0 : xn = x . transpose ( 0 , 1 ) . unsqueeze ( 2 ) . repeat ( 1 , 1 , beam , 1 ) . view ( - 1 , self . n_bb , self . odim ) else : xn = x . transpose ( 0 , 1 ) xb = xn [:, :, self . blank ] . unsqueeze ( 2 ) . expand ( - 1 , - 1 , self . odim ) self . x = torch . stack ([ xn , xb ]) # (2, T, B, O) or (2, T, BW, O) # Setup CTC windowing self . margin = margin if margin > 0 : self . frame_ids = torch . arange ( self . input_length , dtype = torch . float32 , device = self . device ) # Precompute end frames (BW,) self . end_frames = ( torch . as_tensor ( xlens ) - 1 ) . view ( self . batch , 1 ) . repeat ( 1 , beam ) . view ( - 1 ) # Precompute base indices to convert label ids to corresponding element indices self . pad_b = ( torch . arange ( self . batch , device = self . device ) * beam ) . view ( - 1 , 1 ) self . pad_bo = ( torch . arange ( self . batch , device = self . device ) * ( beam * self . odim )) . view ( - 1 , 1 ) self . pad_o = ( torch . arange ( self . batch , device = self . device ) * self . odim ) . unsqueeze ( 1 ) . repeat ( 1 , beam ) . view ( - 1 , 1 ) self . bb_idx = torch . arange ( self . n_bb , device = self . device ) . view ( - 1 , 1 ) index_select_state ( self , state , best_ids ) \u00b6 Select CTC states according to best ids :param state : CTC state :param best_ids : index numbers selected by beam pruning (B, W) :return selected_state Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def index_select_state ( self , state , best_ids ): \"\"\"Select CTC states according to best ids :param state : CTC state :param best_ids : index numbers selected by beam pruning (B, W) :return selected_state \"\"\" r , s , f_min , f_max , scoring_idmap = state # convert ids to BWO space vidx = ( best_ids + self . pad_bo ) . view ( - 1 ) # select hypothesis scores s_new = torch . index_select ( s . view ( - 1 ), 0 , vidx ) s_new = s_new . view ( - 1 , 1 ) . repeat ( 1 , self . odim ) . view ( self . n_bb , self . odim ) # convert ids to BWS space (S: scoring_num) if scoring_idmap is not None : snum = self . scoring_num beam_idx = ( torch . div ( best_ids , self . odim ) + self . pad_b ) . view ( - 1 ) label_ids = torch . fmod ( best_ids , self . odim ) . view ( - 1 ) score_idx = scoring_idmap [ beam_idx , label_ids ] score_idx [ score_idx == - 1 ] = 0 vidx = score_idx + beam_idx * snum else : snum = self . odim # select forward probabilities r_new = torch . index_select ( r . view ( - 1 , 2 , self . n_bb * snum ), 2 , vidx ) . view ( - 1 , 2 , self . n_bb ) return r_new , s_new , f_min , f_max e2e_asr_common \u00b6 ErrorCalculator \u00b6 Calculate CER and WER for E2E_ASR and CTC models during training :param y_hats: numpy array with predicted text :param y_pads: numpy array with true (target) text :param char_list: :param sym_space: :param sym_blank: :return: __call__ ( self , ys_hat , ys_pad , is_ctc = False ) special \u00b6 Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __call__ ( self , ys_hat , ys_pad , is_ctc = False ): cer , wer = None , None if is_ctc : return self . calculate_cer_ctc ( ys_hat , ys_pad ) elif not self . report_cer and not self . report_wer : return cer , wer seqs_hat , seqs_true = self . convert_to_char ( ys_hat , ys_pad ) if self . report_cer : cer = self . calculate_cer ( seqs_hat , seqs_true ) if self . report_wer : wer = self . calculate_wer ( seqs_hat , seqs_true ) return cer , wer __init__ ( self , char_list , sym_space , sym_blank , report_cer = False , report_wer = False ) special \u00b6 Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , char_list , sym_space , sym_blank , report_cer = False , report_wer = False ): super ( ErrorCalculator , self ) . __init__ () self . char_list = char_list self . space = sym_space self . blank = sym_blank self . report_cer = report_cer self . report_wer = report_wer self . idx_blank = self . char_list . index ( self . blank ) if self . space in self . char_list : self . idx_space = self . char_list . index ( self . space ) else : self . idx_space = None calculate_cer ( self , seqs_hat , seqs_true ) \u00b6 Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 167 168 169 170 171 172 173 174 def calculate_cer ( self , seqs_hat , seqs_true ): char_eds , char_ref_lens = [], [] for i , seq_hat_text in enumerate ( seqs_hat ): seq_true_text = seqs_true [ i ] hyp_chars = seq_hat_text . replace ( ' ' , '' ) ref_chars = seq_true_text . replace ( ' ' , '' ) char_ref_lens . append ( len ( ref_chars )) return float ( sum ( char_eds )) / sum ( char_ref_lens ) calculate_cer_ctc ( self , ys_hat , ys_pad ) \u00b6 Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def calculate_cer_ctc ( self , ys_hat , ys_pad ): cers , char_ref_lens = [], [] for i , y in enumerate ( ys_hat ): y_hat = [ x [ 0 ] for x in groupby ( y )] y_true = ys_pad [ i ] seq_hat , seq_true = [], [] for idx in y_hat : idx = int ( idx ) if idx != - 1 and idx != self . idx_blank and idx != self . idx_space : seq_hat . append ( self . char_list [ int ( idx )]) for idx in y_true : idx = int ( idx ) if idx != - 1 and idx != self . idx_blank and idx != self . idx_space : seq_true . append ( self . char_list [ int ( idx )]) hyp_chars = \"\" . join ( seq_hat ) ref_chars = \"\" . join ( seq_true ) cer_ctc = float ( sum ( cers )) / sum ( char_ref_lens ) if cers else None return cer_ctc calculate_wer ( self , seqs_hat , seqs_true ) \u00b6 Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 176 177 178 179 180 181 182 183 def calculate_wer ( self , seqs_hat , seqs_true ): word_eds , word_ref_lens = [], [] for i , seq_hat_text in enumerate ( seqs_hat ): seq_true_text = seqs_true [ i ] hyp_words = seq_hat_text . split () ref_words = seq_true_text . split () word_ref_lens . append ( len ( ref_words )) return float ( sum ( word_eds )) / sum ( word_ref_lens ) convert_to_char ( self , ys_hat , ys_pad ) \u00b6 Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def convert_to_char ( self , ys_hat , ys_pad ): seqs_hat , seqs_true = [], [] for i , y_hat in enumerate ( ys_hat ): y_true = ys_pad [ i ] eos_true = np . where ( y_true == - 1 )[ 0 ] eos_true = eos_true [ 0 ] if len ( eos_true ) > 0 else len ( y_true ) # To avoid wrong higger WER than the one obtained from the decoding # eos from y_true is used to mark the eos in y_hat # because of that y_hats has not padded outs with -1. seq_hat = [ self . char_list [ int ( idx )] for idx in y_hat [: eos_true ]] seq_true = [ self . char_list [ int ( idx )] for idx in y_true if int ( idx ) != - 1 ] seq_hat_text = \"\" . join ( seq_hat ) . replace ( self . space , ' ' ) seq_hat_text = seq_hat_text . replace ( self . blank , '' ) seq_true_text = \"\" . join ( seq_true ) . replace ( self . space , ' ' ) seqs_hat . append ( seq_hat_text ) seqs_true . append ( seq_true_text ) return seqs_hat , seqs_true end_detect ( ended_hyps , i , M = 3 , D_end =- 10.0 ) \u00b6 End detection desribed in Eq. (50) of S. Watanabe et al \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\" :param ended_hyps: :param i: :param M: :param D_end: :return: Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def end_detect ( ended_hyps , i , M = 3 , D_end = np . log ( 1 * np . exp ( - 10 ))): \"\"\"End detection desribed in Eq. (50) of S. Watanabe et al \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\" :param ended_hyps: :param i: :param M: :param D_end: :return: \"\"\" if len ( ended_hyps ) == 0 : return False count = 0 best_hyp = sorted ( ended_hyps , key = lambda x : x [ 'score' ], reverse = True )[ 0 ] for m in six . moves . range ( M ): # get ended_hyps with their length is i - m hyp_length = i - m hyps_same_length = [ x for x in ended_hyps if len ( x [ 'yseq' ]) == hyp_length ] if len ( hyps_same_length ) > 0 : best_hyp_same_length = sorted ( hyps_same_length , key = lambda x : x [ 'score' ], reverse = True )[ 0 ] if best_hyp_same_length [ 'score' ] - best_hyp [ 'score' ] < D_end : count += 1 if count == M : return True else : return False get_vgg2l_odim ( idim , in_channel = 3 , out_channel = 128 ) \u00b6 Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 80 81 82 83 84 def get_vgg2l_odim ( idim , in_channel = 3 , out_channel = 128 ): idim = idim / in_channel idim = np . ceil ( np . array ( idim , dtype = np . float32 ) / 2 ) # 1st max pooling idim = np . ceil ( np . array ( idim , dtype = np . float32 ) / 2 ) # 2nd max pooling return int ( idim ) * out_channel # numer of channels label_smoothing_dist ( odim , lsm_type , transcript = None , blank = 0 ) \u00b6 Obtain label distribution for loss smoothing :param odim: :param lsm_type: :param blank: :param transcript: :return: Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def label_smoothing_dist ( odim , lsm_type , transcript = None , blank = 0 ): \"\"\"Obtain label distribution for loss smoothing :param odim: :param lsm_type: :param blank: :param transcript: :return: \"\"\" if transcript is not None : with open ( transcript , 'rb' ) as f : trans_json = json . load ( f )[ 'utts' ] if lsm_type == 'unigram' : assert transcript is not None , 'transcript is required for %s label smoothing' % lsm_type labelcount = np . zeros ( odim ) for k , v in trans_json . items (): ids = np . array ([ int ( n ) for n in v [ 'output' ][ 0 ][ 'tokenid' ] . split ()]) # to avoid an error when there is no text in an uttrance if len ( ids ) > 0 : labelcount [ ids ] += 1 labelcount [ odim - 1 ] = len ( transcript ) # count <eos> labelcount [ labelcount == 0 ] = 1 # flooring labelcount [ blank ] = 0 # remove counts for blank labeldist = labelcount . astype ( np . float32 ) / np . sum ( labelcount ) else : logging . error ( \"Error: unexpected label smoothing type: %s \" % lsm_type ) sys . exit () return labeldist mt_interface \u00b6 MT Interface module. MTInterface \u00b6 MT Interface for ESPnet model implementation. attention_plot_class property readonly \u00b6 Get attention plot class. add_arguments ( parser ) staticmethod \u00b6 Add arguments to parser. Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 11 12 13 14 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments to parser.\"\"\" return parser build ( idim , odim , ** kwargs ) classmethod \u00b6 Initialize this class with python-level args. Parameters: Name Type Description Default idim int The number of an input feature dim. required odim int The number of output vocab. required Returns: Type Description ASRinterface A new instance of ASRInterface. Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @classmethod def build ( cls , idim : int , odim : int , ** kwargs ): \"\"\"Initialize this class with python-level args. Args: idim (int): The number of an input feature dim. odim (int): The number of output vocab. Returns: ASRinterface: A new instance of ASRInterface. \"\"\" def wrap ( parser ): return get_parser ( parser , required = False ) args = argparse . Namespace ( ** kwargs ) args = fill_missing_args ( args , wrap ) args = fill_missing_args ( args , cls . add_arguments ) return cls ( idim , odim , args ) calculate_all_attentions ( self , xs , ilens , ys ) \u00b6 Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 78 79 80 81 82 83 84 85 86 87 def calculate_all_attentions ( self , xs , ilens , ys ): \"\"\"Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray \"\"\" raise NotImplementedError ( \"calculate_all_attentions method is not implemented\" ) forward ( self , xs , ilens , ys ) \u00b6 Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def forward ( self , xs , ilens , ys ): \"\"\"Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer \"\"\" raise NotImplementedError ( \"forward method is not implemented\" ) translate ( self , x , trans_args , char_list = None , rnnlm = None ) \u00b6 Translate x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace trans_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 54 55 56 57 58 59 60 61 62 63 64 def translate ( self , x , trans_args , char_list = None , rnnlm = None ): \"\"\"Translate x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace trans_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"translate method is not implemented\" ) translate_batch ( self , x , trans_args , char_list = None , rnnlm = None ) \u00b6 Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace trans_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 66 67 68 69 70 71 72 73 74 75 76 def translate_batch ( self , x , trans_args , char_list = None , rnnlm = None ): \"\"\"Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace trans_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"Batch decoding is not supported yet.\" ) pytorch_backend special \u00b6 ctc \u00b6 CTC \u00b6 CTC module :param int odim: dimension of outputs :param int eprojs: number of encoder projection units :param float dropout_rate: dropout rate (0.0 ~ 1.0) :param str ctc_type: builtin or warpctc :param bool reduce: reduce the CTC loss into a scalar __init__ ( self , odim , eprojs , dropout_rate , ctc_type = 'warpctc' , reduce = True ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , odim , eprojs , dropout_rate , ctc_type = 'warpctc' , reduce = True ): super () . __init__ () self . dropout_rate = dropout_rate self . loss = None self . ctc_lo = torch . nn . Linear ( eprojs , odim ) self . ctc_type = ctc_type if self . ctc_type == 'builtin' : reduction_type = 'sum' if reduce else 'none' self . ctc_loss = torch . nn . CTCLoss ( reduction = reduction_type ) elif self . ctc_type == 'warpctc' : import warpctc_pytorch as warp_ctc self . ctc_loss = warp_ctc . CTCLoss ( size_average = True , reduce = reduce ) else : raise ValueError ( 'ctc_type must be \"builtin\" or \"warpctc\": {} ' . format ( self . ctc_type )) self . ignore_id = - 1 self . reduce = reduce argmax ( self , hs_pad ) argmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: argmax applied 2d tensor (B, Tmax) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 110 111 112 113 114 115 116 117 def argmax ( self , hs_pad ): \"\"\"argmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: argmax applied 2d tensor (B, Tmax) :rtype: torch.Tensor \"\"\" return torch . argmax ( self . ctc_lo ( hs_pad ), dim = 2 ) forward ( self , hs_pad , hlens , ys_pad ) CTC forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :return: ctc loss value :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , hs_pad , hlens , ys_pad ): \"\"\"CTC forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :return: ctc loss value :rtype: torch.Tensor \"\"\" # TODO(kan-bayashi): need to make more smart way ys = [ y [ y != self . ignore_id ] for y in ys_pad ] # parse padded ys self . loss = None hlens = torch . from_numpy ( np . fromiter ( hlens , dtype = np . int32 )) olens = torch . from_numpy ( np . fromiter ( ( x . size ( 0 ) for x in ys ), dtype = np . int32 )) # zero padding for hs ys_hat = self . ctc_lo ( F . dropout ( hs_pad , p = self . dropout_rate )) # zero padding for ys ys_true = torch . cat ( ys ) . cpu () . int () # batch x olen # get length info logging . info ( self . __class__ . __name__ + ' input lengths: ' + '' . join ( str ( hlens ) . split ( ' \\n ' ))) logging . info ( self . __class__ . __name__ + ' output lengths: ' + '' . join ( str ( olens ) . split ( ' \\n ' ))) # get ctc loss # expected shape of seqLength x batchSize x alphabet_size dtype = ys_hat . dtype ys_hat = ys_hat . transpose ( 0 , 1 ) if self . ctc_type == \"warpctc\" : # warpctc only supports float32 ys_hat = ys_hat . to ( dtype = torch . float32 ) else : # use GPU when using the cuDNN implementation ys_true = to_device ( self , ys_true ) self . loss = to_device ( self , self . loss_fn ( ys_hat , ys_true , hlens , olens )) . to ( dtype = dtype ) if self . reduce : # NOTE: sum() is needed to keep consistency since warpctc return as tensor w/ shape (1,) # but builtin return as tensor w/o shape (scalar). self . loss = self . loss . sum () logging . info ( 'ctc loss:' + str ( float ( self . loss ))) return self . loss log_softmax ( self , hs_pad ) log_softmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: log softmax applied 3d tensor (B, Tmax, odim) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 101 102 103 104 105 106 107 108 def log_softmax ( self , hs_pad ): \"\"\"log_softmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: log softmax applied 3d tensor (B, Tmax, odim) :rtype: torch.Tensor \"\"\" return F . log_softmax ( self . ctc_lo ( hs_pad ), dim = 2 ) loss_fn ( self , th_pred , th_target , th_ilen , th_olen ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def loss_fn ( self , th_pred , th_target , th_ilen , th_olen ): if self . ctc_type == 'builtin' : th_pred = th_pred . log_softmax ( 2 ) # Use the deterministic CuDNN implementation of CTC loss to avoid # [issue#17798](https://github.com/pytorch/pytorch/issues/17798) with torch . backends . cudnn . flags ( deterministic = True ): loss = self . ctc_loss ( th_pred , th_target , th_ilen , th_olen ) # Batch-size average loss = loss / th_pred . size ( 1 ) return loss elif self . ctc_type == 'warpctc' : return self . ctc_loss ( th_pred , th_target , th_ilen , th_olen ) else : raise NotImplementedError ctc_for ( args , odim , reduce = True ) \u00b6 Returns the CTC module for the given args and output dimension :param Namespace args: the program args :param int odim : The output dimension :param bool reduce : return the CTC loss in a scalar :return: the corresponding CTC module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def ctc_for ( args , odim , reduce = True ): \"\"\"Returns the CTC module for the given args and output dimension :param Namespace args: the program args :param int odim : The output dimension :param bool reduce : return the CTC loss in a scalar :return: the corresponding CTC module \"\"\" num_encs = getattr ( args , \"num_encs\" , 1 ) # use getattr to keep compatibility if num_encs == 1 : # compatible with single encoder asr mode return CTC ( odim , args . eprojs , args . dropout_rate , ctc_type = 'builtin' , reduce = reduce ) # changed this to use builtin ctc rather # than warpctc, so we have nothing to # install and it's just about the loss anyways. elif num_encs >= 1 : ctcs_list = torch . nn . ModuleList () if args . share_ctc : # use dropout_rate of the first encoder ctc = CTC ( odim , args . eprojs , args . dropout_rate [ 0 ], ctc_type = args . ctc_type , reduce = reduce ) ctcs_list . append ( ctc ) else : for idx in range ( num_encs ): ctc = CTC ( odim , args . eprojs , args . dropout_rate [ idx ], ctc_type = args . ctc_type , reduce = reduce ) ctcs_list . append ( ctc ) return ctcs_list else : raise ValueError ( \"Number of encoders needs to be more than one. {} \" . format ( num_encs )) e2e_asr \u00b6 RNN sequence-to-sequence speech recognition model (pytorch). E2E \u00b6 E2E module. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options __init__ ( self , idim , odim , args ) special Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def __init__ ( self , idim , odim , args ): \"\"\"Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options \"\"\" super ( E2E , self ) . __init__ () # This loads default arguments, # but not calling this yields the same error, so it's not why things break. torch . nn . Module . __init__ ( self ) self . mtlalpha = args . mtlalpha assert 0.0 <= self . mtlalpha <= 1.0 , \"mtlalpha should be [0.0, 1.0]\" self . etype = args . etype self . verbose = args . verbose # NOTE: for self.build method args . char_list = getattr ( args , \"char_list\" , None ) self . char_list = args . char_list self . outdir = args . outdir self . space = args . sym_space self . blank = args . sym_blank # below means the last number becomes eos/sos ID # note that sos/eos IDs are identical self . sos = odim - 1 self . eos = odim - 1 # subsample info # +1 means input (+1) and layers outputs (args.elayer) subsample = np . ones ( args . elayers + 1 , dtype = np . int ) if args . etype . endswith ( \"p\" ) and not args . etype . startswith ( \"vgg\" ): ss = args . subsample . split ( \"_\" ) for j in range ( min ( args . elayers + 1 , len ( ss ))): subsample [ j ] = int ( ss [ j ]) else : logging . warning ( 'Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.' ) logging . info ( 'subsample: ' + ' ' . join ([ str ( x ) for x in subsample ])) self . subsample = subsample # label smoothing info if args . lsm_type and os . path . isfile ( args . train_json ): logging . info ( \"Use label smoothing with \" + args . lsm_type ) labeldist = label_smoothing_dist ( odim , args . lsm_type , transcript = args . train_json ) else : labeldist = None if getattr ( args , \"use_frontend\" , False ): # use getattr to keep compatibility # Relative importing because of using python3 syntax from tools.espnet_minimal.nets.pytorch_backend.frontends.feature_transform \\ import feature_transform_for from tools.espnet_minimal.nets.pytorch_backend.frontends.frontend \\ import frontend_for self . frontend = frontend_for ( args , idim ) self . feature_transform = feature_transform_for ( args , ( idim - 1 ) * 2 ) idim = args . n_mels else : self . frontend = None # encoder self . enc = encoder_for ( args , idim , self . subsample ) # ctc # self.ctc = ctc_for(args, odim) <-- if this is executed, the shapes don't match. # The missing/unexpected arguments are not fixed by removing this however. # attention self . att = att_for ( args ) # decoder self . dec = decoder_for ( args , odim , self . sos , self . eos , self . att , labeldist ) # weight initialization self . init_like_chainer () self . report_cer = False self . report_wer = False self . rnnlm = None self . logzero = - 10000000000.0 self . loss = None self . acc = None add_arguments ( parser ) staticmethod Add arguments. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 44 45 46 47 48 49 50 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments.\"\"\" E2E . encoder_add_arguments ( parser ) E2E . attention_add_arguments ( parser ) E2E . decoder_add_arguments ( parser ) return parser attention_add_arguments ( parser ) staticmethod Add arguments for the attention. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 @staticmethod def attention_add_arguments ( parser ): \"\"\"Add arguments for the attention.\"\"\" group = parser . add_argument_group ( \"E2E attention setting\" ) # attention group . add_argument ( '--atype' , default = 'dot' , type = str , choices = [ 'noatt' , 'dot' , 'add' , 'location' , 'coverage' , 'coverage_location' , 'location2d' , 'location_recurrent' , 'multi_head_dot' , 'multi_head_add' , 'multi_head_loc' , 'multi_head_multi_res_loc' ], help = 'Type of attention architecture' ) group . add_argument ( '--adim' , default = 320 , type = int , help = 'Number of attention transformation dimensions' ) group . add_argument ( '--awin' , default = 5 , type = int , help = 'Window size for location2d attention' ) group . add_argument ( '--aheads' , default = 4 , type = int , help = 'Number of heads for multi head attention' ) group . add_argument ( '--aconv-chans' , default =- 1 , type = int , help = 'Number of attention convolution channels \\ (negative value indicates no location-aware attention)' ) group . add_argument ( '--aconv-filts' , default = 100 , type = int , help = 'Number of attention convolution filters \\ (negative value indicates no location-aware attention)' ) group . add_argument ( '--dropout-rate' , default = 0.0 , type = float , help = 'Dropout rate for the encoder' ) return parser calculate_all_attentions ( self , xs_pad , ilens , ys_pad ) E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 def calculate_all_attentions ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray \"\"\" with torch . no_grad (): # 0. Frontend if self . frontend is not None : hs_pad , hlens , mask = self . frontend ( to_torch_tensor ( xs_pad ), ilens ) hs_pad , hlens = self . feature_transform ( hs_pad , hlens ) else : hs_pad , hlens = xs_pad , ilens # 1. Encoder hpad , hlens , _ = self . enc ( hs_pad , hlens ) # 2. Decoder att_ws = self . dec . calculate_all_attentions ( hpad , hlens , ys_pad ) return att_ws decoder_add_arguments ( parser ) staticmethod Add arguments for the decoder. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @staticmethod def decoder_add_arguments ( parser ): \"\"\"Add arguments for the decoder.\"\"\" group = parser . add_argument_group ( \"E2E encoder setting\" ) group . add_argument ( '--dtype' , default = 'lstm' , type = str , choices = [ 'lstm' , 'gru' ], help = 'Type of decoder network architecture' ) group . add_argument ( '--dlayers' , default = 1 , type = int , help = 'Number of decoder layers' ) group . add_argument ( '--dunits' , default = 320 , type = int , help = 'Number of decoder hidden units' ) group . add_argument ( '--dropout-rate-decoder' , default = 0.0 , type = float , help = 'Dropout rate for the decoder' ) group . add_argument ( '--sampling-probability' , default = 0.0 , type = float , help = 'Ratio of predicted labels fed back to decoder' ) group . add_argument ( '--lsm-type' , const = '' , default = '' , type = str , nargs = '?' , choices = [ '' , 'unigram' ], help = 'Apply label smoothing with a specified distribution type' ) return parser encode ( self , x ) Encode acoustic features. :param ndarray x: input acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def encode ( self , x ): \"\"\"Encode acoustic features. :param ndarray x: input acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor \"\"\" self . eval () ilens = [ x . shape [ 0 ]] # subsample frame x = x [:: self . subsample [ 0 ], :] p = next ( self . parameters ()) h = torch . as_tensor ( x , device = p . device , dtype = p . dtype ) # make a utt list (1) to use the same interface for encoder hs = h . contiguous () . unsqueeze ( 0 ) # 0. Frontend if self . frontend is not None : enhanced , hlens , mask = self . frontend ( hs , ilens ) hs , hlens = self . feature_transform ( enhanced , hlens ) else : hs , hlens = hs , ilens # 1. encoder hs , _ , _ = self . enc ( hs , hlens ) return hs . squeeze ( 0 ) encoder_add_arguments ( parser ) staticmethod Add arguments for the encoder. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @staticmethod def encoder_add_arguments ( parser ): \"\"\"Add arguments for the encoder.\"\"\" group = parser . add_argument_group ( \"E2E encoder setting\" ) # encoder group . add_argument ( '--etype' , default = 'blstmp' , type = str , choices = [ 'lstm' , 'blstm' , 'lstmp' , 'blstmp' , 'vgglstmp' , 'vggblstmp' , 'vgglstm' , 'vggblstm' , 'gru' , 'bgru' , 'grup' , 'bgrup' , 'vgggrup' , 'vggbgrup' , 'vgggru' , 'vggbgru' ], help = 'Type of encoder network architecture' ) group . add_argument ( '--elayers' , default = 4 , type = int , help = 'Number of encoder layers (for shared recognition part in multi-speaker asr mode)' ) group . add_argument ( '--eunits' , '-u' , default = 300 , type = int , help = 'Number of encoder hidden units' ) group . add_argument ( '--eprojs' , default = 320 , type = int , help = 'Number of encoder projection units' ) group . add_argument ( '--subsample' , default = \"1\" , type = str , help = 'Subsample input frames x_y_z means subsample every x frame at 1st layer, ' 'every y frame at 2nd layer etc.' ) return parser enhance ( self , xs ) Forward only in the frontend stage. :param ndarray xs: input acoustic feature (T, C, F) :return: enhaned feature :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def enhance ( self , xs ): \"\"\"Forward only in the frontend stage. :param ndarray xs: input acoustic feature (T, C, F) :return: enhaned feature :rtype: torch.Tensor \"\"\" if self . frontend is None : raise RuntimeError ( 'Frontend does \\' t exist' ) prev = self . training self . eval () ilens = np . fromiter (( xx . shape [ 0 ] for xx in xs ), dtype = np . int64 ) # subsample frame xs = [ xx [:: self . subsample [ 0 ], :] for xx in xs ] xs = [ to_device ( self , to_torch_tensor ( xx ) . float ()) for xx in xs ] xs_pad = pad_list ( xs , 0.0 ) enhanced , hlensm , mask = self . frontend ( xs_pad , ilens ) if prev : self . train () return enhanced . cpu () . numpy (), mask . cpu () . numpy (), ilens forward ( self , xs_pad , ilens , ys_pad ) E2E forward. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: loss value :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def forward ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E forward. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: loss value :rtype: torch.Tensor \"\"\" # 0. Frontend if self . frontend is not None : hs_pad , hlens , mask = self . frontend ( to_torch_tensor ( xs_pad ), ilens ) hs_pad , hlens = self . feature_transform ( hs_pad , hlens ) else : hs_pad , hlens = xs_pad , ilens # 1. Encoder hs_pad , hlens , _ = self . enc ( hs_pad , hlens ) # 2. CTC loss if self . mtlalpha == 0 : self . loss_ctc = None else : self . loss_ctc = self . ctc ( hs_pad , hlens , ys_pad ) # 3. attention loss if self . mtlalpha == 1 : self . loss_att , acc = None , None else : self . loss_att , acc , _ = self . dec ( hs_pad , hlens , ys_pad ) self . acc = acc # 4. compute cer without beam search if self . mtlalpha == 0 or self . char_list is None : cer_ctc = None else : cers = [] y_hats = self . ctc . argmax ( hs_pad ) . data for i , y in enumerate ( y_hats ): y_hat = [ x [ 0 ] for x in groupby ( y )] y_true = ys_pad [ i ] seq_hat = [ self . char_list [ int ( idx )] for idx in y_hat if int ( idx ) != - 1 ] seq_true = [ self . char_list [ int ( idx )] for idx in y_true if int ( idx ) != - 1 ] seq_hat_text = \"\" . join ( seq_hat ) . replace ( self . space , ' ' ) seq_hat_text = seq_hat_text . replace ( self . blank , '' ) seq_true_text = \"\" . join ( seq_true ) . replace ( self . space , ' ' ) hyp_chars = seq_hat_text . replace ( ' ' , '' ) ref_chars = seq_true_text . replace ( ' ' , '' ) cer_ctc = sum ( cers ) / len ( cers ) if cers else None # 5. compute cer/wer if self . training or not ( self . report_cer or self . report_wer ): cer , wer = 0.0 , 0.0 # oracle_cer, oracle_wer = 0.0, 0.0 else : if self . recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( hs_pad ) . data else : lpz = None word_eds , word_ref_lens , char_eds , char_ref_lens = [], [], [], [] nbest_hyps = self . dec . recognize_beam_batch ( hs_pad , torch . tensor ( hlens ), lpz , self . recog_args , self . char_list , self . rnnlm ) # remove <sos> and <eos> y_hats = [ nbest_hyp [ 0 ][ 'yseq' ][ 1 : - 1 ] for nbest_hyp in nbest_hyps ] for i , y_hat in enumerate ( y_hats ): y_true = ys_pad [ i ] seq_hat = [ self . char_list [ int ( idx )] for idx in y_hat if int ( idx ) != - 1 ] seq_true = [ self . char_list [ int ( idx )] for idx in y_true if int ( idx ) != - 1 ] seq_hat_text = \"\" . join ( seq_hat ) . replace ( self . recog_args . space , ' ' ) seq_hat_text = seq_hat_text . replace ( self . recog_args . blank , '' ) seq_true_text = \"\" . join ( seq_true ) . replace ( self . recog_args . space , ' ' ) hyp_words = seq_hat_text . split () ref_words = seq_true_text . split () word_ref_lens . append ( len ( ref_words )) hyp_chars = seq_hat_text . replace ( ' ' , '' ) ref_chars = seq_true_text . replace ( ' ' , '' ) char_ref_lens . append ( len ( ref_chars )) wer = 0.0 if not self . report_wer else float ( sum ( word_eds )) / sum ( word_ref_lens ) cer = 0.0 if not self . report_cer else float ( sum ( char_eds )) / sum ( char_ref_lens ) alpha = self . mtlalpha if alpha == 0 : self . loss = self . loss_att loss_att_data = float ( self . loss_att ) loss_ctc_data = None elif alpha == 1 : self . loss = self . loss_ctc loss_att_data = None loss_ctc_data = float ( self . loss_ctc ) else : self . loss = alpha * self . loss_ctc + ( 1 - alpha ) * self . loss_att loss_att_data = float ( self . loss_att ) loss_ctc_data = float ( self . loss_ctc ) loss_data = float ( self . loss ) if loss_data < CTC_LOSS_THRESHOLD and not math . isnan ( loss_data ): self . reporter . report ( loss_ctc_data , loss_att_data , acc , cer_ctc , cer , wer , loss_data ) else : logging . warning ( 'loss (= %f ) is not correct' , loss_data ) return self . loss init_like_chainer ( self ) Initialize weight like chainer. chainer basically uses LeCun way: W ~ Normal(0, fan_in -0.5), b = 0 pytorch basically uses W, b ~ Uniform(-fan_in -0.5, fan_in**-0.5) however, there are two exceptions as far as I know. - EmbedID.W ~ Normal(0, 1) - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def init_like_chainer ( self ): \"\"\"Initialize weight like chainer. chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0 pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5) however, there are two exceptions as far as I know. - EmbedID.W ~ Normal(0, 1) - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM) \"\"\" lecun_normal_init_parameters ( self ) # exceptions # embed weight ~ Normal(0, 1) self . dec . embed . weight . data . normal_ ( 0 , 1 ) # forget-bias = 1.0 # https://discuss.pytorch.org/t/set-forget-gate-bias-of-lstm/1745 for l in six . moves . range ( len ( self . dec . decoder )): set_forget_bias_to_one ( self . dec . decoder [ l ] . bias_ih ) recognize ( self , x , recog_args , char_list , rnnlm = None ) E2E beam search. :param ndarray x: input acoustic feature (T, D) :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def recognize ( self , x , recog_args , char_list , rnnlm = None ): \"\"\"E2E beam search. :param ndarray x: input acoustic feature (T, D) :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" hs = self . encode ( x ) . unsqueeze ( 0 ) # calculate log P(z_t|X) for CTC scores if recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( hs )[ 0 ] else : lpz = None # 2. Decoder # decode the first utterance y = self . dec . recognize_beam ( hs [ 0 ], lpz , recog_args , char_list , rnnlm ) return y recognize_batch ( self , xs , recog_args , char_list , rnnlm = None ) E2E beam search. :param list xs: list of input acoustic feature arrays [(T_1, D), (T_2, D), ...] :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def recognize_batch ( self , xs , recog_args , char_list , rnnlm = None ): \"\"\"E2E beam search. :param list xs: list of input acoustic feature arrays [(T_1, D), (T_2, D), ...] :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" prev = self . training self . eval () ilens = np . fromiter (( xx . shape [ 0 ] for xx in xs ), dtype = np . int64 ) # subsample frame xs = [ xx [:: self . subsample [ 0 ], :] for xx in xs ] xs = [ to_device ( self , to_torch_tensor ( xx ) . float ()) for xx in xs ] xs_pad = pad_list ( xs , 0.0 ) # 0. Frontend if self . frontend is not None : enhanced , hlens , mask = self . frontend ( xs_pad , ilens ) hs_pad , hlens = self . feature_transform ( enhanced , hlens ) else : hs_pad , hlens = xs_pad , ilens # 1. Encoder hs_pad , hlens , _ = self . enc ( hs_pad , hlens ) # calculate log P(z_t|X) for CTC scores if recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( hs_pad ) normalize_score = False else : lpz = None normalize_score = True # 2. Decoder hlens = torch . tensor ( list ( map ( int , hlens ))) # make sure hlens is tensor y = self . dec . recognize_beam_batch ( hs_pad , hlens , lpz , recog_args , char_list , rnnlm , normalize_score = normalize_score ) if prev : self . train () return y scorers ( self ) Scorers. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 328 329 330 def scorers ( self ): \"\"\"Scorers.\"\"\" return dict ( decoder = self . dec , ctc = CTCPrefixScorer ( self . ctc , self . eos )) subsample_frames ( self , x ) Subsample speeh frames in the encoder. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 477 478 479 480 481 482 483 484 485 def subsample_frames ( self , x ): \"\"\"Subsample speeh frames in the encoder.\"\"\" # subsample frame x = x [:: self . subsample [ 0 ], :] ilen = [ x . shape [ 0 ]] h = to_device ( self , torch . from_numpy ( np . array ( x , dtype = np . float32 ))) h . contiguous () return h , ilen e2e_asr_transformer \u00b6 Transformer speech recognition model (pytorch). E2E \u00b6 E2E module. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options attention_plot_class property readonly Return PlotAttentionReport. __init__ ( self , idim , odim , args , ignore_id =- 1 ) special Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , idim , odim , args , ignore_id =- 1 ): \"\"\"Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options \"\"\" torch . nn . Module . __init__ ( self ) if args . transformer_attn_dropout_rate is None : args . transformer_attn_dropout_rate = args . dropout_rate self . encoder = Encoder ( idim = idim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . eunits , num_blocks = args . elayers , input_layer = args . transformer_input_layer , dropout_rate = args . dropout_rate , positional_dropout_rate = args . dropout_rate , attention_dropout_rate = args . transformer_attn_dropout_rate ) self . decoder = Decoder ( odim = odim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . dunits , num_blocks = args . dlayers , dropout_rate = args . dropout_rate , positional_dropout_rate = args . dropout_rate , self_attention_dropout_rate = args . transformer_attn_dropout_rate , src_attention_dropout_rate = args . transformer_attn_dropout_rate ) self . sos = odim - 1 self . eos = odim - 1 self . odim = odim self . ignore_id = ignore_id self . subsample = [ 1 ] # self.lsm_weight = a self . criterion = LabelSmoothingLoss ( self . odim , self . ignore_id , args . lsm_weight , args . transformer_length_normalized_loss ) # self.verbose = args.verbose self . reset_parameters ( args ) self . adim = args . adim self . mtlalpha = args . mtlalpha if args . mtlalpha > 0.0 : self . ctc = CTC ( odim , args . adim , args . dropout_rate , ctc_type = args . ctc_type , reduce = True ) else : self . ctc = None if args . report_cer or args . report_wer : from tools.espnet_minimal import ErrorCalculator self . error_calculator = ErrorCalculator ( args . char_list , args . sym_space , args . sym_blank , args . report_cer , args . report_wer ) else : self . error_calculator = None self . rnnlm = None add_arguments ( parser ) staticmethod Add arguments. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments.\"\"\" group = parser . add_argument_group ( \"transformer model setting\" ) group . add_argument ( \"--transformer-init\" , type = str , default = \"pytorch\" , choices = [ \"pytorch\" , \"xavier_uniform\" , \"xavier_normal\" , \"kaiming_uniform\" , \"kaiming_normal\" ], help = 'how to initialize transformer parameters' ) group . add_argument ( \"--transformer-input-layer\" , type = str , default = \"conv2d\" , choices = [ \"conv2d\" , \"linear\" , \"embed\" ], help = 'transformer input layer type' ) group . add_argument ( '--transformer-attn-dropout-rate' , default = None , type = float , help = 'dropout in transformer attention. use --dropout-rate if None is set' ) group . add_argument ( '--transformer-lr' , default = 10.0 , type = float , help = 'Initial value of learning rate' ) group . add_argument ( '--transformer-warmup-steps' , default = 25000 , type = int , help = 'optimizer warmup steps' ) group . add_argument ( '--transformer-length-normalized-loss' , default = True , type = strtobool , help = 'normalize loss by length' ) group . add_argument ( '--dropout-rate' , default = 0.0 , type = float , help = 'Dropout rate for the encoder' ) # Encoder group . add_argument ( '--elayers' , default = 4 , type = int , help = 'Number of encoder layers (for shared recognition part in multi-speaker asr mode)' ) group . add_argument ( '--eunits' , '-u' , default = 300 , type = int , help = 'Number of encoder hidden units' ) # Attention group . add_argument ( '--adim' , default = 320 , type = int , help = 'Number of attention transformation dimensions' ) group . add_argument ( '--aheads' , default = 4 , type = int , help = 'Number of heads for multi head attention' ) # Decoder group . add_argument ( '--dlayers' , default = 1 , type = int , help = 'Number of decoder layers' ) group . add_argument ( '--dunits' , default = 320 , type = int , help = 'Number of decoder hidden units' ) return parser calculate_all_attentions ( self , xs_pad , ilens , ys_pad ) E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def calculate_all_attentions ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray \"\"\" with torch . no_grad (): self . forward ( xs_pad , ilens , ys_pad ) ret = dict () for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): ret [ name ] = m . attn . cpu () . numpy () return ret encode ( self , x ) Encode acoustic features. :param ndarray x: source acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 223 224 225 226 227 228 229 230 231 232 233 def encode ( self , x ): \"\"\"Encode acoustic features. :param ndarray x: source acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor \"\"\" self . eval () x = torch . as_tensor ( x ) . unsqueeze ( 0 ) enc_output , _ = self . encoder ( x , None ) return enc_output . squeeze ( 0 ) forward ( self , xs_pad , ilens , ys_pad ) E2E forward. :param torch.Tensor xs_pad: batch of padded source sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of source sequences (B) :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :return: ctc loass value :rtype: torch.Tensor :return: attention loss value :rtype: torch.Tensor :return: accuracy in attention decoder :rtype: float Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def forward ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E forward. :param torch.Tensor xs_pad: batch of padded source sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of source sequences (B) :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :return: ctc loass value :rtype: torch.Tensor :return: attention loss value :rtype: torch.Tensor :return: accuracy in attention decoder :rtype: float \"\"\" # 1. forward encoder xs_pad = xs_pad [:, : max ( ilens )] # for data parallel src_mask = ( ~ make_pad_mask ( ilens . tolist ())) . to ( xs_pad . device ) . unsqueeze ( - 2 ) hs_pad , hs_mask = self . encoder ( xs_pad , src_mask ) self . hs_pad = hs_pad # 2. forward decoder ys_in_pad , ys_out_pad = add_sos_eos ( ys_pad , self . sos , self . eos , self . ignore_id ) ys_mask = target_mask ( ys_in_pad , self . ignore_id ) pred_pad , pred_mask = self . decoder ( ys_in_pad , ys_mask , hs_pad , hs_mask ) self . pred_pad = pred_pad # 3. compute attention loss loss_att = self . criterion ( pred_pad , ys_out_pad ) self . acc = th_accuracy ( pred_pad . view ( - 1 , self . odim ), ys_out_pad , ignore_label = self . ignore_id ) # TODO(karita) show predicted text # TODO(karita) calculate these stats cer_ctc = None if self . mtlalpha == 0.0 : loss_ctc = None else : batch_size = xs_pad . size ( 0 ) hs_len = hs_mask . view ( batch_size , - 1 ) . sum ( 1 ) loss_ctc = self . ctc ( hs_pad . view ( batch_size , - 1 , self . adim ), hs_len , ys_pad ) if self . error_calculator is not None : ys_hat = self . ctc . argmax ( hs_pad . view ( batch_size , - 1 , self . adim )) . data cer_ctc = self . error_calculator ( ys_hat . cpu (), ys_pad . cpu (), is_ctc = True ) # 5. compute cer/wer if self . training or self . error_calculator is None : cer , wer = None , None else : ys_hat = pred_pad . argmax ( dim =- 1 ) cer , wer = self . error_calculator ( ys_hat . cpu (), ys_pad . cpu ()) # copyied from e2e_asr alpha = self . mtlalpha if alpha == 0 : self . loss = loss_att loss_att_data = float ( loss_att ) loss_ctc_data = None elif alpha == 1 : self . loss = loss_ctc loss_att_data = None loss_ctc_data = float ( loss_ctc ) else : self . loss = alpha * loss_ctc + ( 1 - alpha ) * loss_att loss_att_data = float ( loss_att ) loss_ctc_data = float ( loss_ctc ) loss_data = float ( self . loss ) if loss_data < CTC_LOSS_THRESHOLD and not math . isnan ( loss_data ): self . reporter . report ( loss_ctc_data , loss_att_data , self . acc , cer_ctc , cer , wer , loss_data ) else : logging . warning ( 'loss (= %f ) is not correct' , loss_data ) return self . loss recognize ( self , x , recog_args , char_list = None , rnnlm = None , use_jit = False ) Recognize input speech. :param ndnarray x: input acoustic feature (B, T, D) or (T, D) :param Namespace recog_args: argment Namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 def recognize ( self , x , recog_args , char_list = None , rnnlm = None , use_jit = False ): \"\"\"Recognize input speech. :param ndnarray x: input acoustic feature (B, T, D) or (T, D) :param Namespace recog_args: argment Namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" enc_output = self . encode ( x ) . unsqueeze ( 0 ) if recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( enc_output ) lpz = lpz . squeeze ( 0 ) else : lpz = None h = enc_output . squeeze ( 0 ) logging . info ( 'input lengths: ' + str ( h . size ( 0 ))) # search parms beam = recog_args . beam_size penalty = recog_args . penalty ctc_weight = recog_args . ctc_weight # preprare sos y = self . sos vy = h . new_zeros ( 1 ) . long () if recog_args . maxlenratio == 0 : maxlen = h . shape [ 0 ] else : # maxlen >= 1 maxlen = max ( 1 , int ( recog_args . maxlenratio * h . size ( 0 ))) minlen = int ( recog_args . minlenratio * h . size ( 0 )) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # initialize hypothesis if rnnlm : hyp = { 'score' : 0.0 , 'yseq' : [ y ], 'rnnlm_prev' : None } else : hyp = { 'score' : 0.0 , 'yseq' : [ y ]} if lpz is not None : import numpy from tools.espnet_minimal.nets.ctc_prefix_score import CTCPrefixScore ctc_prefix_score = CTCPrefixScore ( lpz . detach () . numpy (), 0 , self . eos , numpy ) hyp [ 'ctc_state_prev' ] = ctc_prefix_score . initial_state () hyp [ 'ctc_score_prev' ] = 0.0 if ctc_weight != 1.0 : # pre-pruning based on attention scores from tools.espnet_minimal.nets.pytorch_backend.rnn.decoders import \\ CTC_SCORING_RATIO ctc_beam = min ( lpz . shape [ - 1 ], int ( beam * CTC_SCORING_RATIO )) else : ctc_beam = lpz . shape [ - 1 ] hyps = [ hyp ] ended_hyps = [] import six traced_decoder = None for i in six . moves . range ( maxlen ): logging . debug ( 'position ' + str ( i )) hyps_best_kept = [] for hyp in hyps : vy . unsqueeze ( 1 ) vy [ 0 ] = hyp [ 'yseq' ][ i ] # get nbest local scores and their ids ys_mask = subsequent_mask ( i + 1 ) . unsqueeze ( 0 ) ys = torch . tensor ( hyp [ 'yseq' ]) . unsqueeze ( 0 ) # FIXME: jit does not match non-jit result if use_jit : if traced_decoder is None : traced_decoder = torch . jit . trace ( self . decoder . forward_one_step , ( ys , ys_mask , enc_output )) local_att_scores = traced_decoder ( ys , ys_mask , enc_output )[ 0 ] else : local_att_scores = self . decoder . forward_one_step ( ys , ys_mask , enc_output )[ 0 ] if rnnlm : rnnlm_state , local_lm_scores = rnnlm . predict ( hyp [ 'rnnlm_prev' ], vy ) local_scores = local_att_scores + recog_args . lm_weight * local_lm_scores else : local_scores = local_att_scores if lpz is not None : local_best_scores , local_best_ids = torch . topk ( local_att_scores , ctc_beam , dim = 1 ) ctc_scores , ctc_states = ctc_prefix_score ( hyp [ 'yseq' ], local_best_ids [ 0 ], hyp [ 'ctc_state_prev' ]) local_scores = \\ ( 1.0 - ctc_weight ) * local_att_scores [:, local_best_ids [ 0 ]] \\ + ctc_weight * torch . from_numpy ( ctc_scores - hyp [ 'ctc_score_prev' ]) if rnnlm : local_scores += recog_args . lm_weight * local_lm_scores [:, local_best_ids [ 0 ]] local_best_scores , joint_best_ids = torch . topk ( local_scores , beam , dim = 1 ) local_best_ids = local_best_ids [:, joint_best_ids [ 0 ]] else : local_best_scores , local_best_ids = torch . topk ( local_scores , beam , dim = 1 ) for j in six . moves . range ( beam ): new_hyp = {} new_hyp [ 'score' ] = hyp [ 'score' ] + float ( local_best_scores [ 0 , j ]) new_hyp [ 'yseq' ] = [ 0 ] * ( 1 + len ( hyp [ 'yseq' ])) new_hyp [ 'yseq' ][: len ( hyp [ 'yseq' ])] = hyp [ 'yseq' ] new_hyp [ 'yseq' ][ len ( hyp [ 'yseq' ])] = int ( local_best_ids [ 0 , j ]) if rnnlm : new_hyp [ 'rnnlm_prev' ] = rnnlm_state if lpz is not None : new_hyp [ 'ctc_state_prev' ] = ctc_states [ joint_best_ids [ 0 , j ]] new_hyp [ 'ctc_score_prev' ] = ctc_scores [ joint_best_ids [ 0 , j ]] # will be (2 x beam) hyps at most hyps_best_kept . append ( new_hyp ) hyps_best_kept = sorted ( hyps_best_kept , key = lambda x : x [ 'score' ], reverse = True )[: beam ] # sort and get nbest hyps = hyps_best_kept logging . debug ( 'number of pruned hypothes: ' + str ( len ( hyps ))) if char_list is not None : logging . debug ( 'best hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyps [ 0 ][ 'yseq' ][ 1 :]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( 'adding <eos> in the last postion in the loop' ) for hyp in hyps : hyp [ 'yseq' ] . append ( self . eos ) # add ended hypothes to a final list, and removed them from current hypothes # (this will be a probmlem, number of hyps < beam) remained_hyps = [] for hyp in hyps : if hyp [ 'yseq' ][ - 1 ] == self . eos : # only store the sequence that has more than minlen outputs # also add penalty if len ( hyp [ 'yseq' ]) > minlen : hyp [ 'score' ] += ( i + 1 ) * penalty if rnnlm : # Word LM needs to add final <eos> score hyp [ 'score' ] += recog_args . lm_weight * rnnlm . final ( hyp [ 'rnnlm_prev' ]) ended_hyps . append ( hyp ) else : remained_hyps . append ( hyp ) # end detection from tools.espnet_minimal.nets.e2e_asr_common import end_detect if end_detect ( ended_hyps , i ) and recog_args . maxlenratio == 0.0 : logging . info ( 'end detected at %d ' , i ) break hyps = remained_hyps if len ( hyps ) > 0 : logging . debug ( 'remeined hypothes: ' + str ( len ( hyps ))) else : logging . info ( 'no hypothesis. Finish decoding.' ) break if char_list is not None : for hyp in hyps : logging . debug ( 'hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyp [ 'yseq' ][ 1 :]])) logging . debug ( 'number of ended hypothes: ' + str ( len ( ended_hyps ))) nbest_hyps = sorted ( ended_hyps , key = lambda x : x [ 'score' ], reverse = True )[: min ( len ( ended_hyps ), recog_args . nbest )] # check number of hypotheis if len ( nbest_hyps ) == 0 : logging . warning ( 'there is no N-best results, perform recognition again with smaller minlenratio.' ) # should copy becasuse Namespace will be overwritten globally recog_args = Namespace ( ** vars ( recog_args )) recog_args . minlenratio = max ( 0.0 , recog_args . minlenratio - 0.1 ) return self . recognize ( x , recog_args , char_list , rnnlm ) logging . info ( 'total log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ])) logging . info ( 'normalized log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ] / len ( nbest_hyps [ 0 ][ 'yseq' ]))) return nbest_hyps reset_parameters ( self , args ) Initialize parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 142 143 144 145 def reset_parameters ( self , args ): \"\"\"Initialize parameters.\"\"\" # initialize parameters initialize ( self , args . transformer_init ) scorers ( self ) Scorers. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 219 220 221 def scorers ( self ): \"\"\"Scorers.\"\"\" return dict ( decoder = self . decoder , ctc = CTCPrefixScorer ( self . ctc , self . eos )) e2e_tts_fastspeech \u00b6 FastSpeech related modules. FeedForwardTransformer \u00b6 Feed Forward Transformer for TTS a.k.a. FastSpeech. This is a module of FastSpeech , feed - forward Transformer with duration predictor described in ` FastSpeech : Fast , Robust and Controllable Text to Speech ` _ , which does not require any auto - regressive processing during inference , resulting in fast decoding compared with auto - regressive Transformer . .. _ ` FastSpeech : Fast , Robust and Controllable Text to Speech ` : https : // arxiv . org / pdf / 1905 . 09263 . pdf attention_plot_class property readonly Return plot class for attention weight plot. base_plot_keys property readonly Return base key names to plot during training. keys should match what chainer.reporter reports. If you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list List of strings which are base keys to plot during training. __init__ ( self , idim , odim , args = None ) special Initialize feed-forward Transformer module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required args Namespace elayers (int): Number of encoder layers. eunits (int): Number of encoder hidden units. adim (int): Number of attention transformation dimensions. aheads (int): Number of heads for multi head attention. dlayers (int): Number of decoder layers. dunits (int): Number of decoder hidden units. use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. duration_predictor_layers (int): Number of duration predictor layers. duration_predictor_chans (int): Number of duration predictor channels. duration_predictor_kernel_size (int): Kernel size of duration predictor. spk_embed_dim (int): Number of speaker embedding dimenstions. spk_embed_integration_type: How to integrate speaker embedding. teacher_model (str): Teacher auto-regressive transformer model path. reduction_factor (int): Reduction factor. transformer_init (float): How to initialize transformer parameters. transformer_lr (float): Initial value of learning rate. transformer_warmup_steps (int): Optimizer warmup steps. transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. transfer_encoder_from_teacher: Whether to transfer encoder using teacher encoder parameters. transferred_encoder_module: Encoder module to be initialized using teacher parameters. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def __init__ ( self , idim , odim , args = None ): \"\"\"Initialize feed-forward Transformer module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. args (Namespace, optional): - elayers (int): Number of encoder layers. - eunits (int): Number of encoder hidden units. - adim (int): Number of attention transformation dimensions. - aheads (int): Number of heads for multi head attention. - dlayers (int): Number of decoder layers. - dunits (int): Number of decoder hidden units. - use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. - encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. - decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. - encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. - decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. - duration_predictor_layers (int): Number of duration predictor layers. - duration_predictor_chans (int): Number of duration predictor channels. - duration_predictor_kernel_size (int): Kernel size of duration predictor. - spk_embed_dim (int): Number of speaker embedding dimenstions. - spk_embed_integration_type: How to integrate speaker embedding. - teacher_model (str): Teacher auto-regressive transformer model path. - reduction_factor (int): Reduction factor. - transformer_init (float): How to initialize transformer parameters. - transformer_lr (float): Initial value of learning rate. - transformer_warmup_steps (int): Optimizer warmup steps. - transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. - transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. - transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. - transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. - transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. - transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. - transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. - use_masking (bool): Whether to apply masking for padded part in loss calculation. - use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. - transfer_encoder_from_teacher: Whether to transfer encoder using teacher encoder parameters. - transferred_encoder_module: Encoder module to be initialized using teacher parameters. \"\"\" # initialize base classes TTSInterface . __init__ ( self ) torch . nn . Module . __init__ ( self ) # fill missing arguments args = fill_missing_args ( args , self . add_arguments ) # store hyperparameters self . idim = idim self . odim = odim self . reduction_factor = args . reduction_factor self . use_scaled_pos_enc = args . use_scaled_pos_enc self . spk_embed_dim = args . spk_embed_dim if self . spk_embed_dim is not None : self . spk_embed_integration_type = args . spk_embed_integration_type # use idx 0 as padding idx padding_idx = 0 # get positional encoding class pos_enc_class = ScaledPositionalEncoding if self . use_scaled_pos_enc else PositionalEncoding # define encoder encoder_input_layer = torch . nn . Embedding ( num_embeddings = idim , embedding_dim = args . adim , padding_idx = padding_idx ) self . encoder = Encoder ( idim = idim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . eunits , num_blocks = args . elayers , input_layer = encoder_input_layer , dropout_rate = args . transformer_enc_dropout_rate , positional_dropout_rate = args . transformer_enc_positional_dropout_rate , attention_dropout_rate = args . transformer_enc_attn_dropout_rate , pos_enc_class = pos_enc_class , normalize_before = args . encoder_normalize_before , concat_after = args . encoder_concat_after , positionwise_layer_type = args . positionwise_layer_type , positionwise_conv_kernel_size = args . positionwise_conv_kernel_size ) # define additional projection for speaker embedding if self . spk_embed_dim is not None : if self . spk_embed_integration_type == \"add\" : self . projection = torch . nn . Linear ( self . spk_embed_dim , args . adim ) else : self . projection = torch . nn . Linear ( args . adim + self . spk_embed_dim , args . adim ) # define duration predictor self . duration_predictor = DurationPredictor ( idim = args . adim , n_layers = args . duration_predictor_layers , n_chans = args . duration_predictor_chans , kernel_size = args . duration_predictor_kernel_size , dropout_rate = args . duration_predictor_dropout_rate , ) # define length regulator self . length_regulator = LengthRegulator () # define decoder # NOTE: we use encoder as decoder because fastspeech's decoder is the same as encoder self . decoder = Encoder ( idim = 0 , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . dunits , num_blocks = args . dlayers , input_layer = None , dropout_rate = args . transformer_dec_dropout_rate , positional_dropout_rate = args . transformer_dec_positional_dropout_rate , attention_dropout_rate = args . transformer_dec_attn_dropout_rate , pos_enc_class = pos_enc_class , normalize_before = args . decoder_normalize_before , concat_after = args . decoder_concat_after , positionwise_layer_type = args . positionwise_layer_type , positionwise_conv_kernel_size = args . positionwise_conv_kernel_size ) # define final projection self . feat_out = torch . nn . Linear ( args . adim , odim * args . reduction_factor ) # define postnet self . postnet = None if args . postnet_layers == 0 else Postnet ( idim = idim , odim = odim , n_layers = args . postnet_layers , n_chans = args . postnet_chans , n_filts = args . postnet_filts , use_batch_norm = args . use_batch_norm , dropout_rate = args . postnet_dropout_rate ) # initialize parameters self . _reset_parameters ( init_type = args . transformer_init , init_enc_alpha = args . initial_encoder_alpha , init_dec_alpha = args . initial_decoder_alpha ) # define teacher model if args . teacher_model is not None : self . teacher = self . _load_teacher_model ( args . teacher_model ) else : self . teacher = None # define duration calculator if self . teacher is not None : self . duration_calculator = DurationCalculator ( self . teacher ) else : self . duration_calculator = None # transfer teacher parameters if self . teacher is not None and args . transfer_encoder_from_teacher : self . _transfer_from_teacher ( args . transferred_encoder_module ) # define criterions self . criterion = FeedForwardTransformerLoss ( use_masking = args . use_masking , use_weighted_masking = args . use_weighted_masking ) add_arguments ( parser ) staticmethod Add model-specific arguments to the parser. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 @staticmethod def add_arguments ( parser ): \"\"\"Add model-specific arguments to the parser.\"\"\" group = parser . add_argument_group ( \"feed-forward transformer model setting\" ) # network structure related group . add_argument ( \"--adim\" , default = 384 , type = int , help = \"Number of attention transformation dimensions\" ) group . add_argument ( \"--aheads\" , default = 4 , type = int , help = \"Number of heads for multi head attention\" ) group . add_argument ( \"--elayers\" , default = 6 , type = int , help = \"Number of encoder layers\" ) group . add_argument ( \"--eunits\" , default = 1536 , type = int , help = \"Number of encoder hidden units\" ) group . add_argument ( \"--dlayers\" , default = 6 , type = int , help = \"Number of decoder layers\" ) group . add_argument ( \"--dunits\" , default = 1536 , type = int , help = \"Number of decoder hidden units\" ) group . add_argument ( \"--positionwise-layer-type\" , default = \"linear\" , type = str , choices = [ \"linear\" , \"conv1d\" , \"conv1d-linear\" ], help = \"Positionwise layer type.\" ) group . add_argument ( \"--positionwise-conv-kernel-size\" , default = 3 , type = int , help = \"Kernel size of positionwise conv1d layer\" ) group . add_argument ( \"--postnet-layers\" , default = 0 , type = int , help = \"Number of postnet layers\" ) group . add_argument ( \"--postnet-chans\" , default = 256 , type = int , help = \"Number of postnet channels\" ) group . add_argument ( \"--postnet-filts\" , default = 5 , type = int , help = \"Filter size of postnet\" ) group . add_argument ( \"--use-batch-norm\" , default = True , type = strtobool , help = \"Whether to use batch normalization\" ) group . add_argument ( \"--use-scaled-pos-enc\" , default = True , type = strtobool , help = \"Use trainable scaled positional encoding instead of the fixed scale one\" ) group . add_argument ( \"--encoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before encoder block\" ) group . add_argument ( \"--decoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before decoder block\" ) group . add_argument ( \"--encoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in encoder\" ) group . add_argument ( \"--decoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in decoder\" ) group . add_argument ( \"--duration-predictor-layers\" , default = 2 , type = int , help = \"Number of layers in duration predictor\" ) group . add_argument ( \"--duration-predictor-chans\" , default = 384 , type = int , help = \"Number of channels in duration predictor\" ) group . add_argument ( \"--duration-predictor-kernel-size\" , default = 3 , type = int , help = \"Kernel size in duration predictor\" ) group . add_argument ( \"--teacher-model\" , default = None , type = str , nargs = \"?\" , help = \"Teacher model file path\" ) group . add_argument ( \"--reduction-factor\" , default = 1 , type = int , help = \"Reduction factor\" ) group . add_argument ( \"--spk-embed-dim\" , default = None , type = int , help = \"Number of speaker embedding dimensions\" ) group . add_argument ( \"--spk-embed-integration-type\" , type = str , default = \"add\" , choices = [ \"add\" , \"concat\" ], help = \"How to integrate speaker embedding\" ) # training related group . add_argument ( \"--transformer-init\" , type = str , default = \"pytorch\" , choices = [ \"pytorch\" , \"xavier_uniform\" , \"xavier_normal\" , \"kaiming_uniform\" , \"kaiming_normal\" ], help = \"How to initialize transformer parameters\" ) group . add_argument ( \"--initial-encoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in encoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--initial-decoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in decoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--transformer-lr\" , default = 1.0 , type = float , help = \"Initial value of learning rate\" ) group . add_argument ( \"--transformer-warmup-steps\" , default = 4000 , type = int , help = \"Optimizer warmup steps\" ) group . add_argument ( \"--transformer-enc-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder except for attention\" ) group . add_argument ( \"--transformer-enc-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder positional encoding\" ) group . add_argument ( \"--transformer-enc-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder self-attention\" ) group . add_argument ( \"--transformer-dec-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder except for attention and pos encoding\" ) group . add_argument ( \"--transformer-dec-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder positional encoding\" ) group . add_argument ( \"--transformer-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder self-attention\" ) group . add_argument ( \"--transformer-enc-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder-decoder attention\" ) group . add_argument ( \"--duration-predictor-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for duration predictor\" ) group . add_argument ( \"--postnet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in postnet\" ) group . add_argument ( \"--transfer-encoder-from-teacher\" , default = True , type = strtobool , help = \"Whether to transfer teacher's parameters\" ) group . add_argument ( \"--transferred-encoder-module\" , default = \"all\" , type = str , choices = [ \"all\" , \"embed\" ], help = \"Encoder modeules to be trasferred from teacher\" ) # loss related group . add_argument ( \"--use-masking\" , default = True , type = strtobool , help = \"Whether to use masking in calculation of loss\" ) group . add_argument ( \"--use-weighted-masking\" , default = False , type = strtobool , help = \"Whether to use weighted masking in calculation of loss\" ) return parser calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ) Calculate all of the attention weights. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None extras Tensor Batch of precalculated durations (B, Tmax, 1). None Returns: Type Description dict Dict of attention weights and outputs. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ): \"\"\"Calculate all of the attention weights. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). extras (Tensor, optional): Batch of precalculated durations (B, Tmax, 1). Returns: dict: Dict of attention weights and outputs. \"\"\" with torch . no_grad (): # remove unnecessary padded part (for multi-gpus) xs = xs [:, : max ( ilens )] ys = ys [:, : max ( olens )] if extras is not None : extras = extras [:, : max ( ilens )] . squeeze ( - 1 ) # forward propagation outs = self . _forward ( xs , ilens , ys , olens , spembs = spembs , ds = extras , is_inference = False )[ 1 ] att_ws_dict = dict () for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): attn = m . attn . cpu () . numpy () if \"encoder\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , ilens . tolist ())] elif \"decoder\" in name : if \"src\" in name : attn = [ a [:, : ol , : il ] for a , il , ol in zip ( attn , ilens . tolist (), olens . tolist ())] elif \"self\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , olens . tolist ())] else : logging . warning ( \"unknown attention module: \" + name ) else : logging . warning ( \"unknown attention module: \" + name ) att_ws_dict [ name ] = attn att_ws_dict [ \"predicted_fbank\" ] = [ m [: l ] . T for m , l in zip ( outs . cpu () . numpy (), olens . tolist ())] return att_ws_dict forward ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None extras Tensor Batch of precalculated durations (B, Tmax, 1). None Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def forward ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). extras (Tensor, optional): Batch of precalculated durations (B, Tmax, 1). Returns: Tensor: Loss value. \"\"\" # remove unnecessary padded part (for multi-gpus) xs = xs [:, : max ( ilens )] ys = ys [:, : max ( olens )] if extras is not None : extras = extras [:, : max ( ilens )] . squeeze ( - 1 ) # forward propagation before_outs , after_outs , ds , d_outs = self . _forward ( xs , ilens , ys , olens , spembs = spembs , ds = extras , is_inference = False ) # modifiy mod part of groundtruth if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) max_olen = max ( olens ) ys = ys [:, : max_olen ] # calculate loss if self . postnet is None : l1_loss , duration_loss = self . criterion ( None , before_outs , d_outs , ys , ds , ilens , olens ) else : l1_loss , duration_loss = self . criterion ( after_outs , before_outs , d_outs , ys , ds , ilens , olens ) loss = l1_loss + duration_loss report_keys = [ { \"l1_loss\" : l1_loss . item ()}, { \"duration_loss\" : duration_loss . item ()}, { \"loss\" : loss . item ()}, ] # report extra information if self . use_scaled_pos_enc : report_keys += [ { \"encoder_alpha\" : self . encoder . embed [ - 1 ] . alpha . data . item ()}, { \"decoder_alpha\" : self . decoder . embed [ - 1 ] . alpha . data . item ()}, ] self . reporter . report ( report_keys ) return loss inference ( self , x , inference_args , spemb = None , * args , ** kwargs ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default x Tensor Input sequence of characters (T,). required inference_args Namespace Dummy for compatibility. required spemb Tensor Speaker embedding vector (spk_embed_dim). None Returns: Type Description Tensor Output sequence of features (L, odim). None: Dummy for compatibility. None: Dummy for compatibility. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 def inference ( self , x , inference_args , spemb = None , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Args: x (Tensor): Input sequence of characters (T,). inference_args (Namespace): Dummy for compatibility. spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim). Returns: Tensor: Output sequence of features (L, odim). None: Dummy for compatibility. None: Dummy for compatibility. \"\"\" # setup batch axis ilens = torch . tensor ([ x . shape [ 0 ]], dtype = torch . long , device = x . device ) xs = x . unsqueeze ( 0 ) if spemb is not None : spembs = spemb . unsqueeze ( 0 ) else : spembs = None # inference _ , outs , _ = self . _forward ( xs , ilens , spembs = spembs , is_inference = True ) # (1, L, odim) return outs [ 0 ], None , None FeedForwardTransformerLoss \u00b6 Loss function module for feed-forward Transformer. __init__ ( self , use_masking = True , use_weighted_masking = False ) special Initialize feed-forward Transformer loss module. Parameters: Name Type Description Default use_masking bool Whether to apply masking for padded part in loss calculation. True use_weighted_masking bool Whether to weighted masking in loss calculation. False Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , use_masking = True , use_weighted_masking = False ): \"\"\"Initialize feed-forward Transformer loss module. Args: use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to weighted masking in loss calculation. \"\"\" super ( FeedForwardTransformerLoss , self ) . __init__ () assert ( use_masking != use_weighted_masking ) or not use_masking self . use_masking = use_masking self . use_weighted_masking = use_weighted_masking # define criterions reduction = \"none\" if self . use_weighted_masking else \"mean\" self . l1_criterion = torch . nn . L1Loss ( reduction = reduction ) self . duration_criterion = DurationPredictorLoss ( reduction = reduction ) forward ( self , after_outs , before_outs , d_outs , ys , ds , ilens , olens ) Calculate forward propagation. Parameters: Name Type Description Default after_outs Tensor Batch of outputs after postnets (B, Lmax, odim). required before_outs Tensor Batch of outputs before postnets (B, Lmax, odim). required d_outs Tensor Batch of outputs of duration predictor (B, Tmax). required ys Tensor Batch of target features (B, Lmax, odim). required ds Tensor Batch of durations (B, Tmax). required ilens LongTensor Batch of the lengths of each input (B,). required olens LongTensor Batch of the lengths of each target (B,). required Returns: Type Description Tensor L1 loss value. Tensor: Duration predictor loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , after_outs , before_outs , d_outs , ys , ds , ilens , olens ): \"\"\"Calculate forward propagation. Args: after_outs (Tensor): Batch of outputs after postnets (B, Lmax, odim). before_outs (Tensor): Batch of outputs before postnets (B, Lmax, odim). d_outs (Tensor): Batch of outputs of duration predictor (B, Tmax). ys (Tensor): Batch of target features (B, Lmax, odim). ds (Tensor): Batch of durations (B, Tmax). ilens (LongTensor): Batch of the lengths of each input (B,). olens (LongTensor): Batch of the lengths of each target (B,). Returns: Tensor: L1 loss value. Tensor: Duration predictor loss value. \"\"\" # apply mask to remove padded part if self . use_masking : duration_masks = make_non_pad_mask ( ilens ) . to ( ys . device ) d_outs = d_outs . masked_select ( duration_masks ) ds = ds . masked_select ( duration_masks ) out_masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) before_outs = before_outs . masked_select ( out_masks ) after_outs = after_outs . masked_select ( out_masks ) if after_outs is not None else None ys = ys . masked_select ( out_masks ) # calculate loss l1_loss = self . l1_criterion ( before_outs , ys ) if after_outs is not None : l1_loss += self . l1_criterion ( after_outs , ys ) duration_loss = self . duration_criterion ( d_outs , ds ) # make weighted mask and apply it if self . use_weighted_masking : out_masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) out_weights = out_masks . float () / out_masks . sum ( dim = 1 , keepdim = True ) . float () out_weights /= ys . size ( 0 ) * ys . size ( 2 ) duration_masks = make_non_pad_mask ( ilens ) . to ( ys . device ) duration_weights = duration_masks . float () / duration_masks . sum ( dim = 1 , keepdim = True ) . float () duration_weights /= ds . size ( 0 ) # apply weight l1_loss = l1_loss . mul ( out_weights ) . masked_select ( out_masks ) . sum () duration_loss = duration_loss . mul ( duration_weights ) . masked_select ( duration_masks ) . sum () return l1_loss , duration_loss e2e_tts_tacotron2 \u00b6 Tacotron 2 related modules. GuidedAttentionLoss \u00b6 Guided attention loss function module. This module calculates the guided attention loss described in ` Efficiently Trainable Text - to - Speech System Based on Deep Convolutional Networks with Guided Attention ` _ , which forces the attention to be diagonal . .. _ ` Efficiently Trainable Text - to - Speech System Based on Deep Convolutional Networks with Guided Attention ` : https : // arxiv . org / abs / 1710 . 08969 __init__ ( self , sigma = 0.4 , alpha = 1.0 , reset_always = True ) special Initialize guided attention loss module. Parameters: Name Type Description Default sigma float Standard deviation to control how close attention to a diagonal. 0.4 alpha float Scaling coefficient (lambda). 1.0 reset_always bool Whether to always reset masks. True Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , sigma = 0.4 , alpha = 1.0 , reset_always = True ): \"\"\"Initialize guided attention loss module. Args: sigma (float, optional): Standard deviation to control how close attention to a diagonal. alpha (float, optional): Scaling coefficient (lambda). reset_always (bool, optional): Whether to always reset masks. \"\"\" super ( GuidedAttentionLoss , self ) . __init__ () self . sigma = sigma self . alpha = alpha self . reset_always = reset_always self . guided_attn_masks = None self . masks = None forward ( self , att_ws , ilens , olens ) Calculate forward propagation. Parameters: Name Type Description Default att_ws Tensor Batch of attention weights (B, T_max_out, T_max_in). required ilens LongTensor Batch of input lenghts (B,). required olens LongTensor Batch of output lenghts (B,). required Returns: Type Description Tensor Guided attention loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def forward ( self , att_ws , ilens , olens ): \"\"\"Calculate forward propagation. Args: att_ws (Tensor): Batch of attention weights (B, T_max_out, T_max_in). ilens (LongTensor): Batch of input lenghts (B,). olens (LongTensor): Batch of output lenghts (B,). Returns: Tensor: Guided attention loss value. \"\"\" if self . guided_attn_masks is None : self . guided_attn_masks = self . _make_guided_attention_masks ( ilens , olens ) . to ( att_ws . device ) if self . masks is None : self . masks = self . _make_masks ( ilens , olens ) . to ( att_ws . device ) losses = self . guided_attn_masks * att_ws loss = torch . mean ( losses . masked_select ( self . masks )) if self . reset_always : self . _reset_masks () return self . alpha * loss Tacotron2 \u00b6 Tacotron2 module for end-to-end text-to-speech (E2E-TTS). This is a module of Spectrogram prediction network in Tacotron2 described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ , which converts the sequence of characters into the sequence of Mel - filterbanks . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 base_plot_keys property readonly Return base key names to plot during training. keys should match what chainer.reporter reports. If you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list List of strings which are base keys to plot during training. __init__ ( self , idim , odim , args = None ) special Initialize Tacotron2 module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required args Namespace spk_embed_dim (int): Dimension of the speaker embedding. embed_dim (int): Dimension of character embedding. elayers (int): The number of encoder blstm layers. eunits (int): The number of encoder blstm units. econv_layers (int): The number of encoder conv layers. econv_filts (int): The number of encoder conv filter size. econv_chans (int): The number of encoder conv filter channels. dlayers (int): The number of decoder lstm layers. dunits (int): The number of decoder lstm units. prenet_layers (int): The number of prenet layers. prenet_units (int): The number of prenet units. postnet_layers (int): The number of postnet layers. postnet_filts (int): The number of postnet filter size. postnet_chans (int): The number of postnet filter channels. output_activation (int): The name of activation function for outputs. adim (int): The number of dimension of mlp in attention. aconv_chans (int): The number of attention conv filter channels. aconv_filts (int): The number of attention conv filter size. cumulate_att_w (bool): Whether to cumulate previous attention weight. use_batch_norm (bool): Whether to use batch normalization. use_concate (int): Whether to concatenate encoder embedding with decoder lstm outputs. dropout_rate (float): Dropout rate. zoneout_rate (float): Zoneout rate. reduction_factor (int): Reduction factor. spk_embed_dim (int): Number of speaker embedding dimenstions. spc_dim (int): Number of spectrogram embedding dimenstions (only for use_cbhg=True). use_cbhg (bool): Whether to use CBHG module. cbhg_conv_bank_layers (int): The number of convoluional banks in CBHG. cbhg_conv_bank_chans (int): The number of channels of convolutional bank in CBHG. cbhg_proj_filts (int): The number of filter size of projection layeri in CBHG. cbhg_proj_chans (int): The number of channels of projection layer in CBHG. cbhg_highway_layers (int): The number of layers of highway network in CBHG. cbhg_highway_units (int): The number of units of highway network in CBHG. cbhg_gru_units (int): The number of units of GRU in CBHG. use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. bce_pos_weight (float): Weight of positive sample of stop token (only for use_masking=True). use-guided-attn-loss (bool): Whether to use guided attention loss. guided-attn-loss-sigma (float) Sigma in guided attention loss. guided-attn-loss-lamdba (float): Lambda in guided attention loss. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def __init__ ( self , idim , odim , args = None ): \"\"\"Initialize Tacotron2 module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. args (Namespace, optional): - spk_embed_dim (int): Dimension of the speaker embedding. - embed_dim (int): Dimension of character embedding. - elayers (int): The number of encoder blstm layers. - eunits (int): The number of encoder blstm units. - econv_layers (int): The number of encoder conv layers. - econv_filts (int): The number of encoder conv filter size. - econv_chans (int): The number of encoder conv filter channels. - dlayers (int): The number of decoder lstm layers. - dunits (int): The number of decoder lstm units. - prenet_layers (int): The number of prenet layers. - prenet_units (int): The number of prenet units. - postnet_layers (int): The number of postnet layers. - postnet_filts (int): The number of postnet filter size. - postnet_chans (int): The number of postnet filter channels. - output_activation (int): The name of activation function for outputs. - adim (int): The number of dimension of mlp in attention. - aconv_chans (int): The number of attention conv filter channels. - aconv_filts (int): The number of attention conv filter size. - cumulate_att_w (bool): Whether to cumulate previous attention weight. - use_batch_norm (bool): Whether to use batch normalization. - use_concate (int): Whether to concatenate encoder embedding with decoder lstm outputs. - dropout_rate (float): Dropout rate. - zoneout_rate (float): Zoneout rate. - reduction_factor (int): Reduction factor. - spk_embed_dim (int): Number of speaker embedding dimenstions. - spc_dim (int): Number of spectrogram embedding dimenstions (only for use_cbhg=True). - use_cbhg (bool): Whether to use CBHG module. - cbhg_conv_bank_layers (int): The number of convoluional banks in CBHG. - cbhg_conv_bank_chans (int): The number of channels of convolutional bank in CBHG. - cbhg_proj_filts (int): The number of filter size of projection layeri in CBHG. - cbhg_proj_chans (int): The number of channels of projection layer in CBHG. - cbhg_highway_layers (int): The number of layers of highway network in CBHG. - cbhg_highway_units (int): The number of units of highway network in CBHG. - cbhg_gru_units (int): The number of units of GRU in CBHG. - use_masking (bool): Whether to apply masking for padded part in loss calculation. - use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. - bce_pos_weight (float): Weight of positive sample of stop token (only for use_masking=True). - use-guided-attn-loss (bool): Whether to use guided attention loss. - guided-attn-loss-sigma (float) Sigma in guided attention loss. - guided-attn-loss-lamdba (float): Lambda in guided attention loss. \"\"\" # initialize base classes TTSInterface . __init__ ( self ) torch . nn . Module . __init__ ( self ) # fill missing arguments args = fill_missing_args ( args , self . add_arguments ) # store hyperparameters self . idim = idim self . odim = odim self . spk_embed_dim = args . spk_embed_dim self . cumulate_att_w = args . cumulate_att_w self . reduction_factor = args . reduction_factor self . use_cbhg = args . use_cbhg self . use_guided_attn_loss = args . use_guided_attn_loss # define activation function for the final output if args . output_activation is None : self . output_activation_fn = None elif hasattr ( F , args . output_activation ): self . output_activation_fn = getattr ( F , args . output_activation ) else : raise ValueError ( 'there is no such an activation function. ( %s )' % args . output_activation ) # set padding idx padding_idx = 0 # define network modules self . enc = Encoder ( idim = idim , embed_dim = args . embed_dim , elayers = args . elayers , eunits = args . eunits , econv_layers = args . econv_layers , econv_chans = args . econv_chans , econv_filts = args . econv_filts , use_batch_norm = args . use_batch_norm , use_residual = args . use_residual , dropout_rate = args . dropout_rate , padding_idx = padding_idx ) dec_idim = args . eunits if args . spk_embed_dim is None else args . eunits + args . spk_embed_dim if args . atype == \"location\" : att = AttLoc ( dec_idim , args . dunits , args . adim , args . aconv_chans , args . aconv_filts ) elif args . atype == \"forward\" : att = AttForward ( dec_idim , args . dunits , args . adim , args . aconv_chans , args . aconv_filts ) if self . cumulate_att_w : logging . warning ( \"cumulation of attention weights is disabled in forward attention.\" ) self . cumulate_att_w = False elif args . atype == \"forward_ta\" : att = AttForwardTA ( dec_idim , args . dunits , args . adim , args . aconv_chans , args . aconv_filts , odim ) if self . cumulate_att_w : logging . warning ( \"cumulation of attention weights is disabled in forward attention.\" ) self . cumulate_att_w = False else : raise NotImplementedError ( \"Support only location or forward\" ) self . dec = Decoder ( idim = dec_idim , odim = odim , att = att , dlayers = args . dlayers , dunits = args . dunits , prenet_layers = args . prenet_layers , prenet_units = args . prenet_units , postnet_layers = args . postnet_layers , postnet_chans = args . postnet_chans , postnet_filts = args . postnet_filts , output_activation_fn = self . output_activation_fn , cumulate_att_w = self . cumulate_att_w , use_batch_norm = args . use_batch_norm , use_concate = args . use_concate , dropout_rate = args . dropout_rate , zoneout_rate = args . zoneout_rate , reduction_factor = args . reduction_factor ) self . taco2_loss = Tacotron2Loss ( use_masking = args . use_masking , use_weighted_masking = args . use_weighted_masking , bce_pos_weight = args . bce_pos_weight ) if self . use_guided_attn_loss : self . attn_loss = GuidedAttentionLoss ( sigma = args . guided_attn_loss_sigma , alpha = args . guided_attn_loss_lambda , ) if self . use_cbhg : self . cbhg = CBHG ( idim = odim , odim = args . spc_dim , conv_bank_layers = args . cbhg_conv_bank_layers , conv_bank_chans = args . cbhg_conv_bank_chans , conv_proj_filts = args . cbhg_conv_proj_filts , conv_proj_chans = args . cbhg_conv_proj_chans , highway_layers = args . cbhg_highway_layers , highway_units = args . cbhg_highway_units , gru_units = args . cbhg_gru_units ) self . cbhg_loss = CBHGLoss ( use_masking = args . use_masking ) # load pretrained model if args . pretrained_model is not None : self . load_pretrained_model ( args . pretrained_model ) add_arguments ( parser ) staticmethod Add model-specific arguments to the parser. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 @staticmethod def add_arguments ( parser ): \"\"\"Add model-specific arguments to the parser.\"\"\" group = parser . add_argument_group ( \"tacotron 2 model setting\" ) # encoder group . add_argument ( '--embed-dim' , default = 512 , type = int , help = 'Number of dimension of embedding' ) group . add_argument ( '--elayers' , default = 1 , type = int , help = 'Number of encoder layers' ) group . add_argument ( '--eunits' , '-u' , default = 512 , type = int , help = 'Number of encoder hidden units' ) group . add_argument ( '--econv-layers' , default = 3 , type = int , help = 'Number of encoder convolution layers' ) group . add_argument ( '--econv-chans' , default = 512 , type = int , help = 'Number of encoder convolution channels' ) group . add_argument ( '--econv-filts' , default = 5 , type = int , help = 'Filter size of encoder convolution' ) # attention group . add_argument ( '--atype' , default = \"location\" , type = str , choices = [ \"forward_ta\" , \"forward\" , \"location\" ], help = 'Type of attention mechanism' ) group . add_argument ( '--adim' , default = 512 , type = int , help = 'Number of attention transformation dimensions' ) group . add_argument ( '--aconv-chans' , default = 32 , type = int , help = 'Number of attention convolution channels' ) group . add_argument ( '--aconv-filts' , default = 15 , type = int , help = 'Filter size of attention convolution' ) group . add_argument ( '--cumulate-att-w' , default = True , type = strtobool , help = \"Whether or not to cumulate attention weights\" ) # decoder group . add_argument ( '--dlayers' , default = 2 , type = int , help = 'Number of decoder layers' ) group . add_argument ( '--dunits' , default = 1024 , type = int , help = 'Number of decoder hidden units' ) group . add_argument ( '--prenet-layers' , default = 2 , type = int , help = 'Number of prenet layers' ) group . add_argument ( '--prenet-units' , default = 256 , type = int , help = 'Number of prenet hidden units' ) group . add_argument ( '--postnet-layers' , default = 5 , type = int , help = 'Number of postnet layers' ) group . add_argument ( '--postnet-chans' , default = 512 , type = int , help = 'Number of postnet channels' ) group . add_argument ( '--postnet-filts' , default = 5 , type = int , help = 'Filter size of postnet' ) group . add_argument ( '--output-activation' , default = None , type = str , nargs = '?' , help = 'Output activation function' ) # cbhg group . add_argument ( '--use-cbhg' , default = False , type = strtobool , help = 'Whether to use CBHG module' ) group . add_argument ( '--cbhg-conv-bank-layers' , default = 8 , type = int , help = 'Number of convoluional bank layers in CBHG' ) group . add_argument ( '--cbhg-conv-bank-chans' , default = 128 , type = int , help = 'Number of convoluional bank channles in CBHG' ) group . add_argument ( '--cbhg-conv-proj-filts' , default = 3 , type = int , help = 'Filter size of convoluional projection layer in CBHG' ) group . add_argument ( '--cbhg-conv-proj-chans' , default = 256 , type = int , help = 'Number of convoluional projection channels in CBHG' ) group . add_argument ( '--cbhg-highway-layers' , default = 4 , type = int , help = 'Number of highway layers in CBHG' ) group . add_argument ( '--cbhg-highway-units' , default = 128 , type = int , help = 'Number of highway units in CBHG' ) group . add_argument ( '--cbhg-gru-units' , default = 256 , type = int , help = 'Number of GRU units in CBHG' ) # model (parameter) related group . add_argument ( '--use-batch-norm' , default = True , type = strtobool , help = 'Whether to use batch normalization' ) group . add_argument ( '--use-concate' , default = True , type = strtobool , help = 'Whether to concatenate encoder embedding with decoder outputs' ) group . add_argument ( '--use-residual' , default = True , type = strtobool , help = 'Whether to use residual connection in conv layer' ) group . add_argument ( '--dropout-rate' , default = 0.5 , type = float , help = 'Dropout rate' ) group . add_argument ( '--zoneout-rate' , default = 0.1 , type = float , help = 'Zoneout rate' ) group . add_argument ( '--reduction-factor' , default = 1 , type = int , help = 'Reduction factor' ) group . add_argument ( \"--spk-embed-dim\" , default = None , type = int , help = \"Number of speaker embedding dimensions\" ) group . add_argument ( \"--spc-dim\" , default = None , type = int , help = \"Number of spectrogram dimensions\" ) group . add_argument ( \"--pretrained-model\" , default = None , type = str , help = \"Pretrained model path\" ) # loss related group . add_argument ( '--use-masking' , default = False , type = strtobool , help = 'Whether to use masking in calculation of loss' ) group . add_argument ( '--use-weighted-masking' , default = False , type = strtobool , help = 'Whether to use weighted masking in calculation of loss' ) group . add_argument ( '--bce-pos-weight' , default = 20.0 , type = float , help = 'Positive sample weight in BCE calculation (only for use-masking=True)' ) group . add_argument ( \"--use-guided-attn-loss\" , default = False , type = strtobool , help = \"Whether to use guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-sigma\" , default = 0.4 , type = float , help = \"Sigma in guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-lambda\" , default = 1.0 , type = float , help = \"Lambda in guided attention loss\" ) return parser calculate_all_attentions ( self , xs , ilens , ys , spembs = None , keep_tensor = False , * args , ** kwargs ) Calculate all of the attention weights. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None keep_tensor bool Whether to keep original tensor. False Returns: Type Description Union[ndarray, Tensor] Batch of attention weights (B, Lmax, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 def calculate_all_attentions ( self , xs , ilens , ys , spembs = None , keep_tensor = False , * args , ** kwargs ): \"\"\"Calculate all of the attention weights. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). keep_tensor (bool, optional): Whether to keep original tensor. Returns: Union[ndarray, Tensor]: Batch of attention weights (B, Lmax, Tmax). \"\"\" # check ilens type (should be list of int) if isinstance ( ilens , torch . Tensor ) or isinstance ( ilens , np . ndarray ): ilens = list ( map ( int , ilens )) self . eval () with torch . no_grad (): hs , hlens = self . enc ( xs , ilens ) if self . spk_embed_dim is not None : spembs = F . normalize ( spembs ) . unsqueeze ( 1 ) . expand ( - 1 , hs . size ( 1 ), - 1 ) hs = torch . cat ([ hs , spembs ], dim =- 1 ) att_ws = self . dec . calculate_all_attentions ( hs , hlens , ys ) self . train () if keep_tensor : return att_ws else : return att_ws . cpu () . numpy () forward ( self , xs , ilens , ys , labels , olens , spembs = None , extras = None , * args , ** kwargs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None extras Tensor Batch of groundtruth spectrograms (B, Lmax, spc_dim). None Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def forward ( self , xs , ilens , ys , labels , olens , spembs = None , extras = None , * args , ** kwargs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). extras (Tensor, optional): Batch of groundtruth spectrograms (B, Lmax, spc_dim). Returns: Tensor: Loss value. \"\"\" # remove unnecessary padded part (for multi-gpus) max_in = max ( ilens ) max_out = max ( olens ) if max_in != xs . shape [ 1 ]: xs = xs [:, : max_in ] if max_out != ys . shape [ 1 ]: ys = ys [:, : max_out ] labels = labels [:, : max_out ] # calculate tacotron2 outputs hs , hlens = self . enc ( xs , ilens ) if self . spk_embed_dim is not None : spembs = F . normalize ( spembs ) . unsqueeze ( 1 ) . expand ( - 1 , hs . size ( 1 ), - 1 ) hs = torch . cat ([ hs , spembs ], dim =- 1 ) after_outs , before_outs , logits , att_ws = self . dec ( hs , hlens , ys ) # modifiy mod part of groundtruth if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) max_out = max ( olens ) ys = ys [:, : max_out ] labels = labels [:, : max_out ] labels [:, - 1 ] = 1.0 # make sure at least one frame has 1 # caluculate taco2 loss l1_loss , mse_loss , bce_loss = self . taco2_loss ( after_outs , before_outs , logits , ys , labels , olens ) loss = l1_loss + mse_loss + bce_loss report_keys = [ { 'l1_loss' : l1_loss . item ()}, { 'mse_loss' : mse_loss . item ()}, { 'bce_loss' : bce_loss . item ()}, ] # caluculate attention loss if self . use_guided_attn_loss : # NOTE(kan-bayashi): length of output for auto-regressive input will be changed when r > 1 if self . reduction_factor > 1 : olens_in = olens . new ([ olen // self . reduction_factor for olen in olens ]) else : olens_in = olens attn_loss = self . attn_loss ( att_ws , ilens , olens_in ) loss = loss + attn_loss report_keys += [ { 'attn_loss' : attn_loss . item ()}, ] # caluculate cbhg loss if self . use_cbhg : # remove unnecessary padded part (for multi-gpus) if max_out != extras . shape [ 1 ]: extras = extras [:, : max_out ] # caluculate cbhg outputs & loss and report them cbhg_outs , _ = self . cbhg ( after_outs , olens ) cbhg_l1_loss , cbhg_mse_loss = self . cbhg_loss ( cbhg_outs , extras , olens ) loss = loss + cbhg_l1_loss + cbhg_mse_loss report_keys += [ { 'cbhg_l1_loss' : cbhg_l1_loss . item ()}, { 'cbhg_mse_loss' : cbhg_mse_loss . item ()}, ] report_keys += [{ 'loss' : loss . item ()}] self . reporter . report ( report_keys ) return loss inference ( self , x , inference_args , spemb = None , * args , ** kwargs ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default x Tensor Input sequence of characters (T,). required inference_args Namespace threshold (float): Threshold in inference. minlenratio (float): Minimum length ratio in inference. maxlenratio (float): Maximum length ratio in inference. required spemb Tensor Speaker embedding vector (spk_embed_dim). None Returns: Type Description Tensor Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 def inference ( self , x , inference_args , spemb = None , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Args: x (Tensor): Input sequence of characters (T,). inference_args (Namespace): - threshold (float): Threshold in inference. - minlenratio (float): Minimum length ratio in inference. - maxlenratio (float): Maximum length ratio in inference. spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim). Returns: Tensor: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). \"\"\" # get options threshold = inference_args . threshold minlenratio = inference_args . minlenratio maxlenratio = inference_args . maxlenratio use_att_constraint = getattr ( inference_args , \"use_att_constraint\" , False ) # keep compatibility backward_window = inference_args . backward_window if use_att_constraint else 0 forward_window = inference_args . forward_window if use_att_constraint else 0 # inference h = self . enc . inference ( x ) if self . spk_embed_dim is not None : spemb = F . normalize ( spemb , dim = 0 ) . unsqueeze ( 0 ) . expand ( h . size ( 0 ), - 1 ) h = torch . cat ([ h , spemb ], dim =- 1 ) outs , probs , att_ws = self . dec . inference ( h , threshold , minlenratio , maxlenratio , use_att_constraint = use_att_constraint , backward_window = backward_window , forward_window = forward_window ) if self . use_cbhg : cbhg_outs = self . cbhg . inference ( outs ) return cbhg_outs , probs , att_ws else : return outs , probs , att_ws Tacotron2Loss \u00b6 Loss function module for Tacotron2. __init__ ( self , use_masking = True , use_weighted_masking = False , bce_pos_weight = 20.0 ) special Initialize Tactoron2 loss module. Parameters: Name Type Description Default use_masking bool Whether to apply masking for padded part in loss calculation. True use_weighted_masking bool Whether to apply weighted masking in loss calculation. False bce_pos_weight float Weight of positive sample of stop token. 20.0 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __init__ ( self , use_masking = True , use_weighted_masking = False , bce_pos_weight = 20.0 ): \"\"\"Initialize Tactoron2 loss module. Args: use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. bce_pos_weight (float): Weight of positive sample of stop token. \"\"\" super ( Tacotron2Loss , self ) . __init__ () assert ( use_masking != use_weighted_masking ) or not use_masking self . use_masking = use_masking self . use_weighted_masking = use_weighted_masking # define criterions reduction = \"none\" if self . use_weighted_masking else \"mean\" self . l1_criterion = torch . nn . L1Loss ( reduction = reduction ) self . mse_criterion = torch . nn . MSELoss ( reduction = reduction ) self . bce_criterion = torch . nn . BCEWithLogitsLoss ( reduction = reduction , pos_weight = torch . tensor ( bce_pos_weight )) # NOTE(kan-bayashi): register pre hook function for the compatibility self . _register_load_state_dict_pre_hook ( self . _load_state_dict_pre_hook ) forward ( self , after_outs , before_outs , logits , ys , labels , olens ) Calculate forward propagation. Parameters: Name Type Description Default after_outs Tensor Batch of outputs after postnets (B, Lmax, odim). required before_outs Tensor Batch of outputs before postnets (B, Lmax, odim). required logits Tensor Batch of stop logits (B, Lmax). required ys Tensor Batch of padded target features (B, Lmax, odim). required labels LongTensor Batch of the sequences of stop token labels (B, Lmax). required olens LongTensor Batch of the lengths of each target (B,). required Returns: Type Description Tensor L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def forward ( self , after_outs , before_outs , logits , ys , labels , olens ): \"\"\"Calculate forward propagation. Args: after_outs (Tensor): Batch of outputs after postnets (B, Lmax, odim). before_outs (Tensor): Batch of outputs before postnets (B, Lmax, odim). logits (Tensor): Batch of stop logits (B, Lmax). ys (Tensor): Batch of padded target features (B, Lmax, odim). labels (LongTensor): Batch of the sequences of stop token labels (B, Lmax). olens (LongTensor): Batch of the lengths of each target (B,). Returns: Tensor: L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value. \"\"\" # make mask and apply it if self . use_masking : masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) ys = ys . masked_select ( masks ) after_outs = after_outs . masked_select ( masks ) before_outs = before_outs . masked_select ( masks ) labels = labels . masked_select ( masks [:, :, 0 ]) logits = logits . masked_select ( masks [:, :, 0 ]) # calculate loss l1_loss = self . l1_criterion ( after_outs , ys ) + self . l1_criterion ( before_outs , ys ) mse_loss = self . mse_criterion ( after_outs , ys ) + self . mse_criterion ( before_outs , ys ) bce_loss = self . bce_criterion ( logits , labels ) # make weighted mask and apply it if self . use_weighted_masking : masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) weights = masks . float () / masks . sum ( dim = 1 , keepdim = True ) . float () out_weights = weights . div ( ys . size ( 0 ) * ys . size ( 2 )) logit_weights = weights . div ( ys . size ( 0 )) # apply weight l1_loss = l1_loss . mul ( out_weights ) . masked_select ( masks ) . sum () mse_loss = mse_loss . mul ( out_weights ) . masked_select ( masks ) . sum () bce_loss = bce_loss . mul ( logit_weights . squeeze ( - 1 )) . masked_select ( masks . squeeze ( - 1 )) . sum () return l1_loss , mse_loss , bce_loss e2e_tts_transformer \u00b6 TTS-Transformer related modules. GuidedMultiHeadAttentionLoss \u00b6 Guided attention loss function module for multi head attention. !!! args sigma (float, optional): Standard deviation to control how close attention to a diagonal. alpha (float, optional): Scaling coefficient (lambda). reset_always (bool, optional): Whether to always reset masks. forward ( self , att_ws , ilens , olens ) Calculate forward propagation. Parameters: Name Type Description Default att_ws Tensor Batch of multi head attention weights (B, H, T_max_out, T_max_in). required ilens LongTensor Batch of input lenghts (B,). required olens LongTensor Batch of output lenghts (B,). required Returns: Type Description Tensor Guided attention loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def forward ( self , att_ws , ilens , olens ): \"\"\"Calculate forward propagation. Args: att_ws (Tensor): Batch of multi head attention weights (B, H, T_max_out, T_max_in). ilens (LongTensor): Batch of input lenghts (B,). olens (LongTensor): Batch of output lenghts (B,). Returns: Tensor: Guided attention loss value. \"\"\" if self . guided_attn_masks is None : self . guided_attn_masks = self . _make_guided_attention_masks ( ilens , olens ) . to ( att_ws . device ) . unsqueeze ( 1 ) if self . masks is None : self . masks = self . _make_masks ( ilens , olens ) . to ( att_ws . device ) . unsqueeze ( 1 ) losses = self . guided_attn_masks * att_ws loss = torch . mean ( losses . masked_select ( self . masks )) if self . reset_always : self . _reset_masks () return self . alpha * loss Transformer \u00b6 Text-to-Speech Transformer module. This is a module of text - to - speech Transformer described in ` Neural Speech Synthesis with Transformer Network ` _ , which convert the sequence of characters or phonemes into the sequence of Mel - filterbanks . .. _ ` Neural Speech Synthesis with Transformer Network ` : https : // arxiv . org / pdf / 1809 . 08895 . pdf attention_plot_class property readonly Return plot class for attention weight plot. base_plot_keys property readonly Return base key names to plot during training. keys should match what chainer.reporter reports. If you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list List of strings which are base keys to plot during training. __init__ ( self , idim , odim , args = None ) special Initialize TTS-Transformer module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required args Namespace embed_dim (int): Dimension of character embedding. eprenet_conv_layers (int): Number of encoder prenet convolution layers. eprenet_conv_chans (int): Number of encoder prenet convolution channels. eprenet_conv_filts (int): Filter size of encoder prenet convolution. dprenet_layers (int): Number of decoder prenet layers. dprenet_units (int): Number of decoder prenet hidden units. elayers (int): Number of encoder layers. eunits (int): Number of encoder hidden units. adim (int): Number of attention transformation dimensions. aheads (int): Number of heads for multi head attention. dlayers (int): Number of decoder layers. dunits (int): Number of decoder hidden units. postnet_layers (int): Number of postnet layers. postnet_chans (int): Number of postnet channels. postnet_filts (int): Filter size of postnet. use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. use_batch_norm (bool): Whether to use batch normalization in encoder prenet. encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. reduction_factor (int): Reduction factor. spk_embed_dim (int): Number of speaker embedding dimenstions. spk_embed_integration_type: How to integrate speaker embedding. transformer_init (float): How to initialize transformer parameters. transformer_lr (float): Initial value of learning rate. transformer_warmup_steps (int): Optimizer warmup steps. transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. eprenet_dropout_rate (float): Dropout rate in encoder prenet. dprenet_dropout_rate (float): Dropout rate in decoder prenet. postnet_dropout_rate (float): Dropout rate in postnet. use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. bce_pos_weight (float): Positive sample weight in bce calculation (only for use_masking=true). loss_type (str): How to calculate loss. use_guided_attn_loss (bool): Whether to use guided attention loss. num_heads_applied_guided_attn (int): Number of heads in each layer to apply guided attention loss. num_layers_applied_guided_attn (int): Number of layers to apply guided attention loss. modules_applied_guided_attn (list): List of module names to apply guided attention loss. guided-attn-loss-sigma (float) Sigma in guided attention loss. guided-attn-loss-lambda (float): Lambda in guided attention loss. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def __init__ ( self , idim , odim , args = None ): \"\"\"Initialize TTS-Transformer module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. args (Namespace, optional): - embed_dim (int): Dimension of character embedding. - eprenet_conv_layers (int): Number of encoder prenet convolution layers. - eprenet_conv_chans (int): Number of encoder prenet convolution channels. - eprenet_conv_filts (int): Filter size of encoder prenet convolution. - dprenet_layers (int): Number of decoder prenet layers. - dprenet_units (int): Number of decoder prenet hidden units. - elayers (int): Number of encoder layers. - eunits (int): Number of encoder hidden units. - adim (int): Number of attention transformation dimensions. - aheads (int): Number of heads for multi head attention. - dlayers (int): Number of decoder layers. - dunits (int): Number of decoder hidden units. - postnet_layers (int): Number of postnet layers. - postnet_chans (int): Number of postnet channels. - postnet_filts (int): Filter size of postnet. - use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. - use_batch_norm (bool): Whether to use batch normalization in encoder prenet. - encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. - decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. - encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. - decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. - reduction_factor (int): Reduction factor. - spk_embed_dim (int): Number of speaker embedding dimenstions. - spk_embed_integration_type: How to integrate speaker embedding. - transformer_init (float): How to initialize transformer parameters. - transformer_lr (float): Initial value of learning rate. - transformer_warmup_steps (int): Optimizer warmup steps. - transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. - transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. - transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. - transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. - transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. - transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. - transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. - eprenet_dropout_rate (float): Dropout rate in encoder prenet. - dprenet_dropout_rate (float): Dropout rate in decoder prenet. - postnet_dropout_rate (float): Dropout rate in postnet. - use_masking (bool): Whether to apply masking for padded part in loss calculation. - use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. - bce_pos_weight (float): Positive sample weight in bce calculation (only for use_masking=true). - loss_type (str): How to calculate loss. - use_guided_attn_loss (bool): Whether to use guided attention loss. - num_heads_applied_guided_attn (int): Number of heads in each layer to apply guided attention loss. - num_layers_applied_guided_attn (int): Number of layers to apply guided attention loss. - modules_applied_guided_attn (list): List of module names to apply guided attention loss. - guided-attn-loss-sigma (float) Sigma in guided attention loss. - guided-attn-loss-lambda (float): Lambda in guided attention loss. \"\"\" # initialize base classes TTSInterface . __init__ ( self ) torch . nn . Module . __init__ ( self ) # fill missing arguments args = fill_missing_args ( args , self . add_arguments ) # store hyperparameters self . idim = idim self . odim = odim self . spk_embed_dim = args . spk_embed_dim if self . spk_embed_dim is not None : self . spk_embed_integration_type = args . spk_embed_integration_type self . use_scaled_pos_enc = args . use_scaled_pos_enc self . reduction_factor = args . reduction_factor self . loss_type = args . loss_type self . use_guided_attn_loss = args . use_guided_attn_loss if self . use_guided_attn_loss : if args . num_layers_applied_guided_attn == - 1 : self . num_layers_applied_guided_attn = args . elayers else : self . num_layers_applied_guided_attn = args . num_layers_applied_guided_attn if args . num_heads_applied_guided_attn == - 1 : self . num_heads_applied_guided_attn = args . aheads else : self . num_heads_applied_guided_attn = args . num_heads_applied_guided_attn self . modules_applied_guided_attn = args . modules_applied_guided_attn # use idx 0 as padding idx padding_idx = 0 # get positional encoding class pos_enc_class = ScaledPositionalEncoding if self . use_scaled_pos_enc else PositionalEncoding # define transformer encoder if args . eprenet_conv_layers != 0 : # encoder prenet encoder_input_layer = torch . nn . Sequential ( EncoderPrenet ( idim = idim , embed_dim = args . embed_dim , elayers = 0 , econv_layers = args . eprenet_conv_layers , econv_chans = args . eprenet_conv_chans , econv_filts = args . eprenet_conv_filts , use_batch_norm = args . use_batch_norm , dropout_rate = args . eprenet_dropout_rate , padding_idx = padding_idx ), torch . nn . Linear ( args . eprenet_conv_chans , args . adim ) ) else : encoder_input_layer = torch . nn . Embedding ( num_embeddings = idim , embedding_dim = args . adim , padding_idx = padding_idx ) self . encoder = Encoder ( idim = idim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . eunits , num_blocks = args . elayers , input_layer = encoder_input_layer , dropout_rate = args . transformer_enc_dropout_rate , positional_dropout_rate = args . transformer_enc_positional_dropout_rate , attention_dropout_rate = args . transformer_enc_attn_dropout_rate , pos_enc_class = pos_enc_class , normalize_before = args . encoder_normalize_before , concat_after = args . encoder_concat_after , positionwise_layer_type = args . positionwise_layer_type , positionwise_conv_kernel_size = args . positionwise_conv_kernel_size , ) # define projection layer if self . spk_embed_dim is not None : if self . spk_embed_integration_type == \"add\" : self . projection = torch . nn . Linear ( self . spk_embed_dim , args . adim ) else : self . projection = torch . nn . Linear ( args . adim + self . spk_embed_dim , args . adim ) # define transformer decoder if args . dprenet_layers != 0 : # decoder prenet decoder_input_layer = torch . nn . Sequential ( DecoderPrenet ( idim = odim , n_layers = args . dprenet_layers , n_units = args . dprenet_units , dropout_rate = args . dprenet_dropout_rate ), torch . nn . Linear ( args . dprenet_units , args . adim ) ) else : decoder_input_layer = \"linear\" self . decoder = Decoder ( odim =- 1 , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . dunits , num_blocks = args . dlayers , dropout_rate = args . transformer_dec_dropout_rate , positional_dropout_rate = args . transformer_dec_positional_dropout_rate , self_attention_dropout_rate = args . transformer_dec_attn_dropout_rate , src_attention_dropout_rate = args . transformer_enc_dec_attn_dropout_rate , input_layer = decoder_input_layer , use_output_layer = False , pos_enc_class = pos_enc_class , normalize_before = args . decoder_normalize_before , concat_after = args . decoder_concat_after ) # define final projection self . feat_out = torch . nn . Linear ( args . adim , odim * args . reduction_factor ) self . prob_out = torch . nn . Linear ( args . adim , args . reduction_factor ) # define postnet self . postnet = None if args . postnet_layers == 0 else Postnet ( idim = idim , odim = odim , n_layers = args . postnet_layers , n_chans = args . postnet_chans , n_filts = args . postnet_filts , use_batch_norm = args . use_batch_norm , dropout_rate = args . postnet_dropout_rate ) # define loss function self . criterion = TransformerLoss ( use_masking = args . use_masking , use_weighted_masking = args . use_weighted_masking , bce_pos_weight = args . bce_pos_weight ) if self . use_guided_attn_loss : self . attn_criterion = GuidedMultiHeadAttentionLoss ( sigma = args . guided_attn_loss_sigma , alpha = args . guided_attn_loss_lambda , ) # initialize parameters self . _reset_parameters ( init_type = args . transformer_init , init_enc_alpha = args . initial_encoder_alpha , init_dec_alpha = args . initial_decoder_alpha ) # load pretrained model if args . pretrained_model is not None : self . load_pretrained_model ( args . pretrained_model ) add_arguments ( parser ) staticmethod Add model-specific arguments to the parser. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 @staticmethod def add_arguments ( parser ): \"\"\"Add model-specific arguments to the parser.\"\"\" group = parser . add_argument_group ( \"transformer model setting\" ) # network structure related group . add_argument ( \"--embed-dim\" , default = 512 , type = int , help = \"Dimension of character embedding in encoder prenet\" ) group . add_argument ( \"--eprenet-conv-layers\" , default = 3 , type = int , help = \"Number of encoder prenet convolution layers\" ) group . add_argument ( \"--eprenet-conv-chans\" , default = 256 , type = int , help = \"Number of encoder prenet convolution channels\" ) group . add_argument ( \"--eprenet-conv-filts\" , default = 5 , type = int , help = \"Filter size of encoder prenet convolution\" ) group . add_argument ( \"--dprenet-layers\" , default = 2 , type = int , help = \"Number of decoder prenet layers\" ) group . add_argument ( \"--dprenet-units\" , default = 256 , type = int , help = \"Number of decoder prenet hidden units\" ) group . add_argument ( \"--elayers\" , default = 3 , type = int , help = \"Number of encoder layers\" ) group . add_argument ( \"--eunits\" , default = 1536 , type = int , help = \"Number of encoder hidden units\" ) group . add_argument ( \"--adim\" , default = 384 , type = int , help = \"Number of attention transformation dimensions\" ) group . add_argument ( \"--aheads\" , default = 4 , type = int , help = \"Number of heads for multi head attention\" ) group . add_argument ( \"--dlayers\" , default = 3 , type = int , help = \"Number of decoder layers\" ) group . add_argument ( \"--dunits\" , default = 1536 , type = int , help = \"Number of decoder hidden units\" ) group . add_argument ( \"--positionwise-layer-type\" , default = \"linear\" , type = str , choices = [ \"linear\" , \"conv1d\" , \"conv1d-linear\" ], help = \"Positionwise layer type.\" ) group . add_argument ( \"--positionwise-conv-kernel-size\" , default = 1 , type = int , help = \"Kernel size of positionwise conv1d layer\" ) group . add_argument ( \"--postnet-layers\" , default = 5 , type = int , help = \"Number of postnet layers\" ) group . add_argument ( \"--postnet-chans\" , default = 256 , type = int , help = \"Number of postnet channels\" ) group . add_argument ( \"--postnet-filts\" , default = 5 , type = int , help = \"Filter size of postnet\" ) group . add_argument ( \"--use-scaled-pos-enc\" , default = True , type = strtobool , help = \"Use trainable scaled positional encoding instead of the fixed scale one.\" ) group . add_argument ( \"--use-batch-norm\" , default = True , type = strtobool , help = \"Whether to use batch normalization\" ) group . add_argument ( \"--encoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before encoder block\" ) group . add_argument ( \"--decoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before decoder block\" ) group . add_argument ( \"--encoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in encoder\" ) group . add_argument ( \"--decoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in decoder\" ) group . add_argument ( \"--reduction-factor\" , default = 1 , type = int , help = \"Reduction factor\" ) group . add_argument ( \"--spk-embed-dim\" , default = None , type = int , help = \"Number of speaker embedding dimensions\" ) group . add_argument ( \"--spk-embed-integration-type\" , type = str , default = \"add\" , choices = [ \"add\" , \"concat\" ], help = \"How to integrate speaker embedding\" ) # training related group . add_argument ( \"--transformer-init\" , type = str , default = \"pytorch\" , choices = [ \"pytorch\" , \"xavier_uniform\" , \"xavier_normal\" , \"kaiming_uniform\" , \"kaiming_normal\" ], help = \"How to initialize transformer parameters\" ) group . add_argument ( \"--initial-encoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in encoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--initial-decoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in decoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--transformer-lr\" , default = 1.0 , type = float , help = \"Initial value of learning rate\" ) group . add_argument ( \"--transformer-warmup-steps\" , default = 4000 , type = int , help = \"Optimizer warmup steps\" ) group . add_argument ( \"--transformer-enc-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder except for attention\" ) group . add_argument ( \"--transformer-enc-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder positional encoding\" ) group . add_argument ( \"--transformer-enc-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder self-attention\" ) group . add_argument ( \"--transformer-dec-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder except for attention and pos encoding\" ) group . add_argument ( \"--transformer-dec-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder positional encoding\" ) group . add_argument ( \"--transformer-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder self-attention\" ) group . add_argument ( \"--transformer-enc-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder-decoder attention\" ) group . add_argument ( \"--eprenet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in encoder prenet\" ) group . add_argument ( \"--dprenet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in decoder prenet\" ) group . add_argument ( \"--postnet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in postnet\" ) group . add_argument ( \"--pretrained-model\" , default = None , type = str , help = \"Pretrained model path\" ) # loss related group . add_argument ( \"--use-masking\" , default = True , type = strtobool , help = \"Whether to use masking in calculation of loss\" ) group . add_argument ( \"--use-weighted-masking\" , default = False , type = strtobool , help = \"Whether to use weighted masking in calculation of loss\" ) group . add_argument ( \"--loss-type\" , default = \"L1\" , choices = [ \"L1\" , \"L2\" , \"L1+L2\" ], help = \"How to calc loss\" ) group . add_argument ( \"--bce-pos-weight\" , default = 5.0 , type = float , help = \"Positive sample weight in BCE calculation (only for use-masking=True)\" ) group . add_argument ( \"--use-guided-attn-loss\" , default = False , type = strtobool , help = \"Whether to use guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-sigma\" , default = 0.4 , type = float , help = \"Sigma in guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-lambda\" , default = 1.0 , type = float , help = \"Lambda in guided attention loss\" ) group . add_argument ( \"--num-heads-applied-guided-attn\" , default = 2 , type = int , help = \"Number of heads in each layer to be applied guided attention loss\" \"if set -1, all of the heads will be applied.\" ) group . add_argument ( \"--num-layers-applied-guided-attn\" , default = 2 , type = int , help = \"Number of layers to be applied guided attention loss\" \"if set -1, all of the layers will be applied.\" ) group . add_argument ( \"--modules-applied-guided-attn\" , type = str , nargs = \"+\" , default = [ \"encoder-decoder\" ], help = \"Module name list to be applied guided attention loss\" ) return parser calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , skip_output = False , keep_tensor = False , * args , ** kwargs ) Calculate all of the attention weights. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None skip_output bool Whether to skip calculate the final output. False keep_tensor bool Whether to keep original tensor. False Returns: Type Description dict Dict of attention weights and outputs. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , skip_output = False , keep_tensor = False , * args , ** kwargs ): \"\"\"Calculate all of the attention weights. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). skip_output (bool, optional): Whether to skip calculate the final output. keep_tensor (bool, optional): Whether to keep original tensor. Returns: dict: Dict of attention weights and outputs. \"\"\" with torch . no_grad (): # forward encoder x_masks = self . _source_mask ( ilens ) hs , _ = self . encoder ( xs , x_masks ) # integrate speaker embedding if self . spk_embed_dim is not None : hs = self . _integrate_with_spk_embed ( hs , spembs ) # thin out frames for reduction factor (B, Lmax, odim) -> (B, Lmax//r, odim) if self . reduction_factor > 1 : ys_in = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] olens_in = olens . new ([ olen // self . reduction_factor for olen in olens ]) else : ys_in , olens_in = ys , olens # add first zero frame and remove last frame for auto-regressive ys_in = self . _add_first_frame_and_remove_last_frame ( ys_in ) # forward decoder y_masks = self . _target_mask ( olens_in ) xy_masks = self . _source_to_target_mask ( ilens , olens_in ) zs , _ = self . decoder ( ys_in , y_masks , hs , xy_masks ) # calculate final outputs if not skip_output : before_outs = self . feat_out ( zs ) . view ( zs . size ( 0 ), - 1 , self . odim ) if self . postnet is None : after_outs = before_outs else : after_outs = before_outs + self . postnet ( before_outs . transpose ( 1 , 2 )) . transpose ( 1 , 2 ) # modifiy mod part of output lengths due to reduction factor > 1 if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) # store into dict att_ws_dict = dict () if keep_tensor : for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): att_ws_dict [ name ] = m . attn if not skip_output : att_ws_dict [ \"before_postnet_fbank\" ] = before_outs att_ws_dict [ \"after_postnet_fbank\" ] = after_outs else : for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): attn = m . attn . cpu () . numpy () if \"encoder\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , ilens . tolist ())] elif \"decoder\" in name : if \"src\" in name : attn = [ a [:, : ol , : il ] for a , il , ol in zip ( attn , ilens . tolist (), olens_in . tolist ())] elif \"self\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , olens_in . tolist ())] else : logging . warning ( \"unknown attention module: \" + name ) else : logging . warning ( \"unknown attention module: \" + name ) att_ws_dict [ name ] = attn if not skip_output : before_outs = before_outs . cpu () . numpy () after_outs = after_outs . cpu () . numpy () att_ws_dict [ \"before_postnet_fbank\" ] = [ m [: l ] . T for m , l in zip ( before_outs , olens . tolist ())] att_ws_dict [ \"after_postnet_fbank\" ] = [ m [: l ] . T for m , l in zip ( after_outs , olens . tolist ())] return att_ws_dict forward ( self , xs , ilens , ys , labels , olens , spembs = None , * args , ** kwargs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def forward ( self , xs , ilens , ys , labels , olens , spembs = None , * args , ** kwargs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). Returns: Tensor: Loss value. \"\"\" # remove unnecessary padded part (for multi-gpus) max_ilen = max ( ilens ) max_olen = max ( olens ) if max_ilen != xs . shape [ 1 ]: xs = xs [:, : max_ilen ] if max_olen != ys . shape [ 1 ]: ys = ys [:, : max_olen ] labels = labels [:, : max_olen ] # forward encoder x_masks = self . _source_mask ( ilens ) hs , _ = self . encoder ( xs , x_masks ) # integrate speaker embedding if self . spk_embed_dim is not None : hs = self . _integrate_with_spk_embed ( hs , spembs ) # thin out frames for reduction factor (B, Lmax, odim) -> (B, Lmax//r, odim) if self . reduction_factor > 1 : ys_in = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] olens_in = olens . new ([ olen // self . reduction_factor for olen in olens ]) else : ys_in , olens_in = ys , olens # add first zero frame and remove last frame for auto-regressive ys_in = self . _add_first_frame_and_remove_last_frame ( ys_in ) # forward decoder y_masks = self . _target_mask ( olens_in ) xy_masks = self . _source_to_target_mask ( ilens , olens_in ) zs , _ = self . decoder ( ys_in , y_masks , hs , xy_masks ) # (B, Lmax//r, odim * r) -> (B, Lmax//r * r, odim) before_outs = self . feat_out ( zs ) . view ( zs . size ( 0 ), - 1 , self . odim ) # (B, Lmax//r, r) -> (B, Lmax//r * r) logits = self . prob_out ( zs ) . view ( zs . size ( 0 ), - 1 ) # postnet -> (B, Lmax//r * r, odim) if self . postnet is None : after_outs = before_outs else : after_outs = before_outs + self . postnet ( before_outs . transpose ( 1 , 2 )) . transpose ( 1 , 2 ) # modifiy mod part of groundtruth if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) max_olen = max ( olens ) ys = ys [:, : max_olen ] labels = labels [:, : max_olen ] labels [:, - 1 ] = 1.0 # make sure at least one frame has 1 # caluculate loss values l1_loss , l2_loss , bce_loss = self . criterion ( after_outs , before_outs , logits , ys , labels , olens ) if self . loss_type == \"L1\" : loss = l1_loss + bce_loss elif self . loss_type == \"L2\" : loss = l2_loss + bce_loss elif self . loss_type == \"L1+L2\" : loss = l1_loss + l2_loss + bce_loss else : raise ValueError ( \"unknown --loss-type \" + self . loss_type ) report_keys = [ { \"l1_loss\" : l1_loss . item ()}, { \"l2_loss\" : l2_loss . item ()}, { \"bce_loss\" : bce_loss . item ()}, { \"loss\" : loss . item ()}, ] # calculate guided attention loss if self . use_guided_attn_loss : # calculate for encoder if \"encoder\" in self . modules_applied_guided_attn : att_ws = [] for idx , layer_idx in enumerate ( reversed ( range ( len ( self . encoder . encoders )))): att_ws += [ self . encoder . encoders [ layer_idx ] . self_attn . attn [:, : self . num_heads_applied_guided_attn ]] if idx + 1 == self . num_layers_applied_guided_attn : break att_ws = torch . cat ( att_ws , dim = 1 ) # (B, H*L, T_in, T_in) enc_attn_loss = self . attn_criterion ( att_ws , ilens , ilens ) loss = loss + enc_attn_loss report_keys += [{ \"enc_attn_loss\" : enc_attn_loss . item ()}] # calculate for decoder if \"decoder\" in self . modules_applied_guided_attn : att_ws = [] for idx , layer_idx in enumerate ( reversed ( range ( len ( self . decoder . decoders )))): att_ws += [ self . decoder . decoders [ layer_idx ] . self_attn . attn [:, : self . num_heads_applied_guided_attn ]] if idx + 1 == self . num_layers_applied_guided_attn : break att_ws = torch . cat ( att_ws , dim = 1 ) # (B, H*L, T_out, T_out) dec_attn_loss = self . attn_criterion ( att_ws , olens_in , olens_in ) loss = loss + dec_attn_loss report_keys += [{ \"dec_attn_loss\" : dec_attn_loss . item ()}] # calculate for encoder-decoder if \"encoder-decoder\" in self . modules_applied_guided_attn : att_ws = [] for idx , layer_idx in enumerate ( reversed ( range ( len ( self . decoder . decoders )))): att_ws += [ self . decoder . decoders [ layer_idx ] . src_attn . attn [:, : self . num_heads_applied_guided_attn ]] if idx + 1 == self . num_layers_applied_guided_attn : break att_ws = torch . cat ( att_ws , dim = 1 ) # (B, H*L, T_out, T_in) enc_dec_attn_loss = self . attn_criterion ( att_ws , ilens , olens_in ) loss = loss + enc_dec_attn_loss report_keys += [{ \"enc_dec_attn_loss\" : enc_dec_attn_loss . item ()}] # report extra information if self . use_scaled_pos_enc : report_keys += [ { \"encoder_alpha\" : self . encoder . embed [ - 1 ] . alpha . data . item ()}, { \"decoder_alpha\" : self . decoder . embed [ - 1 ] . alpha . data . item ()}, ] self . reporter . report ( report_keys ) return loss inference ( self , x , inference_args , spemb = None , * args , ** kwargs ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default x Tensor Input sequence of characters (T,). required inference_args Namespace threshold (float): Threshold in inference. minlenratio (float): Minimum length ratio in inference. maxlenratio (float): Maximum length ratio in inference. required spemb Tensor Speaker embedding vector (spk_embed_dim). None Returns: Type Description Tensor Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Encoder-decoder (source) attention weights (#layers, #heads, L, T). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def inference ( self , x , inference_args , spemb = None , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Args: x (Tensor): Input sequence of characters (T,). inference_args (Namespace): - threshold (float): Threshold in inference. - minlenratio (float): Minimum length ratio in inference. - maxlenratio (float): Maximum length ratio in inference. spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim). Returns: Tensor: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Encoder-decoder (source) attention weights (#layers, #heads, L, T). \"\"\" # get options threshold = inference_args . threshold minlenratio = inference_args . minlenratio maxlenratio = inference_args . maxlenratio use_att_constraint = getattr ( inference_args , \"use_att_constraint\" , False ) # keep compatibility if use_att_constraint : logging . warning ( \"Attention constraint is not yet supported in Transformer. Not enabled.\" ) # forward encoder xs = x . unsqueeze ( 0 ) hs , _ = self . encoder ( xs , None ) # integrate speaker embedding if self . spk_embed_dim is not None : spembs = spemb . unsqueeze ( 0 ) hs = self . _integrate_with_spk_embed ( hs , spembs ) # set limits of length maxlen = int ( hs . size ( 1 ) * maxlenratio / self . reduction_factor ) minlen = int ( hs . size ( 1 ) * minlenratio / self . reduction_factor ) # initialize idx = 0 ys = hs . new_zeros ( 1 , 1 , self . odim ) outs , probs = [], [] # forward decoder step-by-step z_cache = self . decoder . init_state ( x ) while True : # update index idx += 1 # calculate output and stop prob at idx-th step y_masks = subsequent_mask ( idx ) . unsqueeze ( 0 ) . to ( x . device ) z , z_cache = self . decoder . forward_one_step ( ys , y_masks , hs , cache = z_cache ) # (B, adim) outs += [ self . feat_out ( z ) . view ( self . reduction_factor , self . odim )] # [(r, odim), ...] probs += [ torch . sigmoid ( self . prob_out ( z ))[ 0 ]] # [(r), ...] # update next inputs ys = torch . cat (( ys , outs [ - 1 ][ - 1 ] . view ( 1 , 1 , self . odim )), dim = 1 ) # (1, idx + 1, odim) # get attention weights att_ws_ = [] for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ) and \"src\" in name : att_ws_ += [ m . attn [ 0 , :, - 1 ] . unsqueeze ( 1 )] # [(#heads, 1, T),...] if idx == 1 : att_ws = att_ws_ else : # [(#heads, l, T), ...] att_ws = [ torch . cat ([ att_w , att_w_ ], dim = 1 ) for att_w , att_w_ in zip ( att_ws , att_ws_ )] # check whether to finish generation if int ( sum ( probs [ - 1 ] >= threshold )) > 0 or idx >= maxlen : # check mininum length if idx < minlen : continue outs = torch . cat ( outs , dim = 0 ) . unsqueeze ( 0 ) . transpose ( 1 , 2 ) # (L, odim) -> (1, L, odim) -> (1, odim, L) if self . postnet is not None : outs = outs + self . postnet ( outs ) # (1, odim, L) outs = outs . transpose ( 2 , 1 ) . squeeze ( 0 ) # (L, odim) probs = torch . cat ( probs , dim = 0 ) break # concatenate attention weights -> (#layers, #heads, L, T) att_ws = torch . stack ( att_ws , dim = 0 ) return outs , probs , att_ws TTSPlot \u00b6 Attention plot module for TTS-Transformer. plotfn ( self , data , attn_dict , outdir , suffix = 'png' , savefn = None ) Plot multi head attentions. Parameters: Name Type Description Default data dict Utts info from json file. required attn_dict dict Multi head attention dict. Values should be numpy.ndarray (H, L, T) required outdir str Directory name to save figures. required suffix str Filename suffix including image type (e.g., png). 'png' savefn function Function to save figures. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def plotfn ( self , data , attn_dict , outdir , suffix = \"png\" , savefn = None ): \"\"\"Plot multi head attentions. Args: data (dict): Utts info from json file. attn_dict (dict): Multi head attention dict. Values should be numpy.ndarray (H, L, T) outdir (str): Directory name to save figures. suffix (str): Filename suffix including image type (e.g., png). savefn (function): Function to save figures. \"\"\" import matplotlib.pyplot as plt for name , att_ws in attn_dict . items (): for idx , att_w in enumerate ( att_ws ): filename = \" %s / %s . %s . %s \" % ( outdir , data [ idx ][ 0 ], name , suffix ) if \"fbank\" in name : fig = plt . Figure () ax = fig . subplots ( 1 , 1 ) ax . imshow ( att_w , aspect = \"auto\" ) ax . set_xlabel ( \"frames\" ) ax . set_ylabel ( \"fbank coeff\" ) fig . tight_layout () else : fig = _plot_and_save_attention ( att_w , filename ) savefn ( fig , filename ) fastspeech special \u00b6 duration_calculator \u00b6 Duration calculator related modules. DurationCalculator Duration calculator module for FastSpeech. !!! todo * Fix the duplicated calculation of diagonal head decision __init__ ( self , teacher_model ) special Initialize duration calculator module. Parameters: Name Type Description Default teacher_model e2e_tts_transformer.Transformer Pretrained auto-regressive Transformer. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_calculator.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , teacher_model ): \"\"\"Initialize duration calculator module. Args: teacher_model (e2e_tts_transformer.Transformer): Pretrained auto-regressive Transformer. \"\"\" super ( DurationCalculator , self ) . __init__ () if isinstance ( teacher_model , Transformer ): self . register_buffer ( \"diag_head_idx\" , torch . tensor ( - 1 )) elif isinstance ( teacher_model , Tacotron2 ): pass else : raise ValueError ( \"teacher model should be the instance of e2e_tts_transformer.Transformer \" \"or e2e_tts_tacotron2.Tacotron2.\" ) self . teacher_model = teacher_model forward ( self , xs , ilens , ys , olens , spembs = None ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the padded sequences of character ids (B, Tmax). required ilens Tensor Batch of lengths of each input sequence (B,). required ys Tensor Batch of the padded sequence of target features (B, Lmax, odim). required olens Tensor Batch of lengths of each output sequence (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None Returns: Type Description Tensor Batch of durations (B, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_calculator.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def forward ( self , xs , ilens , ys , olens , spembs = None ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the padded sequences of character ids (B, Tmax). ilens (Tensor): Batch of lengths of each input sequence (B,). ys (Tensor): Batch of the padded sequence of target features (B, Lmax, odim). olens (Tensor): Batch of lengths of each output sequence (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). Returns: Tensor: Batch of durations (B, Tmax). \"\"\" if isinstance ( self . teacher_model , Transformer ): att_ws = self . _calculate_encoder_decoder_attentions ( xs , ilens , ys , olens , spembs = spembs ) # TODO(kan-bayashi): fix this issue # this does not work in multi-gpu case. registered buffer is not saved. if int ( self . diag_head_idx ) == - 1 : self . _init_diagonal_head ( att_ws ) att_ws = att_ws [:, self . diag_head_idx ] else : # NOTE(kan-bayashi): Here we assume that the teacher is tacotron 2 att_ws = self . teacher_model . calculate_all_attentions ( xs , ilens , ys , spembs = spembs , keep_tensor = True ) durations = [ self . _calculate_duration ( att_w , ilen , olen ) for att_w , ilen , olen in zip ( att_ws , ilens , olens )] return pad_list ( durations , 0 ) duration_predictor \u00b6 Duration predictor related modules. DurationPredictor Duration predictor module. This is a module of duration predictor described in `FastSpeech: Fast, Robust and Controllable Text to Speech` _ . The duration predictor predicts a duration of each frame in log domain from the hidden embeddings of encoder . .. _ `FastSpeech: Fast, Robust and Controllable Text to Speech` : https : // arxiv . org / pdf / 1905 . 09263 . pdf !!! note The calculation domain of outputs is different between in `forward` and in `inference` . In `forward` , the outputs are calculated in log domain but in `inference` , those are calculated in linear domain . __init__ ( self , idim , n_layers = 2 , n_chans = 384 , kernel_size = 3 , dropout_rate = 0.1 , offset = 1.0 ) special Initilize duration predictor module. Parameters: Name Type Description Default idim int Input dimension. required n_layers int Number of convolutional layers. 2 n_chans int Number of channels of convolutional layers. 384 kernel_size int Kernel size of convolutional layers. 3 dropout_rate float Dropout rate. 0.1 offset float Offset value to avoid nan in log domain. 1.0 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , idim , n_layers = 2 , n_chans = 384 , kernel_size = 3 , dropout_rate = 0.1 , offset = 1.0 ): \"\"\"Initilize duration predictor module. Args: idim (int): Input dimension. n_layers (int, optional): Number of convolutional layers. n_chans (int, optional): Number of channels of convolutional layers. kernel_size (int, optional): Kernel size of convolutional layers. dropout_rate (float, optional): Dropout rate. offset (float, optional): Offset value to avoid nan in log domain. \"\"\" super ( DurationPredictor , self ) . __init__ () self . offset = offset self . conv = torch . nn . ModuleList () for idx in range ( n_layers ): in_chans = idim if idx == 0 else n_chans self . conv += [ torch . nn . Sequential ( torch . nn . Conv1d ( in_chans , n_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ), torch . nn . ReLU (), LayerNorm ( n_chans , dim = 1 ), torch . nn . Dropout ( dropout_rate ) )] self . linear = torch . nn . Linear ( n_chans , 1 ) forward ( self , xs , x_masks = None ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of input sequences (B, Tmax, idim). required x_masks ByteTensor Batch of masks indicating padded part (B, Tmax). None Returns: Type Description Tensor Batch of predicted durations in log domain (B, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 71 72 73 74 75 76 77 78 79 80 81 82 def forward ( self , xs , x_masks = None ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of input sequences (B, Tmax, idim). x_masks (ByteTensor, optional): Batch of masks indicating padded part (B, Tmax). Returns: Tensor: Batch of predicted durations in log domain (B, Tmax). \"\"\" return self . _forward ( xs , x_masks , False ) inference ( self , xs , x_masks = None ) Inference duration. Parameters: Name Type Description Default xs Tensor Batch of input sequences (B, Tmax, idim). required x_masks ByteTensor Batch of masks indicating padded part (B, Tmax). None Returns: Type Description LongTensor Batch of predicted durations in linear domain (B, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 84 85 86 87 88 89 90 91 92 93 94 95 def inference ( self , xs , x_masks = None ): \"\"\"Inference duration. Args: xs (Tensor): Batch of input sequences (B, Tmax, idim). x_masks (ByteTensor, optional): Batch of masks indicating padded part (B, Tmax). Returns: LongTensor: Batch of predicted durations in linear domain (B, Tmax). \"\"\" return self . _forward ( xs , x_masks , True ) DurationPredictorLoss Loss function module for duration predictor. The loss value is Calculated in log domain to make it Gaussian. __init__ ( self , offset = 1.0 , reduction = 'mean' ) special Initilize duration predictor loss module. Parameters: Name Type Description Default offset float Offset value to avoid nan in log domain. 1.0 reduction str Reduction type in loss calculation. 'mean' Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , offset = 1.0 , reduction = \"mean\" ): \"\"\"Initilize duration predictor loss module. Args: offset (float, optional): Offset value to avoid nan in log domain. reduction (str): Reduction type in loss calculation. \"\"\" super ( DurationPredictorLoss , self ) . __init__ () self . criterion = torch . nn . MSELoss ( reduction = reduction ) self . offset = offset forward ( self , outputs , targets ) Calculate forward propagation. Parameters: Name Type Description Default outputs Tensor Batch of prediction durations in log domain (B, T) required targets LongTensor Batch of groundtruth durations in linear domain (B, T) required Returns: Type Description Tensor Mean squared error loss value. Note outputs is in log domain but targets is in linear domain. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def forward ( self , outputs , targets ): \"\"\"Calculate forward propagation. Args: outputs (Tensor): Batch of prediction durations in log domain (B, T) targets (LongTensor): Batch of groundtruth durations in linear domain (B, T) Returns: Tensor: Mean squared error loss value. Note: `outputs` is in log domain but `targets` is in linear domain. \"\"\" # NOTE: outputs is in log domain while targets in linear targets = torch . log ( targets . float () + self . offset ) loss = self . criterion ( outputs , targets ) return loss length_regulator \u00b6 Length regulator related modules. LengthRegulator Length regulator module for feed-forward Transformer. This is a module of length regulator described in ` FastSpeech : Fast , Robust and Controllable Text to Speech ` _ . The length regulator expands char or phoneme - level embedding features to frame - level by repeating each feature based on the corresponding predicted durations . .. _ ` FastSpeech : Fast , Robust and Controllable Text to Speech ` : https : // arxiv . org / pdf / 1905 . 09263 . pdf __init__ ( self , pad_value = 0.0 ) special Initilize length regulator module. Parameters: Name Type Description Default pad_value float Value used for padding. 0.0 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/length_regulator.py 28 29 30 31 32 33 34 35 36 def __init__ ( self , pad_value = 0.0 ): \"\"\"Initilize length regulator module. Args: pad_value (float, optional): Value used for padding. \"\"\" super ( LengthRegulator , self ) . __init__ () self . pad_value = pad_value forward ( self , xs , ds , ilens , alpha = 1.0 ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of sequences of char or phoneme embeddings (B, Tmax, D). required ds LongTensor Batch of durations of each frame (B, T). required ilens LongTensor Batch of input lengths (B,). required alpha float Alpha value to control speed of speech. 1.0 Returns: Type Description Tensor replicated input tensor based on durations (B, T*, D). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/length_regulator.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def forward ( self , xs , ds , ilens , alpha = 1.0 ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of sequences of char or phoneme embeddings (B, Tmax, D). ds (LongTensor): Batch of durations of each frame (B, T). ilens (LongTensor): Batch of input lengths (B,). alpha (float, optional): Alpha value to control speed of speech. Returns: Tensor: replicated input tensor based on durations (B, T*, D). \"\"\" assert alpha > 0 if alpha != 1.0 : ds = torch . round ( ds . float () * alpha ) . long () xs = [ x [: ilen ] for x , ilen in zip ( xs , ilens )] ds = [ d [: ilen ] for d , ilen in zip ( ds , ilens )] xs = [ self . _repeat_one_sequence ( x , d ) for x , d in zip ( xs , ds )] return pad_list ( xs , self . pad_value ) initialization \u00b6 Initialization functions for RNN sequence-to-sequence models. lecun_normal_init_parameters ( module ) \u00b6 Initialize parameters in the LeCun's manner. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/initialization.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def lecun_normal_init_parameters ( module ): \"\"\"Initialize parameters in the LeCun's manner.\"\"\" for p in module . parameters (): data = p . data if data . dim () == 1 : # bias data . zero_ () elif data . dim () == 2 : # linear weight n = data . size ( 1 ) stdv = 1. / math . sqrt ( n ) data . normal_ ( 0 , stdv ) elif data . dim () in ( 3 , 4 ): # conv weight n = data . size ( 1 ) for k in data . size ()[ 2 :]: n *= k stdv = 1. / math . sqrt ( n ) data . normal_ ( 0 , stdv ) else : raise NotImplementedError set_forget_bias_to_one ( bias ) \u00b6 Initialize a bias vector in the forget gate with one. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/initialization.py 51 52 53 54 55 def set_forget_bias_to_one ( bias ): \"\"\"Initialize a bias vector in the forget gate with one.\"\"\" n = bias . size ( 0 ) start , end = n // 4 , n // 2 bias . data [ start : end ] . fill_ ( 1. ) uniform_init_parameters ( module ) \u00b6 Initialize parameters with an uniform distribution. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/initialization.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def uniform_init_parameters ( module ): \"\"\"Initialize parameters with an uniform distribution.\"\"\" for p in module . parameters (): data = p . data if data . dim () == 1 : # bias data . uniform_ ( - 0.1 , 0.1 ) elif data . dim () == 2 : # linear weight data . uniform_ ( - 0.1 , 0.1 ) elif data . dim () in ( 3 , 4 ): # conv weight pass # use the pytorch default else : raise NotImplementedError nets_utils \u00b6 Network related utility tools. make_non_pad_mask ( lengths , xs = None , length_dim =- 1 ) \u00b6 Make mask tensor containing indices of non-padded part. Parameters: Name Type Description Default lengths LongTensor or List Batch of lengths (B,). required xs Tensor The reference tensor. If set, masks will be the same shape as this tensor. None length_dim int Dimension indicator of the above tensor. See the example. -1 Returns: Type Description ByteTensor mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples With only lengths. lengths = [5, 3, 2] make_non_pad_mask(lengths) masks = [[1, 1, 1, 1 ,1], [1, 1, 1, 0, 0], [1, 1, 0, 0, 0]] With the reference tensor. xs = torch.zeros((3, 2, 4)) make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 0], [1, 1, 1, 0]], [[1, 1, 0, 0], [1, 1, 0, 0]]], dtype=torch.uint8) xs = torch.zeros((3, 2, 6)) make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) With the reference tensor and dimension indicator. xs = torch.zeros((3, 6, 6)) make_non_pad_mask(lengths, xs, 1) tensor([[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) make_non_pad_mask(lengths, xs, 2) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def make_non_pad_mask ( lengths , xs = None , length_dim =- 1 ): \"\"\"Make mask tensor containing indices of non-padded part. Args: lengths (LongTensor or List): Batch of lengths (B,). xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor. length_dim (int, optional): Dimension indicator of the above tensor. See the example. Returns: ByteTensor: mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples: With only lengths. >>> lengths = [5, 3, 2] >>> make_non_pad_mask(lengths) masks = [[1, 1, 1, 1 ,1], [1, 1, 1, 0, 0], [1, 1, 0, 0, 0]] With the reference tensor. >>> xs = torch.zeros((3, 2, 4)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 0], [1, 1, 1, 0]], [[1, 1, 0, 0], [1, 1, 0, 0]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) With the reference tensor and dimension indicator. >>> xs = torch.zeros((3, 6, 6)) >>> make_non_pad_mask(lengths, xs, 1) tensor([[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) >>> make_non_pad_mask(lengths, xs, 2) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) \"\"\" return ~ make_pad_mask ( lengths , xs , length_dim ) make_pad_mask ( lengths , xs = None , length_dim =- 1 ) \u00b6 Make mask tensor containing indices of padded part. Parameters: Name Type Description Default lengths LongTensor or List Batch of lengths (B,). required xs Tensor The reference tensor. If set, masks will be the same shape as this tensor. None length_dim int Dimension indicator of the above tensor. See the example. -1 Returns: Type Description Tensor Mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples With only lengths. lengths = [5, 3, 2] make_non_pad_mask(lengths) masks = [[0, 0, 0, 0 ,0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 1]] With the reference tensor. xs = torch.zeros((3, 2, 4)) make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 1], [0, 0, 0, 1]], [[0, 0, 1, 1], [0, 0, 1, 1]]], dtype=torch.uint8) xs = torch.zeros((3, 2, 6)) make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) With the reference tensor and dimension indicator. xs = torch.zeros((3, 6, 6)) make_pad_mask(lengths, xs, 1) tensor([[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) make_pad_mask(lengths, xs, 2) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def make_pad_mask ( lengths , xs = None , length_dim =- 1 ): \"\"\"Make mask tensor containing indices of padded part. Args: lengths (LongTensor or List): Batch of lengths (B,). xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor. length_dim (int, optional): Dimension indicator of the above tensor. See the example. Returns: Tensor: Mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples: With only lengths. >>> lengths = [5, 3, 2] >>> make_non_pad_mask(lengths) masks = [[0, 0, 0, 0 ,0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 1]] With the reference tensor. >>> xs = torch.zeros((3, 2, 4)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 1], [0, 0, 0, 1]], [[0, 0, 1, 1], [0, 0, 1, 1]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) With the reference tensor and dimension indicator. >>> xs = torch.zeros((3, 6, 6)) >>> make_pad_mask(lengths, xs, 1) tensor([[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) >>> make_pad_mask(lengths, xs, 2) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) \"\"\" if length_dim == 0 : raise ValueError ( 'length_dim cannot be 0: {} ' . format ( length_dim )) if not isinstance ( lengths , list ): lengths = lengths . tolist () bs = int ( len ( lengths )) if xs is None : maxlen = int ( max ( lengths )) else : maxlen = xs . size ( length_dim ) seq_range = torch . arange ( 0 , maxlen , dtype = torch . int64 ) seq_range_expand = seq_range . unsqueeze ( 0 ) . expand ( bs , maxlen ) seq_length_expand = seq_range_expand . new ( lengths ) . unsqueeze ( - 1 ) mask = seq_range_expand >= seq_length_expand if xs is not None : assert xs . size ( 0 ) == bs , ( xs . size ( 0 ), bs ) if length_dim < 0 : length_dim = xs . dim () + length_dim # ind = (:, None, ..., None, :, , None, ..., None) ind = tuple ( slice ( None ) if i in ( 0 , length_dim ) else None for i in range ( xs . dim ())) mask = mask [ ind ] . expand_as ( xs ) . to ( xs . device ) return mask mask_by_length ( xs , lengths , fill = 0 ) \u00b6 Mask tensor according to length. Parameters: Name Type Description Default xs Tensor Batch of input tensor (B, * ). required lengths LongTensor or List Batch of lengths (B,). required fill int or float Value to fill masked part. 0 Returns: Type Description Tensor Batch of masked input tensor (B, * ). Examples x = torch.arange(5).repeat(3, 1) + 1 x tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]) lengths = [5, 3, 2] mask_by_length(x, lengths) tensor([[1, 2, 3, 4, 5], [1, 2, 3, 0, 0], [1, 2, 0, 0, 0]]) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def mask_by_length ( xs , lengths , fill = 0 ): \"\"\"Mask tensor according to length. Args: xs (Tensor): Batch of input tensor (B, `*`). lengths (LongTensor or List): Batch of lengths (B,). fill (int or float): Value to fill masked part. Returns: Tensor: Batch of masked input tensor (B, `*`). Examples: >>> x = torch.arange(5).repeat(3, 1) + 1 >>> x tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]) >>> lengths = [5, 3, 2] >>> mask_by_length(x, lengths) tensor([[1, 2, 3, 4, 5], [1, 2, 3, 0, 0], [1, 2, 0, 0, 0]]) \"\"\" assert xs . size ( 0 ) == len ( lengths ) ret = xs . data . new ( * xs . size ()) . fill_ ( fill ) for i , l in enumerate ( lengths ): ret [ i , : l ] = xs [ i , : l ] return ret pad_list ( xs , pad_value ) \u00b6 Perform padding for the list of tensors. Parameters: Name Type Description Default xs List List of Tensors [(T_1, * ), (T_2, * ), ..., (T_B, * )]. required pad_value float Value for padding. required Returns: Type Description Tensor Padded tensor (B, Tmax, * ). Examples x = [torch.ones(4), torch.ones(2), torch.ones(1)] x [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])] pad_list(x, 0) tensor([[1., 1., 1., 1.], [1., 1., 0., 0.], [1., 0., 0., 0.]]) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def pad_list ( xs , pad_value ): \"\"\"Perform padding for the list of tensors. Args: xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)]. pad_value (float): Value for padding. Returns: Tensor: Padded tensor (B, Tmax, `*`). Examples: >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)] >>> x [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])] >>> pad_list(x, 0) tensor([[1., 1., 1., 1.], [1., 1., 0., 0.], [1., 0., 0., 0.]]) \"\"\" n_batch = len ( xs ) max_len = max ( x . size ( 0 ) for x in xs ) pad = xs [ 0 ] . new ( n_batch , max_len , * xs [ 0 ] . size ()[ 1 :]) . fill_ ( pad_value ) for i in range ( n_batch ): pad [ i , : xs [ i ] . size ( 0 )] = xs [ i ] return pad th_accuracy ( pad_outputs , pad_targets , ignore_label ) \u00b6 Calculate accuracy. Parameters: Name Type Description Default pad_outputs Tensor Prediction tensors (B * Lmax, D). required pad_targets LongTensor Target label tensors (B, Lmax, D). required ignore_label int Ignore label id. required Returns: Type Description float Accuracy value (0.0 - 1.0). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def th_accuracy ( pad_outputs , pad_targets , ignore_label ): \"\"\"Calculate accuracy. Args: pad_outputs (Tensor): Prediction tensors (B * Lmax, D). pad_targets (LongTensor): Target label tensors (B, Lmax, D). ignore_label (int): Ignore label id. Returns: float: Accuracy value (0.0 - 1.0). \"\"\" pad_pred = pad_outputs . view ( pad_targets . size ( 0 ), pad_targets . size ( 1 ), pad_outputs . size ( 1 )) . argmax ( 2 ) mask = pad_targets != ignore_label numerator = torch . sum ( pad_pred . masked_select ( mask ) == pad_targets . masked_select ( mask )) denominator = torch . sum ( mask ) return float ( numerator ) / float ( denominator ) to_device ( m , x ) \u00b6 Send tensor into the device of the module. Parameters: Name Type Description Default m torch.nn.Module Torch module. required x Tensor Torch tensor. required Returns: Type Description Tensor Torch tensor located in the same place as torch module. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def to_device ( m , x ): \"\"\"Send tensor into the device of the module. Args: m (torch.nn.Module): Torch module. x (Tensor): Torch tensor. Returns: Tensor: Torch tensor located in the same place as torch module. \"\"\" assert isinstance ( m , torch . nn . Module ) device = next ( m . parameters ()) . device return x . to ( device ) to_torch_tensor ( x ) \u00b6 Change to torch.Tensor or ComplexTensor from numpy.ndarray. Parameters: Name Type Description Default x Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict. required Returns: Type Description Tensor or ComplexTensor Type converted inputs. Examples xs = np.ones(3, dtype=np.float32) xs = to_torch_tensor(xs) tensor([1., 1., 1.]) xs = torch.ones(3, 4, 5) assert to_torch_tensor(xs) is xs xs = {'real': xs, 'imag': xs} to_torch_tensor(xs) ComplexTensor( Real: tensor([1., 1., 1.]) Imag; tensor([1., 1., 1.]) ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def to_torch_tensor ( x ): \"\"\"Change to torch.Tensor or ComplexTensor from numpy.ndarray. Args: x: Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict. Returns: Tensor or ComplexTensor: Type converted inputs. Examples: >>> xs = np.ones(3, dtype=np.float32) >>> xs = to_torch_tensor(xs) tensor([1., 1., 1.]) >>> xs = torch.ones(3, 4, 5) >>> assert to_torch_tensor(xs) is xs >>> xs = {'real': xs, 'imag': xs} >>> to_torch_tensor(xs) ComplexTensor( Real: tensor([1., 1., 1.]) Imag; tensor([1., 1., 1.]) ) \"\"\" # If numpy, change to torch tensor if isinstance ( x , np . ndarray ): if x . dtype . kind == 'c' : # Dynamically importing because torch_complex requires python3 from torch_complex.tensor import ComplexTensor return ComplexTensor ( x ) else : return torch . from_numpy ( x ) # If {'real': ..., 'imag': ...}, convert to ComplexTensor elif isinstance ( x , dict ): # Dynamically importing because torch_complex requires python3 from torch_complex.tensor import ComplexTensor if 'real' not in x or 'imag' not in x : raise ValueError ( \"has 'real' and 'imag' keys: {} \" . format ( list ( x ))) # Relative importing because of using python3 syntax return ComplexTensor ( x [ 'real' ], x [ 'imag' ]) # If torch.Tensor, as it is elif isinstance ( x , torch . Tensor ): return x else : error = ( \"x must be numpy.ndarray, torch.Tensor or a dict like \" \"{{'real': torch.Tensor, 'imag': torch.Tensor}}, \" \"but got {} \" . format ( type ( x ))) try : from torch_complex.tensor import ComplexTensor except Exception : # If PY2 raise ValueError ( error ) else : # If PY3 if isinstance ( x , ComplexTensor ): return x else : raise ValueError ( error ) rnn special \u00b6 attentions \u00b6 Attention modules for RNN. AttAdd Additive attention :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int att_dim: attention dimension :param bool han_mode: flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 171 172 173 174 175 176 177 178 179 180 181 182 183 def __init__ ( self , eprojs , dunits , att_dim , han_mode = False ): super ( AttAdd , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 185 186 187 188 189 190 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttCov Coverage mechanism attention Reference : Get To The Point : Summarization with Pointer - Generator Network ( https :// arxiv . org /abs/ 1704.04368 ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def __init__ ( self , eprojs , dunits , att_dim , han_mode = False ): super ( AttCov , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . wvec = torch . nn . Linear ( 1 , att_dim ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ) AttCov forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ): \"\"\"AttCov forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev_list is None : # if no bias, 0 0-pad goes 0 att_prev_list = to_device ( self , ( 1. - make_pad_mask ( enc_hs_len ) . float ())) att_prev_list = [ att_prev_list / att_prev_list . new ( enc_hs_len ) . unsqueeze ( - 1 )] # att_prev_list: L' * [B x T] => cov_vec B x T cov_vec = sum ( att_prev_list ) # cov_vec: B x T => B x T x 1 => B x T x att_dim cov_vec = self . wvec ( cov_vec . unsqueeze ( - 1 )) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( cov_vec + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) att_prev_list += [ w ] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , att_prev_list reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 376 377 378 379 380 381 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttCovLoc Coverage mechanism location aware attention This attention is a combination of coverage and location - aware attentions . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ): super ( AttCovLoc , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . aconv_chans = aconv_chans self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ) AttCovLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ): \"\"\"AttCovLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev_list is None : # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev_list = [ to_device ( self , mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ))] # att_prev_list: L' * [B x T] => cov_vec B x T cov_vec = sum ( att_prev_list ) # cov_vec: B x T -> B x 1 x 1 x T -> B x C x 1 x T att_conv = self . loc_conv ( cov_vec . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) att_prev_list += [ w ] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , att_prev_list reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 696 697 698 699 700 701 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttDot Dot product attention :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int att_dim: attention dimension :param bool han_mode: flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , eprojs , dunits , att_dim , han_mode = False ): super ( AttDot , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weight (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weight (B x T_max) :rtype: torch.Tensor \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = torch . tanh ( self . mlp_enc ( self . enc_h )) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) e = torch . sum ( self . pre_compute_enc_h * torch . tanh ( self . mlp_dec ( dec_z )) . view ( batch , 1 , self . att_dim ), dim = 2 ) # utt x frame # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 112 113 114 115 116 117 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttForward Forward attention module. !!! reference \"Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\" ( https : // arxiv . org / pdf / 1807 . 06736 . pdf ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts ): super ( AttForward , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ) Calculate AttForward forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: attention weights of previous step :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ): \"\"\"Calculate AttForward forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: attention weights of previous step :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : # initial attention will be [1, 0, 0, ...] att_prev = enc_hs_pad . new_zeros ( * enc_hs_pad . size ()[: 2 ]) att_prev [:, 0 ] = 1.0 # att_prev: utt x frame -> utt x 1 x 1 x frame -> utt x att_conv_chans x 1 x frame att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . unsqueeze ( 1 ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( self . pre_compute_enc_h + dec_z_tiled + att_conv )) . squeeze ( 2 ) # NOTE: consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) # apply monotonic attention constraint (mainly for TTS) if last_attended_idx is not None : e = _apply_attention_constraint ( e , last_attended_idx , backward_window , forward_window ) w = F . softmax ( scaling * e , dim = 1 ) # forward attention att_prev_shift = F . pad ( att_prev , ( 1 , 0 ))[:, : - 1 ] w = ( att_prev + att_prev_shift ) * w # NOTE: clamp is needed to avoid nan gradient w = F . normalize ( torch . clamp ( w , 1e-6 ), p = 1 , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . unsqueeze ( - 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1260 1261 1262 1263 1264 1265 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttForwardTA Forward attention with transition agent module. !!! reference \"Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\" ( https : // arxiv . org / pdf / 1807 . 06736 . pdf ) : param int eunits : # units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param int odim : output dimension __init__ ( self , eunits , dunits , att_dim , aconv_chans , aconv_filts , odim ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 def __init__ ( self , eunits , dunits , att_dim , aconv_chans , aconv_filts , odim ): super ( AttForwardTA , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eunits , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_ta = torch . nn . Linear ( eunits + dunits + odim , 1 ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eunits = eunits self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . trans_agent_prob = 0.5 forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , out_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ) Calculate AttForwardTA forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, Tmax, eunits) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B, dunits) :param torch.Tensor att_prev: attention weights of previous step :param torch.Tensor out_prev: decoder outputs of previous step (B, odim) :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, dunits) :rtype: torch.Tensor :return: previous attention weights (B, Tmax) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , out_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ): \"\"\"Calculate AttForwardTA forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, Tmax, eunits) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B, dunits) :param torch.Tensor att_prev: attention weights of previous step :param torch.Tensor out_prev: decoder outputs of previous step (B, odim) :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, dunits) :rtype: torch.Tensor :return: previous attention weights (B, Tmax) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : # initial attention will be [1, 0, 0, ...] att_prev = enc_hs_pad . new_zeros ( * enc_hs_pad . size ()[: 2 ]) att_prev [:, 0 ] = 1.0 # att_prev: utt x frame -> utt x 1 x 1 x frame -> utt x att_conv_chans x 1 x frame att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) # apply monotonic attention constraint (mainly for TTS) if last_attended_idx is not None : e = _apply_attention_constraint ( e , last_attended_idx , backward_window , forward_window ) w = F . softmax ( scaling * e , dim = 1 ) # forward attention att_prev_shift = F . pad ( att_prev , ( 1 , 0 ))[:, : - 1 ] w = ( self . trans_agent_prob * att_prev + ( 1 - self . trans_agent_prob ) * att_prev_shift ) * w # NOTE: clamp is needed to avoid nan gradient w = F . normalize ( torch . clamp ( w , 1e-6 ), p = 1 , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) # update transition agent prob self . trans_agent_prob = torch . sigmoid ( self . mlp_ta ( torch . cat ([ c , out_prev , dec_z ], dim = 1 ))) return c , w reset ( self ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1373 1374 1375 1376 1377 1378 def reset ( self ): self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . trans_agent_prob = 0.5 AttLoc location-aware attention module. !!! reference \"Attention-Based Models for Speech Recognition\" ( https : // arxiv . org / pdf / 1506 . 07503 . pdf ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ): super ( AttLoc , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ) Calcualte AttLoc forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x T_max) :param float scaling: scaling parameter before applying softmax :param torch.Tensor forward_window: forward window size when constraining attention :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ): \"\"\"Calcualte AttLoc forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x T_max) :param float scaling: scaling parameter before applying softmax :param torch.Tensor forward_window: forward window size when constraining attention :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev is None : # if no bias, 0 0-pad goes 0 att_prev = ( 1. - make_pad_mask ( enc_hs_len ) . to ( device = dec_z . device , dtype = dec_z . dtype )) att_prev = att_prev / att_prev . new ( enc_hs_len ) . unsqueeze ( - 1 ) # att_prev: utt x frame -> utt x 1 x 1 x frame -> utt x att_conv_chans x 1 x frame att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE: consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) # apply monotonic attention constraint (mainly for TTS) if last_attended_idx is not None : e = _apply_attention_constraint ( e , last_attended_idx , backward_window , forward_window ) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 272 273 274 275 276 277 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttLoc2D 2D location-aware attention This attention is an extended version of location aware attention . It take not only one frame before attention weights , but also earlier frames into account . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param int att_win : attention window size ( default = 5 ) : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , att_win , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def __init__ ( self , eprojs , dunits , att_dim , att_win , aconv_chans , aconv_filts , han_mode = False ): super ( AttLoc2D , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( att_win , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . aconv_chans = aconv_chans self . att_win = att_win self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttLoc2D forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x att_win x T_max) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x att_win x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttLoc2D forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x att_win x T_max) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x att_win x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev is None : # B * [Li x att_win] # if no bias, 0 0-pad goes 0 att_prev = to_device ( self , ( 1. - make_pad_mask ( enc_hs_len ) . float ())) att_prev = att_prev / att_prev . new ( enc_hs_len ) . unsqueeze ( - 1 ) att_prev = att_prev . unsqueeze ( 1 ) . expand ( - 1 , self . att_win , - 1 ) # att_prev: B x att_win x Tmax -> B x 1 x att_win x Tmax -> B x C x 1 x Tmax att_conv = self . loc_conv ( att_prev . unsqueeze ( 1 )) # att_conv: B x C x 1 x Tmax -> B x Tmax x C att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) # update att_prev: B x att_win x Tmax -> B x att_win+1 x Tmax -> B x att_win x Tmax att_prev = torch . cat ([ att_prev , w . unsqueeze ( 1 )], dim = 1 ) att_prev = att_prev [:, 1 :] return c , att_prev reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 478 479 480 481 482 483 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttLocRec location-aware recurrent attention This attention is an extended version of location aware attention . With the use of RNN , it take the effect of the history of attention weights into account . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ): super ( AttLocRec , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . att_lstm = torch . nn . LSTMCell ( aconv_chans , att_dim , bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_states , scaling = 2.0 ) AttLocRec forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param tuple att_prev_states: previous attention weight and lstm states ((B, T_max), ((B, att_dim), (B, att_dim))) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights and lstm states (w, (hx, cx)) ((B, T_max), ((B, att_dim), (B, att_dim))) :rtype: tuple Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_states , scaling = 2.0 ): \"\"\"AttLocRec forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param tuple att_prev_states: previous attention weight and lstm states ((B, T_max), ((B, att_dim), (B, att_dim))) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights and lstm states (w, (hx, cx)) ((B, T_max), ((B, att_dim), (B, att_dim))) :rtype: tuple \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev_states is None : # initialize attention weight with uniform dist. # if no bias, 0 0-pad goes 0 att_prev = to_device ( self , ( 1. - make_pad_mask ( enc_hs_len ) . float ())) att_prev = att_prev / att_prev . new ( enc_hs_len ) . unsqueeze ( - 1 ) # initialize lstm states att_h = enc_hs_pad . new_zeros ( batch , self . att_dim ) att_c = enc_hs_pad . new_zeros ( batch , self . att_dim ) att_states = ( att_h , att_c ) else : att_prev = att_prev_states [ 0 ] att_states = att_prev_states [ 1 ] # B x 1 x 1 x T -> B x C x 1 x T att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # apply non-linear att_conv = F . relu ( att_conv ) # B x C x 1 x T -> B x C x 1 x 1 -> B x C att_conv = F . max_pool2d ( att_conv , ( 1 , att_conv . size ( 3 ))) . view ( batch , - 1 ) att_h , att_c = self . att_lstm ( att_conv , att_states ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_h . unsqueeze ( 1 ) + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , ( w , ( att_h , att_c )) reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 584 585 586 587 588 589 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttMultiHeadAdd Multi head additive attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) This attention is multi head attention using additive attention for each head . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ): super ( AttMultiHeadAdd , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () self . gvec = torch . nn . ModuleList () for _ in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . gvec += [ torch . nn . Linear ( att_dim_k , 1 )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) AttMultiHeadAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"AttMultiHeadAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ self . mlp_k [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) c = [] w = [] for h in six . moves . range ( self . aheads ): e = self . gvec [ h ]( torch . tanh ( self . pre_compute_k [ h ] + self . mlp_q [ h ]( dec_z ) . view ( batch , 1 , self . att_dim_k ))) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( self . scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 911 912 913 914 915 916 917 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None AttMultiHeadDot Multi head dot product attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ): super ( AttMultiHeadDot , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () for _ in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) AttMultiHeadDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"AttMultiHeadDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ torch . tanh ( self . mlp_k [ h ]( self . enc_h )) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) c = [] w = [] for h in six . moves . range ( self . aheads ): e = torch . sum ( self . pre_compute_k [ h ] * torch . tanh ( self . mlp_q [ h ]( dec_z )) . view ( batch , 1 , self . att_dim_k ), dim = 2 ) # utt x frame # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( self . scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 805 806 807 808 809 810 811 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None AttMultiHeadLoc Multi head location based attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) This attention is multi head attention using location - aware attention for each head . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ): super ( AttMultiHeadLoc , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () self . gvec = torch . nn . ModuleList () self . loc_conv = torch . nn . ModuleList () self . mlp_att = torch . nn . ModuleList () for _ in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . gvec += [ torch . nn . Linear ( att_dim_k , 1 )] self . loc_conv += [ torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False )] self . mlp_att += [ torch . nn . Linear ( aconv_chans , att_dim_k , bias = False )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttMultiHeadLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttMultiHeadLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ self . mlp_k [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : att_prev = [] for _ in six . moves . range ( self . aheads ): # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev += [ to_device ( self , mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ))] c = [] w = [] for h in six . moves . range ( self . aheads ): att_conv = self . loc_conv [ h ]( att_prev [ h ] . view ( batch , 1 , 1 , self . h_length )) att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) att_conv = self . mlp_att [ h ]( att_conv ) e = self . gvec [ h ]( torch . tanh ( self . pre_compute_k [ h ] + att_conv + self . mlp_q [ h ]( dec_z ) . view ( batch , 1 , self . att_dim_k ))) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1024 1025 1026 1027 1028 1029 1030 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None AttMultiHeadMultiResLoc Multi head multi resolution location based attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) This attention is multi head attention using location - aware attention for each head . Furthermore , it uses different filter size for each head . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param int aconv_chans : maximum # channels of attention convolution each head use # ch = aconv_chans * ( head + 1 ) / aheads e . g . aheads = 4 , aconv_chans = 100 => filter size = 25 , 50 , 75 , 100 : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ): super ( AttMultiHeadMultiResLoc , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () self . gvec = torch . nn . ModuleList () self . loc_conv = torch . nn . ModuleList () self . mlp_att = torch . nn . ModuleList () for h in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . gvec += [ torch . nn . Linear ( att_dim_k , 1 )] afilts = aconv_filts * ( h + 1 ) // aheads self . loc_conv += [ torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * afilts + 1 ), padding = ( 0 , afilts ), bias = False )] self . mlp_att += [ torch . nn . Linear ( aconv_chans , att_dim_k , bias = False )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) AttMultiHeadMultiResLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"AttMultiHeadMultiResLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ self . mlp_k [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : att_prev = [] for _ in six . moves . range ( self . aheads ): # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev += [ to_device ( self , mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ))] c = [] w = [] for h in six . moves . range ( self . aheads ): att_conv = self . loc_conv [ h ]( att_prev [ h ] . view ( batch , 1 , 1 , self . h_length )) att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) att_conv = self . mlp_att [ h ]( att_conv ) e = self . gvec [ h ]( torch . tanh ( self . pre_compute_k [ h ] + att_conv + self . mlp_q [ h ]( dec_z ) . view ( batch , 1 , self . att_dim_k ))) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( self . scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1154 1155 1156 1157 1158 1159 1160 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None NoAtt No attention __init__ ( self ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 46 47 48 49 50 51 def __init__ ( self ): super ( NoAtt , self ) . __init__ () self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . c = None forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) NoAtt forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, T_max, D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"NoAtt forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, T_max, D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # initialize attention weight with uniform dist. if att_prev is None : # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev = mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ) att_prev = att_prev . to ( self . enc_h ) self . c = torch . sum ( self . enc_h * att_prev . view ( batch , self . h_length , 1 ), dim = 1 ) return self . c , att_prev reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 53 54 55 56 57 58 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . c = None att_for ( args , num_att = 1 , han_mode = False ) Instantiates an attention module given the program arguments :param Namespace args: The arguments :param int num_att: number of attention modules (in multi-speaker case, it can be 2 or more) :param bool han_mode: switch on/off mode of hierarchical attention network (HAN) :rtype torch.nn.Module :return: The attention module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 def att_for ( args , num_att = 1 , han_mode = False ): \"\"\"Instantiates an attention module given the program arguments :param Namespace args: The arguments :param int num_att: number of attention modules (in multi-speaker case, it can be 2 or more) :param bool han_mode: switch on/off mode of hierarchical attention network (HAN) :rtype torch.nn.Module :return: The attention module \"\"\" att_list = torch . nn . ModuleList () num_encs = getattr ( args , \"num_encs\" , 1 ) # use getattr to keep compatibility aheads = getattr ( args , 'aheads' , None ) awin = getattr ( args , 'awin' , None ) aconv_chans = getattr ( args , 'aconv_chans' , None ) aconv_filts = getattr ( args , 'aconv_filts' , None ) if num_encs == 1 : for i in range ( num_att ): att = initial_att ( args . atype , args . eprojs , args . dunits , aheads , args . adim , awin , aconv_chans , aconv_filts ) att_list . append ( att ) elif num_encs > 1 : # no multi-speaker mode if han_mode : att = initial_att ( args . han_type , args . eprojs , args . dunits , args . han_heads , args . han_dim , args . han_win , args . han_conv_chans , args . han_conv_filts , han_mode = True ) return att else : att_list = torch . nn . ModuleList () for idx in range ( num_encs ): att = initial_att ( args . atype [ idx ], args . eprojs , args . dunits , aheads [ idx ], args . adim [ idx ], awin [ idx ], aconv_chans [ idx ], aconv_filts [ idx ]) att_list . append ( att ) else : raise ValueError ( \"Number of encoders needs to be more than one. {} \" . format ( num_encs )) return att_list att_to_numpy ( att_ws , att ) Converts attention weights to a numpy array given the attention :param list att_ws: The attention weights :param torch.nn.Module att: The attention :rtype: np.ndarray :return: The numpy array of the attention weights Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 def att_to_numpy ( att_ws , att ): \"\"\"Converts attention weights to a numpy array given the attention :param list att_ws: The attention weights :param torch.nn.Module att: The attention :rtype: np.ndarray :return: The numpy array of the attention weights \"\"\" # convert to numpy array with the shape (B, Lmax, Tmax) if isinstance ( att , AttLoc2D ): # att_ws => list of previous concate attentions att_ws = torch . stack ([ aw [:, - 1 ] for aw in att_ws ], dim = 1 ) . cpu () . numpy () elif isinstance ( att , ( AttCov , AttCovLoc )): # att_ws => list of list of previous attentions att_ws = torch . stack ([ aw [ idx ] for idx , aw in enumerate ( att_ws )], dim = 1 ) . cpu () . numpy () elif isinstance ( att , AttLocRec ): # att_ws => list of tuple of attention and hidden states att_ws = torch . stack ([ aw [ 0 ] for aw in att_ws ], dim = 1 ) . cpu () . numpy () elif isinstance ( att , ( AttMultiHeadDot , AttMultiHeadAdd , AttMultiHeadLoc , AttMultiHeadMultiResLoc )): # att_ws => list of list of each head attention n_heads = len ( att_ws [ 0 ]) att_ws_sorted_by_head = [] for h in six . moves . range ( n_heads ): att_ws_head = torch . stack ([ aw [ h ] for aw in att_ws ], dim = 1 ) att_ws_sorted_by_head += [ att_ws_head ] att_ws = torch . stack ( att_ws_sorted_by_head , dim = 1 ) . cpu () . numpy () else : # att_ws => list of attentions att_ws = torch . stack ( att_ws , dim = 1 ) . cpu () . numpy () return att_ws initial_att ( atype , eprojs , dunits , aheads , adim , awin , aconv_chans , aconv_filts , han_mode = False ) Instantiates a single attention module :param str atype: attention type :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int aheads: # heads of multi head attention :param int adim: attention dimension :param int awin: attention window size :param int aconv_chans: # channels of attention convolution :param int aconv_filts: filter size of attention convolution :param bool han_mode: flag to swith on mode of hierarchical attention :return: The attention module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 def initial_att ( atype , eprojs , dunits , aheads , adim , awin , aconv_chans , aconv_filts , han_mode = False ): \"\"\"Instantiates a single attention module :param str atype: attention type :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int aheads: # heads of multi head attention :param int adim: attention dimension :param int awin: attention window size :param int aconv_chans: # channels of attention convolution :param int aconv_filts: filter size of attention convolution :param bool han_mode: flag to swith on mode of hierarchical attention :return: The attention module \"\"\" if atype == 'noatt' : att = NoAtt () elif atype == 'dot' : att = AttDot ( eprojs , dunits , adim , han_mode ) elif atype == 'add' : att = AttAdd ( eprojs , dunits , adim , han_mode ) elif atype == 'location' : att = AttLoc ( eprojs , dunits , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'location2d' : att = AttLoc2D ( eprojs , dunits , adim , awin , aconv_chans , aconv_filts , han_mode ) elif atype == 'location_recurrent' : att = AttLocRec ( eprojs , dunits , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'coverage' : att = AttCov ( eprojs , dunits , adim , han_mode ) elif atype == 'coverage_location' : att = AttCovLoc ( eprojs , dunits , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'multi_head_dot' : att = AttMultiHeadDot ( eprojs , dunits , aheads , adim , adim , han_mode ) elif atype == 'multi_head_add' : att = AttMultiHeadAdd ( eprojs , dunits , aheads , adim , adim , han_mode ) elif atype == 'multi_head_loc' : att = AttMultiHeadLoc ( eprojs , dunits , aheads , adim , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'multi_head_multi_res_loc' : att = AttMultiHeadMultiResLoc ( eprojs , dunits , aheads , adim , adim , aconv_chans , aconv_filts , han_mode ) return att decoders \u00b6 Decoder Decoder module :param int eprojs: encoder projection units :param int odim: dimension of outputs :param str dtype: gru or lstm :param int dlayers: decoder layers :param int dunits: decoder units :param int sos: start of sequence symbol id :param int eos: end of sequence symbol id :param torch.nn.Module att: attention module :param int verbose: verbose level :param list char_list: list of character strings :param ndarray labeldist: distribution of label smoothing :param float lsm_weight: label smoothing weight :param float sampling_probability: scheduled sampling probability :param float dropout: dropout rate :param float context_residual: if True, use context vector for token generation :param float replace_sos: use for multilingual (speech/text) translation __init__ ( self , eprojs , odim , dtype , dlayers , dunits , sos , eos , att , verbose = 0 , char_list = None , labeldist = None , lsm_weight = 0.0 , sampling_probability = 0.0 , dropout = 0.0 , context_residual = False , replace_sos = False , num_encs = 1 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , eprojs , odim , dtype , dlayers , dunits , sos , eos , att , verbose = 0 , char_list = None , labeldist = None , lsm_weight = 0. , sampling_probability = 0.0 , dropout = 0.0 , context_residual = False , replace_sos = False , num_encs = 1 ): torch . nn . Module . __init__ ( self ) self . dtype = dtype self . dunits = dunits self . dlayers = dlayers self . context_residual = context_residual self . embed = torch . nn . Embedding ( odim , dunits ) self . dropout_emb = torch . nn . Dropout ( p = dropout ) self . decoder = torch . nn . ModuleList () self . dropout_dec = torch . nn . ModuleList () self . decoder += [ torch . nn . LSTMCell ( dunits + eprojs , dunits ) if self . dtype == \"lstm\" else torch . nn . GRUCell ( dunits + eprojs , dunits )] self . dropout_dec += [ torch . nn . Dropout ( p = dropout )] for _ in six . moves . range ( 1 , self . dlayers ): self . decoder += [ torch . nn . LSTMCell ( dunits , dunits ) if self . dtype == \"lstm\" else torch . nn . GRUCell ( dunits , dunits )] self . dropout_dec += [ torch . nn . Dropout ( p = dropout )] # NOTE: dropout is applied only for the vertical connections # see https://arxiv.org/pdf/1409.2329.pdf self . ignore_id = - 1 if context_residual : self . output = torch . nn . Linear ( dunits + eprojs , odim ) else : self . output = torch . nn . Linear ( dunits , odim ) self . loss = None self . att = att self . dunits = dunits self . sos = sos self . eos = eos self . odim = odim self . verbose = verbose self . char_list = char_list # for label smoothing self . labeldist = labeldist self . vlabeldist = None self . lsm_weight = lsm_weight self . sampling_probability = sampling_probability self . dropout = dropout self . num_encs = num_encs # for multilingual E2E-ST self . replace_sos = replace_sos self . logzero = - 10000000000.0 calculate_all_attentions ( self , hs_pad , hlen , ys_pad , strm_idx = 0 , lang_ids = None ) Calculate all of attentions :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlen: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index for parallel speaker attention in multi-speaker case :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) multi-encoder case => [(B, Lmax, Tmax1), (B, Lmax, Tmax2), ..., (B, Lmax, NumEncs)] 3) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 def calculate_all_attentions ( self , hs_pad , hlen , ys_pad , strm_idx = 0 , lang_ids = None ): \"\"\"Calculate all of attentions :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlen: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index for parallel speaker attention in multi-speaker case :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) multi-encoder case => [(B, Lmax, Tmax1), (B, Lmax, Tmax2), ..., (B, Lmax, NumEncs)] 3) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray \"\"\" # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : hs_pad = [ hs_pad ] hlen = [ hlen ] # TODO(kan-bayashi): need to make more smart way ys = [ y [ y != self . ignore_id ] for y in ys_pad ] # parse padded ys att_idx = min ( strm_idx , len ( self . att ) - 1 ) # hlen should be list of list of integer hlen = [ list ( map ( int , hlen [ idx ])) for idx in range ( self . num_encs )] self . loss = None # prepare input and output word sequences with sos/eos IDs eos = ys [ 0 ] . new ([ self . eos ]) sos = ys [ 0 ] . new ([ self . sos ]) if self . replace_sos : ys_in = [ torch . cat ([ idx , y ], dim = 0 ) for idx , y in zip ( lang_ids , ys )] else : ys_in = [ torch . cat ([ sos , y ], dim = 0 ) for y in ys ] ys_out = [ torch . cat ([ y , eos ], dim = 0 ) for y in ys ] # padding for ys with -1 # pys: utt x olen ys_in_pad = pad_list ( ys_in , self . eos ) ys_out_pad = pad_list ( ys_out , self . ignore_id ) # get length info olength = ys_out_pad . size ( 1 ) # initialization c_list = [ self . zero_state ( hs_pad [ 0 ])] z_list = [ self . zero_state ( hs_pad [ 0 ])] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( hs_pad [ 0 ])) z_list . append ( self . zero_state ( hs_pad [ 0 ])) att_ws = [] if self . num_encs == 1 : att_w = None self . att [ att_idx ] . reset () # reset pre-computation of h else : att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han # pre-computation of embedding eys = self . dropout_emb ( self . embed ( ys_in_pad )) # utt x olen x zdim # loop for an output sequence for i in six . moves . range ( olength ): if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( hs_pad [ 0 ], hlen [ 0 ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w ) att_ws . append ( att_w ) else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( hs_pad [ idx ], hlen [ idx ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ idx ]) hs_pad_han = torch . stack ( att_c_list , dim = 1 ) hlen_han = [ self . num_encs ] * len ( ys_in ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( hs_pad_han , hlen_han , self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ self . num_encs ]) att_ws . append ( att_w_list ) ey = torch . cat (( eys [:, i , :], att_c ), dim = 1 ) # utt x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , z_list , c_list ) if self . num_encs == 1 : # convert to numpy array with the shape (B, Lmax, Tmax) att_ws = att_to_numpy ( att_ws , self . att [ att_idx ]) else : _att_ws = [] for idx , ws in enumerate ( zip ( * att_ws )): ws = att_to_numpy ( ws , self . att [ idx ]) _att_ws . append ( ws ) att_ws = _att_ws return att_ws forward ( self , hs_pad , hlens , ys_pad , strm_idx = 0 , lang_ids = None ) Decoder forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index indicates the index of decoding stream. :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention loss value :rtype: torch.Tensor :return: accuracy :rtype: float Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def forward ( self , hs_pad , hlens , ys_pad , strm_idx = 0 , lang_ids = None ): \"\"\"Decoder forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index indicates the index of decoding stream. :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention loss value :rtype: torch.Tensor :return: accuracy :rtype: float \"\"\" # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : hs_pad = [ hs_pad ] hlens = [ hlens ] # TODO(kan-bayashi): need to make more smart way ys = [ y [ y != self . ignore_id ] for y in ys_pad ] # parse padded ys # attention index for the attention module # in SPA (speaker parallel attention), att_idx is used to select attention module. In other cases, it is 0. att_idx = min ( strm_idx , len ( self . att ) - 1 ) # hlens should be list of list of integer hlens = [ list ( map ( int , hlens [ idx ])) for idx in range ( self . num_encs )] self . loss = None # prepare input and output word sequences with sos/eos IDs eos = ys [ 0 ] . new ([ self . eos ]) sos = ys [ 0 ] . new ([ self . sos ]) if self . replace_sos : ys_in = [ torch . cat ([ idx , y ], dim = 0 ) for idx , y in zip ( lang_ids , ys )] else : ys_in = [ torch . cat ([ sos , y ], dim = 0 ) for y in ys ] ys_out = [ torch . cat ([ y , eos ], dim = 0 ) for y in ys ] # padding for ys with -1 # pys: utt x olen ys_in_pad = pad_list ( ys_in , self . eos ) ys_out_pad = pad_list ( ys_out , self . ignore_id ) # get dim, length info batch = ys_out_pad . size ( 0 ) olength = ys_out_pad . size ( 1 ) for idx in range ( self . num_encs ): logging . info ( self . __class__ . __name__ + 'Number of Encoder: {} ; enc {} : input lengths: {} .' . format ( self . num_encs , idx + 1 , hlens [ idx ])) logging . info ( self . __class__ . __name__ + ' output lengths: ' + str ([ y . size ( 0 ) for y in ys_out ])) # initialization c_list = [ self . zero_state ( hs_pad [ 0 ])] z_list = [ self . zero_state ( hs_pad [ 0 ])] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( hs_pad [ 0 ])) z_list . append ( self . zero_state ( hs_pad [ 0 ])) z_all = [] if self . num_encs == 1 : att_w = None self . att [ att_idx ] . reset () # reset pre-computation of h else : att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han # pre-computation of embedding eys = self . dropout_emb ( self . embed ( ys_in_pad )) # utt x olen x zdim # loop for an output sequence for i in six . moves . range ( olength ): if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( hs_pad [ 0 ], hlens [ 0 ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w ) else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( hs_pad [ idx ], hlens [ idx ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ idx ]) hs_pad_han = torch . stack ( att_c_list , dim = 1 ) hlens_han = [ self . num_encs ] * len ( ys_in ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( hs_pad_han , hlens_han , self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ self . num_encs ]) if i > 0 and random . random () < self . sampling_probability : logging . info ( ' scheduled sampling ' ) z_out = self . output ( z_all [ - 1 ]) z_out = np . argmax ( z_out . detach () . cpu (), axis = 1 ) z_out = self . dropout_emb ( self . embed ( to_device ( self , z_out ))) ey = torch . cat (( z_out , att_c ), dim = 1 ) # utt x (zdim + hdim) else : ey = torch . cat (( eys [:, i , :], att_c ), dim = 1 ) # utt x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , z_list , c_list ) if self . context_residual : z_all . append ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) # utt x (zdim + hdim) else : z_all . append ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) # utt x (zdim) z_all = torch . stack ( z_all , dim = 1 ) . view ( batch * olength , - 1 ) # compute loss y_all = self . output ( z_all ) if LooseVersion ( torch . __version__ ) < LooseVersion ( '1.0' ): reduction_str = 'elementwise_mean' else : reduction_str = 'mean' self . loss = F . cross_entropy ( y_all , ys_out_pad . view ( - 1 ), ignore_index = self . ignore_id , reduction = reduction_str ) # compute perplexity ppl = math . exp ( self . loss . item ()) # -1: eos, which is removed in the loss computation self . loss *= ( np . mean ([ len ( x ) for x in ys_in ]) - 1 ) acc = th_accuracy ( y_all , ys_out_pad , ignore_label = self . ignore_id ) logging . info ( 'att loss:' + '' . join ( str ( self . loss . item ()) . split ( ' \\n ' ))) # show predicted character sequence for debug if self . verbose > 0 and self . char_list is not None : ys_hat = y_all . view ( batch , olength , - 1 ) ys_true = ys_out_pad for ( i , y_hat ), y_true in zip ( enumerate ( ys_hat . detach () . cpu () . numpy ()), ys_true . detach () . cpu () . numpy ()): if i == MAX_DECODER_OUTPUT : break idx_hat = np . argmax ( y_hat [ y_true != self . ignore_id ], axis = 1 ) idx_true = y_true [ y_true != self . ignore_id ] seq_hat = [ self . char_list [ int ( idx )] for idx in idx_hat ] seq_true = [ self . char_list [ int ( idx )] for idx in idx_true ] seq_hat = \"\" . join ( seq_hat ) seq_true = \"\" . join ( seq_true ) logging . info ( \"groundtruth[ %d ]: \" % i + seq_true ) logging . info ( \"prediction [ %d ]: \" % i + seq_hat ) if self . labeldist is not None : if self . vlabeldist is None : self . vlabeldist = to_device ( self , torch . from_numpy ( self . labeldist )) loss_reg = - torch . sum (( F . log_softmax ( y_all , dim = 1 ) * self . vlabeldist ) . view ( - 1 ), dim = 0 ) / len ( ys_in ) self . loss = ( 1. - self . lsm_weight ) * self . loss + self . lsm_weight * loss_reg return self . loss , acc , ppl init_state ( self , x ) Get an initial state for decoding (optional). Parameters: Name Type Description Default x torch.Tensor The encoded feature tensor required Returns: initial state Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 def init_state ( self , x ): # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : x = [ x ] c_list = [ self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))] z_list = [ self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))) z_list . append ( self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))) # TODO(karita): support strm_index for `asr_mix` strm_index = 0 att_idx = min ( strm_index , len ( self . att ) - 1 ) if self . num_encs == 1 : a = None self . att [ att_idx ] . reset () # reset pre-computation of h else : a = [ None ] * ( self . num_encs + 1 ) # atts + han for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han return dict ( c_prev = c_list [:], z_prev = z_list [:], a_prev = a , workspace = ( att_idx , z_list , c_list )) recognize_beam ( self , h , lpz , recog_args , char_list , rnnlm = None , strm_idx = 0 ) beam search implementation :param torch.Tensor h: encoder hidden state (T, eprojs) [in multi-encoder case, list of torch.Tensor, [(T1, eprojs), (T2, eprojs), ...] ] :param torch.Tensor lpz: ctc log softmax output (T, odim) [in multi-encoder case, list of torch.Tensor, [(T1, odim), (T2, odim), ...] ] :param Namespace recog_args: argument Namespace containing options :param char_list: list of character strings :param torch.nn.Module rnnlm: language module :param int strm_idx: stream index for speaker parallel attention in multi-speaker case :return: N-best decoding results :rtype: list of dicts Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def recognize_beam ( self , h , lpz , recog_args , char_list , rnnlm = None , strm_idx = 0 ): \"\"\"beam search implementation :param torch.Tensor h: encoder hidden state (T, eprojs) [in multi-encoder case, list of torch.Tensor, [(T1, eprojs), (T2, eprojs), ...] ] :param torch.Tensor lpz: ctc log softmax output (T, odim) [in multi-encoder case, list of torch.Tensor, [(T1, odim), (T2, odim), ...] ] :param Namespace recog_args: argument Namespace containing options :param char_list: list of character strings :param torch.nn.Module rnnlm: language module :param int strm_idx: stream index for speaker parallel attention in multi-speaker case :return: N-best decoding results :rtype: list of dicts \"\"\" # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : h = [ h ] lpz = [ lpz ] if self . num_encs > 1 and lpz is None : lpz = [ lpz ] * self . num_encs for idx in range ( self . num_encs ): logging . info ( 'Number of Encoder: {} ; enc {} : input lengths: {} .' . format ( self . num_encs , idx + 1 , h [ 0 ] . size ( 0 ))) att_idx = min ( strm_idx , len ( self . att ) - 1 ) # initialization c_list = [ self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))] z_list = [ self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))) z_list . append ( self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))) if self . num_encs == 1 : a = None self . att [ att_idx ] . reset () # reset pre-computation of h else : a = [ None ] * ( self . num_encs + 1 ) # atts + han att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han # search parms beam = recog_args . beam_size penalty = recog_args . penalty ctc_weight = getattr ( recog_args , \"ctc_weight\" , False ) # for NMT if lpz [ 0 ] is not None and self . num_encs > 1 : # weights-ctc, e.g. ctc_loss = w_1*ctc_1_loss + w_2 * ctc_2_loss + w_N * ctc_N_loss weights_ctc_dec = recog_args . weights_ctc_dec / np . sum ( recog_args . weights_ctc_dec ) # normalize logging . info ( 'ctc weights (decoding): ' + ' ' . join ([ str ( x ) for x in weights_ctc_dec ])) else : weights_ctc_dec = [ 1.0 ] # preprate sos if self . replace_sos and recog_args . tgt_lang : y = char_list . index ( recog_args . tgt_lang ) else : y = self . sos logging . info ( '<sos> index: ' + str ( y )) logging . info ( '<sos> mark: ' + char_list [ y ]) vy = h [ 0 ] . new_zeros ( 1 ) . long () maxlen = np . amin ([ h [ idx ] . size ( 0 ) for idx in range ( self . num_encs )]) if recog_args . maxlenratio != 0 : # maxlen >= 1 maxlen = max ( 1 , int ( recog_args . maxlenratio * maxlen )) minlen = int ( recog_args . minlenratio * maxlen ) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # initialize hypothesis if rnnlm : hyp = { 'score' : 0.0 , 'yseq' : [ y ], 'c_prev' : c_list , 'z_prev' : z_list , 'a_prev' : a , 'rnnlm_prev' : None } else : hyp = { 'score' : 0.0 , 'yseq' : [ y ], 'c_prev' : c_list , 'z_prev' : z_list , 'a_prev' : a } if lpz [ 0 ] is not None : ctc_prefix_score = [ CTCPrefixScore ( lpz [ idx ] . detach () . numpy (), 0 , self . eos , np ) for idx in range ( self . num_encs )] hyp [ 'ctc_state_prev' ] = [ ctc_prefix_score [ idx ] . initial_state () for idx in range ( self . num_encs )] hyp [ 'ctc_score_prev' ] = [ 0.0 ] * self . num_encs if ctc_weight != 1.0 : # pre-pruning based on attention scores ctc_beam = min ( lpz [ 0 ] . shape [ - 1 ], int ( beam * CTC_SCORING_RATIO )) else : ctc_beam = lpz [ 0 ] . shape [ - 1 ] hyps = [ hyp ] ended_hyps = [] for i in six . moves . range ( maxlen ): logging . debug ( 'position ' + str ( i )) hyps_best_kept = [] for hyp in hyps : vy . unsqueeze ( 1 ) vy [ 0 ] = hyp [ 'yseq' ][ i ] ey = self . dropout_emb ( self . embed ( vy )) # utt list (1) x zdim ey . unsqueeze ( 0 ) if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( h [ 0 ] . unsqueeze ( 0 ), [ h [ 0 ] . size ( 0 )], self . dropout_dec [ 0 ]( hyp [ 'z_prev' ][ 0 ]), hyp [ 'a_prev' ]) else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( h [ idx ] . unsqueeze ( 0 ), [ h [ idx ] . size ( 0 )], self . dropout_dec [ 0 ]( hyp [ 'z_prev' ][ 0 ]), hyp [ 'a_prev' ][ idx ]) h_han = torch . stack ( att_c_list , dim = 1 ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( h_han , [ self . num_encs ], self . dropout_dec [ 0 ]( hyp [ 'z_prev' ][ 0 ]), hyp [ 'a_prev' ][ self . num_encs ]) ey = torch . cat (( ey , att_c ), dim = 1 ) # utt(1) x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , hyp [ 'z_prev' ], hyp [ 'c_prev' ]) # get nbest local scores and their ids if self . context_residual : logits = self . output ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) else : logits = self . output ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) local_att_scores = F . log_softmax ( logits , dim = 1 ) if rnnlm : rnnlm_state , local_lm_scores = rnnlm . predict ( hyp [ 'rnnlm_prev' ], vy ) local_scores = local_att_scores + recog_args . lm_weight * local_lm_scores else : local_scores = local_att_scores if lpz [ 0 ] is not None : local_best_scores , local_best_ids = torch . topk ( local_att_scores , ctc_beam , dim = 1 ) ctc_scores , ctc_states = [ None ] * self . num_encs , [ None ] * self . num_encs for idx in range ( self . num_encs ): ctc_scores [ idx ], ctc_states [ idx ] = ctc_prefix_score [ idx ]( hyp [ 'yseq' ], local_best_ids [ 0 ], hyp [ 'ctc_state_prev' ][ idx ]) local_scores = \\ ( 1.0 - ctc_weight ) * local_att_scores [:, local_best_ids [ 0 ]] if self . num_encs == 1 : local_scores += ctc_weight * torch . from_numpy ( ctc_scores [ 0 ] - hyp [ 'ctc_score_prev' ][ 0 ]) else : for idx in range ( self . num_encs ): local_scores += ctc_weight * weights_ctc_dec [ idx ] * torch . from_numpy ( ctc_scores [ idx ] - hyp [ 'ctc_score_prev' ][ idx ]) if rnnlm : local_scores += recog_args . lm_weight * local_lm_scores [:, local_best_ids [ 0 ]] local_best_scores , joint_best_ids = torch . topk ( local_scores , beam , dim = 1 ) local_best_ids = local_best_ids [:, joint_best_ids [ 0 ]] else : local_best_scores , local_best_ids = torch . topk ( local_scores , beam , dim = 1 ) for j in six . moves . range ( beam ): new_hyp = {} # [:] is needed! new_hyp [ 'z_prev' ] = z_list [:] new_hyp [ 'c_prev' ] = c_list [:] if self . num_encs == 1 : new_hyp [ 'a_prev' ] = att_w [:] else : new_hyp [ 'a_prev' ] = [ att_w_list [ idx ][:] for idx in range ( self . num_encs + 1 )] new_hyp [ 'score' ] = hyp [ 'score' ] + local_best_scores [ 0 , j ] new_hyp [ 'yseq' ] = [ 0 ] * ( 1 + len ( hyp [ 'yseq' ])) new_hyp [ 'yseq' ][: len ( hyp [ 'yseq' ])] = hyp [ 'yseq' ] new_hyp [ 'yseq' ][ len ( hyp [ 'yseq' ])] = int ( local_best_ids [ 0 , j ]) if rnnlm : new_hyp [ 'rnnlm_prev' ] = rnnlm_state if lpz [ 0 ] is not None : new_hyp [ 'ctc_state_prev' ] = [ ctc_states [ idx ][ joint_best_ids [ 0 , j ]] for idx in range ( self . num_encs )] new_hyp [ 'ctc_score_prev' ] = [ ctc_scores [ idx ][ joint_best_ids [ 0 , j ]] for idx in range ( self . num_encs )] # will be (2 x beam) hyps at most hyps_best_kept . append ( new_hyp ) hyps_best_kept = sorted ( hyps_best_kept , key = lambda x : x [ 'score' ], reverse = True )[: beam ] # sort and get nbest hyps = hyps_best_kept logging . debug ( 'number of pruned hypotheses: ' + str ( len ( hyps ))) logging . debug ( 'best hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyps [ 0 ][ 'yseq' ][ 1 :]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( 'adding <eos> in the last position in the loop' ) for hyp in hyps : hyp [ 'yseq' ] . append ( self . eos ) # add ended hypotheses to a final list, and removed them from current hypotheses # (this will be a problem, number of hyps < beam) remained_hyps = [] for hyp in hyps : if hyp [ 'yseq' ][ - 1 ] == self . eos : # only store the sequence that has more than minlen outputs # also add penalty if len ( hyp [ 'yseq' ]) > minlen : hyp [ 'score' ] += ( i + 1 ) * penalty if rnnlm : # Word LM needs to add final <eos> score hyp [ 'score' ] += recog_args . lm_weight * rnnlm . final ( hyp [ 'rnnlm_prev' ]) ended_hyps . append ( hyp ) else : remained_hyps . append ( hyp ) # end detection if end_detect ( ended_hyps , i ) and recog_args . maxlenratio == 0.0 : logging . info ( 'end detected at %d ' , i ) break hyps = remained_hyps if len ( hyps ) > 0 : logging . debug ( 'remaining hypotheses: ' + str ( len ( hyps ))) else : logging . info ( 'no hypothesis. Finish decoding.' ) break for hyp in hyps : logging . debug ( 'hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyp [ 'yseq' ][ 1 :]])) logging . debug ( 'number of ended hypotheses: ' + str ( len ( ended_hyps ))) nbest_hyps = sorted ( ended_hyps , key = lambda x : x [ 'score' ], reverse = True )[: min ( len ( ended_hyps ), recog_args . nbest )] # check number of hypotheses if len ( nbest_hyps ) == 0 : logging . warning ( 'there is no N-best results, perform recognition again with smaller minlenratio.' ) # should copy because Namespace will be overwritten globally recog_args = Namespace ( ** vars ( recog_args )) recog_args . minlenratio = max ( 0.0 , recog_args . minlenratio - 0.1 ) if self . num_encs == 1 : return self . recognize_beam ( h [ 0 ], lpz [ 0 ], recog_args , char_list , rnnlm ) else : return self . recognize_beam ( h , lpz , recog_args , char_list , rnnlm ) logging . info ( 'total log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ])) logging . info ( 'normalized log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ] / len ( nbest_hyps [ 0 ][ 'yseq' ]))) # remove sos return nbest_hyps recognize_beam_batch ( self , h , hlens , lpz , recog_args , char_list , rnnlm = None , normalize_score = True , strm_idx = 0 , lang_ids = None ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 def recognize_beam_batch ( self , h , hlens , lpz , recog_args , char_list , rnnlm = None , normalize_score = True , strm_idx = 0 , lang_ids = None ): # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : h = [ h ] hlens = [ hlens ] lpz = [ lpz ] if self . num_encs > 1 and lpz is None : lpz = [ lpz ] * self . num_encs att_idx = min ( strm_idx , len ( self . att ) - 1 ) for idx in range ( self . num_encs ): logging . info ( 'Number of Encoder: {} ; enc {} : input lengths: {} .' . format ( self . num_encs , idx + 1 , h [ idx ] . size ( 1 ))) h [ idx ] = mask_by_length ( h [ idx ], hlens [ idx ], 0.0 ) # search params batch = len ( hlens [ 0 ]) beam = recog_args . beam_size penalty = recog_args . penalty ctc_weight = getattr ( recog_args , \"ctc_weight\" , 0 ) # for NMT att_weight = 1.0 - ctc_weight ctc_margin = getattr ( recog_args , \"ctc_window_margin\" , 0 ) # use getattr to keep compatibility # weights-ctc, e.g. ctc_loss = w_1*ctc_1_loss + w_2 * ctc_2_loss + w_N * ctc_N_loss if lpz [ 0 ] is not None and self . num_encs > 1 : weights_ctc_dec = recog_args . weights_ctc_dec / np . sum ( recog_args . weights_ctc_dec ) # normalize logging . info ( 'ctc weights (decoding): ' + ' ' . join ([ str ( x ) for x in weights_ctc_dec ])) else : weights_ctc_dec = [ 1.0 ] n_bb = batch * beam pad_b = to_device ( self , torch . arange ( batch ) * beam ) . view ( - 1 , 1 ) max_hlen = np . amin ([ max ( hlens [ idx ]) for idx in range ( self . num_encs )]) if recog_args . maxlenratio == 0 : maxlen = max_hlen else : maxlen = max ( 1 , int ( recog_args . maxlenratio * max_hlen )) minlen = int ( recog_args . minlenratio * max_hlen ) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # initialization c_prev = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] z_prev = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] c_list = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] z_list = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] vscores = to_device ( self , torch . zeros ( batch , beam )) rnnlm_state = None if self . num_encs == 1 : a_prev = [ None ] att_w_list , ctc_scorer , ctc_state = [ None ], [ None ], [ None ] self . att [ att_idx ] . reset () # reset pre-computation of h else : a_prev = [ None ] * ( self . num_encs + 1 ) # atts + han att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts ctc_scorer , ctc_state = [ None ] * ( self . num_encs ), [ None ] * ( self . num_encs ) for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han if self . replace_sos and recog_args . tgt_lang : logging . info ( '<sos> index: ' + str ( char_list . index ( recog_args . tgt_lang ))) logging . info ( '<sos> mark: ' + recog_args . tgt_lang ) yseq = [[ char_list . index ( recog_args . tgt_lang )] for _ in six . moves . range ( n_bb )] elif lang_ids is not None : # NOTE: used for evaluation during training yseq = [[ lang_ids [ b // recog_args . beam_size ]] for b in six . moves . range ( n_bb )] else : logging . info ( '<sos> index: ' + str ( self . sos )) logging . info ( '<sos> mark: ' + char_list [ self . sos ]) yseq = [[ self . sos ] for _ in six . moves . range ( n_bb )] accum_odim_ids = [ self . sos for _ in six . moves . range ( n_bb )] stop_search = [ False for _ in six . moves . range ( batch )] nbest_hyps = [[] for _ in six . moves . range ( batch )] ended_hyps = [[] for _ in range ( batch )] exp_hlens = [ hlens [ idx ] . repeat ( beam ) . view ( beam , batch ) . transpose ( 0 , 1 ) . contiguous () for idx in range ( self . num_encs )] exp_hlens = [ exp_hlens [ idx ] . view ( - 1 ) . tolist () for idx in range ( self . num_encs )] exp_h = [ h [ idx ] . unsqueeze ( 1 ) . repeat ( 1 , beam , 1 , 1 ) . contiguous () for idx in range ( self . num_encs )] exp_h = [ exp_h [ idx ] . view ( n_bb , h [ idx ] . size ()[ 1 ], h [ idx ] . size ()[ 2 ]) for idx in range ( self . num_encs )] if lpz [ 0 ] is not None : scoring_ratio = CTC_SCORING_RATIO if att_weight > 0.0 and not lpz [ 0 ] . is_cuda else 0 ctc_scorer = [ CTCPrefixScoreTH ( lpz [ idx ], hlens [ idx ], 0 , self . eos , beam , scoring_ratio , margin = ctc_margin ) for idx in range ( self . num_encs )] for i in six . moves . range ( maxlen ): logging . debug ( 'position ' + str ( i )) vy = to_device ( self , torch . LongTensor ( self . _get_last_yseq ( yseq ))) ey = self . dropout_emb ( self . embed ( vy )) if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( exp_h [ 0 ], exp_hlens [ 0 ], self . dropout_dec [ 0 ]( z_prev [ 0 ]), a_prev [ 0 ]) att_w_list = [ att_w ] else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( exp_h [ idx ], exp_hlens [ idx ], self . dropout_dec [ 0 ]( z_prev [ 0 ]), a_prev [ idx ]) exp_h_han = torch . stack ( att_c_list , dim = 1 ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( exp_h_han , [ self . num_encs ] * n_bb , self . dropout_dec [ 0 ]( z_prev [ 0 ]), a_prev [ self . num_encs ]) ey = torch . cat (( ey , att_c ), dim = 1 ) # attention decoder z_list , c_list = self . rnn_forward ( ey , z_list , c_list , z_prev , c_prev ) if self . context_residual : logits = self . output ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) else : logits = self . output ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) local_scores = att_weight * F . log_softmax ( logits , dim = 1 ) # rnnlm if rnnlm : rnnlm_state , local_lm_scores = rnnlm . buff_predict ( rnnlm_state , vy , n_bb ) local_scores = local_scores + recog_args . lm_weight * local_lm_scores # ctc if ctc_scorer [ 0 ]: for idx in range ( self . num_encs ): att_w = att_w_list [ idx ] att_w_ = att_w if isinstance ( att_w , torch . Tensor ) else att_w [ 0 ] ctc_state [ idx ], local_ctc_scores = ctc_scorer [ idx ]( yseq , ctc_state [ idx ], local_scores , att_w_ ) local_scores = local_scores + ctc_weight * weights_ctc_dec [ idx ] * local_ctc_scores local_scores = local_scores . view ( batch , beam , self . odim ) if i == 0 : local_scores [:, 1 :, :] = self . logzero # accumulate scores eos_vscores = local_scores [:, :, self . eos ] + vscores vscores = vscores . view ( batch , beam , 1 ) . repeat ( 1 , 1 , self . odim ) vscores [:, :, self . eos ] = self . logzero vscores = ( vscores + local_scores ) . view ( batch , - 1 ) # global pruning accum_best_scores , accum_best_ids = torch . topk ( vscores , beam , 1 ) accum_odim_ids = torch . fmod ( accum_best_ids , self . odim ) . view ( - 1 ) . data . cpu () . tolist () accum_padded_beam_ids = ( torch . div ( accum_best_ids , self . odim ) + pad_b ) . view ( - 1 ) . data . cpu () . tolist () y_prev = yseq [:][:] yseq = self . _index_select_list ( yseq , accum_padded_beam_ids ) yseq = self . _append_ids ( yseq , accum_odim_ids ) vscores = accum_best_scores vidx = to_device ( self , torch . LongTensor ( accum_padded_beam_ids )) a_prev = [] num_atts = self . num_encs if self . num_encs == 1 else self . num_encs + 1 for idx in range ( num_atts ): if isinstance ( att_w_list [ idx ], torch . Tensor ): _a_prev = torch . index_select ( att_w_list [ idx ] . view ( n_bb , * att_w_list [ idx ] . shape [ 1 :]), 0 , vidx ) elif isinstance ( att_w_list [ idx ], list ): # handle the case of multi-head attention _a_prev = [ torch . index_select ( att_w_one . view ( n_bb , - 1 ), 0 , vidx ) for att_w_one in att_w_list [ idx ]] else : # handle the case of location_recurrent when return is a tuple _a_prev_ = torch . index_select ( att_w_list [ idx ][ 0 ] . view ( n_bb , - 1 ), 0 , vidx ) _h_prev_ = torch . index_select ( att_w_list [ idx ][ 1 ][ 0 ] . view ( n_bb , - 1 ), 0 , vidx ) _c_prev_ = torch . index_select ( att_w_list [ idx ][ 1 ][ 1 ] . view ( n_bb , - 1 ), 0 , vidx ) _a_prev = ( _a_prev_ , ( _h_prev_ , _c_prev_ )) a_prev . append ( _a_prev ) z_prev = [ torch . index_select ( z_list [ li ] . view ( n_bb , - 1 ), 0 , vidx ) for li in range ( self . dlayers )] c_prev = [ torch . index_select ( c_list [ li ] . view ( n_bb , - 1 ), 0 , vidx ) for li in range ( self . dlayers )] # pick ended hyps if i > minlen : k = 0 penalty_i = ( i + 1 ) * penalty thr = accum_best_scores [:, - 1 ] for samp_i in six . moves . range ( batch ): if stop_search [ samp_i ]: k = k + beam continue for beam_j in six . moves . range ( beam ): if eos_vscores [ samp_i , beam_j ] > thr [ samp_i ]: yk = y_prev [ k ][:] yk . append ( self . eos ) if len ( yk ) < min ( hlens [ idx ][ samp_i ] for idx in range ( self . num_encs )): _vscore = eos_vscores [ samp_i ][ beam_j ] + penalty_i if rnnlm : _vscore += recog_args . lm_weight * rnnlm . final ( rnnlm_state , index = k ) _score = _vscore . data . cpu () . numpy () ended_hyps [ samp_i ] . append ({ 'yseq' : yk , 'vscore' : _vscore , 'score' : _score }) k = k + 1 # end detection stop_search = [ stop_search [ samp_i ] or end_detect ( ended_hyps [ samp_i ], i ) for samp_i in six . moves . range ( batch )] stop_search_summary = list ( set ( stop_search )) if len ( stop_search_summary ) == 1 and stop_search_summary [ 0 ]: break if rnnlm : rnnlm_state = self . _index_select_lm_state ( rnnlm_state , 0 , vidx ) if ctc_scorer [ 0 ]: for idx in range ( self . num_encs ): ctc_state [ idx ] = ctc_scorer [ idx ] . index_select_state ( ctc_state [ idx ], accum_best_ids ) torch . cuda . empty_cache () dummy_hyps = [{ 'yseq' : [ self . sos , self . eos ], 'score' : np . array ([ - float ( 'inf' )])}] ended_hyps = [ ended_hyps [ samp_i ] if len ( ended_hyps [ samp_i ]) != 0 else dummy_hyps for samp_i in six . moves . range ( batch )] if normalize_score : for samp_i in six . moves . range ( batch ): for x in ended_hyps [ samp_i ]: x [ 'score' ] /= len ( x [ 'yseq' ]) nbest_hyps = [ sorted ( ended_hyps [ samp_i ], key = lambda x : x [ 'score' ], reverse = True )[: min ( len ( ended_hyps [ samp_i ]), recog_args . nbest )] for samp_i in six . moves . range ( batch )] return nbest_hyps rnn_forward ( self , ey , z_list , c_list , z_prev , c_prev ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 102 103 104 105 106 107 108 109 110 111 112 def rnn_forward ( self , ey , z_list , c_list , z_prev , c_prev ): if self . dtype == \"lstm\" : z_list [ 0 ], c_list [ 0 ] = self . decoder [ 0 ]( ey , ( z_prev [ 0 ], c_prev [ 0 ])) for l in six . moves . range ( 1 , self . dlayers ): z_list [ l ], c_list [ l ] = self . decoder [ l ]( self . dropout_dec [ l - 1 ]( z_list [ l - 1 ]), ( z_prev [ l ], c_prev [ l ])) else : z_list [ 0 ] = self . decoder [ 0 ]( ey , z_prev [ 0 ]) for l in six . moves . range ( 1 , self . dlayers ): z_list [ l ] = self . decoder [ l ]( self . dropout_dec [ l - 1 ]( z_list [ l - 1 ]), z_prev [ l ]) return z_list , c_list score ( self , yseq , state , x ) Score new token (required). Parameters: Name Type Description Default y torch.Tensor 1D torch.int64 prefix tokens. required state Scorer state for prefix tokens required x torch.Tensor The encoder feature that generates ys. required Returns: Type Description tuple[torch.Tensor, Any] Tuple of scores for next token that has a shape of (n_vocab) and next state for ys Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 def score ( self , yseq , state , x ): # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : x = [ x ] att_idx , z_list , c_list = state [ \"workspace\" ] vy = yseq [ - 1 ] . unsqueeze ( 0 ) ey = self . dropout_emb ( self . embed ( vy )) # utt list (1) x zdim if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( x [ 0 ] . unsqueeze ( 0 ), [ x [ 0 ] . size ( 0 )], self . dropout_dec [ 0 ]( state [ 'z_prev' ][ 0 ]), state [ 'a_prev' ]) else : att_w = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs ): att_c_list [ idx ], att_w [ idx ] = self . att [ idx ]( x [ idx ] . unsqueeze ( 0 ), [ x [ idx ] . size ( 0 )], self . dropout_dec [ 0 ]( state [ 'z_prev' ][ 0 ]), state [ 'a_prev' ][ idx ]) h_han = torch . stack ( att_c_list , dim = 1 ) att_c , att_w [ self . num_encs ] = self . att [ self . num_encs ]( h_han , [ self . num_encs ], self . dropout_dec [ 0 ]( state [ 'z_prev' ][ 0 ]), state [ 'a_prev' ][ self . num_encs ]) ey = torch . cat (( ey , att_c ), dim = 1 ) # utt(1) x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , state [ 'z_prev' ], state [ 'c_prev' ]) if self . context_residual : logits = self . output ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) else : logits = self . output ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) logp = F . log_softmax ( logits , dim = 1 ) . squeeze ( 0 ) return logp , dict ( c_prev = c_list [:], z_prev = z_list [:], a_prev = att_w , workspace = ( att_idx , z_list , c_list )) zero_state ( self , hs_pad ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 99 100 def zero_state ( self , hs_pad ): return hs_pad . new_zeros ( hs_pad . size ( 0 ), self . dunits ) decoder_for ( args , odim , sos , eos , att , labeldist ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 899 900 901 902 903 904 905 def decoder_for ( args , odim , sos , eos , att , labeldist ): return Decoder ( args . eprojs , odim , args . dtype , args . dlayers , args . dunits , sos , eos , att , args . verbose , args . char_list , labeldist , args . lsm_weight , args . sampling_probability , args . dropout_rate_decoder , getattr ( args , \"context_residual\" , False ), # use getattr to keep compatibility getattr ( args , \"replace_sos\" , False ), # use getattr to keep compatibility getattr ( args , \"num_encs\" , 1 )) # use getattr to keep compatibility encoders \u00b6 Encoder Encoder module :param str etype: type of encoder network :param int idim: number of dimensions of encoder network :param int elayers: number of layers of encoder network :param int eunits: number of lstm units of encoder network :param int eprojs: number of projection units of encoder network :param np.ndarray subsample: list of subsampling numbers :param float dropout: dropout rate :param int in_channel: number of input channels __init__ ( self , etype , idim , elayers , eunits , eprojs , subsample , dropout , in_channel = 1 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def __init__ ( self , etype , idim , elayers , eunits , eprojs , subsample , dropout , in_channel = 1 ): super ( Encoder , self ) . __init__ () typ = etype . lstrip ( \"vgg\" ) . rstrip ( \"p\" ) if typ not in [ 'lstm' , 'gru' , 'blstm' , 'bgru' ]: logging . error ( \"Error: need to specify an appropriate encoder architecture\" ) if etype . startswith ( \"vgg\" ): if etype [ - 1 ] == \"p\" : self . enc = torch . nn . ModuleList ([ VGG2L ( in_channel ), RNNP ( get_vgg2l_odim ( idim , in_channel = in_channel ), elayers , eunits , eprojs , subsample , dropout , typ = typ )]) logging . info ( 'Use CNN-VGG + ' + typ . upper () + 'P for encoder' ) else : self . enc = torch . nn . ModuleList ([ VGG2L ( in_channel ), RNN ( get_vgg2l_odim ( idim , in_channel = in_channel ), elayers , eunits , eprojs , dropout , typ = typ )]) logging . info ( 'Use CNN-VGG + ' + typ . upper () + ' for encoder' ) else : if etype [ - 1 ] == \"p\" : self . enc = torch . nn . ModuleList ( [ RNNP ( idim , elayers , eunits , eprojs , subsample , dropout , typ = typ )]) logging . info ( typ . upper () + ' with every-layer projection for encoder' ) else : self . enc = torch . nn . ModuleList ([ RNN ( idim , elayers , eunits , eprojs , dropout , typ = typ )]) logging . info ( typ . upper () + ' without projection for encoder' ) forward ( self , xs_pad , ilens , prev_states = None ) Encoder forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous encoder hidden states (?, ...) :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def forward ( self , xs_pad , ilens , prev_states = None ): \"\"\"Encoder forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous encoder hidden states (?, ...) :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor \"\"\" if prev_states is None : prev_states = [ None ] * len ( self . enc ) assert len ( prev_states ) == len ( self . enc ) current_states = [] for module , prev_state in zip ( self . enc , prev_states ): xs_pad , ilens , states = module ( xs_pad , ilens , prev_state = prev_state ) current_states . append ( states ) # make mask to remove bias value in padded part mask = to_device ( self , make_pad_mask ( ilens ) . unsqueeze ( - 1 )) return xs_pad . masked_fill ( mask , 0.0 ), ilens , current_states RNN RNN module :param int idim: dimension of inputs :param int elayers: number of encoder layers :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional) :param int hdim: number of final projection units :param float dropout: dropout rate :param str typ: The RNN type __init__ ( self , idim , elayers , cdim , hdim , dropout , typ = 'blstm' ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , idim , elayers , cdim , hdim , dropout , typ = \"blstm\" ): super ( RNN , self ) . __init__ () bidir = typ [ 0 ] == \"b\" self . nbrnn = torch . nn . LSTM ( idim , cdim , elayers , batch_first = True , dropout = dropout , bidirectional = bidir ) if \"lstm\" in typ \\ else torch . nn . GRU ( idim , cdim , elayers , batch_first = True , dropout = dropout , bidirectional = bidir ) if bidir : self . l_last = torch . nn . Linear ( cdim * 2 , hdim ) else : self . l_last = torch . nn . Linear ( cdim , hdim ) self . typ = typ forward ( self , xs_pad , ilens , prev_state = None ) RNN forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def forward ( self , xs_pad , ilens , prev_state = None ): \"\"\"RNN forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor \"\"\" logging . debug ( self . __class__ . __name__ + ' input lengths: ' + str ( ilens )) xs_pack = pack_padded_sequence ( xs_pad , ilens , batch_first = True ) self . nbrnn . flatten_parameters () if prev_state is not None and self . nbrnn . bidirectional : # We assume that when previous state is passed, it means that we're streaming the input # and therefore cannot propagate backward BRNN state (otherwise it goes in the wrong direction) prev_state = reset_backward_rnn_state ( prev_state ) ys , states = self . nbrnn ( xs_pack , hx = prev_state ) # ys: utt list of frame x cdim x 2 (2: means bidirectional) ys_pad , ilens = pad_packed_sequence ( ys , batch_first = True ) # (sum _utt frame_utt) x dim projected = torch . tanh ( self . l_last ( ys_pad . contiguous () . view ( - 1 , ys_pad . size ( 2 )))) xs_pad = projected . view ( ys_pad . size ( 0 ), ys_pad . size ( 1 ), - 1 ) return xs_pad , ilens , states # x: utt list of frame x dim RNNP RNN with projection layer module :param int idim: dimension of inputs :param int elayers: number of encoder layers :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional) :param int hdim: number of projection units :param np.ndarray subsample: list of subsampling numbers :param float dropout: dropout rate :param str typ: The RNN type __init__ ( self , idim , elayers , cdim , hdim , subsample , dropout , typ = 'blstm' ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , idim , elayers , cdim , hdim , subsample , dropout , typ = \"blstm\" ): super ( RNNP , self ) . __init__ () bidir = typ [ 0 ] == \"b\" for i in six . moves . range ( elayers ): if i == 0 : inputdim = idim else : inputdim = hdim rnn = torch . nn . LSTM ( inputdim , cdim , dropout = dropout , num_layers = 1 , bidirectional = bidir , batch_first = True ) if \"lstm\" in typ \\ else torch . nn . GRU ( inputdim , cdim , dropout = dropout , num_layers = 1 , bidirectional = bidir , batch_first = True ) setattr ( self , \" %s%d \" % ( \"birnn\" if bidir else \"rnn\" , i ), rnn ) # bottleneck layer to merge if bidir : setattr ( self , \"bt %d \" % i , torch . nn . Linear ( 2 * cdim , hdim )) else : setattr ( self , \"bt %d \" % i , torch . nn . Linear ( cdim , hdim )) self . elayers = elayers self . cdim = cdim self . subsample = subsample self . typ = typ self . bidir = bidir forward ( self , xs_pad , ilens , prev_state = None ) RNNP forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, hdim) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def forward ( self , xs_pad , ilens , prev_state = None ): \"\"\"RNNP forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, hdim) :rtype: torch.Tensor \"\"\" logging . debug ( self . __class__ . __name__ + ' input lengths: ' + str ( ilens )) elayer_states = [] for layer in six . moves . range ( self . elayers ): xs_pack = pack_padded_sequence ( xs_pad , ilens , batch_first = True ) rnn = getattr ( self , ( \"birnn\" if self . bidir else \"rnn\" ) + str ( layer )) rnn . flatten_parameters () if prev_state is not None and rnn . bidirectional : prev_state = reset_backward_rnn_state ( prev_state ) ys , states = rnn ( xs_pack , hx = None if prev_state is None else prev_state [ layer ]) elayer_states . append ( states ) # ys: utt list of frame x cdim x 2 (2: means bidirectional) ys_pad , ilens = pad_packed_sequence ( ys , batch_first = True ) sub = self . subsample [ layer + 1 ] if sub > 1 : ys_pad = ys_pad [:, :: sub ] ilens = [ int ( i + 1 ) // sub for i in ilens ] # (sum _utt frame_utt) x dim projected = getattr ( self , 'bt' + str ( layer ) )( ys_pad . contiguous () . view ( - 1 , ys_pad . size ( 2 ))) if layer == self . elayers - 1 : xs_pad = projected . view ( ys_pad . size ( 0 ), ys_pad . size ( 1 ), - 1 ) else : xs_pad = torch . tanh ( projected . view ( ys_pad . size ( 0 ), ys_pad . size ( 1 ), - 1 )) return xs_pad , ilens , elayer_states # x: utt list of frame x dim VGG2L VGG-like module :param int in_channel: number of input channels __init__ ( self , in_channel = 1 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 153 154 155 156 157 158 159 160 161 def __init__ ( self , in_channel = 1 ): super ( VGG2L , self ) . __init__ () # CNN layer (VGG motivated) self . conv1_1 = torch . nn . Conv2d ( in_channel , 64 , 3 , stride = 1 , padding = 1 ) self . conv1_2 = torch . nn . Conv2d ( 64 , 64 , 3 , stride = 1 , padding = 1 ) self . conv2_1 = torch . nn . Conv2d ( 64 , 128 , 3 , stride = 1 , padding = 1 ) self . conv2_2 = torch . nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ) self . in_channel = in_channel forward ( self , xs_pad , ilens , ** kwargs ) VGG2L forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :return: batch of padded hidden state sequences (B, Tmax // 4, 128 * D // 4) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def forward ( self , xs_pad , ilens , ** kwargs ): \"\"\"VGG2L forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :return: batch of padded hidden state sequences (B, Tmax // 4, 128 * D // 4) :rtype: torch.Tensor \"\"\" logging . debug ( self . __class__ . __name__ + ' input lengths: ' + str ( ilens )) # x: utt x frame x dim # xs_pad = F.pad_sequence(xs_pad) # x: utt x 1 (input channel num) x frame x dim xs_pad = xs_pad . view ( xs_pad . size ( 0 ), xs_pad . size ( 1 ), self . in_channel , xs_pad . size ( 2 ) // self . in_channel ) . transpose ( 1 , 2 ) # NOTE: max_pool1d ? xs_pad = F . relu ( self . conv1_1 ( xs_pad )) xs_pad = F . relu ( self . conv1_2 ( xs_pad )) xs_pad = F . max_pool2d ( xs_pad , 2 , stride = 2 , ceil_mode = True ) xs_pad = F . relu ( self . conv2_1 ( xs_pad )) xs_pad = F . relu ( self . conv2_2 ( xs_pad )) xs_pad = F . max_pool2d ( xs_pad , 2 , stride = 2 , ceil_mode = True ) if torch . is_tensor ( ilens ): ilens = ilens . cpu () . numpy () else : ilens = np . array ( ilens , dtype = np . float32 ) ilens = np . array ( np . ceil ( ilens / 2 ), dtype = np . int64 ) ilens = np . array ( np . ceil ( np . array ( ilens , dtype = np . float32 ) / 2 ), dtype = np . int64 ) . tolist () # x: utt_list of frame (remove zeropaded frames) x (input channel num x dim) xs_pad = xs_pad . transpose ( 1 , 2 ) xs_pad = xs_pad . contiguous () . view ( xs_pad . size ( 0 ), xs_pad . size ( 1 ), xs_pad . size ( 2 ) * xs_pad . size ( 3 )) return xs_pad , ilens , None # no state in this layer encoder_for ( args , idim , subsample ) Instantiates an encoder module given the program arguments :param Namespace args: The arguments :param int or List of integer idim: dimension of input, e.g. 83, or List of dimensions of inputs, e.g. [83,83] :param List or List of List subsample: subsample factors, e.g. [1,2,2,1,1], or List of subsample factors of each encoder. e.g. [[1,2,2,1,1], [1,2,2,1,1]] :rtype torch.nn.Module :return: The encoder module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def encoder_for ( args , idim , subsample ): \"\"\"Instantiates an encoder module given the program arguments :param Namespace args: The arguments :param int or List of integer idim: dimension of input, e.g. 83, or List of dimensions of inputs, e.g. [83,83] :param List or List of List subsample: subsample factors, e.g. [1,2,2,1,1], or List of subsample factors of each encoder. e.g. [[1,2,2,1,1], [1,2,2,1,1]] :rtype torch.nn.Module :return: The encoder module \"\"\" num_encs = getattr ( args , \"num_encs\" , 1 ) # use getattr to keep compatibility if num_encs == 1 : # compatible with single encoder asr mode return Encoder ( args . etype , idim , args . elayers , args . eunits , args . eprojs , subsample , args . dropout_rate ) elif num_encs >= 1 : enc_list = torch . nn . ModuleList () for idx in range ( num_encs ): enc = Encoder ( args . etype [ idx ], idim [ idx ], args . elayers [ idx ], args . eunits [ idx ], args . eprojs , subsample [ idx ], args . dropout_rate [ idx ]) enc_list . append ( enc ) return enc_list else : raise ValueError ( \"Number of encoders needs to be more than one. {} \" . format ( num_encs )) reset_backward_rnn_state ( states ) Sets backward BRNN states to zeroes - useful in processing of sliding windows over the inputs Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 137 138 139 140 141 142 143 144 def reset_backward_rnn_state ( states ): \"\"\"Sets backward BRNN states to zeroes - useful in processing of sliding windows over the inputs\"\"\" if isinstance ( states , ( list , tuple )): for state in states : state [ 1 :: 2 ] = 0. else : states [ 1 :: 2 ] = 0. return states streaming special \u00b6 segment \u00b6 SegmentStreamingE2E SegmentStreamingE2E constructor. :param E2E e2e: E2E ASR object :param recog_args: arguments for \"recognize\" method of E2E __init__ ( self , e2e , recog_args , rnnlm = None ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/segment.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , e2e , recog_args , rnnlm = None ): self . _e2e = e2e self . _recog_args = recog_args self . _char_list = e2e . char_list self . _rnnlm = rnnlm self . _e2e . eval () self . _blank_idx_in_char_list = - 1 for idx in range ( len ( self . _char_list )): if self . _char_list [ idx ] == self . _e2e . blank : self . _blank_idx_in_char_list = idx break self . _subsampling_factor = np . prod ( e2e . subsample ) self . _activates = 0 self . _blank_dur = 0 self . _previous_input = [] self . _previous_encoder_recurrent_state = None self . _encoder_states = [] self . _ctc_posteriors = [] assert self . _recog_args . batchsize <= 1 , \\ \"SegmentStreamingE2E works only with batch size <= 1\" assert \"b\" not in self . _e2e . etype , \\ \"SegmentStreamingE2E works only with uni-directional encoders\" accept_input ( self , x ) Call this method each time a new batch of input is available. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/segment.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def accept_input ( self , x ): \"\"\"Call this method each time a new batch of input is available.\"\"\" self . _previous_input . extend ( x ) h , ilen = self . _e2e . subsample_frames ( x ) # Run encoder and apply greedy search on CTC softmax output h , _ , self . _previous_encoder_recurrent_state = self . _e2e . enc ( h . unsqueeze ( 0 ), ilen , self . _previous_encoder_recurrent_state ) z = self . _e2e . ctc . argmax ( h ) . squeeze ( 0 ) if self . _activates == 0 and z [ 0 ] != self . _blank_idx_in_char_list : self . _activates = 1 # Rerun encoder with zero state at onset of detection tail_len = self . _subsampling_factor * ( self . _recog_args . streaming_onset_margin + 1 ) h , ilen = self . _e2e . subsample_frames ( np . reshape ( self . _previous_input [ - tail_len :], [ - 1 , len ( self . _previous_input [ 0 ])])) h , _ , self . _previous_encoder_recurrent_state = self . _e2e . enc ( h . unsqueeze ( 0 ), ilen , None ) hyp = None if self . _activates == 1 : self . _encoder_states . extend ( h . squeeze ( 0 )) self . _ctc_posteriors . extend ( self . _e2e . ctc . log_softmax ( h ) . squeeze ( 0 )) if z [ 0 ] == self . _blank_idx_in_char_list : self . _blank_dur += 1 else : self . _blank_dur = 0 if self . _blank_dur >= self . _recog_args . streaming_min_blank_dur : seg_len = len ( self . _encoder_states ) - self . _blank_dur + self . _recog_args . streaming_offset_margin if seg_len > 0 : # Run decoder with a detected segment h = torch . cat ( self . _encoder_states [: seg_len ], dim = 0 ) . view ( - 1 , self . _encoder_states [ 0 ] . size ( 0 )) if self . _recog_args . ctc_weight > 0.0 : lpz = torch . cat ( self . _ctc_posteriors [: seg_len ], dim = 0 ) . view ( - 1 , self . _ctc_posteriors [ 0 ] . size ( 0 )) if self . _recog_args . batchsize > 0 : lpz = lpz . unsqueeze ( 0 ) normalize_score = False else : lpz = None normalize_score = True if self . _recog_args . batchsize == 0 : hyp = self . _e2e . dec . recognize_beam ( h , lpz , self . _recog_args , self . _char_list , self . _rnnlm ) else : hlens = torch . tensor ([ h . shape [ 0 ]]) hyp = self . _e2e . dec . recognize_beam_batch ( h . unsqueeze ( 0 ), hlens , lpz , self . _recog_args , self . _char_list , self . _rnnlm , normalize_score = normalize_score )[ 0 ] self . _activates = 0 self . _blank_dur = 0 tail_len = self . _subsampling_factor * self . _recog_args . streaming_onset_margin self . _previous_input = self . _previous_input [ - tail_len :] self . _encoder_states = [] self . _ctc_posteriors = [] return hyp window \u00b6 WindowStreamingE2E WindowStreamingE2E constructor. :param E2E e2e: E2E ASR object :param recog_args: arguments for \"recognize\" method of E2E __init__ ( self , e2e , recog_args , rnnlm = None ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/window.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , e2e , recog_args , rnnlm = None ): self . _e2e = e2e self . _recog_args = recog_args self . _char_list = e2e . char_list self . _rnnlm = rnnlm self . _e2e . eval () self . _offset = 0 self . _previous_encoder_recurrent_state = None self . _encoder_states = [] self . _ctc_posteriors = [] self . _last_recognition = None assert self . _recog_args . ctc_weight > 0.0 , \\ \"WindowStreamingE2E works only with combined CTC and attention decoders.\" accept_input ( self , x ) Call this method each time a new batch of input is available. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/window.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def accept_input ( self , x ): \"\"\"Call this method each time a new batch of input is available.\"\"\" h , ilen = self . _e2e . subsample_frames ( x ) # Streaming encoder h , _ , self . _previous_encoder_recurrent_state = self . _e2e . enc ( h . unsqueeze ( 0 ), ilen , self . _previous_encoder_recurrent_state ) self . _encoder_states . append ( h . squeeze ( 0 )) # CTC posteriors for the incoming audio self . _ctc_posteriors . append ( self . _e2e . ctc . log_softmax ( h ) . squeeze ( 0 )) decode_with_attention_offline ( self ) Run the attention decoder offline. Works even if the previous layers (encoder and CTC decoder) were being run in the online mode. This method should be run after all the audio has been consumed. This is used mostly to compare the results between offline and online implementation of the previous layers. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/window.py 65 66 67 68 69 70 71 72 73 74 def decode_with_attention_offline ( self ): \"\"\"Run the attention decoder offline. Works even if the previous layers (encoder and CTC decoder) were being run in the online mode. This method should be run after all the audio has been consumed. This is used mostly to compare the results between offline and online implementation of the previous layers. \"\"\" h , lpz = self . _input_window_for_decoder ( use_all = True ) return self . _e2e . dec . recognize_beam ( h , lpz , self . _recog_args , self . _char_list , self . _rnnlm ) tacotron2 special \u00b6 cbhg \u00b6 CBHG related modules. CBHG CBHG module to convert log Mel-filterbanks to linear spectrogram. This is a module of CBHG introduced in ` Tacotron : Towards End - to - End Speech Synthesis ` _ . The CBHG converts the sequence of log Mel - filterbanks into linear spectrogram . .. _ ` Tacotron : Towards End - to - End Speech Synthesis ` : https : // arxiv . org / abs / 1703 . 10135 __init__ ( self , idim , odim , conv_bank_layers = 8 , conv_bank_chans = 128 , conv_proj_filts = 3 , conv_proj_chans = 256 , highway_layers = 4 , highway_units = 128 , gru_units = 256 ) special Initialize CBHG module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required conv_bank_layers int The number of convolution bank layers. 8 conv_bank_chans int The number of channels in convolution bank. 128 conv_proj_filts int Kernel size of convolutional projection layer. 3 conv_proj_chans int The number of channels in convolutional projection layer. 256 highway_layers int The number of highway network layers. 4 highway_units int The number of highway network units. 128 gru_units int The number of GRU units (for both directions). 256 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , idim , odim , conv_bank_layers = 8 , conv_bank_chans = 128 , conv_proj_filts = 3 , conv_proj_chans = 256 , highway_layers = 4 , highway_units = 128 , gru_units = 256 ): \"\"\"Initialize CBHG module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. conv_bank_layers (int, optional): The number of convolution bank layers. conv_bank_chans (int, optional): The number of channels in convolution bank. conv_proj_filts (int, optional): Kernel size of convolutional projection layer. conv_proj_chans (int, optional): The number of channels in convolutional projection layer. highway_layers (int, optional): The number of highway network layers. highway_units (int, optional): The number of highway network units. gru_units (int, optional): The number of GRU units (for both directions). \"\"\" super ( CBHG , self ) . __init__ () self . idim = idim self . odim = odim self . conv_bank_layers = conv_bank_layers self . conv_bank_chans = conv_bank_chans self . conv_proj_filts = conv_proj_filts self . conv_proj_chans = conv_proj_chans self . highway_layers = highway_layers self . highway_units = highway_units self . gru_units = gru_units # define 1d convolution bank self . conv_bank = torch . nn . ModuleList () for k in range ( 1 , self . conv_bank_layers + 1 ): if k % 2 != 0 : padding = ( k - 1 ) // 2 else : padding = (( k - 1 ) // 2 , ( k - 1 ) // 2 + 1 ) self . conv_bank += [ torch . nn . Sequential ( torch . nn . ConstantPad1d ( padding , 0.0 ), torch . nn . Conv1d ( idim , self . conv_bank_chans , k , stride = 1 , padding = 0 , bias = True ), torch . nn . BatchNorm1d ( self . conv_bank_chans ), torch . nn . ReLU ())] # define max pooling (need padding for one-side to keep same length) self . max_pool = torch . nn . Sequential ( torch . nn . ConstantPad1d (( 0 , 1 ), 0.0 ), torch . nn . MaxPool1d ( 2 , stride = 1 )) # define 1d convolution projection self . projections = torch . nn . Sequential ( torch . nn . Conv1d ( self . conv_bank_chans * self . conv_bank_layers , self . conv_proj_chans , self . conv_proj_filts , stride = 1 , padding = ( self . conv_proj_filts - 1 ) // 2 , bias = True ), torch . nn . BatchNorm1d ( self . conv_proj_chans ), torch . nn . ReLU (), torch . nn . Conv1d ( self . conv_proj_chans , self . idim , self . conv_proj_filts , stride = 1 , padding = ( self . conv_proj_filts - 1 ) // 2 , bias = True ), torch . nn . BatchNorm1d ( self . idim ), ) # define highway network self . highways = torch . nn . ModuleList () self . highways += [ torch . nn . Linear ( idim , self . highway_units )] for _ in range ( self . highway_layers ): self . highways += [ HighwayNet ( self . highway_units )] # define bidirectional GRU self . gru = torch . nn . GRU ( self . highway_units , gru_units // 2 , num_layers = 1 , batch_first = True , bidirectional = True ) # define final projection self . output = torch . nn . Linear ( gru_units , odim , bias = True ) forward ( self , xs , ilens ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the padded sequences of inputs (B, Tmax, idim). required ilens LongTensor Batch of lengths of each input sequence (B,). required Returns: Type Description Tensor Batch of the padded sequence of outputs (B, Tmax, odim). LongTensor: Batch of lengths of each output sequence (B,). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def forward ( self , xs , ilens ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the padded sequences of inputs (B, Tmax, idim). ilens (LongTensor): Batch of lengths of each input sequence (B,). Return: Tensor: Batch of the padded sequence of outputs (B, Tmax, odim). LongTensor: Batch of lengths of each output sequence (B,). \"\"\" xs = xs . transpose ( 1 , 2 ) # (B, idim, Tmax) convs = [] for k in range ( self . conv_bank_layers ): convs += [ self . conv_bank [ k ]( xs )] convs = torch . cat ( convs , dim = 1 ) # (B, #CH * #BANK, Tmax) convs = self . max_pool ( convs ) convs = self . projections ( convs ) . transpose ( 1 , 2 ) # (B, Tmax, idim) xs = xs . transpose ( 1 , 2 ) + convs # + 1 for dimension adjustment layer for l in range ( self . highway_layers + 1 ): xs = self . highways [ l ]( xs ) # sort by length xs , ilens , sort_idx = self . _sort_by_length ( xs , ilens ) # total_length needs for DataParallel # (see https://github.com/pytorch/pytorch/pull/6327) total_length = xs . size ( 1 ) xs = pack_padded_sequence ( xs , ilens , batch_first = True ) self . gru . flatten_parameters () xs , _ = self . gru ( xs ) xs , ilens = pad_packed_sequence ( xs , batch_first = True , total_length = total_length ) # revert sorting by length xs , ilens = self . _revert_sort_by_length ( xs , ilens , sort_idx ) xs = self . output ( xs ) # (B, Tmax, odim) return xs , ilens inference ( self , x ) Inference. Parameters: Name Type Description Default x Tensor The sequences of inputs (T, idim). required Returns: Type Description Tensor The sequence of outputs (T, odim). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def inference ( self , x ): \"\"\"Inference. Args: x (Tensor): The sequences of inputs (T, idim). Return: Tensor: The sequence of outputs (T, odim). \"\"\" assert len ( x . size ()) == 2 xs = x . unsqueeze ( 0 ) ilens = x . new ([ x . size ( 0 )]) . long () return self . forward ( xs , ilens )[ 0 ][ 0 ] CBHGLoss Loss function module for CBHG. __init__ ( self , use_masking = True ) special Initialize CBHG loss module. Parameters: Name Type Description Default use_masking bool Whether to mask padded part in loss calculation. True Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 20 21 22 23 24 25 26 27 28 def __init__ ( self , use_masking = True ): \"\"\"Initialize CBHG loss module. Args: use_masking (bool): Whether to mask padded part in loss calculation. \"\"\" super ( CBHGLoss , self ) . __init__ () self . use_masking = use_masking forward ( self , cbhg_outs , spcs , olens ) Calculate forward propagation. Parameters: Name Type Description Default cbhg_outs Tensor Batch of CBHG outputs (B, Lmax, spc_dim). required spcs Tensor Batch of groundtruth of spectrogram (B, Lmax, spc_dim). required olens LongTensor Batch of the lengths of each sequence (B,). required Returns: Type Description Tensor L1 loss value Tensor: Mean square error loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , cbhg_outs , spcs , olens ): \"\"\"Calculate forward propagation. Args: cbhg_outs (Tensor): Batch of CBHG outputs (B, Lmax, spc_dim). spcs (Tensor): Batch of groundtruth of spectrogram (B, Lmax, spc_dim). olens (LongTensor): Batch of the lengths of each sequence (B,). Returns: Tensor: L1 loss value Tensor: Mean square error loss value. \"\"\" # perform masking for padded values if self . use_masking : mask = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( spcs . device ) spcs = spcs . masked_select ( mask ) cbhg_outs = cbhg_outs . masked_select ( mask ) # calculate loss cbhg_l1_loss = F . l1_loss ( cbhg_outs , spcs ) cbhg_mse_loss = F . mse_loss ( cbhg_outs , spcs ) return cbhg_l1_loss , cbhg_mse_loss HighwayNet Highway Network module. This is a module of Highway Network introduced in ` Highway Networks ` _ . .. _ ` Highway Networks ` : https : // arxiv . org / abs / 1505 . 00387 __init__ ( self , idim ) special Initialize Highway Network module. Parameters: Name Type Description Default idim int Dimension of the inputs. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def __init__ ( self , idim ): \"\"\"Initialize Highway Network module. Args: idim (int): Dimension of the inputs. \"\"\" super ( HighwayNet , self ) . __init__ () self . idim = idim self . projection = torch . nn . Sequential ( torch . nn . Linear ( idim , idim ), torch . nn . ReLU ()) self . gate = torch . nn . Sequential ( torch . nn . Linear ( idim , idim ), torch . nn . Sigmoid ()) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of inputs (B, ..., idim). required Returns: Type Description Tensor Batch of outputs, which are the same shape as inputs (B, ..., idim). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 238 239 240 241 242 243 244 245 246 247 248 249 250 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of inputs (B, ..., idim). Returns: Tensor: Batch of outputs, which are the same shape as inputs (B, ..., idim). \"\"\" proj = self . projection ( x ) gate = self . gate ( x ) return proj * gate + x * ( 1.0 - gate ) decoder \u00b6 Tacotron2 decoder related modules. Decoder Decoder module of Spectrogram prediction network. This is a module of decoder of Spectrogram prediction network in Tacotron2 , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . The decoder generates the sequence of features from the sequence of the hidden states . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , odim , att , dlayers = 2 , dunits = 1024 , prenet_layers = 2 , prenet_units = 256 , postnet_layers = 5 , postnet_chans = 512 , postnet_filts = 5 , output_activation_fn = None , cumulate_att_w = True , use_batch_norm = True , use_concate = True , dropout_rate = 0.5 , zoneout_rate = 0.1 , reduction_factor = 1 ) special Initialize Tacotron2 decoder module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required att torch.nn.Module Instance of attention class. required dlayers int The number of decoder lstm layers. 2 dunits int The number of decoder lstm units. 1024 prenet_layers int The number of prenet layers. 2 prenet_units int The number of prenet units. 256 postnet_layers int The number of postnet layers. 5 postnet_filts int The number of postnet filter size. 5 postnet_chans int The number of postnet filter channels. 512 output_activation_fn torch.nn.Module Activation function for outputs. None cumulate_att_w bool Whether to cumulate previous attention weight. True use_batch_norm bool Whether to use batch normalization. True use_concate bool Whether to concatenate encoder embedding with decoder lstm outputs. True dropout_rate float Dropout rate. 0.5 zoneout_rate float Zoneout rate. 0.1 reduction_factor int Reduction factor. 1 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __init__ ( self , idim , odim , att , dlayers = 2 , dunits = 1024 , prenet_layers = 2 , prenet_units = 256 , postnet_layers = 5 , postnet_chans = 512 , postnet_filts = 5 , output_activation_fn = None , cumulate_att_w = True , use_batch_norm = True , use_concate = True , dropout_rate = 0.5 , zoneout_rate = 0.1 , reduction_factor = 1 ): \"\"\"Initialize Tacotron2 decoder module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. att (torch.nn.Module): Instance of attention class. dlayers (int, optional): The number of decoder lstm layers. dunits (int, optional): The number of decoder lstm units. prenet_layers (int, optional): The number of prenet layers. prenet_units (int, optional): The number of prenet units. postnet_layers (int, optional): The number of postnet layers. postnet_filts (int, optional): The number of postnet filter size. postnet_chans (int, optional): The number of postnet filter channels. output_activation_fn (torch.nn.Module, optional): Activation function for outputs. cumulate_att_w (bool, optional): Whether to cumulate previous attention weight. use_batch_norm (bool, optional): Whether to use batch normalization. use_concate (bool, optional): Whether to concatenate encoder embedding with decoder lstm outputs. dropout_rate (float, optional): Dropout rate. zoneout_rate (float, optional): Zoneout rate. reduction_factor (int, optional): Reduction factor. \"\"\" super ( Decoder , self ) . __init__ () # store the hyperparameters self . idim = idim self . odim = odim self . att = att self . output_activation_fn = output_activation_fn self . cumulate_att_w = cumulate_att_w self . use_concate = use_concate self . reduction_factor = reduction_factor # check attention type if isinstance ( self . att , AttForwardTA ): self . use_att_extra_inputs = True else : self . use_att_extra_inputs = False # define lstm network prenet_units = prenet_units if prenet_layers != 0 else odim self . lstm = torch . nn . ModuleList () for layer in six . moves . range ( dlayers ): iunits = idim + prenet_units if layer == 0 else dunits lstm = torch . nn . LSTMCell ( iunits , dunits ) if zoneout_rate > 0.0 : lstm = ZoneOutCell ( lstm , zoneout_rate ) self . lstm += [ lstm ] # define prenet if prenet_layers > 0 : self . prenet = Prenet ( idim = odim , n_layers = prenet_layers , n_units = prenet_units , dropout_rate = dropout_rate ) else : self . prenet = None # define postnet if postnet_layers > 0 : self . postnet = Postnet ( idim = idim , odim = odim , n_layers = postnet_layers , n_chans = postnet_chans , n_filts = postnet_filts , use_batch_norm = use_batch_norm , dropout_rate = dropout_rate ) else : self . postnet = None # define projection layers iunits = idim + dunits if use_concate else dunits self . feat_out = torch . nn . Linear ( iunits , odim * reduction_factor , bias = False ) self . prob_out = torch . nn . Linear ( iunits , reduction_factor ) # initialize self . apply ( decoder_init ) calculate_all_attentions ( self , hs , hlens , ys ) Calculate all of the attention weights. Parameters: Name Type Description Default hs Tensor Batch of the sequences of padded hidden states (B, Tmax, idim). required hlens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of the sequences of padded target features (B, Lmax, odim). required Returns: Type Description numpy.ndarray Batch of attention weights (B, Lmax, Tmax). Note This computation is performed in teacher-forcing manner. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 def calculate_all_attentions ( self , hs , hlens , ys ): \"\"\"Calculate all of the attention weights. Args: hs (Tensor): Batch of the sequences of padded hidden states (B, Tmax, idim). hlens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of the sequences of padded target features (B, Lmax, odim). Returns: numpy.ndarray: Batch of attention weights (B, Lmax, Tmax). Note: This computation is performed in teacher-forcing manner. \"\"\" # thin out frames (B, Lmax, odim) -> (B, Lmax/r, odim) if self . reduction_factor > 1 : ys = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] # length list should be list of int hlens = list ( map ( int , hlens )) # initialize hidden states of decoder c_list = [ self . _zero_state ( hs )] z_list = [ self . _zero_state ( hs )] for _ in six . moves . range ( 1 , len ( self . lstm )): c_list += [ self . _zero_state ( hs )] z_list += [ self . _zero_state ( hs )] prev_out = hs . new_zeros ( hs . size ( 0 ), self . odim ) # initialize attention prev_att_w = None self . att . reset () # loop for an output sequence att_ws = [] for y in ys . transpose ( 0 , 1 ): if self . use_att_extra_inputs : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w , prev_out ) else : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w ) att_ws += [ att_w ] prenet_out = self . prenet ( prev_out ) if self . prenet is not None else prev_out xs = torch . cat ([ att_c , prenet_out ], dim = 1 ) z_list [ 0 ], c_list [ 0 ] = self . lstm [ 0 ]( xs , ( z_list [ 0 ], c_list [ 0 ])) for l in six . moves . range ( 1 , len ( self . lstm )): z_list [ l ], c_list [ l ] = self . lstm [ l ]( z_list [ l - 1 ], ( z_list [ l ], c_list [ l ])) prev_out = y # teacher forcing if self . cumulate_att_w and prev_att_w is not None : prev_att_w = prev_att_w + att_w # Note: error when use += else : prev_att_w = att_w att_ws = torch . stack ( att_ws , dim = 1 ) # (B, Lmax, Tmax) return att_ws forward ( self , hs , hlens , ys ) Calculate forward propagation. Parameters: Name Type Description Default hs Tensor Batch of the sequences of padded hidden states (B, Tmax, idim). required hlens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of the sequences of padded target features (B, Lmax, odim). required Returns: Type Description Tensor Batch of output tensors after postnet (B, Lmax, odim). Tensor: Batch of output tensors before postnet (B, Lmax, odim). Tensor: Batch of logits of stop prediction (B, Lmax). Tensor: Batch of attention weights (B, Lmax, Tmax). Note This computation is performed in teacher-forcing manner. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 def forward ( self , hs , hlens , ys ): \"\"\"Calculate forward propagation. Args: hs (Tensor): Batch of the sequences of padded hidden states (B, Tmax, idim). hlens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of the sequences of padded target features (B, Lmax, odim). Returns: Tensor: Batch of output tensors after postnet (B, Lmax, odim). Tensor: Batch of output tensors before postnet (B, Lmax, odim). Tensor: Batch of logits of stop prediction (B, Lmax). Tensor: Batch of attention weights (B, Lmax, Tmax). Note: This computation is performed in teacher-forcing manner. \"\"\" # thin out frames (B, Lmax, odim) -> (B, Lmax/r, odim) if self . reduction_factor > 1 : ys = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] # length list should be list of int hlens = list ( map ( int , hlens )) # initialize hidden states of decoder c_list = [ self . _zero_state ( hs )] z_list = [ self . _zero_state ( hs )] for _ in six . moves . range ( 1 , len ( self . lstm )): c_list += [ self . _zero_state ( hs )] z_list += [ self . _zero_state ( hs )] prev_out = hs . new_zeros ( hs . size ( 0 ), self . odim ) # initialize attention prev_att_w = None self . att . reset () # loop for an output sequence outs , logits , att_ws = [], [], [] for y in ys . transpose ( 0 , 1 ): if self . use_att_extra_inputs : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w , prev_out ) else : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w ) prenet_out = self . prenet ( prev_out ) if self . prenet is not None else prev_out xs = torch . cat ([ att_c , prenet_out ], dim = 1 ) z_list [ 0 ], c_list [ 0 ] = self . lstm [ 0 ]( xs , ( z_list [ 0 ], c_list [ 0 ])) for l in six . moves . range ( 1 , len ( self . lstm )): z_list [ l ], c_list [ l ] = self . lstm [ l ]( z_list [ l - 1 ], ( z_list [ l ], c_list [ l ])) zcs = torch . cat ([ z_list [ - 1 ], att_c ], dim = 1 ) if self . use_concate else z_list [ - 1 ] outs += [ self . feat_out ( zcs ) . view ( hs . size ( 0 ), self . odim , - 1 )] logits += [ self . prob_out ( zcs )] att_ws += [ att_w ] prev_out = y # teacher forcing if self . cumulate_att_w and prev_att_w is not None : prev_att_w = prev_att_w + att_w # Note: error when use += else : prev_att_w = att_w logits = torch . cat ( logits , dim = 1 ) # (B, Lmax) before_outs = torch . cat ( outs , dim = 2 ) # (B, odim, Lmax) att_ws = torch . stack ( att_ws , dim = 1 ) # (B, Lmax, Tmax) if self . reduction_factor > 1 : before_outs = before_outs . view ( before_outs . size ( 0 ), self . odim , - 1 ) # (B, odim, Lmax) if self . postnet is not None : after_outs = before_outs + self . postnet ( before_outs ) # (B, odim, Lmax) else : after_outs = before_outs before_outs = before_outs . transpose ( 2 , 1 ) # (B, Lmax, odim) after_outs = after_outs . transpose ( 2 , 1 ) # (B, Lmax, odim) logits = logits # apply activation function for scaling if self . output_activation_fn is not None : before_outs = self . output_activation_fn ( before_outs ) after_outs = self . output_activation_fn ( after_outs ) return after_outs , before_outs , logits , att_ws inference ( self , h , threshold = 0.5 , minlenratio = 0.0 , maxlenratio = 10.0 , use_att_constraint = False , backward_window = None , forward_window = None ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default h Tensor Input sequence of encoder hidden states (T, C). required threshold float Threshold to stop generation. 0.5 minlenratio float Minimum length ratio. If set to 1.0 and the length of input is 10, the minimum length of outputs will be 10 * 1 = 10. 0.0 minlenratio float Minimum length ratio. If set to 10 and the length of input is 10, the maximum length of outputs will be 10 * 10 = 100. 0.0 use_att_constraint bool Whether to apply attention constraint introduced in Deep Voice 3 _. False backward_window int Backward window size in attention constraint. None forward_window int Forward window size in attention constraint. None Returns: Type Description Tensor Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). Note This computation is performed in auto-regressive manner. .. _ Deep Voice 3 : https://arxiv.org/abs/1710.07654 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def inference ( self , h , threshold = 0.5 , minlenratio = 0.0 , maxlenratio = 10.0 , use_att_constraint = False , backward_window = None , forward_window = None ): \"\"\"Generate the sequence of features given the sequences of characters. Args: h (Tensor): Input sequence of encoder hidden states (T, C). threshold (float, optional): Threshold to stop generation. minlenratio (float, optional): Minimum length ratio. If set to 1.0 and the length of input is 10, the minimum length of outputs will be 10 * 1 = 10. minlenratio (float, optional): Minimum length ratio. If set to 10 and the length of input is 10, the maximum length of outputs will be 10 * 10 = 100. use_att_constraint (bool): Whether to apply attention constraint introduced in `Deep Voice 3`_. backward_window (int): Backward window size in attention constraint. forward_window (int): Forward window size in attention constraint. Returns: Tensor: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). Note: This computation is performed in auto-regressive manner. .. _`Deep Voice 3`: https://arxiv.org/abs/1710.07654 \"\"\" # setup assert len ( h . size ()) == 2 hs = h . unsqueeze ( 0 ) ilens = [ h . size ( 0 )] maxlen = int ( h . size ( 0 ) * maxlenratio ) minlen = int ( h . size ( 0 ) * minlenratio ) # initialize hidden states of decoder c_list = [ self . _zero_state ( hs )] z_list = [ self . _zero_state ( hs )] for _ in six . moves . range ( 1 , len ( self . lstm )): c_list += [ self . _zero_state ( hs )] z_list += [ self . _zero_state ( hs )] prev_out = hs . new_zeros ( 1 , self . odim ) # initialize attention prev_att_w = None self . att . reset () # setup for attention constraint if use_att_constraint : last_attended_idx = 0 else : last_attended_idx = None # loop for an output sequence idx = 0 outs , att_ws , probs = [], [], [] while True : # updated index idx += self . reduction_factor # decoder calculation if self . use_att_extra_inputs : att_c , att_w = self . att ( hs , ilens , z_list [ 0 ], prev_att_w , prev_out , last_attended_idx = last_attended_idx , backward_window = backward_window , forward_window = forward_window ) else : att_c , att_w = self . att ( hs , ilens , z_list [ 0 ], prev_att_w , last_attended_idx = last_attended_idx , backward_window = backward_window , forward_window = forward_window ) att_ws += [ att_w ] prenet_out = self . prenet ( prev_out ) if self . prenet is not None else prev_out xs = torch . cat ([ att_c , prenet_out ], dim = 1 ) z_list [ 0 ], c_list [ 0 ] = self . lstm [ 0 ]( xs , ( z_list [ 0 ], c_list [ 0 ])) for l in six . moves . range ( 1 , len ( self . lstm )): z_list [ l ], c_list [ l ] = self . lstm [ l ]( z_list [ l - 1 ], ( z_list [ l ], c_list [ l ])) zcs = torch . cat ([ z_list [ - 1 ], att_c ], dim = 1 ) if self . use_concate else z_list [ - 1 ] outs += [ self . feat_out ( zcs ) . view ( 1 , self . odim , - 1 )] # [(1, odim, r), ...] probs += [ torch . sigmoid ( self . prob_out ( zcs ))[ 0 ]] # [(r), ...] if self . output_activation_fn is not None : prev_out = self . output_activation_fn ( outs [ - 1 ][:, :, - 1 ]) # (1, odim) else : prev_out = outs [ - 1 ][:, :, - 1 ] # (1, odim) if self . cumulate_att_w and prev_att_w is not None : prev_att_w = prev_att_w + att_w # Note: error when use += else : prev_att_w = att_w if use_att_constraint : last_attended_idx = int ( att_w . argmax ()) # check whether to finish generation if int ( sum ( probs [ - 1 ] >= threshold )) > 0 or idx >= maxlen : # check mininum length if idx < minlen : continue outs = torch . cat ( outs , dim = 2 ) # (1, odim, L) if self . postnet is not None : outs = outs + self . postnet ( outs ) # (1, odim, L) outs = outs . transpose ( 2 , 1 ) . squeeze ( 0 ) # (L, odim) probs = torch . cat ( probs , dim = 0 ) att_ws = torch . cat ( att_ws , dim = 0 ) break if self . output_activation_fn is not None : outs = self . output_activation_fn ( outs ) return outs , probs , att_ws Postnet Postnet module for Spectrogram prediction network. This is a module of Postnet in Spectrogram prediction network , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . The Postnet predicts refines the predicted Mel - filterbank of the decoder , which helps to compensate the detail sturcture of spectrogram . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , odim , n_layers = 5 , n_chans = 512 , n_filts = 5 , dropout_rate = 0.5 , use_batch_norm = True ) special Initialize postnet module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required n_layers int The number of layers. 5 n_filts int The number of filter size. 5 n_units int The number of filter channels. required use_batch_norm bool Whether to use batch normalization.. True dropout_rate float Dropout rate.. 0.5 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __init__ ( self , idim , odim , n_layers = 5 , n_chans = 512 , n_filts = 5 , dropout_rate = 0.5 , use_batch_norm = True ): \"\"\"Initialize postnet module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. n_layers (int, optional): The number of layers. n_filts (int, optional): The number of filter size. n_units (int, optional): The number of filter channels. use_batch_norm (bool, optional): Whether to use batch normalization.. dropout_rate (float, optional): Dropout rate.. \"\"\" super ( Postnet , self ) . __init__ () self . postnet = torch . nn . ModuleList () for layer in six . moves . range ( n_layers - 1 ): ichans = odim if layer == 0 else n_chans ochans = odim if layer == n_layers - 1 else n_chans if use_batch_norm : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , ochans , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . BatchNorm1d ( ochans ), torch . nn . Tanh (), torch . nn . Dropout ( dropout_rate ))] else : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , ochans , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . Tanh (), torch . nn . Dropout ( dropout_rate ))] ichans = n_chans if n_layers != 1 else odim if use_batch_norm : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , odim , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . BatchNorm1d ( odim ), torch . nn . Dropout ( dropout_rate ))] else : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , odim , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . Dropout ( dropout_rate ))] forward ( self , xs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the sequences of padded input tensors (B, idim, Tmax). required Returns: Type Description Tensor Batch of padded output tensor. (B, odim, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def forward ( self , xs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the sequences of padded input tensors (B, idim, Tmax). Returns: Tensor: Batch of padded output tensor. (B, odim, Tmax). \"\"\" for l in six . moves . range ( len ( self . postnet )): xs = self . postnet [ l ]( xs ) return xs Prenet Prenet module for decoder of Spectrogram prediction network. This is a module of Prenet in the decoder of Spectrogram prediction network , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . The Prenet preforms nonlinear conversion of inputs before input to auto - regressive lstm , which helps to learn diagonal attentions . !!! note This module alway applies dropout even in evaluation . See the detail in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , n_layers = 2 , n_units = 256 , dropout_rate = 0.5 ) special Initialize prenet module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required n_layers int The number of prenet layers. 2 n_units int The number of prenet units. 256 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , idim , n_layers = 2 , n_units = 256 , dropout_rate = 0.5 ): \"\"\"Initialize prenet module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. n_layers (int, optional): The number of prenet layers. n_units (int, optional): The number of prenet units. \"\"\" super ( Prenet , self ) . __init__ () self . dropout_rate = dropout_rate self . prenet = torch . nn . ModuleList () for layer in six . moves . range ( n_layers ): n_inputs = idim if layer == 0 else n_units self . prenet += [ torch . nn . Sequential ( torch . nn . Linear ( n_inputs , n_units ), torch . nn . ReLU ())] forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of input tensors (B, ..., idim). required Returns: Type Description Tensor Batch of output tensors (B, ..., odim). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 124 125 126 127 128 129 130 131 132 133 134 135 136 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of input tensors (B, ..., idim). Returns: Tensor: Batch of output tensors (B, ..., odim). \"\"\" for l in six . moves . range ( len ( self . prenet )): x = F . dropout ( self . prenet [ l ]( x ), self . dropout_rate ) return x ZoneOutCell ZoneOut Cell module. This is a module of zoneout described in ` Zoneout : Regularizing RNNs by Randomly Preserving Hidden Activations ` _ . This code is modified from ` eladhoffer / seq2seq . pytorch ` _ . !!! examples >>> lstm = torch . nn . LSTMCell ( 16 , 32 ) >>> lstm = ZoneOutCell ( lstm , 0 . 5 ) .. _ ` Zoneout : Regularizing RNNs by Randomly Preserving Hidden Activations ` : https : // arxiv . org / abs / 1606 . 01305 .. _ ` eladhoffer / seq2seq . pytorch ` : https : // github . com / eladhoffer / seq2seq . pytorch __init__ ( self , cell , zoneout_rate = 0.1 ) special Initialize zone out cell module. Parameters: Name Type Description Default cell torch.nn.Module Pytorch recurrent cell module e.g. torch.nn.Module.LSTMCell . required zoneout_rate float Probability of zoneout from 0.0 to 1.0. 0.1 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , cell , zoneout_rate = 0.1 ): \"\"\"Initialize zone out cell module. Args: cell (torch.nn.Module): Pytorch recurrent cell module e.g. `torch.nn.Module.LSTMCell`. zoneout_rate (float, optional): Probability of zoneout from 0.0 to 1.0. \"\"\" super ( ZoneOutCell , self ) . __init__ () self . cell = cell self . hidden_size = cell . hidden_size self . zoneout_rate = zoneout_rate if zoneout_rate > 1.0 or zoneout_rate < 0.0 : raise ValueError ( \"zoneout probability must be in the range from 0.0 to 1.0.\" ) forward ( self , inputs , hidden ) Calculate forward propagation. Parameters: Name Type Description Default inputs Tensor Batch of input tensor (B, input_size). required hidden tuple Tensor: Batch of initial hidden states (B, hidden_size). Tensor: Batch of initial cell states (B, hidden_size). required Returns: Type Description tuple Tensor: Batch of next hidden states (B, hidden_size). Tensor: Batch of next cell states (B, hidden_size). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def forward ( self , inputs , hidden ): \"\"\"Calculate forward propagation. Args: inputs (Tensor): Batch of input tensor (B, input_size). hidden (tuple): - Tensor: Batch of initial hidden states (B, hidden_size). - Tensor: Batch of initial cell states (B, hidden_size). Returns: tuple: - Tensor: Batch of next hidden states (B, hidden_size). - Tensor: Batch of next cell states (B, hidden_size). \"\"\" next_hidden = self . cell ( inputs , hidden ) next_hidden = self . _zoneout ( hidden , next_hidden , self . zoneout_rate ) return next_hidden decoder_init ( m ) Initialize decoder parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 16 17 18 19 def decoder_init ( m ): \"\"\"Initialize decoder parameters.\"\"\" if isinstance ( m , torch . nn . Conv1d ): torch . nn . init . xavier_uniform_ ( m . weight , torch . nn . init . calculate_gain ( 'tanh' )) encoder \u00b6 Tacotron2 encoder related modules. Encoder Encoder module of Spectrogram prediction network. This is a module of encoder of Spectrogram prediction network in Tacotron2 , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . This is the encoder which converts the sequence of characters into the sequence of hidden states . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , embed_dim = 512 , elayers = 1 , eunits = 512 , econv_layers = 3 , econv_chans = 512 , econv_filts = 5 , use_batch_norm = True , use_residual = False , dropout_rate = 0.5 , padding_idx = 0 ) special Initialize Tacotron2 encoder module. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , idim , embed_dim = 512 , elayers = 1 , eunits = 512 , econv_layers = 3 , econv_chans = 512 , econv_filts = 5 , use_batch_norm = True , use_residual = False , dropout_rate = 0.5 , padding_idx = 0 ): \"\"\"Initialize Tacotron2 encoder module. Args: idim (int) Dimension of the inputs. embed_dim (int, optional) Dimension of character embedding. elayers (int, optional) The number of encoder blstm layers. eunits (int, optional) The number of encoder blstm units. econv_layers (int, optional) The number of encoder conv layers. econv_filts (int, optional) The number of encoder conv filter size. econv_chans (int, optional) The number of encoder conv filter channels. use_batch_norm (bool, optional) Whether to use batch normalization. use_residual (bool, optional) Whether to use residual connection. dropout_rate (float, optional) Dropout rate. \"\"\" super ( Encoder , self ) . __init__ () # store the hyperparameters self . idim = idim self . use_residual = use_residual # define network layer modules self . embed = torch . nn . Embedding ( idim , embed_dim , padding_idx = padding_idx ) if econv_layers > 0 : self . convs = torch . nn . ModuleList () for layer in six . moves . range ( econv_layers ): ichans = embed_dim if layer == 0 else econv_chans if use_batch_norm : self . convs += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , econv_chans , econv_filts , stride = 1 , padding = ( econv_filts - 1 ) // 2 , bias = False ), torch . nn . BatchNorm1d ( econv_chans ), torch . nn . ReLU (), torch . nn . Dropout ( dropout_rate ))] else : self . convs += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , econv_chans , econv_filts , stride = 1 , padding = ( econv_filts - 1 ) // 2 , bias = False ), torch . nn . ReLU (), torch . nn . Dropout ( dropout_rate ))] else : self . convs = None if elayers > 0 : iunits = econv_chans if econv_layers != 0 else embed_dim self . blstm = torch . nn . LSTM ( iunits , eunits // 2 , elayers , batch_first = True , bidirectional = True ) else : self . blstm = None # initialize self . apply ( encoder_init ) forward ( self , xs , ilens = None ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the padded sequence of character ids (B, Tmax). Padded value should be 0. required ilens LongTensor Batch of lengths of each input batch (B,). None Returns: Type Description Tensor Batch of the sequences of encoder states(B, Tmax, eunits). LongTensor: Batch of lengths of each sequence (B,) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , xs , ilens = None ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the padded sequence of character ids (B, Tmax). Padded value should be 0. ilens (LongTensor): Batch of lengths of each input batch (B,). Returns: Tensor: Batch of the sequences of encoder states(B, Tmax, eunits). LongTensor: Batch of lengths of each sequence (B,) \"\"\" xs = self . embed ( xs ) . transpose ( 1 , 2 ) if self . convs is not None : for l in six . moves . range ( len ( self . convs )): if self . use_residual : xs += self . convs [ l ]( xs ) else : xs = self . convs [ l ]( xs ) if self . blstm is None : return xs . transpose ( 1 , 2 ) xs = pack_padded_sequence ( xs . transpose ( 1 , 2 ), ilens , batch_first = True ) self . blstm . flatten_parameters () xs , _ = self . blstm ( xs ) # (B, Tmax, C) xs , hlens = pad_packed_sequence ( xs , batch_first = True ) return xs , hlens inference ( self , x ) Inference. Parameters: Name Type Description Default x Tensor The sequeunce of character ids (T,). required Returns: Type Description Tensor The sequences of encoder states(T, eunits). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def inference ( self , x ): \"\"\"Inference. Args: x (Tensor): The sequeunce of character ids (T,). Returns: Tensor: The sequences of encoder states(T, eunits). \"\"\" assert len ( x . size ()) == 1 xs = x . unsqueeze ( 0 ) ilens = [ x . size ( 0 )] return self . forward ( xs , ilens )[ 0 ][ 0 ] encoder_init ( m ) Initialize encoder parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 17 18 19 20 def encoder_init ( m ): \"\"\"Initialize encoder parameters.\"\"\" if isinstance ( m , torch . nn . Conv1d ): torch . nn . init . xavier_uniform_ ( m . weight , torch . nn . init . calculate_gain ( 'relu' )) transformer special \u00b6 add_sos_eos \u00b6 Unility functions for Transformer. add_sos_eos ( ys_pad , sos , eos , ignore_id ) Add <sos> and <eos> labels. Parameters: Name Type Description Default ys_pad torch.Tensor batch of padded target sequences (B, Lmax) required sos int index of <sos> required eos int index of <eos> required ignore_id int index of padding required Returns: Type Description torch.Tensor padded tensor (B, Lmax) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/add_sos_eos.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def add_sos_eos ( ys_pad , sos , eos , ignore_id ): \"\"\" Add `<sos>` and `<eos>` labels. Arguments: ys_pad (torch.Tensor): batch of padded target sequences (B, Lmax) sos (int): index of `<sos>` eos (int): index of `<eos>` ignore_id (int): index of padding Returns: torch.Tensor: padded tensor (B, Lmax) \"\"\" from tools.espnet_minimal import pad_list _sos = ys_pad . new ([ sos ]) _eos = ys_pad . new ([ eos ]) ys = [ y [ y != ignore_id ] for y in ys_pad ] # parse padded ys ys_in = [ torch . cat ([ _sos , y ], dim = 0 ) for y in ys ] ys_out = [ torch . cat ([ y , _eos ], dim = 0 ) for y in ys ] return pad_list ( ys_in , eos ), pad_list ( ys_out , ignore_id ) attention \u00b6 Multi-Head Attention layer definition. MultiHeadedAttention Multi-Head Attention layer. :param int n_head: the number of head s :param int n_feat: the number of features :param float dropout_rate: dropout rate __init__ ( self , n_head , n_feat , dropout_rate ) special Construct an MultiHeadedAttention object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/attention.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , n_head , n_feat , dropout_rate ): \"\"\"Construct an MultiHeadedAttention object.\"\"\" super ( MultiHeadedAttention , self ) . __init__ () assert n_feat % n_head == 0 # We assume d_v always equals d_k self . d_k = n_feat // n_head self . h = n_head self . linear_q = nn . Linear ( n_feat , n_feat ) self . linear_k = nn . Linear ( n_feat , n_feat ) self . linear_v = nn . Linear ( n_feat , n_feat ) self . linear_out = nn . Linear ( n_feat , n_feat ) self . attn = None self . dropout = nn . Dropout ( p = dropout_rate ) forward ( self , query , key , value , mask ) Compute 'Scaled Dot Product Attention'. :param torch.Tensor query: (batch, time1, size) :param torch.Tensor key: (batch, time2, size) :param torch.Tensor value: (batch, time2, size) :param torch.Tensor mask: (batch, time1, time2) :param torch.nn.Dropout dropout: :return torch.Tensor: attentined and transformed value (batch, time1, d_model) weighted by the query dot key attention (batch, head, time1, time2) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/attention.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def forward ( self , query , key , value , mask ): \"\"\"Compute 'Scaled Dot Product Attention'. :param torch.Tensor query: (batch, time1, size) :param torch.Tensor key: (batch, time2, size) :param torch.Tensor value: (batch, time2, size) :param torch.Tensor mask: (batch, time1, time2) :param torch.nn.Dropout dropout: :return torch.Tensor: attentined and transformed `value` (batch, time1, d_model) weighted by the query dot key attention (batch, head, time1, time2) \"\"\" n_batch = query . size ( 0 ) q = self . linear_q ( query ) . view ( n_batch , - 1 , self . h , self . d_k ) k = self . linear_k ( key ) . view ( n_batch , - 1 , self . h , self . d_k ) v = self . linear_v ( value ) . view ( n_batch , - 1 , self . h , self . d_k ) q = q . transpose ( 1 , 2 ) # (batch, head, time1, d_k) k = k . transpose ( 1 , 2 ) # (batch, head, time2, d_k) v = v . transpose ( 1 , 2 ) # (batch, head, time2, d_k) scores = torch . matmul ( q , k . transpose ( - 2 , - 1 )) / math . sqrt ( self . d_k ) # (batch, head, time1, time2) if mask is not None : mask = mask . unsqueeze ( 1 ) . eq ( 0 ) # (batch, 1, time1, time2) min_value = float ( numpy . finfo ( torch . tensor ( 0 , dtype = scores . dtype ) . numpy () . dtype ) . min ) scores = scores . masked_fill ( mask , min_value ) self . attn = torch . softmax ( scores , dim =- 1 ) . masked_fill ( mask , 0.0 ) # (batch, head, time1, time2) else : self . attn = torch . softmax ( scores , dim =- 1 ) # (batch, head, time1, time2) p_attn = self . dropout ( self . attn ) x = torch . matmul ( p_attn , v ) # (batch, head, time1, d_k) x = x . transpose ( 1 , 2 ) . contiguous () . view ( n_batch , - 1 , self . h * self . d_k ) # (batch, time1, d_model) return self . linear_out ( x ) # (batch, time1, d_model) decoder \u00b6 Decoder definition. Decoder Transfomer decoder module. :param int odim: output dim :param int attention_dim: dimention of attention :param int attention_heads: the number of heads of multi head attention :param int linear_units: the number of units of position-wise feed forward :param int num_blocks: the number of decoder blocks :param float dropout_rate: dropout rate :param float attention_dropout_rate: dropout rate for attention :param str or torch.nn.Module input_layer: input layer type :param bool use_output_layer: whether to use output layer :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) __init__ ( self , odim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , self_attention_dropout_rate = 0.0 , src_attention_dropout_rate = 0.0 , input_layer = 'embed' , use_output_layer = True , pos_enc_class =< class ' tools . espnet_minimal . nets . pytorch_backend . transformer . embedding . PositionalEncoding '>, normalize_before=True, concat_after=False) special Construct an Decoder object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , odim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , self_attention_dropout_rate = 0.0 , src_attention_dropout_rate = 0.0 , input_layer = \"embed\" , use_output_layer = True , pos_enc_class = PositionalEncoding , normalize_before = True , concat_after = False ): \"\"\"Construct an Decoder object.\"\"\" torch . nn . Module . __init__ ( self ) if input_layer == \"embed\" : self . embed = torch . nn . Sequential ( torch . nn . Embedding ( odim , attention_dim ), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif input_layer == \"linear\" : self . embed = torch . nn . Sequential ( torch . nn . Linear ( odim , attention_dim ), torch . nn . LayerNorm ( attention_dim ), torch . nn . Dropout ( dropout_rate ), torch . nn . ReLU (), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif isinstance ( input_layer , torch . nn . Module ): self . embed = torch . nn . Sequential ( input_layer , pos_enc_class ( attention_dim , positional_dropout_rate ) ) else : raise NotImplementedError ( \"only `embed` or torch.nn.Module is supported.\" ) self . normalize_before = normalize_before self . decoders = repeat ( num_blocks , lambda : DecoderLayer ( attention_dim , MultiHeadedAttention ( attention_heads , attention_dim , self_attention_dropout_rate ), MultiHeadedAttention ( attention_heads , attention_dim , src_attention_dropout_rate ), PositionwiseFeedForward ( attention_dim , linear_units , dropout_rate ), dropout_rate , normalize_before , concat_after ) ) if self . normalize_before : self . after_norm = LayerNorm ( attention_dim ) if use_output_layer : self . output_layer = torch . nn . Linear ( attention_dim , odim ) else : self . output_layer = None forward ( self , tgt , tgt_mask , memory , memory_mask ) Forward decoder. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) if input_layer == \"embed\" input tensor (batch, maxlen_out, #mels) in the other cases :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param torch.Tensor memory_mask: encoded memory mask, (batch, maxlen_in) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :return x: decoded token score before softmax (batch, maxlen_out, token) if use_output_layer is True, final block outputs (batch, maxlen_out, attention_dim) in the other cases :rtype: torch.Tensor :return tgt_mask: score mask before softmax (batch, maxlen_out) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def forward ( self , tgt , tgt_mask , memory , memory_mask ): \"\"\"Forward decoder. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) if input_layer == \"embed\" input tensor (batch, maxlen_out, #mels) in the other cases :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param torch.Tensor memory_mask: encoded memory mask, (batch, maxlen_in) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :return x: decoded token score before softmax (batch, maxlen_out, token) if use_output_layer is True, final block outputs (batch, maxlen_out, attention_dim) in the other cases :rtype: torch.Tensor :return tgt_mask: score mask before softmax (batch, maxlen_out) :rtype: torch.Tensor \"\"\" x = self . embed ( tgt ) x , tgt_mask , memory , memory_mask = self . decoders ( x , tgt_mask , memory , memory_mask ) if self . normalize_before : x = self . after_norm ( x ) if self . output_layer is not None : x = self . output_layer ( x ) return x , tgt_mask forward_one_step ( self , tgt , tgt_mask , memory , cache = None ) Forward one step. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param List[torch.Tensor] cache: cached output list of (batch, max_time_out-1, size) :return y, cache: NN output value and cache per self.decoders . y.shape is (batch, maxlen_out, token) :rtype: Tuple[torch.Tensor, List[torch.Tensor]] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def forward_one_step ( self , tgt , tgt_mask , memory , cache = None ): \"\"\"Forward one step. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param List[torch.Tensor] cache: cached output list of (batch, max_time_out-1, size) :return y, cache: NN output value and cache per `self.decoders`. `y.shape` is (batch, maxlen_out, token) :rtype: Tuple[torch.Tensor, List[torch.Tensor]] \"\"\" x = self . embed ( tgt ) if cache is None : cache = [ None ] * len ( self . decoders ) new_cache = [] for c , decoder in zip ( cache , self . decoders ): x , tgt_mask , memory , memory_mask = decoder ( x , tgt_mask , memory , None , cache = c ) new_cache . append ( x ) if self . normalize_before : y = self . after_norm ( x [:, - 1 ]) else : y = x [:, - 1 ] if self . output_layer is not None : y = torch . log_softmax ( self . output_layer ( y ), dim =- 1 ) return y , new_cache score ( self , ys , state , x ) Score. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def score ( self , ys , state , x ): \"\"\"Score.\"\"\" # TODO(karita): remove this section after all ScorerInterface implements batch decoding if ys . dim () == 1 : ys_mask = subsequent_mask ( len ( ys ), device = x . device ) . unsqueeze ( 0 ) logp , state = self . forward_one_step ( ys . unsqueeze ( 0 ), ys_mask , x . unsqueeze ( 0 ), cache = state ) return logp . squeeze ( 0 ), state # merge states n_batch = len ( ys ) n_layers = len ( self . decoders ) if state [ 0 ] is None : batch_state = None else : # transpose state of [batch, layer] into [layer, batch] batch_state = [ torch . stack ([ state [ b ][ l ] for b in range ( n_batch )]) for l in range ( n_layers )] # batch decoding ys_mask = subsequent_mask ( ys . size ( - 1 ), device = x . device ) . unsqueeze ( 0 ) logp , state = self . forward_one_step ( ys , ys_mask , x , cache = batch_state ) # transpose state of [layer, batch] into [batch, layer] state_list = [[ state [ l ][ b ] for l in range ( n_layers )] for b in range ( n_batch )] return logp , state_list decoder_layer \u00b6 Decoder self-attention layer definition. DecoderLayer Single decoder layer module. :param int size: input dim :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.attention.MultiHeadedAttention self_attn: self attention module :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.attention.MultiHeadedAttention src_attn: source attention module :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward feed_forward: feed forward layer module :param float dropout_rate: dropout rate :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) __init__ ( self , size , self_attn , src_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ) special Construct an DecoderLayer object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder_layer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , size , self_attn , src_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ): \"\"\"Construct an DecoderLayer object.\"\"\" super ( DecoderLayer , self ) . __init__ () self . size = size self . self_attn = self_attn self . src_attn = src_attn self . feed_forward = feed_forward self . norm1 = LayerNorm ( size ) self . norm2 = LayerNorm ( size ) self . norm3 = LayerNorm ( size ) self . dropout = nn . Dropout ( dropout_rate ) self . normalize_before = normalize_before self . concat_after = concat_after if self . concat_after : self . concat_linear1 = nn . Linear ( size + size , size ) self . concat_linear2 = nn . Linear ( size + size , size ) forward ( self , tgt , tgt_mask , memory , memory_mask , cache = None ) Compute decoded features. Parameters: Name Type Description Default tgt torch.Tensor decoded previous target features (batch, max_time_out, size) required tgt_mask torch.Tensor mask for x (batch, max_time_out) required memory torch.Tensor encoded source features (batch, max_time_in, size) required memory_mask torch.Tensor mask for memory (batch, max_time_in) required cache torch.Tensor cached output (batch, max_time_out-1, size) None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder_layer.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def forward ( self , tgt , tgt_mask , memory , memory_mask , cache = None ): \"\"\"Compute decoded features. Args: tgt (torch.Tensor): decoded previous target features (batch, max_time_out, size) tgt_mask (torch.Tensor): mask for x (batch, max_time_out) memory (torch.Tensor): encoded source features (batch, max_time_in, size) memory_mask (torch.Tensor): mask for memory (batch, max_time_in) cache (torch.Tensor): cached output (batch, max_time_out-1, size) \"\"\" residual = tgt if self . normalize_before : tgt = self . norm1 ( tgt ) if cache is None : tgt_q = tgt tgt_q_mask = tgt_mask else : # compute only the last frame query keeping dim: max_time_out -> 1 assert cache . shape == ( tgt . shape [ 0 ], tgt . shape [ 1 ] - 1 , self . size ), \\ f \" { cache . shape } == { ( tgt . shape [ 0 ], tgt . shape [ 1 ] - 1 , self . size ) } \" tgt_q = tgt [:, - 1 :, :] residual = residual [:, - 1 :, :] tgt_q_mask = None if tgt_mask is not None : tgt_q_mask = tgt_mask [:, - 1 :, :] if self . concat_after : tgt_concat = torch . cat (( tgt_q , self . self_attn ( tgt_q , tgt , tgt , tgt_q_mask )), dim =- 1 ) x = residual + self . concat_linear1 ( tgt_concat ) else : x = residual + self . dropout ( self . self_attn ( tgt_q , tgt , tgt , tgt_q_mask )) if not self . normalize_before : x = self . norm1 ( x ) residual = x if self . normalize_before : x = self . norm2 ( x ) if self . concat_after : x_concat = torch . cat (( x , self . src_attn ( x , memory , memory , memory_mask )), dim =- 1 ) x = residual + self . concat_linear2 ( x_concat ) else : x = residual + self . dropout ( self . src_attn ( x , memory , memory , memory_mask )) if not self . normalize_before : x = self . norm2 ( x ) residual = x if self . normalize_before : x = self . norm3 ( x ) x = residual + self . dropout ( self . feed_forward ( x )) if not self . normalize_before : x = self . norm3 ( x ) if cache is not None : x = torch . cat ([ cache , x ], dim = 1 ) return x , tgt_mask , memory , memory_mask embedding \u00b6 Positonal Encoding Module. PositionalEncoding Positional encoding. :param int d_model: embedding dim :param float dropout_rate: dropout rate :param int max_len: maximum input length __init__ ( self , d_model , dropout_rate , max_len = 5000 ) special Construct an PositionalEncoding object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 37 38 39 40 41 42 43 44 45 def __init__ ( self , d_model , dropout_rate , max_len = 5000 ): \"\"\"Construct an PositionalEncoding object.\"\"\" super ( PositionalEncoding , self ) . __init__ () self . d_model = d_model self . xscale = math . sqrt ( self . d_model ) self . dropout = torch . nn . Dropout ( p = dropout_rate ) self . pe = None self . extend_pe ( torch . tensor ( 0.0 ) . expand ( 1 , max_len )) self . _register_load_state_dict_pre_hook ( _pre_hook ) extend_pe ( self , x ) Reset the positional encodings. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def extend_pe ( self , x ): \"\"\"Reset the positional encodings.\"\"\" if self . pe is not None : if self . pe . size ( 1 ) >= x . size ( 1 ): if self . pe . dtype != x . dtype or self . pe . device != x . device : self . pe = self . pe . to ( dtype = x . dtype , device = x . device ) return pe = torch . zeros ( x . size ( 1 ), self . d_model ) position = torch . arange ( 0 , x . size ( 1 ), dtype = torch . float32 ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , self . d_model , 2 , dtype = torch . float32 ) * - ( math . log ( 10000.0 ) / self . d_model )) pe [:, 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 1 :: 2 ] = torch . cos ( position * div_term ) pe = pe . unsqueeze ( 0 ) self . pe = pe . to ( device = x . device , dtype = x . dtype ) forward ( self , x ) Add positional encoding. Parameters: Name Type Description Default x Tensor Input. Its shape is (batch, time, ...) required Returns: Type Description torch.Tensor Encoded tensor. Its shape is (batch, time, ...) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 63 64 65 66 67 68 69 70 71 72 73 74 75 def forward ( self , x : torch . Tensor ): \"\"\"Add positional encoding. Args: x (torch.Tensor): Input. Its shape is (batch, time, ...) Returns: torch.Tensor: Encoded tensor. Its shape is (batch, time, ...) \"\"\" self . extend_pe ( x ) x = x * self . xscale + self . pe [:, : x . size ( 1 )] return self . dropout ( x ) ScaledPositionalEncoding Scaled positional encoding module. See also: Sec. 3.2 https://arxiv.org/pdf/1809.08895.pdf __init__ ( self , d_model , dropout_rate , max_len = 5000 ) special Initialize class. :param int d_model: embedding dim :param float dropout_rate: dropout rate :param int max_len: maximum input length Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , d_model , dropout_rate , max_len = 5000 ): \"\"\"Initialize class. :param int d_model: embedding dim :param float dropout_rate: dropout rate :param int max_len: maximum input length \"\"\" super () . __init__ ( d_model = d_model , dropout_rate = dropout_rate , max_len = max_len ) self . alpha = torch . nn . Parameter ( torch . tensor ( 1.0 )) forward ( self , x ) Add positional encoding. Parameters: Name Type Description Default x torch.Tensor Input. Its shape is (batch, time, ...) required Returns: Type Description torch.Tensor Encoded tensor. Its shape is (batch, time, ...) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 100 101 102 103 104 105 106 107 108 109 110 111 112 def forward ( self , x ): \"\"\"Add positional encoding. Args: x (torch.Tensor): Input. Its shape is (batch, time, ...) Returns: torch.Tensor: Encoded tensor. Its shape is (batch, time, ...) \"\"\" self . extend_pe ( x ) x = x + self . alpha * self . pe [:, : x . size ( 1 )] return self . dropout ( x ) reset_parameters ( self ) Reset parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 96 97 98 def reset_parameters ( self ): \"\"\"Reset parameters.\"\"\" self . alpha . data = torch . tensor ( 1.0 ) encoder \u00b6 Encoder definition. Encoder Transformer encoder module. :param int idim: input dim :param int attention_dim: dimention of attention :param int attention_heads: the number of heads of multi head attention :param int linear_units: the number of units of position-wise feed forward :param int num_blocks: the number of decoder blocks :param float dropout_rate: dropout rate :param float attention_dropout_rate: dropout rate in attention :param float positional_dropout_rate: dropout rate after adding positional encoding :param str or torch.nn.Module input_layer: input layer type :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) :param str positionwise_layer_type: linear of conv1d :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer :param int padding_idx: padding_idx for input_layer=embed __init__ ( self , idim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , attention_dropout_rate = 0.0 , input_layer = 'conv2d' , pos_enc_class =< class ' tools . espnet_minimal . nets . pytorch_backend . transformer . embedding . PositionalEncoding '>, normalize_before=True, concat_after=False, positionwise_layer_type=' linear ', positionwise_conv_kernel_size=1, padding_idx=-1) special Construct an Encoder object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , idim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , attention_dropout_rate = 0.0 , input_layer = \"conv2d\" , pos_enc_class = PositionalEncoding , normalize_before = True , concat_after = False , positionwise_layer_type = \"linear\" , positionwise_conv_kernel_size = 1 , padding_idx =- 1 ): \"\"\"Construct an Encoder object.\"\"\" super ( Encoder , self ) . __init__ () if input_layer == \"linear\" : self . embed = torch . nn . Sequential ( torch . nn . Linear ( idim , attention_dim ), torch . nn . LayerNorm ( attention_dim ), torch . nn . Dropout ( dropout_rate ), torch . nn . ReLU (), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif input_layer == \"conv2d\" : self . embed = Conv2dSubsampling ( idim , attention_dim , dropout_rate ) elif input_layer == \"embed\" : self . embed = torch . nn . Sequential ( torch . nn . Embedding ( idim , attention_dim , padding_idx = padding_idx ), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif isinstance ( input_layer , torch . nn . Module ): self . embed = torch . nn . Sequential ( input_layer , pos_enc_class ( attention_dim , positional_dropout_rate ), ) elif input_layer is None : self . embed = torch . nn . Sequential ( pos_enc_class ( attention_dim , positional_dropout_rate ) ) else : raise ValueError ( \"unknown input_layer: \" + input_layer ) self . normalize_before = normalize_before if positionwise_layer_type == \"linear\" : positionwise_layer = PositionwiseFeedForward positionwise_layer_args = ( attention_dim , linear_units , dropout_rate ) elif positionwise_layer_type == \"conv1d\" : positionwise_layer = MultiLayeredConv1d positionwise_layer_args = ( attention_dim , linear_units , positionwise_conv_kernel_size , dropout_rate ) elif positionwise_layer_type == \"conv1d-linear\" : positionwise_layer = Conv1dLinear positionwise_layer_args = ( attention_dim , linear_units , positionwise_conv_kernel_size , dropout_rate ) else : raise NotImplementedError ( \"Support only linear or conv1d.\" ) self . encoders = repeat ( num_blocks , lambda : EncoderLayer ( attention_dim , MultiHeadedAttention ( attention_heads , attention_dim , attention_dropout_rate ), positionwise_layer ( * positionwise_layer_args ), dropout_rate , normalize_before , concat_after ) ) if self . normalize_before : self . after_norm = LayerNorm ( attention_dim ) forward ( self , xs , masks ) Embed positions in tensor. :param torch.Tensor xs: input tensor :param torch.Tensor masks: input mask :return: position embedded tensor and mask :rtype Tuple[torch.Tensor, torch.Tensor]: Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def forward ( self , xs , masks ): \"\"\"Embed positions in tensor. :param torch.Tensor xs: input tensor :param torch.Tensor masks: input mask :return: position embedded tensor and mask :rtype Tuple[torch.Tensor, torch.Tensor]: \"\"\" if isinstance ( self . embed , Conv2dSubsampling ): xs , masks = self . embed ( xs , masks ) else : xs = self . embed ( xs ) xs , masks = self . encoders ( xs , masks ) if self . normalize_before : xs = self . after_norm ( xs ) return xs , masks encoder_layer \u00b6 Encoder self-attention layer definition. EncoderLayer Encoder layer module. :param int size: input dim :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.attention.MultiHeadedAttention self_attn: self attention module :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward feed_forward: feed forward module :param float dropout_rate: dropout rate :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) __init__ ( self , size , self_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ) special Construct an EncoderLayer object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder_layer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , size , self_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ): \"\"\"Construct an EncoderLayer object.\"\"\" super ( EncoderLayer , self ) . __init__ () self . self_attn = self_attn self . feed_forward = feed_forward self . norm1 = LayerNorm ( size ) self . norm2 = LayerNorm ( size ) self . dropout = nn . Dropout ( dropout_rate ) self . size = size self . normalize_before = normalize_before self . concat_after = concat_after if self . concat_after : self . concat_linear = nn . Linear ( size + size , size ) forward ( self , x , mask ) Compute encoded features. :param torch.Tensor x: encoded source features (batch, max_time_in, size) :param torch.Tensor mask: mask for x (batch, max_time_in) :rtype: Tuple[torch.Tensor, torch.Tensor] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder_layer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def forward ( self , x , mask ): \"\"\"Compute encoded features. :param torch.Tensor x: encoded source features (batch, max_time_in, size) :param torch.Tensor mask: mask for x (batch, max_time_in) :rtype: Tuple[torch.Tensor, torch.Tensor] \"\"\" residual = x if self . normalize_before : x = self . norm1 ( x ) if self . concat_after : x_concat = torch . cat (( x , self . self_attn ( x , x , x , mask )), dim =- 1 ) x = residual + self . concat_linear ( x_concat ) else : x = residual + self . dropout ( self . self_attn ( x , x , x , mask )) if not self . normalize_before : x = self . norm1 ( x ) residual = x if self . normalize_before : x = self . norm2 ( x ) x = residual + self . dropout ( self . feed_forward ( x )) if not self . normalize_before : x = self . norm2 ( x ) return x , mask initializer \u00b6 Parameter initialization. initialize ( model , init_type = 'pytorch' ) Initialize Transformer module. :param torch.nn.Module model: transformer instance :param str init_type: initialization type Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/initializer.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def initialize ( model , init_type = \"pytorch\" ): \"\"\"Initialize Transformer module. :param torch.nn.Module model: transformer instance :param str init_type: initialization type \"\"\" if init_type == \"pytorch\" : return # weight init for p in model . parameters (): if p . dim () > 1 : if init_type == \"xavier_uniform\" : torch . nn . init . xavier_uniform_ ( p . data ) elif init_type == \"xavier_normal\" : torch . nn . init . xavier_normal_ ( p . data ) elif init_type == \"kaiming_uniform\" : torch . nn . init . kaiming_uniform_ ( p . data , nonlinearity = \"relu\" ) elif init_type == \"kaiming_normal\" : torch . nn . init . kaiming_normal_ ( p . data , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown initialization: \" + init_type ) # bias init for p in model . parameters (): if p . dim () == 1 : p . data . zero_ () # reset some modules with default init for m in model . modules (): if isinstance ( m , ( torch . nn . Embedding , LayerNorm )): m . reset_parameters () label_smoothing_loss \u00b6 Label smoothing module. LabelSmoothingLoss Label-smoothing loss. :param int size: the number of class :param int padding_idx: ignored class id :param float smoothing: smoothing rate (0.0 means the conventional CE) :param bool normalize_length: normalize loss by sequence length if True :param torch.nn.Module criterion: loss function to be smoothed __init__ ( self , size , padding_idx , smoothing , normalize_length = False , criterion = KLDivLoss ()) special Construct an LabelSmoothingLoss object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/label_smoothing_loss.py 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , size , padding_idx , smoothing , normalize_length = False , criterion = nn . KLDivLoss ( reduce = False )): \"\"\"Construct an LabelSmoothingLoss object.\"\"\" super ( LabelSmoothingLoss , self ) . __init__ () self . criterion = criterion self . padding_idx = padding_idx self . confidence = 1.0 - smoothing self . smoothing = smoothing self . size = size self . true_dist = None self . normalize_length = normalize_length forward ( self , x , target ) Compute loss between x and target. :param torch.Tensor x: prediction (batch, seqlen, class) :param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen) :return: scalar float value :rtype torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/label_smoothing_loss.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , x , target ): \"\"\"Compute loss between x and target. :param torch.Tensor x: prediction (batch, seqlen, class) :param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen) :return: scalar float value :rtype torch.Tensor \"\"\" assert x . size ( 2 ) == self . size batch_size = x . size ( 0 ) x = x . view ( - 1 , self . size ) target = target . view ( - 1 ) with torch . no_grad (): true_dist = x . clone () true_dist . fill_ ( self . smoothing / ( self . size - 1 )) ignore = target == self . padding_idx # (B,) total = len ( target ) - ignore . sum () . item () target = target . masked_fill ( ignore , 0 ) # avoid -1 index true_dist . scatter_ ( 1 , target . unsqueeze ( 1 ), self . confidence ) kl = self . criterion ( torch . log_softmax ( x , dim = 1 ), true_dist ) denom = total if self . normalize_length else batch_size return kl . masked_fill ( ignore . unsqueeze ( 1 ), 0 ) . sum () / denom layer_norm \u00b6 Layer normalization module. LayerNorm Layer normalization module. :param int nout: output dim size :param int dim: dimension to be normalized __init__ ( self , nout , dim =- 1 ) special Construct an LayerNorm object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/layer_norm.py 19 20 21 22 def __init__ ( self , nout , dim =- 1 ): \"\"\"Construct an LayerNorm object.\"\"\" super ( LayerNorm , self ) . __init__ ( nout , eps = 1e-12 ) self . dim = dim forward ( self , x ) Apply layer normalization. :param torch.Tensor x: input tensor :return: layer normalized tensor :rtype torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/layer_norm.py 24 25 26 27 28 29 30 31 32 33 def forward ( self , x ): \"\"\"Apply layer normalization. :param torch.Tensor x: input tensor :return: layer normalized tensor :rtype torch.Tensor \"\"\" if self . dim == - 1 : return super ( LayerNorm , self ) . forward ( x ) return super ( LayerNorm , self ) . forward ( x . transpose ( 1 , - 1 )) . transpose ( 1 , - 1 ) mask \u00b6 Mask module. subsequent_mask ( size , device = 'cpu' , dtype = torch . bool ) Create mask for subsequent steps (1, size, size). :param int size: size of mask :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device :param torch.dtype dtype: result dtype :rtype: torch.Tensor subsequent_mask(3) [[1, 0, 0], [1, 1, 0], [1, 1, 1]] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/mask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def subsequent_mask ( size , device = \"cpu\" , dtype = datatype ): \"\"\"Create mask for subsequent steps (1, size, size). :param int size: size of mask :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device :param torch.dtype dtype: result dtype :rtype: torch.Tensor >>> subsequent_mask(3) [[1, 0, 0], [1, 1, 0], [1, 1, 1]] \"\"\" ret = torch . ones ( size , size , device = device , dtype = dtype ) return torch . tril ( ret , out = ret ) target_mask ( ys_in_pad , ignore_id ) Create mask for decoder self-attention. :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :param int ignore_id: index of padding :param torch.dtype dtype: result dtype :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/mask.py 32 33 34 35 36 37 38 39 40 41 42 def target_mask ( ys_in_pad , ignore_id ): \"\"\"Create mask for decoder self-attention. :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :param int ignore_id: index of padding :param torch.dtype dtype: result dtype :rtype: torch.Tensor \"\"\" ys_mask = ys_in_pad != ignore_id m = subsequent_mask ( ys_mask . size ( - 1 ), device = ys_mask . device ) . unsqueeze ( 0 ) return ys_mask . unsqueeze ( - 2 ) & m multi_layer_conv \u00b6 Layer modules for FFT block in FastSpeech (Feed-forward Transformer). Conv1dLinear Conv1D + Linear for Transformer block. A variant of MultiLayeredConv1d, which replaces second conv-layer to linear. __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ) special Initialize Conv1dLinear module. Parameters: Name Type Description Default in_chans int Number of input channels. required hidden_chans int Number of hidden channels. required kernel_size int Kernel size of conv1d. required dropout_rate float Dropout rate. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ): \"\"\"Initialize Conv1dLinear module. Args: in_chans (int): Number of input channels. hidden_chans (int): Number of hidden channels. kernel_size (int): Kernel size of conv1d. dropout_rate (float): Dropout rate. \"\"\" super ( Conv1dLinear , self ) . __init__ () self . w_1 = torch . nn . Conv1d ( in_chans , hidden_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ) self . w_2 = torch . nn . Linear ( hidden_chans , in_chans ) self . dropout = torch . nn . Dropout ( dropout_rate ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of input tensors (B, ..., in_chans). required Returns: Type Description Tensor Batch of output tensors (B, ..., hidden_chans). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 77 78 79 80 81 82 83 84 85 86 87 88 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of input tensors (B, ..., in_chans). Returns: Tensor: Batch of output tensors (B, ..., hidden_chans). \"\"\" x = torch . relu ( self . w_1 ( x . transpose ( - 1 , 1 ))) . transpose ( - 1 , 1 ) return self . w_2 ( self . dropout ( x )) MultiLayeredConv1d Multi-layered conv1d for Transformer block. This is a module of multi - leyered conv1d designed to replace positionwise feed - forward network in Transforner block , which is introduced in ` FastSpeech : Fast , Robust and Controllable Text to Speech ` _ . .. _ ` FastSpeech : Fast , Robust and Controllable Text to Speech ` : https : // arxiv . org / pdf / 1905 . 09263 . pdf __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ) special Initialize MultiLayeredConv1d module. Parameters: Name Type Description Default in_chans int Number of input channels. required hidden_chans int Number of hidden channels. required kernel_size int Kernel size of conv1d. required dropout_rate float Dropout rate. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ): \"\"\"Initialize MultiLayeredConv1d module. Args: in_chans (int): Number of input channels. hidden_chans (int): Number of hidden channels. kernel_size (int): Kernel size of conv1d. dropout_rate (float): Dropout rate. \"\"\" super ( MultiLayeredConv1d , self ) . __init__ () self . w_1 = torch . nn . Conv1d ( in_chans , hidden_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ) self . w_2 = torch . nn . Conv1d ( hidden_chans , in_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ) self . dropout = torch . nn . Dropout ( dropout_rate ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of input tensors (B, ..., in_chans). required Returns: Type Description Tensor Batch of output tensors (B, ..., hidden_chans). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 40 41 42 43 44 45 46 47 48 49 50 51 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of input tensors (B, ..., in_chans). Returns: Tensor: Batch of output tensors (B, ..., hidden_chans). \"\"\" x = torch . relu ( self . w_1 ( x . transpose ( - 1 , 1 ))) . transpose ( - 1 , 1 ) return self . w_2 ( self . dropout ( x ) . transpose ( - 1 , 1 )) . transpose ( - 1 , 1 ) optimizer \u00b6 Optimizer module. NoamOpt Optim wrapper that implements rate. param_groups property readonly Return param_groups. __init__ ( self , model_size , factor , warmup , optimizer ) special Construct an NoamOpt object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 15 16 17 18 19 20 21 22 def __init__ ( self , model_size , factor , warmup , optimizer ): \"\"\"Construct an NoamOpt object.\"\"\" self . optimizer = optimizer self . _step = 0 self . warmup = warmup self . factor = factor self . model_size = model_size self . _rate = 0 load_state_dict ( self , state_dict ) Load state_dict. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 60 61 62 63 64 65 66 def load_state_dict ( self , state_dict ): \"\"\"Load state_dict.\"\"\" for key , value in state_dict . items (): if key == \"optimizer\" : self . optimizer . load_state_dict ( state_dict [ \"optimizer\" ]) else : setattr ( self , key , value ) rate ( self , step = None ) Implement lrate above. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 38 39 40 41 42 43 def rate ( self , step = None ): \"\"\"Implement `lrate` above.\"\"\" if step is None : step = self . _step return self . factor * self . model_size ** ( - 0.5 ) \\ * min ( step ** ( - 0.5 ), step * self . warmup ** ( - 1.5 )) state_dict ( self ) Return state_dict. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 49 50 51 52 53 54 55 56 57 58 def state_dict ( self ): \"\"\"Return state_dict.\"\"\" return { \"_step\" : self . _step , \"warmup\" : self . warmup , \"factor\" : self . factor , \"model_size\" : self . model_size , \"_rate\" : self . _rate , \"optimizer\" : self . optimizer . state_dict () } step ( self ) Update parameters and rate. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 29 30 31 32 33 34 35 36 def step ( self ): \"\"\"Update parameters and rate.\"\"\" self . _step += 1 rate = self . rate () for p in self . optimizer . param_groups : p [ 'lr' ] = rate self . _rate = rate self . optimizer . step () zero_grad ( self ) Reset gradient. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 45 46 47 def zero_grad ( self ): \"\"\"Reset gradient.\"\"\" self . optimizer . zero_grad () get_std_opt ( model , d_model , warmup , factor ) Get standard NoamOpt. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 69 70 71 72 def get_std_opt ( model , d_model , warmup , factor ): \"\"\"Get standard NoamOpt.\"\"\" base = torch . optim . Adam ( model . parameters (), lr = 0 , betas = ( 0.9 , 0.98 ), eps = 1e-9 ) return NoamOpt ( d_model , factor , warmup , base ) plot \u00b6 PlotAttentionReport __call__ ( self , trainer ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 70 71 72 73 def __call__ ( self , trainer ): attn_dict = self . get_attention_weights () suffix = \"ep.{.updater.epoch}.png\" . format ( trainer ) self . plotfn ( self . data , attn_dict , self . outdir , suffix , savefig ) get_attention_weights ( self ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 75 76 77 78 79 80 81 def get_attention_weights ( self ): batch = self . converter ([ self . transform ( self . data )], self . device ) if isinstance ( batch , tuple ): att_ws = self . att_vis_fn ( * batch ) elif isinstance ( batch , dict ): att_ws = self . att_vis_fn ( ** batch ) return att_ws log_attentions ( self , logger , step ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 83 84 85 86 87 88 89 90 def log_attentions ( self , logger , step ): def log_fig ( plot , filename ): from os.path import basename logger . add_figure ( basename ( filename ), plot , step ) plt . clf () attn_dict = self . get_attention_weights () self . plotfn ( self . data , attn_dict , self . outdir , \"\" , log_fig ) plotfn ( self , * args , ** kwargs ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 67 68 def plotfn ( self , * args , ** kwargs ): plot_multi_head_attention ( * args , ** kwargs ) plot_multi_head_attention ( data , attn_dict , outdir , suffix = 'png' , savefn =< function savefig at 0x7f918a81a700 > ) Plot multi head attentions :param dict data: utts info from json file :param dict[str, torch.Tensor] attn_dict: multi head attention dict. values should be torch.Tensor (head, input_length, output_length) :param str outdir: dir to save fig :param str suffix: filename suffix including image type (e.g., png) :param savefn: function to save Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def plot_multi_head_attention ( data , attn_dict , outdir , suffix = \"png\" , savefn = savefig ): \"\"\"Plot multi head attentions :param dict data: utts info from json file :param dict[str, torch.Tensor] attn_dict: multi head attention dict. values should be torch.Tensor (head, input_length, output_length) :param str outdir: dir to save fig :param str suffix: filename suffix including image type (e.g., png) :param savefn: function to save \"\"\" for name , att_ws in attn_dict . items (): for idx , att_w in enumerate ( att_ws ): filename = \" %s / %s . %s . %s \" % ( outdir , data [ idx ][ 0 ], name , suffix ) dec_len = int ( data [ idx ][ 1 ][ 'output' ][ 0 ][ 'shape' ][ 0 ]) enc_len = int ( data [ idx ][ 1 ][ 'input' ][ 0 ][ 'shape' ][ 0 ]) if \"encoder\" in name : att_w = att_w [:, : enc_len , : enc_len ] elif \"decoder\" in name : if \"self\" in name : att_w = att_w [:, : dec_len , : dec_len ] else : att_w = att_w [:, : dec_len , : enc_len ] else : logging . warning ( \"unknown name for shaping attention\" ) fig = _plot_and_save_attention ( att_w , filename ) savefn ( fig , filename ) savefig ( plot , filename ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 32 33 34 def savefig ( plot , filename ): plot . savefig ( filename ) plt . clf () positionwise_feed_forward \u00b6 Positionwise feed forward layer definition. PositionwiseFeedForward Positionwise feed forward layer. :param int idim: input dimenstion :param int hidden_units: number of hidden units :param float dropout_rate: dropout rate __init__ ( self , idim , hidden_units , dropout_rate ) special Construct an PositionwiseFeedForward object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/positionwise_feed_forward.py 21 22 23 24 25 26 def __init__ ( self , idim , hidden_units , dropout_rate ): \"\"\"Construct an PositionwiseFeedForward object.\"\"\" super ( PositionwiseFeedForward , self ) . __init__ () self . w_1 = torch . nn . Linear ( idim , hidden_units ) self . w_2 = torch . nn . Linear ( hidden_units , idim ) self . dropout = torch . nn . Dropout ( dropout_rate ) forward ( self , x ) Forward funciton. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/positionwise_feed_forward.py 28 29 30 def forward ( self , x ): \"\"\"Forward funciton.\"\"\" return self . w_2 ( self . dropout ( torch . relu ( self . w_1 ( x )))) repeat \u00b6 Repeat the same layer definition. MultiSequential Multi-input multi-output torch.nn.Sequential. forward ( self , * args ) Repeat. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/repeat.py 15 16 17 18 19 def forward ( self , * args ): \"\"\"Repeat.\"\"\" for m in self : args = m ( * args ) return args repeat ( N , fn ) Repeat module N times. :param int N: repeat time :param function fn: function to generate module :return: repeated modules :rtype: MultiSequential Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/repeat.py 22 23 24 25 26 27 28 29 30 def repeat ( N , fn ): \"\"\"Repeat module N times. :param int N: repeat time :param function fn: function to generate module :return: repeated modules :rtype: MultiSequential \"\"\" return MultiSequential ( * [ fn () for _ in range ( N )]) subsampling \u00b6 Subsampling layer definition. Conv2dSubsampling Convolutional 2D subsampling (to 1/4 length). :param int idim: input dim :param int odim: output dim :param flaot dropout_rate: dropout rate __init__ ( self , idim , odim , dropout_rate ) special Construct an Conv2dSubsampling object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/subsampling.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , idim , odim , dropout_rate ): \"\"\"Construct an Conv2dSubsampling object.\"\"\" super ( Conv2dSubsampling , self ) . __init__ () self . conv = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , odim , 3 , 2 ), torch . nn . ReLU (), torch . nn . Conv2d ( odim , odim , 3 , 2 ), torch . nn . ReLU () ) self . out = torch . nn . Sequential ( torch . nn . Linear ( odim * ((( idim - 1 ) // 2 - 1 ) // 2 ), odim ), PositionalEncoding ( odim , dropout_rate ) ) forward ( self , x , x_mask ) Subsample x. :param torch.Tensor x: input tensor :param torch.Tensor x_mask: input mask :return: subsampled x and mask :rtype Tuple[torch.Tensor, torch.Tensor] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/subsampling.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def forward ( self , x , x_mask ): \"\"\"Subsample x. :param torch.Tensor x: input tensor :param torch.Tensor x_mask: input mask :return: subsampled x and mask :rtype Tuple[torch.Tensor, torch.Tensor] \"\"\" x = x . unsqueeze ( 1 ) # (b, c, t, f) x = self . conv ( x ) b , c , t , f = x . size () x = self . out ( x . transpose ( 1 , 2 ) . contiguous () . view ( b , t , c * f )) if x_mask is None : return x , None return x , x_mask [:, :, : - 2 : 2 ][:, :, : - 2 : 2 ] wavenet \u00b6 This code is based on https://github.com/kan-bayashi/PytorchWaveNetVocoder. CausalConv1d \u00b6 1D dilated causal convolution. __init__ ( self , in_channels , out_channels , kernel_size , dilation = 1 , bias = True ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 99 100 101 102 103 104 105 106 107 def __init__ ( self , in_channels , out_channels , kernel_size , dilation = 1 , bias = True ): super ( CausalConv1d , self ) . __init__ () self . in_channels = in_channels self . out_channels = out_channels self . kernel_size = kernel_size self . dilation = dilation self . padding = padding = ( kernel_size - 1 ) * dilation self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , padding = padding , dilation = dilation , bias = bias ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Input tensor with the shape (B, in_channels, T). required Returns: Type Description Tensor Tensor with the shape (B, out_channels, T) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Input tensor with the shape (B, in_channels, T). Returns: Tensor: Tensor with the shape (B, out_channels, T) \"\"\" x = self . conv ( x ) if self . padding != 0 : x = x [:, :, : - self . padding ] return x OneHot \u00b6 Convert to one-hot vector. !!! args depth (int): Dimension of one-hot vector. __init__ ( self , depth ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 75 76 77 def __init__ ( self , depth ): super ( OneHot , self ) . __init__ () self . depth = depth forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x LongTensor long tensor variable with the shape (B, T) required Returns: Type Description Tensor float tensor variable with the shape (B, depth, T) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (LongTensor): long tensor variable with the shape (B, T) Returns: Tensor: float tensor variable with the shape (B, depth, T) \"\"\" x = x % self . depth x = torch . unsqueeze ( x , 2 ) x_onehot = x . new_zeros ( x . size ( 0 ), x . size ( 1 ), self . depth ) . float () return x_onehot . scatter_ ( 2 , x , 1 ) UpSampling \u00b6 Upsampling layer with deconvolution. !!! args upsampling_factor (int): Upsampling factor. __init__ ( self , upsampling_factor , bias = True ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 133 134 135 136 137 138 139 140 def __init__ ( self , upsampling_factor , bias = True ): super ( UpSampling , self ) . __init__ () self . upsampling_factor = upsampling_factor self . bias = bias self . conv = nn . ConvTranspose2d ( 1 , 1 , kernel_size = ( 1 , self . upsampling_factor ), stride = ( 1 , self . upsampling_factor ), bias = self . bias ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Input tensor with the shape (B, C, T) required Returns: Type Description Tensor Tensor with the shape (B, C, T') where T' = T * upsampling_factor. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 142 143 144 145 146 147 148 149 150 151 152 153 154 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Input tensor with the shape (B, C, T) Returns: Tensor: Tensor with the shape (B, C, T') where T' = T * upsampling_factor. \"\"\" x = x . unsqueeze ( 1 ) # B x 1 x C x T x = self . conv ( x ) # B x 1 x C x T' return x . squeeze ( 1 ) WaveNet \u00b6 Conditional wavenet. !!! args n_quantize (int): Number of quantization. n_aux (int): Number of aux feature dimension. n_resch (int): Number of filter channels for residual block. n_skipch (int): Number of filter channels for skip connection. dilation_depth (int): Number of dilation depth (e.g. if set 10, max dilation = 2^(10-1)). dilation_repeat (int): Number of dilation repeat. kernel_size (int): Filter size of dilated causal convolution. upsampling_factor (int): Upsampling factor. __init__ ( self , n_quantize = 256 , n_aux = 28 , n_resch = 512 , n_skipch = 256 , dilation_depth = 10 , dilation_repeat = 3 , kernel_size = 2 , upsampling_factor = 0 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def __init__ ( self , n_quantize = 256 , n_aux = 28 , n_resch = 512 , n_skipch = 256 , dilation_depth = 10 , dilation_repeat = 3 , kernel_size = 2 , upsampling_factor = 0 ): super ( WaveNet , self ) . __init__ () self . n_aux = n_aux self . n_quantize = n_quantize self . n_resch = n_resch self . n_skipch = n_skipch self . kernel_size = kernel_size self . dilation_depth = dilation_depth self . dilation_repeat = dilation_repeat self . upsampling_factor = upsampling_factor self . dilations = [ 2 ** i for i in range ( self . dilation_depth )] * self . dilation_repeat self . receptive_field = ( self . kernel_size - 1 ) * sum ( self . dilations ) + 1 # for preprocessing self . onehot = OneHot ( self . n_quantize ) self . causal = CausalConv1d ( self . n_quantize , self . n_resch , self . kernel_size ) if self . upsampling_factor > 0 : self . upsampling = UpSampling ( self . upsampling_factor ) # for residual blocks self . dil_sigmoid = nn . ModuleList () self . dil_tanh = nn . ModuleList () self . aux_1x1_sigmoid = nn . ModuleList () self . aux_1x1_tanh = nn . ModuleList () self . skip_1x1 = nn . ModuleList () self . res_1x1 = nn . ModuleList () for d in self . dilations : self . dil_sigmoid += [ CausalConv1d ( self . n_resch , self . n_resch , self . kernel_size , d )] self . dil_tanh += [ CausalConv1d ( self . n_resch , self . n_resch , self . kernel_size , d )] self . aux_1x1_sigmoid += [ nn . Conv1d ( self . n_aux , self . n_resch , 1 )] self . aux_1x1_tanh += [ nn . Conv1d ( self . n_aux , self . n_resch , 1 )] self . skip_1x1 += [ nn . Conv1d ( self . n_resch , self . n_skipch , 1 )] self . res_1x1 += [ nn . Conv1d ( self . n_resch , self . n_resch , 1 )] # for postprocessing self . conv_post_1 = nn . Conv1d ( self . n_skipch , self . n_skipch , 1 ) self . conv_post_2 = nn . Conv1d ( self . n_skipch , self . n_quantize , 1 ) forward ( self , x , h ) Calculate forward propagation. Parameters: Name Type Description Default x LongTensor Quantized input waveform tensor with the shape (B, T). required h Tensor Auxiliary feature tensor with the shape (B, n_aux, T). required Returns: Type Description Tensor Logits with the shape (B, T, n_quantize). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def forward ( self , x , h ): \"\"\"Calculate forward propagation. Args: x (LongTensor): Quantized input waveform tensor with the shape (B, T). h (Tensor): Auxiliary feature tensor with the shape (B, n_aux, T). Returns: Tensor: Logits with the shape (B, T, n_quantize). \"\"\" # preprocess output = self . _preprocess ( x ) if self . upsampling_factor > 0 : h = self . upsampling ( h ) # residual block skip_connections = [] for l in range ( len ( self . dilations )): output , skip = self . _residual_forward ( output , h , self . dil_sigmoid [ l ], self . dil_tanh [ l ], self . aux_1x1_sigmoid [ l ], self . aux_1x1_tanh [ l ], self . skip_1x1 [ l ], self . res_1x1 [ l ]) skip_connections . append ( skip ) # skip-connection part output = sum ( skip_connections ) output = self . _postprocess ( output ) return output generate ( self , x , h , n_samples , interval = None , mode = 'sampling' ) Generate a waveform with fast genration algorithm. This generation based on Fast WaveNet Generation Algorithm _. Parameters: Name Type Description Default x LongTensor Initial waveform tensor with the shape (T,). required h Tensor Auxiliary feature tensor with the shape (n_samples + T, n_aux). required n_samples int Number of samples to be generated. required interval int Log interval. None mode str \"sampling\" or \"argmax\". 'sampling' Returns: Type Description ndarray Generated quantized waveform (n_samples). .. _ Fast WaveNet Generation Algorithm : https://arxiv.org/abs/1611.09482 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def generate ( self , x , h , n_samples , interval = None , mode = \"sampling\" ): \"\"\"Generate a waveform with fast genration algorithm. This generation based on `Fast WaveNet Generation Algorithm`_. Args: x (LongTensor): Initial waveform tensor with the shape (T,). h (Tensor): Auxiliary feature tensor with the shape (n_samples + T, n_aux). n_samples (int): Number of samples to be generated. interval (int, optional): Log interval. mode (str, optional): \"sampling\" or \"argmax\". Return: ndarray: Generated quantized waveform (n_samples). .. _`Fast WaveNet Generation Algorithm`: https://arxiv.org/abs/1611.09482 \"\"\" # reshape inputs assert len ( x . shape ) == 1 assert len ( h . shape ) == 2 and h . shape [ 1 ] == self . n_aux x = x . unsqueeze ( 0 ) h = h . transpose ( 0 , 1 ) . unsqueeze ( 0 ) # perform upsampling if self . upsampling_factor > 0 : h = self . upsampling ( h ) # padding for shortage if n_samples > h . shape [ 2 ]: h = F . pad ( h , ( 0 , n_samples - h . shape [ 2 ]), \"replicate\" ) # padding if the length less than n_pad = self . receptive_field - x . size ( 1 ) if n_pad > 0 : x = F . pad ( x , ( n_pad , 0 ), \"constant\" , self . n_quantize // 2 ) h = F . pad ( h , ( n_pad , 0 ), \"replicate\" ) # prepare buffer output = self . _preprocess ( x ) h_ = h [:, :, : x . size ( 1 )] output_buffer = [] buffer_size = [] for l , d in enumerate ( self . dilations ): output , _ = self . _residual_forward ( output , h_ , self . dil_sigmoid [ l ], self . dil_tanh [ l ], self . aux_1x1_sigmoid [ l ], self . aux_1x1_tanh [ l ], self . skip_1x1 [ l ], self . res_1x1 [ l ]) if d == 2 ** ( self . dilation_depth - 1 ): buffer_size . append ( self . kernel_size - 1 ) else : buffer_size . append ( d * 2 * ( self . kernel_size - 1 )) output_buffer . append ( output [:, :, - buffer_size [ l ] - 1 : - 1 ]) # generate samples = x [ 0 ] start_time = time . time () for i in range ( n_samples ): output = samples [ - self . kernel_size * 2 + 1 :] . unsqueeze ( 0 ) output = self . _preprocess ( output ) h_ = h [:, :, samples . size ( 0 ) - 1 ] . contiguous () . view ( 1 , self . n_aux , 1 ) output_buffer_next = [] skip_connections = [] for l , d in enumerate ( self . dilations ): output , skip = self . _generate_residual_forward ( output , h_ , self . dil_sigmoid [ l ], self . dil_tanh [ l ], self . aux_1x1_sigmoid [ l ], self . aux_1x1_tanh [ l ], self . skip_1x1 [ l ], self . res_1x1 [ l ]) output = torch . cat ([ output_buffer [ l ], output ], dim = 2 ) output_buffer_next . append ( output [:, :, - buffer_size [ l ]:]) skip_connections . append ( skip ) # update buffer output_buffer = output_buffer_next # get predicted sample output = sum ( skip_connections ) output = self . _postprocess ( output )[ 0 ] if mode == \"sampling\" : posterior = F . softmax ( output [ - 1 ], dim = 0 ) dist = torch . distributions . Categorical ( posterior ) sample = dist . sample () . unsqueeze ( 0 ) elif mode == \"argmax\" : sample = output . argmax ( - 1 ) else : logging . error ( \"mode should be sampling or argmax\" ) sys . exit ( 1 ) samples = torch . cat ([ samples , sample ], dim = 0 ) # show progress if interval is not None and ( i + 1 ) % interval == 0 : elapsed_time_per_sample = ( time . time () - start_time ) / interval logging . info ( \" %d / %d estimated time = %.3f sec ( %.3f sec / sample)\" % ( i + 1 , n_samples , ( n_samples - i - 1 ) * elapsed_time_per_sample , elapsed_time_per_sample )) start_time = time . time () return samples [ - n_samples :] . cpu () . numpy () decode_mu_law ( y , mu = 256 ) \u00b6 Perform mu-law decoding. Parameters: Name Type Description Default x ndarray Quantized audio signal with the range from 0 to mu - 1. required mu int Quantized level. 256 Returns: Type Description ndarray Audio signal with the range from -1 to 1. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def decode_mu_law ( y , mu = 256 ): \"\"\"Perform mu-law decoding. Args: x (ndarray): Quantized audio signal with the range from 0 to mu - 1. mu (int): Quantized level. Returns: ndarray: Audio signal with the range from -1 to 1. \"\"\" mu = mu - 1 fx = ( y - 0.5 ) / mu * 2 - 1 x = np . sign ( fx ) / mu * (( 1 + mu ) ** np . abs ( fx ) - 1 ) return x encode_mu_law ( x , mu = 256 ) \u00b6 Perform mu-law encoding. Parameters: Name Type Description Default x ndarray Audio signal with the range from -1 to 1. required mu int Quantized level. 256 Returns: Type Description ndarray Quantized audio signal with the range from 0 to mu - 1. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def encode_mu_law ( x , mu = 256 ): \"\"\"Perform mu-law encoding. Args: x (ndarray): Audio signal with the range from -1 to 1. mu (int): Quantized level. Returns: ndarray: Quantized audio signal with the range from 0 to mu - 1. \"\"\" mu = mu - 1 fx = np . sign ( x ) * np . log ( 1 + mu * np . abs ( x )) / np . log ( 1 + mu ) return np . floor (( fx + 1 ) / 2 * mu + 0.5 ) . astype ( np . int64 ) initialize ( m ) \u00b6 Initilize conv layers with xavier. Parameters: Name Type Description Default m torch.nn.Module Torch module. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def initialize ( m ): \"\"\"Initilize conv layers with xavier. Args: m (torch.nn.Module): Torch module. \"\"\" if isinstance ( m , nn . Conv1d ): nn . init . xavier_uniform_ ( m . weight ) nn . init . constant_ ( m . bias , 0.0 ) if isinstance ( m , nn . ConvTranspose2d ): nn . init . constant_ ( m . weight , 1.0 ) nn . init . constant_ ( m . bias , 0.0 ) scorer_interface \u00b6 Scorer interface module. BatchScorerInterface \u00b6 Batch scorer interface. score ( self , ys , states , xs ) \u00b6 Score new token batch (required). Parameters: Name Type Description Default ys Tensor torch.int64 prefix tokens (n_batch, ylen). required states List[Any] Scorer states for prefix tokens. required xs Tensor The encoder feature that generates ys (n_batch, xlen, n_feat). required Returns: Type Description Tuple[torch.Tensor, List[Any]] tuple[torch.Tensor, List[Any]]: Tuple of batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys. Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def score ( self , ys : torch . Tensor , states : List [ Any ], xs : torch . Tensor ) -> Tuple [ torch . Tensor , List [ Any ]]: \"\"\"Score new token batch (required). Args: ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen). states (List[Any]): Scorer states for prefix tokens. xs (torch.Tensor): The encoder feature that generates ys (n_batch, xlen, n_feat). Returns: tuple[torch.Tensor, List[Any]]: Tuple of batchfied scores for next token with shape of `(n_batch, n_vocab)` and next state list for ys. \"\"\" raise NotImplementedError PartialScorerInterface \u00b6 Partial scorer interface for beam search. The partial scorer performs scoring when non - partial scorer finished scoring , and recieves pre - pruned next tokens to score because it is too heavy to score all the tokens . !!! examples * Prefix search for connectionist - temporal - classification models * : class : ` services . hci . speech . espnet_minimal . nets . scorers . ctc . CTCPrefixScorer ` score_partial ( self , y , next_tokens , state , x ) \u00b6 Score new token (required). Parameters: Name Type Description Default y Tensor 1D prefix token required next_tokens Tensor torch.int64 next token to score required state Any decoder state for prefix tokens required x Tensor The encoder feature that generates ys required Returns: Type Description Tuple[torch.Tensor, Any] tuple[torch.Tensor, Any]: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def score_partial ( self , y : torch . Tensor , next_tokens : torch . Tensor , state : Any , x : torch . Tensor ) \\ -> Tuple [ torch . Tensor , Any ]: \"\"\"Score new token (required). Args: y (torch.Tensor): 1D prefix token next_tokens (torch.Tensor): torch.int64 next token to score state: decoder state for prefix tokens x (torch.Tensor): The encoder feature that generates ys Returns: tuple[torch.Tensor, Any]: Tuple of a score tensor for y that has a shape `(len(next_tokens),)` and next state for ys \"\"\" raise NotImplementedError ScorerInterface \u00b6 Scorer interface for beam search. The scorer performs scoring of the all tokens in vocabulary . !!! examples * Search heuristics * : class : ` services . hci . speech . espnet_minimal . nets . scorers . length_bonus . LengthBonus ` * Decoder networks of the sequence - to - sequence models * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . nets . transformer . decoder . Decoder ` * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . nets . rnn . decoders . Decoder ` * Neural language models * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . lm . transformer . TransformerLM ` * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . lm . default . DefaultRNNLM ` * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . lm . seq_rnn . SequentialRNNLM ` final_score ( self , state ) \u00b6 Score eos (optional). Parameters: Name Type Description Default state Any Scorer state for prefix tokens required Returns: Type Description float float: final score Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 68 69 70 71 72 73 74 75 76 77 78 def final_score ( self , state : Any ) -> float : \"\"\"Score eos (optional). Args: state: Scorer state for prefix tokens Returns: float: final score \"\"\" return 0.0 init_state ( self , x ) \u00b6 Get an initial state for decoding (optional). Parameters: Name Type Description Default x Tensor The encoded feature tensor required Returns: initial state Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 28 29 30 31 32 33 34 35 36 37 def init_state ( self , x : torch . Tensor ) -> Any : \"\"\"Get an initial state for decoding (optional). Args: x (torch.Tensor): The encoded feature tensor Returns: initial state \"\"\" return None score ( self , y , state , x ) \u00b6 Score new token (required). Parameters: Name Type Description Default y Tensor 1D torch.int64 prefix tokens. required state Any Scorer state for prefix tokens required x Tensor The encoder feature that generates ys. required Returns: Type Description Tuple[torch.Tensor, Any] tuple[torch.Tensor, Any]: Tuple of scores for next token that has a shape of (n_vocab) and next state for ys Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def score ( self , y : torch . Tensor , state : Any , x : torch . Tensor ) -> Tuple [ torch . Tensor , Any ]: \"\"\"Score new token (required). Args: y (torch.Tensor): 1D torch.int64 prefix tokens. state: Scorer state for prefix tokens x (torch.Tensor): The encoder feature that generates ys. Returns: tuple[torch.Tensor, Any]: Tuple of scores for next token that has a shape of `(n_vocab)` and next state for ys \"\"\" raise NotImplementedError select_state ( self , state , i ) \u00b6 Select state with relative ids in the main beam search. Parameters: Name Type Description Default state Any Decoder state for prefix tokens required i int Index to select a state in the main beam search required Returns: Type Description Any state: pruned state Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 39 40 41 42 43 44 45 46 47 48 49 50 def select_state ( self , state : Any , i : int ) -> Any : \"\"\"Select state with relative ids in the main beam search. Args: state: Decoder state for prefix tokens i (int): Index to select a state in the main beam search Returns: state: pruned state \"\"\" return None if state is None else state [ i ] scorers special \u00b6 ctc \u00b6 ScorerInterface implementation for CTC. CTCPrefixScorer \u00b6 Decoder interface wrapper for CTCPrefixScore. __init__ ( self , ctc , eos ) special Initialize class. Parameters: Name Type Description Default ctc Module The CTC implementaiton. For example, :class: services.hci.speech.espnet_minimal.nets.pytorch_backend.ctc.CTC required eos int The end-of-sequence id. required Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , ctc : torch . nn . Module , eos : int ): \"\"\"Initialize class. Args: ctc (torch.nn.Module): The CTC implementaiton. For example, :class:`services.hci.speech.espnet_minimal.nets.pytorch_backend.ctc.CTC` eos (int): The end-of-sequence id. \"\"\" self . ctc = ctc self . eos = eos self . impl = None init_state ( self , x ) Get an initial state for decoding. Parameters: Name Type Description Default x Tensor The encoded feature tensor required Returns: initial state Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def init_state ( self , x : torch . Tensor ): \"\"\"Get an initial state for decoding. Args: x (torch.Tensor): The encoded feature tensor Returns: initial state \"\"\" logp = self . ctc . log_softmax ( x . unsqueeze ( 0 )) . detach () . squeeze ( 0 ) . cpu () . numpy () # TODO(karita): use CTCPrefixScoreTH self . impl = CTCPrefixScore ( logp , 0 , self . eos , np ) return 0 , self . impl . initial_state () score_partial ( self , y , ids , state , x ) Score new token. Parameters: Name Type Description Default y torch.Tensor 1D prefix token required next_tokens torch.Tensor torch.int64 next token to score required state decoder state for prefix tokens required x torch.Tensor 2D encoder feature that generates ys required Returns: Type Description tuple[torch.Tensor, Any] Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def score_partial ( self , y , ids , state , x ): \"\"\"Score new token. Args: y (torch.Tensor): 1D prefix token next_tokens (torch.Tensor): torch.int64 next token to score state: decoder state for prefix tokens x (torch.Tensor): 2D encoder feature that generates ys Returns: tuple[torch.Tensor, Any]: Tuple of a score tensor for y that has a shape `(len(next_tokens),)` and next state for ys \"\"\" prev_score , state = state presub_score , new_st = self . impl ( y . cpu (), ids . cpu (), state ) tscore = torch . as_tensor ( presub_score - prev_score , device = x . device , dtype = x . dtype ) return tscore , ( presub_score , new_st ) select_state ( self , state , i ) Select state with relative ids in the main beam search. Parameters: Name Type Description Default state Decoder state for prefix tokens required i int Index to select a state in the main beam search required Returns: Type Description state pruned state Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 39 40 41 42 43 44 45 46 47 48 49 50 51 def select_state ( self , state , i ): \"\"\"Select state with relative ids in the main beam search. Args: state: Decoder state for prefix tokens i (int): Index to select a state in the main beam search Returns: state: pruned state \"\"\" sc , st = state return sc [ i ], st [ i ] tts_interface \u00b6 TTS Interface realted modules. TTSInterface \u00b6 TTS Interface for ESPnet model implementation. base_plot_keys property readonly \u00b6 Return base key names to plot during training. The keys should match what chainer.reporter reports. if you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list[str] Base keys to plot during training. __init__ ( self ) special \u00b6 Initilize TTS module. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 19 20 21 def __init__ ( self ): \"\"\"Initilize TTS module.\"\"\" self . reporter = None add_arguments ( parser ) staticmethod \u00b6 Add model specific argments to parser. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 14 15 16 17 @staticmethod def add_arguments ( parser ): \"\"\"Add model specific argments to parser.\"\"\" return parser calculate_all_attentions ( self , * args , ** kwargs ) \u00b6 Calculate TTS attention weights. Parameters: Name Type Description Default Tensor Batch of attention weights (B, Lmax, Tmax). required Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 43 44 45 46 47 48 49 50 def calculate_all_attentions ( self , * args , ** kwargs ): \"\"\"Calculate TTS attention weights. Args: Tensor: Batch of attention weights (B, Lmax, Tmax). \"\"\" raise NotImplementedError ( \"calculate_all_attentions method is not implemented\" ) forward ( self , * args , ** kwargs ) \u00b6 Calculate TTS forward propagation. Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 23 24 25 26 27 28 29 30 def forward ( self , * args , ** kwargs ): \"\"\"Calculate TTS forward propagation. Returns: Tensor: Loss value. \"\"\" raise NotImplementedError ( \"forward method is not implemented\" ) inference ( self , * args , ** kwargs ) \u00b6 Generate the sequence of features given the sequences of characters. Returns: Type Description Tensor The sequence of generated features (L, odim). Tensor: The sequence of stop probabilities (L,). Tensor: The sequence of attention weights (L, T). Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 32 33 34 35 36 37 38 39 40 41 def inference ( self , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Returns: Tensor: The sequence of generated features (L, odim). Tensor: The sequence of stop probabilities (L,). Tensor: The sequence of attention weights (L, T). \"\"\" raise NotImplementedError ( \"inference method is not implemented\" ) load_pretrained_model ( self , model_path ) \u00b6 Load pretrained model parameters. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 52 53 54 def load_pretrained_model ( self , model_path ): \"\"\"Load pretrained model parameters.\"\"\" torch_load ( model_path , self ) utils special \u00b6 check_kwargs \u00b6 check_kwargs ( func , kwargs , name = None ) \u00b6 check kwargs are valid for func If kwargs are invalid, raise TypeError as same as python default :param function func: function to be validated :param dict kwargs: keyword arguments for func :param str name: name used in TypeError (default is func name) Source code in adviser/tools/espnet_minimal/utils/check_kwargs.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def check_kwargs ( func , kwargs , name = None ): \"\"\"check kwargs are valid for func If kwargs are invalid, raise TypeError as same as python default :param function func: function to be validated :param dict kwargs: keyword arguments for func :param str name: name used in TypeError (default is func name) \"\"\" try : params = inspect . signature ( func ) . parameters except ValueError : return if name is None : name = func . __name__ for k in kwargs . keys (): if k not in params : raise TypeError ( f \" { name } () got an unexpected keyword argument ' { k } '\" ) cli_readers \u00b6 HDF5Reader \u00b6 __init__ ( self , rspecifier , return_shape = False ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , rspecifier , return_shape = False ): if ':' not in rspecifier : raise ValueError ( 'Give \"rspecifier\" such as \"ark:some.ark: {} \"' . format ( self . rspecifier )) self . rspecifier = rspecifier self . ark_or_scp , self . filepath = self . rspecifier . split ( ':' , 1 ) if self . ark_or_scp not in [ 'ark' , 'scp' ]: raise ValueError ( f 'Must be scp or ark: { self . ark_or_scp } ' ) self . return_shape = return_shape __iter__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __iter__ ( self ): if self . ark_or_scp == 'scp' : hdf5_dict = {} with open ( self . filepath , 'r' , encoding = 'utf-8' ) as f : for line in f : key , value = line . rstrip () . split ( None , 1 ) if ':' not in value : raise RuntimeError ( 'scp file for hdf5 should be like: ' '\"uttid filepath.h5:key\": {} ( {} )' . format ( line , self . filepath )) path , h5_key = value . split ( ':' , 1 ) hdf5_file = hdf5_dict . get ( path ) if hdf5_file is None : try : hdf5_file = h5py . File ( path , 'r' ) except Exception : logging . error ( 'Error when loading {} ' . format ( path )) raise hdf5_dict [ path ] = hdf5_file try : data = hdf5_file [ h5_key ] except Exception : logging . error ( 'Error when loading {} with key= {} ' . format ( path , h5_key )) raise if self . return_shape : yield key , data . shape else : yield key , data [()] # Closing all files for k in hdf5_dict : try : hdf5_dict [ k ] . close () except Exception : pass else : if self . filepath == '-' : # Required h5py>=2.9 filepath = io . BytesIO ( sys . stdin . buffer . read ()) else : filepath = self . filepath with h5py . File ( filepath , 'r' ) as f : for key in f : if self . return_shape : yield key , f [ key ] . shape else : yield key , f [ key ][()] KaldiReader \u00b6 __init__ ( self , rspecifier , return_shape = False , segments = None ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 54 55 56 57 def __init__ ( self , rspecifier , return_shape = False , segments = None ): self . rspecifier = rspecifier self . return_shape = return_shape self . segments = segments __iter__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 59 60 61 62 63 64 65 def __iter__ ( self ): with kaldiio . ReadHelper ( self . rspecifier , segments = self . segments ) as reader : for key , array in reader : if self . return_shape : array = array . shape yield key , array SoundHDF5Reader \u00b6 __init__ ( self , rspecifier , return_shape = False ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 138 139 140 141 142 143 144 145 def __init__ ( self , rspecifier , return_shape = False ): if ':' not in rspecifier : raise ValueError ( 'Give \"rspecifier\" such as \"ark:some.ark: {} \"' . format ( rspecifier )) self . ark_or_scp , self . filepath = rspecifier . split ( ':' , 1 ) if self . ark_or_scp not in [ 'ark' , 'scp' ]: raise ValueError ( f 'Must be scp or ark: { self . ark_or_scp } ' ) self . return_shape = return_shape __iter__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def __iter__ ( self ): if self . ark_or_scp == 'scp' : hdf5_dict = {} with open ( self . filepath , 'r' , encoding = 'utf-8' ) as f : for line in f : key , value = line . rstrip () . split ( None , 1 ) if ':' not in value : raise RuntimeError ( 'scp file for hdf5 should be like: ' '\"uttid filepath.h5:key\": {} ( {} )' . format ( line , self . filepath )) path , h5_key = value . split ( ':' , 1 ) hdf5_file = hdf5_dict . get ( path ) if hdf5_file is None : try : hdf5_file = SoundHDF5File ( path , 'r' ) except Exception : logging . error ( 'Error when loading {} ' . format ( path )) raise hdf5_dict [ path ] = hdf5_file try : data = hdf5_file [ h5_key ] except Exception : logging . error ( 'Error when loading {} with key= {} ' . format ( path , h5_key )) raise # Change Tuple[ndarray, int] -> Tuple[int, ndarray] # (soundfile style -> scipy style) array , rate = data if self . return_shape : array = array . shape yield key , ( rate , array ) # Closing all files for k in hdf5_dict : try : hdf5_dict [ k ] . close () except Exception : pass else : if self . filepath == '-' : # Required h5py>=2.9 filepath = io . BytesIO ( sys . stdin . buffer . read ()) else : filepath = self . filepath for key , ( a , r ) in SoundHDF5File ( filepath , 'r' ) . items (): if self . return_shape : a = a . shape yield key , ( r , a ) SoundReader \u00b6 __init__ ( self , rspecifier , return_shape = False ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 205 206 207 208 209 210 211 212 213 def __init__ ( self , rspecifier , return_shape = False ): if ':' not in rspecifier : raise ValueError ( 'Give \"rspecifier\" such as \"scp:some.scp: {} \"' . format ( rspecifier )) self . ark_or_scp , self . filepath = rspecifier . split ( ':' , 1 ) if self . ark_or_scp != 'scp' : raise ValueError ( 'Only supporting \"scp\" for sound file: {} ' . format ( self . ark_or_scp )) self . return_shape = return_shape __iter__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 215 216 217 218 219 220 221 222 223 224 225 def __iter__ ( self ): with open ( self . filepath , 'r' , encoding = 'utf-8' ) as f : for line in f : key , sound_file_path = line . rstrip () . split ( None , 1 ) # Assume PCM16 array , rate = soundfile . read ( sound_file_path , dtype = 'int16' ) # Change Tuple[ndarray, int] -> Tuple[int, ndarray] # (soundfile style -> scipy style) if self . return_shape : array = array . shape yield key , ( rate , array ) file_reader_helper ( rspecifier , filetype = 'mat' , return_shape = False , segments = None ) \u00b6 Read uttid and array in kaldi style This function might be a bit confusing as \"ark\" is used for HDF5 to imitate \"kaldi-rspecifier\". Parameters: Name Type Description Default rspecifier str Give as \"ark:feats.ark\" or \"scp:feats.scp\" required filetype str \"mat\" is kaldi-martix, \"hdf5\": HDF5 'mat' return_shape bool Return the shape of the matrix, instead of the matrix. This can reduce IO cost for HDF5. False Returns: Type Description Generator[Tuple[str, np.ndarray], None, None] Examples Read from kaldi-matrix ark file: for u, array in file_reader_helper('ark:feats.ark', 'mat'): ... array Read from HDF5 file: for u, array in file_reader_helper('ark:feats.h5', 'hdf5'): ... array Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def file_reader_helper ( rspecifier : str , filetype : str = 'mat' , return_shape : bool = False , segments : str = None ): \"\"\"Read uttid and array in kaldi style This function might be a bit confusing as \"ark\" is used for HDF5 to imitate \"kaldi-rspecifier\". Args: rspecifier: Give as \"ark:feats.ark\" or \"scp:feats.scp\" filetype: \"mat\" is kaldi-martix, \"hdf5\": HDF5 return_shape: Return the shape of the matrix, instead of the matrix. This can reduce IO cost for HDF5. Returns: Generator[Tuple[str, np.ndarray], None, None]: Examples: Read from kaldi-matrix ark file: >>> for u, array in file_reader_helper('ark:feats.ark', 'mat'): ... array Read from HDF5 file: >>> for u, array in file_reader_helper('ark:feats.h5', 'hdf5'): ... array \"\"\" if filetype == 'mat' : return KaldiReader ( rspecifier , return_shape = return_shape , segments = segments ) elif filetype == 'hdf5' : return HDF5Reader ( rspecifier , return_shape = return_shape ) elif filetype == 'sound.hdf5' : return SoundHDF5Reader ( rspecifier , return_shape = return_shape ) elif filetype == 'sound' : return SoundReader ( rspecifier , return_shape = return_shape ) else : raise NotImplementedError ( f 'filetype= { filetype } ' ) cli_utils \u00b6 assert_scipy_wav_style ( value ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 33 34 35 36 37 38 def assert_scipy_wav_style ( value ): assert is_scipy_wav_style ( value ), \\ 'Must be Tuple[int, numpy.ndarray], but got {} ' . format ( type ( value ) if not isinstance ( value , Sequence ) else ' {} [ {} ]' . format ( type ( value ), ', ' . join ( str ( type ( v )) for v in value ))) get_commandline_args () \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 13 14 15 16 17 18 19 20 21 22 23 def get_commandline_args (): extra_chars = [ ' ' , ';' , '&' , '(' , ')' , '|' , '^' , '<' , '>' , '?' , '*' , '[' , ']' , '$' , '`' , '\"' , ' \\\\ ' , '!' , '{' , '}' ] # Escape the extra characters for shell argv = [ arg . replace ( ' \\' ' , ' \\'\\\\\\'\\' ' ) if all ( char not in arg for char in extra_chars ) else ' \\' ' + arg . replace ( ' \\' ' , ' \\'\\\\\\'\\' ' ) + ' \\' ' for arg in sys . argv ] return sys . executable + ' ' + ' ' . join ( argv ) is_scipy_wav_style ( value ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 26 27 28 29 30 def is_scipy_wav_style ( value ): # If Tuple[int, numpy.ndarray] or not return ( isinstance ( value , Sequence ) and len ( value ) == 2 and isinstance ( value [ 0 ], int ) and isinstance ( value [ 1 ], numpy . ndarray )) strtobool ( x ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 8 9 10 def strtobool ( x ): # distutils.util.strtobool returns integer, but it's confusing, return bool ( dist_strtobool ( x )) cli_writers \u00b6 BaseWriter \u00b6 __enter__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 77 78 def __enter__ ( self ): return self __exit__ ( self , exc_type , exc_val , exc_tb ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 80 81 def __exit__ ( self , exc_type , exc_val , exc_tb ): self . close () __setitem__ ( self , key , value ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 74 75 def __setitem__ ( self , key , value ): raise NotImplementedError close ( self ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def close ( self ): try : self . writer . close () except Exception : pass if self . writer_scp is not None : try : self . writer_scp . close () except Exception : pass if self . writer_nframe is not None : try : self . writer_nframe . close () except Exception : pass HDF5Writer \u00b6 HDF5Writer !!! examples >>> with HDF5Writer('ark:out.h5', compress=True) as f: ... f['key'] = array __init__ ( self , wspecifier , write_num_frames = None , compress = False ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , wspecifier , write_num_frames = None , compress = False ): spec_dict = parse_wspecifier ( wspecifier ) self . filename = spec_dict [ 'ark' ] if compress : self . kwargs = { 'compression' : 'gzip' } else : self . kwargs = {} self . writer = h5py . File ( spec_dict [ 'ark' ], 'w' ) if 'scp' in spec_dict : self . writer_scp = open ( spec_dict [ 'scp' ], 'w' , encoding = 'utf-8' ) else : self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None __setitem__ ( self , key , value ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 190 191 192 193 194 195 196 def __setitem__ ( self , key , value ): self . writer . create_dataset ( key , data = value , ** self . kwargs ) if self . writer_scp is not None : self . writer_scp . write ( f ' { key } { self . filename } : { key } \\n ' ) if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( value ) } \\n ' ) KaldiWriter \u00b6 __init__ ( self , wspecifier , write_num_frames = None , compress = False , compression_method = 2 ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , wspecifier , write_num_frames = None , compress = False , compression_method = 2 ): if compress : self . writer = kaldiio . WriteHelper ( wspecifier , compression_method = compression_method ) else : self . writer = kaldiio . WriteHelper ( wspecifier ) self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None __setitem__ ( self , key , value ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 137 138 139 140 def __setitem__ ( self , key , value ): self . writer [ key ] = value if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( value ) } \\n ' ) SoundHDF5Writer \u00b6 SoundHDF5Writer !!! examples >>> fs = 16000 >>> with SoundHDF5Writer('ark:out.h5') as f: ... f['key'] = fs, array __init__ ( self , wspecifier , write_num_frames = None , pcm_format = 'wav' ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , wspecifier , write_num_frames = None , pcm_format = 'wav' ): self . pcm_format = pcm_format spec_dict = parse_wspecifier ( wspecifier ) self . filename = spec_dict [ 'ark' ] self . writer = SoundHDF5File ( spec_dict [ 'ark' ], 'w' , format = self . pcm_format ) if 'scp' in spec_dict : self . writer_scp = open ( spec_dict [ 'scp' ], 'w' , encoding = 'utf-8' ) else : self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None __setitem__ ( self , key , value ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 223 224 225 226 227 228 229 230 231 232 233 def __setitem__ ( self , key , value ): assert_scipy_wav_style ( value ) # Change Tuple[int, ndarray] -> Tuple[ndarray, int] # (scipy style -> soundfile style) value = ( value [ 1 ], value [ 0 ]) self . writer . create_dataset ( key , data = value ) if self . writer_scp is not None : self . writer_scp . write ( f ' { key } { self . filename } : { key } \\n ' ) if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( value [ 0 ]) } \\n ' ) SoundWriter \u00b6 SoundWriter !!! examples >>> fs = 16000 >>> with SoundWriter('ark,scp:outdir,out.scp') as f: ... f['key'] = fs, array __init__ ( self , wspecifier , write_num_frames = None , pcm_format = 'wav' ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def __init__ ( self , wspecifier , write_num_frames = None , pcm_format = 'wav' ): self . pcm_format = pcm_format spec_dict = parse_wspecifier ( wspecifier ) # e.g. ark,scp:dirname,wav.scp # -> The wave files are found in dirname/*.wav self . dirname = spec_dict [ 'ark' ] Path ( self . dirname ) . mkdir ( parents = True , exist_ok = True ) self . writer = None if 'scp' in spec_dict : self . writer_scp = open ( spec_dict [ 'scp' ], 'w' , encoding = 'utf-8' ) else : self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None __setitem__ ( self , key , value ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 263 264 265 266 267 268 269 270 271 272 def __setitem__ ( self , key , value ): assert_scipy_wav_style ( value ) rate , signal = value wavfile = Path ( self . dirname ) / ( key + '.' + self . pcm_format ) soundfile . write ( wavfile , signal . astype ( numpy . int16 ), rate ) if self . writer_scp is not None : self . writer_scp . write ( f ' { key } { wavfile } \\n ' ) if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( signal ) } \\n ' ) file_writer_helper ( wspecifier , filetype = 'mat' , write_num_frames = None , compress = False , compression_method = 2 , pcm_format = 'wav' ) \u00b6 Write matrices in kaldi style Parameters: Name Type Description Default wspecifier str e.g. ark,scp:out.ark,out.scp required filetype str \"mat\" is kaldi-martix, \"hdf5\": HDF5 'mat' write_num_frames str e.g. 'ark,t:num_frames.txt' None compress bool Compress or not False compression_method int Specify compression level 2 Write in kaldi-matrix-ark with \"kaldi-scp\" file: with file_writer_helper('ark,scp:out.ark,out.scp') as f: f['uttid'] = array This \"scp\" has the following format: uttidA out.ark:1234 uttidB out.ark:2222 where, 1234 and 2222 points the strating byte address of the matrix. (For detail, see official documentation of Kaldi) Write in HDF5 with \"scp\" file: with file_writer_helper('ark,scp:out.h5,out.scp', 'hdf5') as f: f['uttid'] = array This \"scp\" file is created as: uttidA out.h5:uttidA uttidB out.h5:uttidB HDF5 can be, unlike \"kaldi-ark\", accessed to any keys, so originally \"scp\" is not required for random-reading. Nevertheless we create \"scp\" for HDF5 because it is useful for some use-case. e.g. Concatenation, Splitting. Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def file_writer_helper ( wspecifier : str , filetype : str = 'mat' , write_num_frames : str = None , compress : bool = False , compression_method : int = 2 , pcm_format : str = 'wav' ): \"\"\"Write matrices in kaldi style Args: wspecifier: e.g. ark,scp:out.ark,out.scp filetype: \"mat\" is kaldi-martix, \"hdf5\": HDF5 write_num_frames: e.g. 'ark,t:num_frames.txt' compress: Compress or not compression_method: Specify compression level Write in kaldi-matrix-ark with \"kaldi-scp\" file: >>> with file_writer_helper('ark,scp:out.ark,out.scp') as f: >>> f['uttid'] = array This \"scp\" has the following format: uttidA out.ark:1234 uttidB out.ark:2222 where, 1234 and 2222 points the strating byte address of the matrix. (For detail, see official documentation of Kaldi) Write in HDF5 with \"scp\" file: >>> with file_writer_helper('ark,scp:out.h5,out.scp', 'hdf5') as f: >>> f['uttid'] = array This \"scp\" file is created as: uttidA out.h5:uttidA uttidB out.h5:uttidB HDF5 can be, unlike \"kaldi-ark\", accessed to any keys, so originally \"scp\" is not required for random-reading. Nevertheless we create \"scp\" for HDF5 because it is useful for some use-case. e.g. Concatenation, Splitting. \"\"\" if filetype == 'mat' : return KaldiWriter ( wspecifier , write_num_frames = write_num_frames , compress = compress , compression_method = compression_method ) elif filetype == 'hdf5' : return HDF5Writer ( wspecifier , write_num_frames = write_num_frames , compress = compress ) elif filetype == 'sound.hdf5' : return SoundHDF5Writer ( wspecifier , write_num_frames = write_num_frames , pcm_format = pcm_format ) elif filetype == 'sound' : return SoundWriter ( wspecifier , write_num_frames = write_num_frames , pcm_format = pcm_format ) else : raise NotImplementedError ( f 'filetype= { filetype } ' ) get_num_frames_writer ( write_num_frames ) \u00b6 get_num_frames_writer Examples get_num_frames_writer('ark,t:num_frames.txt') Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_num_frames_writer ( write_num_frames : str ): \"\"\"get_num_frames_writer Examples: >>> get_num_frames_writer('ark,t:num_frames.txt') \"\"\" if write_num_frames is not None : if ':' not in write_num_frames : raise ValueError ( 'Must include \":\", write_num_frames= {} ' . format ( write_num_frames )) nframes_type , nframes_file = write_num_frames . split ( ':' , 1 ) if nframes_type != 'ark,t' : raise ValueError ( 'Only supporting text mode. ' 'e.g. --write-num-frames=ark,t:foo.txt :' ' {} ' . format ( nframes_type )) return open ( nframes_file , 'w' , encoding = 'utf-8' ) parse_wspecifier ( wspecifier ) \u00b6 Parse wspecifier to dict Examples parse_wspecifier('ark,scp:out.ark,out.scp') {'ark': 'out.ark', 'scp': 'out.scp'} Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def parse_wspecifier ( wspecifier : str ) -> Dict [ str , str ]: \"\"\"Parse wspecifier to dict Examples: >>> parse_wspecifier('ark,scp:out.ark,out.scp') {'ark': 'out.ark', 'scp': 'out.scp'} \"\"\" ark_scp , filepath = wspecifier . split ( ':' , 1 ) if ark_scp not in [ 'ark' , 'scp,ark' , 'ark,scp' ]: raise ValueError ( ' {} is not allowed: {} ' . format ( ark_scp , wspecifier )) ark_scps = ark_scp . split ( ',' ) filepaths = filepath . split ( ',' ) if len ( ark_scps ) != len ( filepaths ): raise ValueError ( 'Mismatch: {} and {} ' . format ( ark_scp , filepath )) spec_dict = dict ( zip ( ark_scps , filepaths )) return spec_dict dataset \u00b6 This module contains pytorch dataset and dataloader implementation for chainer training. ChainerDataLoader \u00b6 Pytorch dataloader in chainer style. !!! args all args for torch.utils.data.dataloader.Dataloader epoch_detail property readonly \u00b6 Epoch_detail required by chainer. __init__ ( self , ** kwargs ) special \u00b6 Init function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 44 45 46 47 48 49 50 51 def __init__ ( self , ** kwargs ): \"\"\"Init function.\"\"\" self . loader = torch . utils . data . dataloader . DataLoader ( ** kwargs ) self . len = len ( kwargs [ 'dataset' ]) self . current_position = 0 self . epoch = 0 self . iter = None self . kwargs = kwargs __iter__ ( self ) special \u00b6 Implement iter function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 68 69 70 71 def __iter__ ( self ): \"\"\"Implement iter function.\"\"\" for batch in self . loader : yield batch finalize ( self ) \u00b6 Implement finalize function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 90 91 92 def finalize ( self ): \"\"\"Implement finalize function.\"\"\" del self . loader next ( self ) \u00b6 Implement next function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def next ( self ): \"\"\"Implement next function.\"\"\" if self . iter is None : self . iter = iter ( self . loader ) try : ret = next ( self . iter ) except StopIteration : self . iter = None return self . next () self . current_position += 1 if self . current_position == self . len : self . epoch = self . epoch + 1 self . current_position = 0 return ret serialize ( self , serializer ) \u00b6 Serialize and deserialize function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 78 79 80 81 82 83 def serialize ( self , serializer ): \"\"\"Serialize and deserialize function.\"\"\" epoch = serializer ( 'epoch' , self . epoch ) current_position = serializer ( 'current_position' , self . current_position ) self . epoch = epoch self . current_position = current_position start_shuffle ( self ) \u00b6 Shuffle function for sortagrad. Source code in adviser/tools/espnet_minimal/utils/dataset.py 85 86 87 88 def start_shuffle ( self ): \"\"\"Shuffle function for sortagrad.\"\"\" self . kwargs [ 'shuffle' ] = True self . loader = torch . utils . data . dataloader . DataLoader ( ** self . kwargs ) TransformDataset \u00b6 Transform Dataset for pytorch backend. !!! args data: list object from make_batchset transfrom: transform function __getitem__ ( self , idx ) special \u00b6 [] operator. Source code in adviser/tools/espnet_minimal/utils/dataset.py 31 32 33 def __getitem__ ( self , idx ): \"\"\"[] operator.\"\"\" return self . transform ( self . data [ idx ]) __init__ ( self , data , transform ) special \u00b6 Init function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 21 22 23 24 25 def __init__ ( self , data , transform ): \"\"\"Init function.\"\"\" super ( TransformDataset ) . __init__ () self . data = data self . transform = transform __len__ ( self ) special \u00b6 Len function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 27 28 29 def __len__ ( self ): \"\"\"Len function.\"\"\" return len ( self . data ) deterministic_utils \u00b6 set_deterministic_chainer ( args ) \u00b6 Ensures chainer produces deterministic results depending on the program arguments :param Namespace args: The program arguments Source code in adviser/tools/espnet_minimal/utils/deterministic_utils.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def set_deterministic_chainer ( args ): \"\"\"Ensures chainer produces deterministic results depending on the program arguments :param Namespace args: The program arguments \"\"\" # seed setting (chainer seed may not need it) os . environ [ 'CHAINER_SEED' ] = str ( args . seed ) logging . info ( 'chainer seed = ' + os . environ [ 'CHAINER_SEED' ]) # debug mode setting # 0 would be fastest, but 1 seems to be reasonable # considering reproducibility # remove type check if args . debugmode < 2 : chainer . config . type_check = False logging . info ( 'chainer type check is disabled' ) # use deterministic computation or not if args . debugmode < 1 : chainer . config . cudnn_deterministic = False logging . info ( 'chainer cudnn deterministic is disabled' ) else : chainer . config . cudnn_deterministic = True set_deterministic_pytorch ( args ) \u00b6 Ensures pytorch produces deterministic results depending on the program arguments :param Namespace args: The program arguments Source code in adviser/tools/espnet_minimal/utils/deterministic_utils.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def set_deterministic_pytorch ( args ): \"\"\"Ensures pytorch produces deterministic results depending on the program arguments :param Namespace args: The program arguments \"\"\" # seed setting torch . manual_seed ( args . seed ) # debug mode setting # 0 would be fastest, but 1 seems to be reasonable # considering reproducibility # remove type check torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False # https://github.com/pytorch/pytorch/issues/6351 if args . debugmode < 2 : chainer . config . type_check = False logging . info ( 'torch type check is disabled' ) # use deterministic computation or not if args . debugmode < 1 : torch . backends . cudnn . deterministic = False torch . backends . cudnn . benchmark = True logging . info ( 'torch cudnn deterministic is disabled' ) dynamic_import \u00b6 dynamic_import ( import_path , alias = {}) \u00b6 dynamic import module and class :param str import_path: syntax 'module_name:class_name' e.g., 'services.hci.speech.espnet_minimal.transform.add_deltas:AddDeltas' :param dict alias: shortcut for registered class :return: imported class Source code in adviser/tools/espnet_minimal/utils/dynamic_import.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def dynamic_import ( import_path , alias = dict ()): \"\"\"dynamic import module and class :param str import_path: syntax 'module_name:class_name' e.g., 'services.hci.speech.espnet_minimal.transform.add_deltas:AddDeltas' :param dict alias: shortcut for registered class :return: imported class \"\"\" if import_path not in alias and ':' not in import_path : raise ValueError ( 'import_path should be one of {} or ' 'include \":\", e.g. \"services.hci.speech.espnet_minimal.transform.add_deltas:AddDeltas\" : ' ' {} ' . format ( set ( alias ), import_path )) if ':' not in import_path : import_path = alias [ import_path ] module_name , objname = import_path . split ( ':' ) m = importlib . import_module ( module_name ) return getattr ( m , objname ) fill_missing_args \u00b6 fill_missing_args ( args , add_arguments ) \u00b6 Fill missing arguments in args. Parameters: Name Type Description Default args Namespace or None Namesapce containing hyperparameters. required add_arguments function Function to add arguments. required Returns: Type Description Namespace Arguments whose missing ones are filled with default value. Examples from argparse import Namespace from services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2 args = Namespace() fill_missing_args(args, Tacotron2.add_arguments_fn) Namespace(aconv_chans=32, aconv_filts=15, adim=512, atype='location', ...) Source code in adviser/tools/espnet_minimal/utils/fill_missing_args.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def fill_missing_args ( args , add_arguments ): \"\"\"Fill missing arguments in args. Args: args (Namespace or None): Namesapce containing hyperparameters. add_arguments (function): Function to add arguments. Returns: Namespace: Arguments whose missing ones are filled with default value. Examples: >>> from argparse import Namespace >>> from services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2 >>> args = Namespace() >>> fill_missing_args(args, Tacotron2.add_arguments_fn) Namespace(aconv_chans=32, aconv_filts=15, adim=512, atype='location', ...) \"\"\" # check argument type assert isinstance ( args , argparse . Namespace ) or args is None assert callable ( add_arguments ) # get default arguments default_args , _ = add_arguments ( argparse . ArgumentParser ()) . parse_known_args () # convert to dict args = {} if args is None else vars ( args ) default_args = vars ( default_args ) for key , value in default_args . items (): if key not in args : logging . info ( \"attribute \\\" %s \\\" does not exist. use default %s .\" % ( key , str ( value ))) args [ key ] = value # Note from Florian: # I believe this is where the wrong # arguments are introduced... no idea # however where the arguments we actually # load go missing. return argparse . Namespace ( ** args ) io_utils \u00b6 LoadInputsAndTargets \u00b6 Create a mini-batch from a list of dicts >>> batch = [ ('utt1', ... dict(input=[dict(feat='some.ark:123', ... filetype='mat', ... name='input1', ... shape=[100, 80 ] ) ] , ... output =[ dict(tokenid='1 2 3 4', ... name='target1', ... shape=[4, 31 ] ) ]] )) >>> l = LoadInputsAndTargets () >>> feat , target = l ( batch ) : param : str mode : Specify the task mode , \"asr\" or \"tts\" : param : str preprocess_conf : The path of a json file for pre - processing : param : bool load_input : If False , not to load the input data : param : bool load_output : If False , not to load the output data : param : bool sort_in_input_length : Sort the mini - batch in descending order of the input length : param : bool use_speaker_embedding : Used for tts mode only : param : bool use_second_target : Used for tts mode only : param : dict preprocess_args : Set some optional arguments for preprocessing : param : Optional [ dict ] preprocess_args : Used for tts mode only __call__ ( self , batch ) special \u00b6 Function to load inputs and targets from list of dicts :param List[Tuple[str, dict]] batch: list of dict which is subset of loaded data.json :return: list of input token id sequences [(L_1), (L_2), ..., (L_B)] :return: list of input feature sequences [(T_1, D), (T_2, D), ..., (T_B, D)] :rtype: list of float ndarray :return: list of target token id sequences [(L_1), (L_2), ..., (L_B)] :rtype: list of int ndarray Source code in adviser/tools/espnet_minimal/utils/io_utils.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def __call__ ( self , batch ): \"\"\"Function to load inputs and targets from list of dicts :param List[Tuple[str, dict]] batch: list of dict which is subset of loaded data.json :return: list of input token id sequences [(L_1), (L_2), ..., (L_B)] :return: list of input feature sequences [(T_1, D), (T_2, D), ..., (T_B, D)] :rtype: list of float ndarray :return: list of target token id sequences [(L_1), (L_2), ..., (L_B)] :rtype: list of int ndarray \"\"\" x_feats_dict = OrderedDict () # OrderedDict[str, List[np.ndarray]] y_feats_dict = OrderedDict () # OrderedDict[str, List[np.ndarray]] uttid_list = [] # List[str] for uttid , info in batch : uttid_list . append ( uttid ) if self . load_input : # Note(kamo): This for-loop is for multiple inputs for idx , inp in enumerate ( info [ 'input' ]): # {\"input\": # [{\"feat\": \"some/path.h5:F01_050C0101_PED_REAL\", # \"filetype\": \"hdf5\", # \"name\": \"input1\", ...}], ...} x = self . _get_from_loader ( filepath = inp [ 'feat' ], filetype = inp . get ( 'filetype' , 'mat' )) x_feats_dict . setdefault ( inp [ 'name' ], []) . append ( x ) # FIXME(kamo): Dirty way to load only speaker_embedding without the other inputs elif self . mode == 'tts' and self . use_speaker_embedding : for idx , inp in enumerate ( info [ 'input' ]): if idx != 1 and len ( info [ 'input' ]) > 1 : x = None else : x = self . _get_from_loader ( filepath = inp [ 'feat' ], filetype = inp . get ( 'filetype' , 'mat' )) x_feats_dict . setdefault ( inp [ 'name' ], []) . append ( x ) if self . load_output : if self . mode == 'mt' : x = np . fromiter ( map ( int , info [ 'output' ][ 1 ][ 'tokenid' ] . split ()), dtype = np . int64 ) x_feats_dict . setdefault ( info [ 'output' ][ 1 ][ 'name' ], []) . append ( x ) for idx , inp in enumerate ( info [ 'output' ]): if 'tokenid' in inp : # ======= Legacy format for output ======= # {\"output\": [{\"tokenid\": \"1 2 3 4\"}]) x = np . fromiter ( map ( int , inp [ 'tokenid' ] . split ()), dtype = np . int64 ) else : # ======= New format ======= # {\"input\": # [{\"feat\": \"some/path.h5:F01_050C0101_PED_REAL\", # \"filetype\": \"hdf5\", # \"name\": \"target1\", ...}], ...} x = self . _get_from_loader ( filepath = inp [ 'feat' ], filetype = inp . get ( 'filetype' , 'mat' )) y_feats_dict . setdefault ( inp [ 'name' ], []) . append ( x ) if self . mode == 'asr' : return_batch , uttid_list = self . _create_batch_asr ( x_feats_dict , y_feats_dict , uttid_list ) elif self . mode == 'tts' : _ , info = batch [ 0 ] eos = int ( info [ 'output' ][ 0 ][ 'shape' ][ 1 ]) - 1 return_batch , uttid_list = self . _create_batch_tts ( x_feats_dict , y_feats_dict , uttid_list , eos ) elif self . mode == 'mt' : return_batch , uttid_list = self . _create_batch_mt ( x_feats_dict , y_feats_dict , uttid_list ) else : raise NotImplementedError if self . preprocessing is not None : # Apply pre-processing all input features for x_name in return_batch . keys (): if x_name . startswith ( \"input\" ): return_batch [ x_name ] = self . preprocessing ( return_batch [ x_name ], uttid_list , ** self . preprocess_args ) # Doesn't return the names now. return tuple ( return_batch . values ()) __init__ ( self , mode = 'asr' , preprocess_conf = None , load_input = True , load_output = True , sort_in_input_length = True , use_speaker_embedding = False , use_second_target = False , preprocess_args = None , keep_all_data_on_mem = False ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , mode = 'asr' , preprocess_conf = None , load_input = True , load_output = True , sort_in_input_length = True , use_speaker_embedding = False , use_second_target = False , preprocess_args = None , keep_all_data_on_mem = False , ): self . _loaders = {} if mode not in [ 'asr' , 'tts' , 'mt' ]: raise ValueError ( 'Only asr or tts are allowed: mode= {} ' . format ( mode )) if preprocess_conf is not None : self . preprocessing = Transformation ( preprocess_conf ) logging . warning ( '[Experimental feature] Some preprocessing will be done ' 'for the mini-batch creation using {} ' . format ( self . preprocessing )) else : # If conf doesn't exist, this function don't touch anything. self . preprocessing = None if use_second_target and use_speaker_embedding and mode == 'tts' : raise ValueError ( 'Choose one of \"use_second_target\" and ' '\"use_speaker_embedding \"' ) if ( use_second_target or use_speaker_embedding ) and mode != 'tts' : logging . warning ( '\"use_second_target\" and \"use_speaker_embedding\" is ' 'used only for tts mode' ) self . mode = mode self . load_output = load_output self . load_input = load_input self . sort_in_input_length = sort_in_input_length self . use_speaker_embedding = use_speaker_embedding self . use_second_target = use_second_target if preprocess_args is None : self . preprocess_args = {} else : assert isinstance ( preprocess_args , dict ), type ( preprocess_args ) self . preprocess_args = dict ( preprocess_args ) self . keep_all_data_on_mem = keep_all_data_on_mem SoundHDF5File \u00b6 Collecting sound files to a HDF5 file >>> f = SoundHDF5File ( 'a.flac.h5' , mode = 'a' ) >>> array = np . random . randint ( 0 , 100 , 100 , dtype = np . int16 ) >>> f [ 'id' ] = ( array , 16000 ) >>> array , rate = f [ 'id' ] : param : str filepath : : param : str mode : : param : str format : The type used when saving wav . flac , nist , htk , etc . : param : str dtype : __contains__ ( self , item ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 536 537 def __contains__ ( self , item ): return item in self . file __enter__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 542 543 def __enter__ ( self ): return self __exit__ ( self , exc_type , exc_val , exc_tb ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 545 546 def __exit__ ( self , exc_type , exc_val , exc_tb ): self . file . close () __getitem__ ( self , key ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 516 517 518 519 520 def __getitem__ ( self , key ): data = self . file [ key ][()] f = io . BytesIO ( data . tobytes ()) array , rate = soundfile . read ( f , dtype = self . dtype ) return array , rate __init__ ( self , filepath , mode = 'r+' , format = None , dtype = 'int16' , ** kwargs ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 def __init__ ( self , filepath , mode = 'r+' , format = None , dtype = 'int16' , ** kwargs ): self . filepath = filepath self . mode = mode self . dtype = dtype self . file = h5py . File ( filepath , mode , ** kwargs ) if format is None : # filepath = a.flac.h5 -> format = flac second_ext = os . path . splitext ( os . path . splitext ( filepath )[ 0 ])[ 1 ] format = second_ext [ 1 :] if format . upper () not in soundfile . available_formats (): # If not found, flac is selected format = 'flac' # This format affects only saving self . format = format __iter__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 533 534 def __iter__ ( self ): return iter ( self . file ) __len__ ( self , item ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 539 540 def __len__ ( self , item ): return len ( self . file ) __repr__ ( self ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 502 503 504 def __repr__ ( self ): return '<SoundHDF5 file \" {} \" (mode {} , format {} , type {} )>' \\ . format ( self . filepath , self . mode , self . format , self . dtype ) __setitem__ ( self , name , data ) special \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 513 514 def __setitem__ ( self , name , data ): self . create_dataset ( name , data = data ) close ( self ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 548 549 def close ( self ): self . file . close () create_dataset ( self , name , shape = None , data = None , ** kwds ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 506 507 508 509 510 511 def create_dataset ( self , name , shape = None , data = None , ** kwds ): f = io . BytesIO () array , rate = data soundfile . write ( f , array , rate , format = self . format ) self . file . create_dataset ( name , shape = shape , data = np . void ( f . getvalue ()), ** kwds ) items ( self ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 529 530 531 def items ( self ): for k in self . file : yield k , self [ k ] keys ( self ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 522 523 def keys ( self ): return self . file . keys () values ( self ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/io_utils.py 525 526 527 def values ( self ): for k in self . file : yield self [ k ] spec_augment \u00b6 This implementation is modified from https://github.com/zcaceres/spec_augment MIT License Copyright (c) 2019 Zach Caceres Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETjjHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. apply_interpolation ( query_points , train_points , w , v , order ) \u00b6 Apply polyharmonic interpolation model to data. Notes Given coefficients w and v for the interpolation model, we evaluate interpolated function values at query_points. Parameters: Name Type Description Default query_points [b, m, d] x values to evaluate the interpolation at required train_points [b, n, d] x values that act as the interpolation centers ( the c variables in the wikipedia article) w: [b, n, k] weights on each interpolation center v: [b, d, k] weights on each input dimension required order order of the interpolation required Returns: Type Description Polyharmonic interpolation evaluated at points defined in query_points. Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def apply_interpolation ( query_points , train_points , w , v , order ): \"\"\"Apply polyharmonic interpolation model to data. Notes: Given coefficients w and v for the interpolation model, we evaluate interpolated function values at query_points. Args: query_points: `[b, m, d]` x values to evaluate the interpolation at train_points: `[b, n, d]` x values that act as the interpolation centers ( the c variables in the wikipedia article) w: `[b, n, k]` weights on each interpolation center v: `[b, d, k]` weights on each input dimension order: order of the interpolation Returns: Polyharmonic interpolation evaluated at points defined in query_points. \"\"\" query_points = query_points . unsqueeze ( 0 ) # First, compute the contribution from the rbf term. pairwise_dists = cross_squared_distance_matrix ( query_points . float (), train_points . float ()) phi_pairwise_dists = phi ( pairwise_dists , order ) rbf_term = torch . matmul ( phi_pairwise_dists , w ) # Then, compute the contribution from the linear term. # Pad query_points with ones, for the bias term in the linear model. ones = torch . ones_like ( query_points [ ... , : 1 ]) query_points_pad = torch . cat (( query_points , ones ), 2 ) . float () linear_term = torch . matmul ( query_points_pad , v ) return rbf_term + linear_term create_dense_flows ( flattened_flows , batch_size , image_height , image_width ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 180 181 182 def create_dense_flows ( flattened_flows , batch_size , image_height , image_width ): # possibly .view return torch . reshape ( flattened_flows , [ batch_size , image_height , image_width , 2 ]) cross_squared_distance_matrix ( x , y ) \u00b6 Pairwise squared distance between two (batch) matrices' rows (2nd dim). Computes the pairwise distances between rows of x and rows of y x: [batch_size, n, d] float Tensor y: [batch_size, m, d] float Tensor squared_dists: [batch_size, n, m] float Tensor , where squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def cross_squared_distance_matrix ( x , y ): \"\"\"Pairwise squared distance between two (batch) matrices' rows (2nd dim). Computes the pairwise distances between rows of x and rows of y Args: x: [batch_size, n, d] float `Tensor` y: [batch_size, m, d] float `Tensor` Returns: squared_dists: [batch_size, n, m] float `Tensor`, where squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2 \"\"\" x_norm_squared = torch . sum ( torch . mul ( x , x )) y_norm_squared = torch . sum ( torch . mul ( y , y )) x_y_transpose = torch . matmul ( x . squeeze ( 0 ), y . squeeze ( 0 ) . transpose ( 0 , 1 )) # squared_dists[b,i,j] = ||x_bi - y_bj||^2 = x_bi'x_bi- 2x_bi'x_bj + x_bj'x_bj squared_dists = x_norm_squared - 2 * x_y_transpose + y_norm_squared return squared_dists . float () dense_image_warp ( image , flow ) \u00b6 Image warping using per-pixel flow vectors. Apply a non-linear warp to the image, where the warp is specified by a dense flow field of offset vectors that define the correspondences of pixel values in the output image back to locations in the source image. Specifically, the pixel value at output[b, j, i, c] is images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c]. The locations specified by this formula do not necessarily map to an int index. Therefore, the pixel value is obtained by bilinear interpolation of the 4 nearest pixels around (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside of the image, we use the nearest pixel values at the image boundary. image: 4-D float Tensor with shape [batch, height, width, channels] . flow: A 4-D float Tensor with shape [batch, height, width, 2] . name: A name for the operation (optional). Note that image and flow can be of type tf.half, tf.float32, or tf.float64, and do not necessarily have to be the same type. A 4-D float Tensor with shape [batch, height, width, channels] and same type as input image. ValueError: if height < 2 or width < 2 or the inputs have the wrong number of dimensions. Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def dense_image_warp ( image , flow ): \"\"\"Image warping using per-pixel flow vectors. Apply a non-linear warp to the image, where the warp is specified by a dense flow field of offset vectors that define the correspondences of pixel values in the output image back to locations in the source image. Specifically, the pixel value at output[b, j, i, c] is images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c]. The locations specified by this formula do not necessarily map to an int index. Therefore, the pixel value is obtained by bilinear interpolation of the 4 nearest pixels around (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside of the image, we use the nearest pixel values at the image boundary. Args: image: 4-D float `Tensor` with shape `[batch, height, width, channels]`. flow: A 4-D float `Tensor` with shape `[batch, height, width, 2]`. name: A name for the operation (optional). Note that image and flow can be of type tf.half, tf.float32, or tf.float64, and do not necessarily have to be the same type. Returns: A 4-D float `Tensor` with shape`[batch, height, width, channels]` and same type as input image. Raises: ValueError: if height < 2 or width < 2 or the inputs have the wrong number of dimensions. \"\"\" image = image . unsqueeze ( 3 ) # add a single channel dimension to image tensor batch_size , height , width , channels = image . shape device = image . device # The flow is defined on the image grid. Turn the flow into a list of query # points in the grid space. grid_x , grid_y = torch . meshgrid ( torch . arange ( width , device = device ), torch . arange ( height , device = device )) stacked_grid = torch . stack (( grid_y , grid_x ), dim = 2 ) . float () batched_grid = stacked_grid . unsqueeze ( - 1 ) . permute ( 3 , 1 , 0 , 2 ) query_points_on_grid = batched_grid - flow query_points_flattened = torch . reshape ( query_points_on_grid , [ batch_size , height * width , 2 ]) # Compute values at the query points, then reshape the result back to the # image grid. interpolated = interpolate_bilinear ( image , query_points_flattened ) interpolated = torch . reshape ( interpolated , [ batch_size , height , width , channels ]) return interpolated flatten_grid_locations ( grid_locations , image_height , image_width ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 169 170 def flatten_grid_locations ( grid_locations , image_height , image_width ): return torch . reshape ( grid_locations , [ image_height * image_width , 2 ]) freq_mask ( spec , F = 30 , num_masks = 1 , replace_with_zero = False ) \u00b6 Frequency masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int F: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def freq_mask ( spec , F = 30 , num_masks = 1 , replace_with_zero = False ): \"\"\"Frequency masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int F: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean \"\"\" cloned = spec . unsqueeze ( 0 ) . clone () num_mel_channels = cloned . shape [ 2 ] for i in range ( 0 , num_masks ): f = random . randrange ( 0 , F ) f_zero = random . randrange ( 0 , num_mel_channels - f ) # avoids randrange error if values are equal and range is empty if ( f_zero == f_zero + f ): return cloned . squeeze ( 0 ) mask_end = random . randrange ( f_zero , f_zero + f ) if ( replace_with_zero ): cloned [ 0 ][:, f_zero : mask_end ] = 0 else : cloned [ 0 ][:, f_zero : mask_end ] = cloned . mean () return cloned . squeeze ( 0 ) get_flat_grid_locations ( image_height , image_width , device ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 173 174 175 176 177 def get_flat_grid_locations ( image_height , image_width , device ): y_range = torch . linspace ( 0 , image_height - 1 , image_height , device = device ) x_range = torch . linspace ( 0 , image_width - 1 , image_width , device = device ) y_grid , x_grid = torch . meshgrid ( y_range , x_range ) return torch . stack (( y_grid , x_grid ), - 1 ) . reshape ([ image_height * image_width , 2 ]) get_grid_locations ( image_height , image_width , device ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 162 163 164 165 166 def get_grid_locations ( image_height , image_width , device ): y_range = torch . linspace ( 0 , image_height - 1 , image_height , device = device ) x_range = torch . linspace ( 0 , image_width - 1 , image_width , device = device ) y_grid , x_grid = torch . meshgrid ( y_range , x_range ) return torch . stack (( y_grid , x_grid ), - 1 ) interpolate_bilinear ( grid , query_points , name = 'interpolate_bilinear' , indexing = 'ij' ) \u00b6 Similar to Matlab's interp2 function. Notes Finds values for query points on a grid using bilinear interpolation. Parameters: Name Type Description Default grid a 4-D float Tensor of shape [batch, height, width, channels] . required query_points a 3-D float Tensor of N points with shape [batch, N, 2] . required name a name for the operation (optional). 'interpolate_bilinear' indexing whether the query points are specified as row and column (ij), or Cartesian coordinates (xy). 'ij' Returns: Type Description values a 3-D Tensor with shape [batch, N, channels] Exceptions: Type Description ValueError if the indexing mode is invalid, or if the shape of the inputs Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def interpolate_bilinear ( grid , query_points , name = 'interpolate_bilinear' , indexing = 'ij' ): \"\"\"Similar to Matlab's interp2 function. Notes: Finds values for query points on a grid using bilinear interpolation. Args: grid: a 4-D float `Tensor` of shape `[batch, height, width, channels]`. query_points: a 3-D float `Tensor` of N points with shape `[batch, N, 2]`. name: a name for the operation (optional). indexing: whether the query points are specified as row and column (ij), or Cartesian coordinates (xy). Returns: values: a 3-D `Tensor` with shape `[batch, N, channels]` Raises: ValueError: if the indexing mode is invalid, or if the shape of the inputs invalid. \"\"\" if indexing != 'ij' and indexing != 'xy' : raise ValueError ( 'Indexing mode must be \\' ij \\' or \\' xy \\' ' ) shape = grid . shape if len ( shape ) != 4 : msg = 'Grid must be 4 dimensional. Received size: ' raise ValueError ( msg + str ( grid . shape )) batch_size , height , width , channels = grid . shape shape = [ batch_size , height , width , channels ] query_type = query_points . dtype grid_type = grid . dtype grid_device = grid . device num_queries = query_points . shape [ 1 ] alphas = [] floors = [] ceils = [] index_order = [ 0 , 1 ] if indexing == 'ij' else [ 1 , 0 ] unstacked_query_points = query_points . unbind ( 2 ) for dim in index_order : queries = unstacked_query_points [ dim ] size_in_indexing_dimension = shape [ dim + 1 ] # max_floor is size_in_indexing_dimension - 2 so that max_floor + 1 # is still a valid index into the grid. max_floor = torch . tensor ( size_in_indexing_dimension - 2 , dtype = query_type , device = grid_device ) min_floor = torch . tensor ( 0.0 , dtype = query_type , device = grid_device ) maxx = torch . max ( min_floor , torch . floor ( queries )) floor = torch . min ( maxx , max_floor ) int_floor = floor . long () floors . append ( int_floor ) ceil = int_floor + 1 ceils . append ( ceil ) # alpha has the same type as the grid, as we will directly use alpha # when taking linear combinations of pixel values from the image. alpha = torch . tensor (( queries - floor ), dtype = grid_type , device = grid_device ) min_alpha = torch . tensor ( 0.0 , dtype = grid_type , device = grid_device ) max_alpha = torch . tensor ( 1.0 , dtype = grid_type , device = grid_device ) alpha = torch . min ( torch . max ( min_alpha , alpha ), max_alpha ) # Expand alpha to [b, n, 1] so we can use broadcasting # (since the alpha values don't depend on the channel). alpha = torch . unsqueeze ( alpha , 2 ) alphas . append ( alpha ) flattened_grid = torch . reshape ( grid , [ batch_size * height * width , channels ]) batch_offsets = torch . reshape ( torch . arange ( batch_size , device = grid_device ) * height * width , [ batch_size , 1 ]) # This wraps array_ops.gather. We reshape the image data such that the # batch, y, and x coordinates are pulled into the first dimension. # Then we gather. Finally, we reshape the output back. It's possible this # code would be made simpler by using array_ops.gather_nd. def gather ( y_coords , x_coords , name ): linear_coordinates = batch_offsets + y_coords * width + x_coords gathered_values = torch . gather ( flattened_grid . t (), 1 , linear_coordinates ) return torch . reshape ( gathered_values , [ batch_size , num_queries , channels ]) # grab the pixel values in the 4 corners around each query point top_left = gather ( floors [ 0 ], floors [ 1 ], 'top_left' ) top_right = gather ( floors [ 0 ], ceils [ 1 ], 'top_right' ) bottom_left = gather ( ceils [ 0 ], floors [ 1 ], 'bottom_left' ) bottom_right = gather ( ceils [ 0 ], ceils [ 1 ], 'bottom_right' ) interp_top = alphas [ 1 ] * ( top_right - top_left ) + top_left interp_bottom = alphas [ 1 ] * ( bottom_right - bottom_left ) + bottom_left interp = alphas [ 0 ] * ( interp_bottom - interp_top ) + interp_top return interp interpolate_spline ( train_points , train_values , query_points , order , regularization_weight = 0.0 ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 185 186 187 188 189 190 191 def interpolate_spline ( train_points , train_values , query_points , order , regularization_weight = 0.0 , ): # First, fit the spline to the observed data. w , v = solve_interpolation ( train_points , train_values , order , regularization_weight ) # Then, evaluate the spline at the query locations. query_values = apply_interpolation ( query_points , train_points , w , v , order ) return query_values phi ( r , order ) \u00b6 Coordinate-wise nonlinearity used to define the order of the interpolation. See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition. r: input op order: interpolation order phi_k evaluated coordinate-wise on r, for k = r Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def phi ( r , order ): \"\"\"Coordinate-wise nonlinearity used to define the order of the interpolation. See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition. Args: r: input op order: interpolation order Returns: phi_k evaluated coordinate-wise on r, for k = r \"\"\" EPSILON = torch . tensor ( 1e-10 , device = r . device ) # using EPSILON prevents log(0), sqrt0), etc. # sqrt(0) is well-defined, but its gradient is not if order == 1 : r = torch . max ( r , EPSILON ) r = torch . sqrt ( r ) return r elif order == 2 : return 0.5 * r * torch . log ( torch . max ( r , EPSILON )) elif order == 4 : return 0.5 * torch . square ( r ) * torch . log ( torch . max ( r , EPSILON )) elif order % 2 == 0 : r = torch . max ( r , EPSILON ) return 0.5 * torch . pow ( r , 0.5 * order ) * torch . log ( r ) else : r = torch . max ( r , EPSILON ) return torch . pow ( r , 0.5 * order ) solve_interpolation ( train_points , train_values , order , regularization_weight ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def solve_interpolation ( train_points , train_values , order , regularization_weight ): device = train_points . device b , n , d = train_points . shape k = train_values . shape [ - 1 ] c = train_points f = train_values . float () matrix_a = phi ( cross_squared_distance_matrix ( c , c ), order ) . unsqueeze ( 0 ) # [b, n, n] # Append ones to the feature values for the bias term in the linear model. ones = torch . ones ( 1 , dtype = train_points . dtype , device = device ) . view ([ - 1 , 1 , 1 ]) matrix_b = torch . cat (( c , ones ), 2 ) . float () # [b, n, d + 1] # [b, n + d + 1, n] left_block = torch . cat (( matrix_a , torch . transpose ( matrix_b , 2 , 1 )), 1 ) num_b_cols = matrix_b . shape [ 2 ] # d + 1 # In Tensorflow, zeros are used here. Pytorch solve fails with zeros for some reason we don't understand. # So instead we use very tiny randn values (variance of one, zero mean) on one side of our multiplication. lhs_zeros = torch . randn (( b , num_b_cols , num_b_cols ), device = device ) / 1e10 right_block = torch . cat (( matrix_b , lhs_zeros ), 1 ) # [b, n + d + 1, d + 1] lhs = torch . cat (( left_block , right_block ), 2 ) # [b, n + d + 1, n + d + 1] rhs_zeros = torch . zeros (( b , d + 1 , k ), dtype = train_points . dtype , device = device ) . float () rhs = torch . cat (( f , rhs_zeros ), 1 ) # [b, n + d + 1, k] # Then, solve the linear system and unpack the results. X , LU = torch . gesv ( rhs , lhs ) w = X [:, : n , :] v = X [:, n :, :] return w , v sparse_image_warp ( img_tensor , source_control_point_locations , dest_control_point_locations , interpolation_order = 2 , regularization_weight = 0.0 , num_boundaries_points = 0 ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def sparse_image_warp ( img_tensor , source_control_point_locations , dest_control_point_locations , interpolation_order = 2 , regularization_weight = 0.0 , num_boundaries_points = 0 ): device = img_tensor . device control_point_flows = dest_control_point_locations - source_control_point_locations batch_size , image_height , image_width = img_tensor . shape flattened_grid_locations = get_flat_grid_locations ( image_height , image_width , device ) flattened_flows = interpolate_spline ( dest_control_point_locations , control_point_flows , flattened_grid_locations , interpolation_order , regularization_weight ) dense_flows = create_dense_flows ( flattened_flows , batch_size , image_height , image_width ) warped_image = dense_image_warp ( img_tensor , dense_flows ) return warped_image , dense_flows specaug ( spec , W = 5 , F = 30 , T = 40 , num_freq_masks = 2 , num_time_masks = 2 , replace_with_zero = False ) \u00b6 SpecAugment SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition (https://arxiv.org/pdf/1904.08779.pdf) This implementation modified from https://github.com/zcaceres/spec_augment :param torch.Tensor spec: input tensor with the shape (T, dim) :param int W: time warp parameter :param int F: maximum width of each freq mask :param int T: maximum width of each time mask :param int num_freq_masks: number of frequency masks :param int num_time_masks: number of time masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def specaug ( spec , W = 5 , F = 30 , T = 40 , num_freq_masks = 2 , num_time_masks = 2 , replace_with_zero = False ): \"\"\"SpecAugment Reference: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition (https://arxiv.org/pdf/1904.08779.pdf) This implementation modified from https://github.com/zcaceres/spec_augment :param torch.Tensor spec: input tensor with the shape (T, dim) :param int W: time warp parameter :param int F: maximum width of each freq mask :param int T: maximum width of each time mask :param int num_freq_masks: number of frequency masks :param int num_time_masks: number of time masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean \"\"\" return time_mask ( freq_mask ( time_warp ( spec , W = W ), F = F , num_masks = num_freq_masks , replace_with_zero = replace_with_zero ), T = T , num_masks = num_time_masks , replace_with_zero = replace_with_zero ) time_mask ( spec , T = 40 , num_masks = 1 , replace_with_zero = False ) \u00b6 Time masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int T: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def time_mask ( spec , T = 40 , num_masks = 1 , replace_with_zero = False ): \"\"\"Time masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int T: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean \"\"\" cloned = spec . unsqueeze ( 0 ) . clone () len_spectro = cloned . shape [ 1 ] for i in range ( 0 , num_masks ): t = random . randrange ( 0 , T ) t_zero = random . randrange ( 0 , len_spectro - t ) # avoids randrange error if values are equal and range is empty if ( t_zero == t_zero + t ): return cloned . squeeze ( 0 ) mask_end = random . randrange ( t_zero , t_zero + t ) if ( replace_with_zero ): cloned [ 0 ][ t_zero : mask_end , :] = 0 else : cloned [ 0 ][ t_zero : mask_end , :] = cloned . mean () return cloned . squeeze ( 0 ) time_warp ( spec , W = 5 ) \u00b6 Time warping :param torch.Tensor spec: input tensor with shape (T, dim) :param int W: time warp parameter Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def time_warp ( spec , W = 5 ): \"\"\"Time warping :param torch.Tensor spec: input tensor with shape (T, dim) :param int W: time warp parameter \"\"\" spec = spec . unsqueeze ( 0 ) spec_len = spec . shape [ 1 ] num_rows = spec . shape [ 2 ] device = spec . device y = num_rows // 2 horizontal_line_at_ctr = spec [ 0 , :, y ] assert len ( horizontal_line_at_ctr ) == spec_len point_to_warp = horizontal_line_at_ctr [ random . randrange ( W , spec_len - W )] assert isinstance ( point_to_warp , torch . Tensor ) # Uniform distribution from (0,W) with chance to be up to W negative dist_to_warp = random . randrange ( - W , W ) src_pts , dest_pts = ( torch . tensor ([[[ point_to_warp , y ]]], device = device ), torch . tensor ([[[ point_to_warp + dist_to_warp , y ]]], device = device )) warped_spectro , dense_flows = sparse_image_warp ( spec , src_pts , dest_pts ) return warped_spectro . squeeze ( 3 ) . squeeze ( 0 ) training special \u00b6 batchfy \u00b6 batchfy_by_bin ( sorted_data , batch_bins , num_batches = 0 , min_batch_size = 1 , shortest_first = False , ikey = 'input' , okey = 'output' ) \u00b6 Make variably sized batch set, which maximizes the number of bins up to batch_bins . :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_bins: Maximum frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every test batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def batchfy_by_bin ( sorted_data , batch_bins , num_batches = 0 , min_batch_size = 1 , shortest_first = False , ikey = \"input\" , okey = \"output\" ): \"\"\"Make variably sized batch set, which maximizes the number of bins up to `batch_bins`. :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_bins: Maximum frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every `test` batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches \"\"\" if batch_bins <= 0 : raise ValueError ( f \"invalid batch_bins= { batch_bins } \" ) length = len ( sorted_data ) idim = int ( sorted_data [ 0 ][ 1 ][ ikey ][ 0 ][ 'shape' ][ 1 ]) odim = int ( sorted_data [ 0 ][ 1 ][ okey ][ 0 ][ 'shape' ][ 1 ]) logging . info ( '# utts: ' + str ( len ( sorted_data ))) minibatches = [] start = 0 n = 0 while True : # Dynamic batch size depending on size of samples b = 0 next_size = 0 max_olen = 0 while next_size < batch_bins and ( start + b ) < length : ilen = int ( sorted_data [ start + b ][ 1 ][ ikey ][ 0 ][ 'shape' ][ 0 ]) * idim olen = int ( sorted_data [ start + b ][ 1 ][ okey ][ 0 ][ 'shape' ][ 0 ]) * odim if olen > max_olen : max_olen = olen next_size = ( max_olen + ilen ) * ( b + 1 ) if next_size <= batch_bins : b += 1 elif next_size == 0 : raise ValueError ( f \"Can't fit one sample in batch_bins ( { batch_bins } ): Please increase the value\" ) end = min ( length , start + max ( min_batch_size , b )) batch = sorted_data [ start : end ] if shortest_first : batch . reverse () minibatches . append ( batch ) # Check for min_batch_size and fixes the batches if needed i = - 1 while len ( minibatches [ i ]) < min_batch_size : missing = min_batch_size - len ( minibatches [ i ]) if - i == len ( minibatches ): minibatches [ i + 1 ] . extend ( minibatches [ i ]) minibatches = minibatches [ 1 :] break else : minibatches [ i ] . extend ( minibatches [ i - 1 ][: missing ]) minibatches [ i - 1 ] = minibatches [ i - 1 ][ missing :] i -= 1 if end == length : break start = end n += 1 if num_batches > 0 : minibatches = minibatches [: num_batches ] lengths = [ len ( x ) for x in minibatches ] logging . info ( str ( len ( minibatches )) + \" batches containing from \" + str ( min ( lengths )) + \" to \" + str ( max ( lengths )) + \" samples \" + \"(avg \" + str ( int ( np . mean ( lengths ))) + \" samples).\" ) return minibatches batchfy_by_frame ( sorted_data , max_frames_in , max_frames_out , max_frames_inout , num_batches = 0 , min_batch_size = 1 , shortest_first = False , ikey = 'input' , okey = 'output' ) \u00b6 Make variably sized batch set, which maximizes the number of frames to max_batch_frame. :param Dict[str, Dict[str, Any]] sorteddata: dictionary loaded from data.json :param int max_frames_in: Maximum input frames of a batch :param int max_frames_out: Maximum output frames of a batch :param int max_frames_inout: Maximum input+output frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every test batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def batchfy_by_frame ( sorted_data , max_frames_in , max_frames_out , max_frames_inout , num_batches = 0 , min_batch_size = 1 , shortest_first = False , ikey = \"input\" , okey = \"output\" ): \"\"\"Make variably sized batch set, which maximizes the number of frames to max_batch_frame. :param Dict[str, Dict[str, Any]] sorteddata: dictionary loaded from data.json :param int max_frames_in: Maximum input frames of a batch :param int max_frames_out: Maximum output frames of a batch :param int max_frames_inout: Maximum input+output frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every `test` batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches \"\"\" if max_frames_in <= 0 and max_frames_out <= 0 and max_frames_inout <= 0 : raise ValueError ( f \"At least, one of `--batch-frames-in`, `--batch-frames-out` or `--batch-frames-inout` should be > 0\" ) length = len ( sorted_data ) minibatches = [] start = 0 end = 0 while end != length : # Dynamic batch size depending on size of samples b = 0 max_olen = 0 max_ilen = 0 while ( start + b ) < length : ilen = int ( sorted_data [ start + b ][ 1 ][ ikey ][ 0 ][ 'shape' ][ 0 ]) if ilen > max_frames_in and max_frames_in != 0 : raise ValueError ( f \"Can't fit one sample in --batch-frames-in ( { max_frames_in } ): Please increase the value\" ) olen = int ( sorted_data [ start + b ][ 1 ][ okey ][ 0 ][ 'shape' ][ 0 ]) if olen > max_frames_out and max_frames_out != 0 : raise ValueError ( f \"Can't fit one sample in --batch-frames-out ( { max_frames_out } ): Please increase the value\" ) if ilen + olen > max_frames_inout and max_frames_inout != 0 : raise ValueError ( f \"Can't fit one sample in --batch-frames-out ( { max_frames_inout } ): Please increase the value\" ) max_olen = max ( max_olen , olen ) max_ilen = max ( max_ilen , ilen ) in_ok = max_ilen * ( b + 1 ) <= max_frames_in or max_frames_in == 0 out_ok = max_olen * ( b + 1 ) <= max_frames_out or max_frames_out == 0 inout_ok = ( max_ilen + max_olen ) * ( b + 1 ) <= max_frames_inout or max_frames_inout == 0 if in_ok and out_ok and inout_ok : # add more seq in the minibatch b += 1 else : # no more seq in the minibatch break end = min ( length , start + b ) batch = sorted_data [ start : end ] if shortest_first : batch . reverse () minibatches . append ( batch ) # Check for min_batch_size and fixes the batches if needed i = - 1 while len ( minibatches [ i ]) < min_batch_size : missing = min_batch_size - len ( minibatches [ i ]) if - i == len ( minibatches ): minibatches [ i + 1 ] . extend ( minibatches [ i ]) minibatches = minibatches [ 1 :] break else : minibatches [ i ] . extend ( minibatches [ i - 1 ][: missing ]) minibatches [ i - 1 ] = minibatches [ i - 1 ][ missing :] i -= 1 start = end if num_batches > 0 : minibatches = minibatches [: num_batches ] lengths = [ len ( x ) for x in minibatches ] logging . info ( str ( len ( minibatches )) + \" batches containing from \" + str ( min ( lengths )) + \" to \" + str ( max ( lengths )) + \" samples\" + \"(avg \" + str ( int ( np . mean ( lengths ))) + \" samples).\" ) return minibatches batchfy_by_seq ( sorted_data , batch_size , max_length_in , max_length_out , min_batch_size = 1 , shortest_first = False , ikey = 'input' , iaxis = 0 , okey = 'output' , oaxis = 0 ) \u00b6 Make batch set from json dictionary :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_size: batch size :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int min_batch_size: mininum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS, MT ikey=\"output\".) :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param str okey: key to access output (for ASR, MT okey=\"output\". for TTS okey=\"input\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) :return: List[List[Tuple[str, dict]]] list of batches Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def batchfy_by_seq ( sorted_data , batch_size , max_length_in , max_length_out , min_batch_size = 1 , shortest_first = False , ikey = \"input\" , iaxis = 0 , okey = \"output\" , oaxis = 0 ): \"\"\"Make batch set from json dictionary :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_size: batch size :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int min_batch_size: mininum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS, MT ikey=\"output\".) :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param str okey: key to access output (for ASR, MT okey=\"output\". for TTS okey=\"input\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) :return: List[List[Tuple[str, dict]]] list of batches \"\"\" if batch_size <= 0 : raise ValueError ( f \"Invalid batch_size= { batch_size } \" ) # check #utts is more than min_batch_size if len ( sorted_data ) < min_batch_size : raise ValueError ( f \"#utts( { len ( sorted_data ) } ) is less than min_batch_size( { min_batch_size } ).\" ) # make list of minibatches minibatches = [] start = 0 while True : _ , info = sorted_data [ start ] ilen = int ( info [ ikey ][ iaxis ][ 'shape' ][ 0 ]) olen = int ( info [ okey ][ oaxis ][ 'shape' ][ 0 ]) if oaxis >= 0 else max ( map ( lambda x : int ( x [ 'shape' ][ 0 ]), info [ okey ])) factor = max ( int ( ilen / max_length_in ), int ( olen / max_length_out )) # change batchsize depending on the input and output length # if ilen = 1000 and max_length_in = 800 # then b = batchsize / 2 # and max(min_batches, .) avoids batchsize = 0 bs = max ( min_batch_size , int ( batch_size / ( 1 + factor ))) end = min ( len ( sorted_data ), start + bs ) minibatch = sorted_data [ start : end ] if shortest_first : minibatch . reverse () # check each batch is more than minimum batchsize if len ( minibatch ) < min_batch_size : mod = min_batch_size - len ( minibatch ) % min_batch_size additional_minibatch = [ sorted_data [ i ] for i in np . random . randint ( 0 , start , mod )] if shortest_first : additional_minibatch . reverse () minibatch . extend ( additional_minibatch ) minibatches . append ( minibatch ) if end == len ( sorted_data ): break start = end # batch: List[List[Tuple[str, dict]]] return minibatches batchfy_shuffle ( data , batch_size , min_batch_size , num_batches , shortest_first ) \u00b6 Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def batchfy_shuffle ( data , batch_size , min_batch_size , num_batches , shortest_first ): import random logging . info ( 'use shuffled batch.' ) sorted_data = random . sample ( data . items (), len ( data . items ())) logging . info ( '# utts: ' + str ( len ( sorted_data ))) # make list of minibatches minibatches = [] start = 0 while True : end = min ( len ( sorted_data ), start + batch_size ) # check each batch is more than minimum batchsize minibatch = sorted_data [ start : end ] if shortest_first : minibatch . reverse () if len ( minibatch ) < min_batch_size : mod = min_batch_size - len ( minibatch ) % min_batch_size additional_minibatch = [ sorted_data [ i ] for i in np . random . randint ( 0 , start , mod )] if shortest_first : additional_minibatch . reverse () minibatch . extend ( additional_minibatch ) minibatches . append ( minibatch ) if end == len ( sorted_data ): break start = end # for debugging if num_batches > 0 : minibatches = minibatches [: num_batches ] logging . info ( '# minibatches: ' + str ( len ( minibatches ))) return minibatches make_batchset ( data , batch_size = 0 , max_length_in = inf , max_length_out = inf , num_batches = 0 , min_batch_size = 1 , shortest_first = False , batch_sort_key = 'input' , swap_io = False , mt = False , count = 'auto' , batch_bins = 0 , batch_frames_in = 0 , batch_frames_out = 0 , batch_frames_inout = 0 , iaxis = 0 , oaxis = 0 ) \u00b6 Make batch set from json dictionary if utts have \"category\" value, >>> data = {'utt1': {'category': 'A', 'input': ...}, ... 'utt2': {'category': 'B', 'input': ...}, ... 'utt3': {'category': 'B', 'input': ...}, ... 'utt4': {'category': 'A', 'input': ...}} >>> make_batchset(data, batchsize=2, ...) [[('utt1', ...), ('utt4', ...)], [('utt2', ...), ('utt3': ...)]] Note that if any utts doesn't have \"category\", perform as same as batchfy_by_{count} :param Dict[str, Dict[str, Any]] data: dictionary loaded from data.json :param int batch_size: maximum number of sequences in a minibatch. :param int batch_bins: maximum number of bins (frames x dim) in a minibatch. :param int batch_frames_in: maximum number of input frames in a minibatch. :param int batch_frames_out: maximum number of output frames in a minibatch. :param int batch_frames_out: maximum number of input+output frames in a minibatch. :param str count: strategy to count maximum size of batch. For choices, see services.hci.speech.espnet_minimal.asr.batchfy.BATCH_COUNT_CHOICES :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :return: List[List[Tuple[str, dict]]] list of batches :param str batch_sort_key: how to sort data before creating minibatches [\"input\", \"output\", \"shuffle\"] :param bool swap_io: if True, use \"input\" as output and \"output\" as input in data dict :param bool mt: if True, use 0-axis of \"output\" as output and 1-axis of \"output\" as input in data dict :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def make_batchset ( data , batch_size = 0 , max_length_in = float ( \"inf\" ), max_length_out = float ( \"inf\" ), num_batches = 0 , min_batch_size = 1 , shortest_first = False , batch_sort_key = \"input\" , swap_io = False , mt = False , count = \"auto\" , batch_bins = 0 , batch_frames_in = 0 , batch_frames_out = 0 , batch_frames_inout = 0 , iaxis = 0 , oaxis = 0 ): \"\"\"Make batch set from json dictionary if utts have \"category\" value, >>> data = {'utt1': {'category': 'A', 'input': ...}, ... 'utt2': {'category': 'B', 'input': ...}, ... 'utt3': {'category': 'B', 'input': ...}, ... 'utt4': {'category': 'A', 'input': ...}} >>> make_batchset(data, batchsize=2, ...) [[('utt1', ...), ('utt4', ...)], [('utt2', ...), ('utt3': ...)]] Note that if any utts doesn't have \"category\", perform as same as batchfy_by_{count} :param Dict[str, Dict[str, Any]] data: dictionary loaded from data.json :param int batch_size: maximum number of sequences in a minibatch. :param int batch_bins: maximum number of bins (frames x dim) in a minibatch. :param int batch_frames_in: maximum number of input frames in a minibatch. :param int batch_frames_out: maximum number of output frames in a minibatch. :param int batch_frames_out: maximum number of input+output frames in a minibatch. :param str count: strategy to count maximum size of batch. For choices, see services.hci.speech.espnet_minimal.asr.batchfy.BATCH_COUNT_CHOICES :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :return: List[List[Tuple[str, dict]]] list of batches :param str batch_sort_key: how to sort data before creating minibatches [\"input\", \"output\", \"shuffle\"] :param bool swap_io: if True, use \"input\" as output and \"output\" as input in `data` dict :param bool mt: if True, use 0-axis of \"output\" as output and 1-axis of \"output\" as input in `data` dict :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) \"\"\" # check args if count not in BATCH_COUNT_CHOICES : raise ValueError ( f \"arg 'count' ( { count } ) should be one of { BATCH_COUNT_CHOICES } \" ) if batch_sort_key not in BATCH_SORT_KEY_CHOICES : raise ValueError ( f \"arg 'batch_sort_key' ( { batch_sort_key } ) should be one of { BATCH_SORT_KEY_CHOICES } \" ) # TODO(karita): remove this by creating converter from ASR to TTS json format batch_sort_axis = 0 if swap_io : # for TTS ikey = \"output\" okey = \"input\" if batch_sort_key == \"input\" : batch_sort_key = \"output\" elif batch_sort_key == \"output\" : batch_sort_key = \"input\" elif mt : # for MT ikey = \"output\" okey = \"output\" batch_sort_key = \"output\" batch_sort_axis = 1 assert iaxis == 1 assert oaxis == 0 # NOTE: input is json['output'][1] and output is json['output'][0] else : ikey = \"input\" okey = \"output\" if count == \"auto\" : if batch_size != 0 : count = \"seq\" elif batch_bins != 0 : count = \"bin\" elif batch_frames_in != 0 or batch_frames_out != 0 or batch_frames_inout != 0 : count = \"frame\" else : raise ValueError ( f \"cannot detect `count` manually set one of { BATCH_COUNT_CHOICES } \" ) logging . info ( f \"count is auto detected as { count } \" ) if count != \"seq\" and batch_sort_key == \"shuffle\" : raise ValueError ( f \"batch_sort_key=shuffle is only available if batch_count=seq\" ) category2data = {} # Dict[str, dict] for k , v in data . items (): category2data . setdefault ( v . get ( 'category' ), {})[ k ] = v batches_list = [] # List[List[List[Tuple[str, dict]]]] for d in category2data . values (): if batch_sort_key == 'shuffle' : batches = batchfy_shuffle ( d , batch_size , min_batch_size , num_batches , shortest_first ) batches_list . append ( batches ) continue # sort it by input lengths (long to short) sorted_data = sorted ( d . items (), key = lambda data : int ( data [ 1 ][ batch_sort_key ][ batch_sort_axis ][ 'shape' ][ 0 ]), reverse = not shortest_first ) logging . info ( '# utts: ' + str ( len ( sorted_data ))) if count == \"seq\" : batches = batchfy_by_seq ( sorted_data , batch_size = batch_size , max_length_in = max_length_in , max_length_out = max_length_out , min_batch_size = min_batch_size , shortest_first = shortest_first , ikey = ikey , iaxis = iaxis , okey = okey , oaxis = oaxis ) if count == \"bin\" : batches = batchfy_by_bin ( sorted_data , batch_bins = batch_bins , min_batch_size = min_batch_size , shortest_first = shortest_first , ikey = ikey , okey = okey ) if count == \"frame\" : batches = batchfy_by_frame ( sorted_data , max_frames_in = batch_frames_in , max_frames_out = batch_frames_out , max_frames_inout = batch_frames_inout , min_batch_size = min_batch_size , shortest_first = shortest_first , ikey = ikey , okey = okey ) batches_list . append ( batches ) if len ( batches_list ) == 1 : batches = batches_list [ 0 ] else : # Concat list. This way is faster than \"sum(batch_list, [])\" batches = list ( itertools . chain ( * batches_list )) # for debugging if num_batches > 0 : batches = batches [: num_batches ] logging . info ( '# minibatches: ' + str ( len ( batches ))) # batch: List[List[Tuple[str, dict]]] return batches evaluator \u00b6 BaseEvaluator \u00b6 Base Evaluator in ESPnet __call__ ( self , trainer = None ) special Source code in adviser/tools/espnet_minimal/utils/training/evaluator.py 9 10 11 12 13 14 15 16 17 18 def __call__ ( self , trainer = None ): ret = super () . __call__ ( trainer ) try : if trainer is not None : # force tensorboard to report evaluation log tb_logger = trainer . get_extension ( TensorboardLogger . default_name ) tb_logger ( trainer ) except ValueError : pass return ret iterators \u00b6 ShufflingEnabler \u00b6 An extension enabling shuffling on an Iterator __call__ ( self , trainer ) special Calls the enabler on the given iterator :param trainer: The iterator Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 20 21 22 23 24 25 26 27 28 def __call__ ( self , trainer ): \"\"\"Calls the enabler on the given iterator :param trainer: The iterator \"\"\" if not self . set : for iterator in self . iterators : iterator . start_shuffle () self . set = True __init__ ( self , iterators ) special Inits the ShufflingEnabler :param list[Iterator] iterators: The iterators to enable shuffling on Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 12 13 14 15 16 17 18 def __init__ ( self , iterators ): \"\"\"Inits the ShufflingEnabler :param list[Iterator] iterators: The iterators to enable shuffling on \"\"\" self . set = False self . iterators = iterators ToggleableShufflingMultiprocessIterator \u00b6 A MultiprocessIterator that can have its shuffling property activated during training __init__ ( self , dataset , batch_size , repeat = True , shuffle = True , n_processes = None , n_prefetch = 1 , shared_mem = None , maxtasksperchild = 20 ) special Init the iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat batches or not (enables multiple epochs) :param bool shuffle: Whether to shuffle the order of the batches :param int n_processes: How many processes to use :param int n_prefetch: The number of prefetch to use :param int shared_mem: How many memory to share between processes :param int maxtasksperchild: Maximum number of tasks per child Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , dataset , batch_size , repeat = True , shuffle = True , n_processes = None , n_prefetch = 1 , shared_mem = None , maxtasksperchild = 20 ): \"\"\"Init the iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat batches or not (enables multiple epochs) :param bool shuffle: Whether to shuffle the order of the batches :param int n_processes: How many processes to use :param int n_prefetch: The number of prefetch to use :param int shared_mem: How many memory to share between processes :param int maxtasksperchild: Maximum number of tasks per child \"\"\" super ( ToggleableShufflingMultiprocessIterator , self ) . __init__ ( dataset = dataset , batch_size = batch_size , repeat = repeat , shuffle = shuffle , n_processes = n_processes , n_prefetch = n_prefetch , shared_mem = shared_mem , maxtasksperchild = maxtasksperchild ) start_shuffle ( self ) Starts shuffling (or reshuffles) the batches Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 76 77 78 79 80 81 82 83 84 def start_shuffle ( self ): \"\"\"Starts shuffling (or reshuffles) the batches\"\"\" self . shuffle = True if int ( chainer . _version . __version__ [ 0 ]) <= 4 : self . _order = np . random . permutation ( len ( self . dataset )) else : self . order_sampler = ShuffleOrderSampler () self . _order = self . order_sampler ( np . arange ( len ( self . dataset )), 0 ) self . _set_prefetch_state () ToggleableShufflingSerialIterator \u00b6 A SerialIterator that can have its shuffling property activated during training __init__ ( self , dataset , batch_size , repeat = True , shuffle = True ) special Init the Iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat data (allow multiple epochs) :param bool shuffle: Whether to shuffle the batches Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 34 35 36 37 38 39 40 41 42 def __init__ ( self , dataset , batch_size , repeat = True , shuffle = True ): \"\"\"Init the Iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat data (allow multiple epochs) :param bool shuffle: Whether to shuffle the batches \"\"\" super ( ToggleableShufflingSerialIterator , self ) . __init__ ( dataset , batch_size , repeat , shuffle ) start_shuffle ( self ) Starts shuffling (or reshuffles) the batches Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 44 45 46 47 48 49 50 51 def start_shuffle ( self ): \"\"\"Starts shuffling (or reshuffles) the batches\"\"\" self . _shuffle = True if int ( chainer . _version . __version__ [ 0 ]) <= 4 : self . _order = np . random . permutation ( len ( self . dataset )) else : self . order_sampler = ShuffleOrderSampler () self . _order = self . order_sampler ( np . arange ( len ( self . dataset )), 0 ) tensorboard_logger \u00b6 TensorboardLogger \u00b6 A tensorboard logger extension __call__ ( self , trainer ) special Updates the events file with the new values :param trainer: The trainer Source code in adviser/tools/espnet_minimal/utils/training/tensorboard_logger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __call__ ( self , trainer ): \"\"\"Updates the events file with the new values :param trainer: The trainer \"\"\" observation = trainer . observation for k , v in observation . items (): if ( self . _entries is not None ) and ( k not in self . _entries ): continue if k is not None and v is not None : if 'cupy' in str ( type ( v )): v = v . get () if 'cupy' in str ( type ( k )): k = k . get () self . _logger . add_scalar ( k , v , trainer . updater . iteration ) if self . _att_reporter is not None and trainer . updater . get_iterator ( 'main' ) . epoch > self . _epoch : self . _epoch = trainer . updater . get_iterator ( 'main' ) . epoch self . _att_reporter . log_attentions ( self . _logger , trainer . updater . iteration ) __init__ ( self , logger , att_reporter = None , entries = None , epoch = 0 ) special Init the extension :param SummaryWriter logger: The logger to use :param PlotAttentionReporter att_reporter: The (optional) PlotAttentionReporter :param entries: The entries to watch :param int epoch: The starting epoch Source code in adviser/tools/espnet_minimal/utils/training/tensorboard_logger.py 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , logger , att_reporter = None , entries = None , epoch = 0 ): \"\"\"Init the extension :param SummaryWriter logger: The logger to use :param PlotAttentionReporter att_reporter: The (optional) PlotAttentionReporter :param entries: The entries to watch :param int epoch: The starting epoch \"\"\" self . _entries = entries self . _att_reporter = att_reporter self . _logger = logger self . _epoch = epoch train_utils \u00b6 check_early_stop ( trainer , epochs ) \u00b6 Checks if the training was stopped by an early stopping trigger and warns the user if it's the case :param trainer: The trainer used for training :param epochs: The maximum number of epochs Source code in adviser/tools/espnet_minimal/utils/training/train_utils.py 6 7 8 9 10 11 12 13 14 15 def check_early_stop ( trainer , epochs ): \"\"\"Checks if the training was stopped by an early stopping trigger and warns the user if it's the case :param trainer: The trainer used for training :param epochs: The maximum number of epochs \"\"\" end_epoch = trainer . updater . get_iterator ( 'main' ) . epoch if end_epoch < ( epochs - 1 ): logging . warning ( \"Hit early stop at epoch \" + str ( end_epoch ) + \" \\n You can change the patience or set it to 0 to run all epochs\" ) set_early_stop ( trainer , args , is_lm = False ) \u00b6 Sets the early stop trigger given the program arguments :param trainer: The trainer used for training :param args: The program arguments :param is_lm: If the trainer is for a LM (epoch instead of epochs) Source code in adviser/tools/espnet_minimal/utils/training/train_utils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def set_early_stop ( trainer , args , is_lm = False ): \"\"\"Sets the early stop trigger given the program arguments :param trainer: The trainer used for training :param args: The program arguments :param is_lm: If the trainer is for a LM (epoch instead of epochs) \"\"\" patience = args . patience criterion = args . early_stop_criterion epochs = args . epoch if is_lm else args . epochs mode = 'max' if 'acc' in criterion else 'min' if patience > 0 : trainer . stop_trigger = chainer . training . triggers . EarlyStoppingTrigger ( monitor = criterion , mode = mode , patients = patience , max_trigger = ( epochs , 'epoch' )) getopensmile \u00b6 get_opensmile_executable_path () \u00b6 Returns the path to the platform-specific openSMILE executable. If it can't be found, will download (and, depending on platform, try to compile) and then return the path. Source code in adviser/tools/getopensmile.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def get_opensmile_executable_path (): \"\"\" Returns the path to the platform-specific openSMILE executable. If it can't be found, will download (and, depending on platform, try to compile) and then return the path. \"\"\" openSmile_path = _get_opensmile_executable () if openSmile_path is None : print ( \"no openSMILE binary found\" ) _download_compile_opensmile () openSmile_path = _get_opensmile_executable () if openSmile_path is None : print ( \"failed to obtain and setup openSMILE. Exiting.\" ) exit () return openSmile_path get_root_dir () \u00b6 Source code in adviser/tools/getopensmile.py 25 26 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))","title":"Tools"},{"location":"api/tools/#tools","text":"","title":"Tools"},{"location":"api/tools/#adviser.tools","text":"","title":"adviser.tools"},{"location":"api/tools/#adviser.tools.create_ontology","text":"","title":"create_ontology"},{"location":"api/tools/#adviser.tools.create_ontology.Database","text":"","title":"Database"},{"location":"api/tools/#adviser.tools.create_ontology.Database.__init__","text":"Source code in adviser/tools/create_ontology.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , path ): conn = sqlite3 . connect ( path ) cursor = conn . cursor () self . tables = {} # result will be (type, name, tbl_name, rootpage, sql) cursor . execute ( \"SELECT * FROM sqlite_master where type='table'\" ) for _ , _ , table , _ , _ in cursor . fetchall (): self . tables [ table ] = DatabaseTable ( table ) for table in self . tables . keys (): # get fields/slots # result will be (id, name, type, not null, default, primary key) cursor . execute ( f \"PRAGMA table_info( { table } );\" ) self . tables [ table ] . fields = cursor . fetchall () # make sure that fields are sorted according to field index (should be already anyway) self . tables [ table ] . fields = sorted ( self . tables [ table ] . fields , key = lambda field : field [ 0 ]) # get entries (especially for possible values) cursor . execute ( f \"SELECT * FROM { table } \" ) self . tables [ table ] . entries = cursor . fetchall ()","title":"__init__()"},{"location":"api/tools/#adviser.tools.create_ontology.Database.get_slot_values","text":"Source code in adviser/tools/create_ontology.py 102 103 def get_slot_values ( self , table , slot ): return self . tables [ table ] . get_slot_values ( slot )","title":"get_slot_values()"},{"location":"api/tools/#adviser.tools.create_ontology.Database.get_slots","text":"Source code in adviser/tools/create_ontology.py 99 100 def get_slots ( self , table ): return self . tables [ table ] . get_slots ()","title":"get_slots()"},{"location":"api/tools/#adviser.tools.create_ontology.Database.get_tables","text":"Source code in adviser/tools/create_ontology.py 96 97 def get_tables ( self ): return list ( self . tables . keys ())","title":"get_tables()"},{"location":"api/tools/#adviser.tools.create_ontology.DatabaseTable","text":"","title":"DatabaseTable"},{"location":"api/tools/#adviser.tools.create_ontology.DatabaseTable.__init__","text":"Source code in adviser/tools/create_ontology.py 47 48 49 50 def __init__ ( self , name ): self . name = name self . fields = [] self . entries = []","title":"__init__()"},{"location":"api/tools/#adviser.tools.create_ontology.DatabaseTable.get_slot_values","text":"Source code in adviser/tools/create_ontology.py 61 62 63 64 65 66 67 68 def get_slot_values ( self , slot , dontcare = False ): # get slot id id = self . _get_slot_id ( slot ) assert id >= 0 , f \"Slot ' { slot } ' is not part of the database table ' { self . name } '\" values = sorted ( list ( set ([ entry [ id ] for entry in self . entries ]))) if dontcare and not ( 'dontcare' in values or \"do n't care\" in values ): values . append ( 'dontcare' ) return values","title":"get_slot_values()"},{"location":"api/tools/#adviser.tools.create_ontology.DatabaseTable.get_slots","text":"Source code in adviser/tools/create_ontology.py 58 59 def get_slots ( self ): return [ field [ 1 ] for field in self . fields ]","title":"get_slots()"},{"location":"api/tools/#adviser.tools.create_ontology.get_defaults","text":"Source code in adviser/tools/create_ontology.py 105 106 107 108 def get_defaults (): return { 'discourseAct' : [ \"ack\" , \"hello\" , \"none\" , \"silence\" , \"thanks\" , \"bad\" ], 'method' : [ \"none\" , \"byconstraints\" , \"byprimarykey\" , \"finished\" , \"byalternatives\" , \"restart\" ], 'key' : 'name' }","title":"get_defaults()"},{"location":"api/tools/#adviser.tools.create_ontology.run_questions","text":"Source code in adviser/tools/create_ontology.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def run_questions ( db : Database ): # initialize with default values answers = get_defaults () questions = [ { 'type' : 'list' , 'qmark' : '>>>' , 'message' : 'Select table to create ontology for' , 'name' : 'table' , 'choices' : [{ 'key' : str ( id ), 'name' : table , 'value' : table } for id , table in enumerate ( db . get_tables ())], 'validate' : lambda answer : 'You must choose at least one table.' \\ if len ( answer ) == 0 else True }, { 'type' : 'input' , 'qmark' : '>>>' , 'message' : 'Enter the name of the domain:' , 'name' : 'domain' , 'default' : lambda answers : answers [ 'table' ] }, { 'type' : 'list' , 'qmark' : '>>>' , 'name' : 'key' , 'message' : 'Which slot will be used as key? (The key uniquely identifies an entity in the database, e.g. the name in case of restaurants)' , 'choices' : lambda answers : [{ 'name' : slot } for slot in db . get_slots ( answers [ 'table' ])] }, { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'requestable' , 'message' : 'Select user requestables' , 'choices' : lambda answers : [{ 'name' : slot , 'checked' : slot != 'id' } for slot in db . get_slots ( answers [ 'table' ])] }, { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'system_requestable' , 'message' : 'Select system requestables' , 'choices' : lambda answers : [{ 'name' : slot } for slot in db . get_slots ( answers [ 'table' ])] }, { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'informable' , 'message' : 'Select informable slots' , 'choices' : lambda answers : [{ 'name' : slot } for slot in db . get_slots ( answers [ 'table' ])] }] answers_ = prompt ( questions , style = custom_style_2 ) # check whether there are answers (e.g. if the user cancels the prompt using Ctrl+c) if not answers_ : exit () answers . update ( answers_ ) # get values for informable slots questions = [ { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : slot , 'message' : f 'Select values for informable slot { slot } ' , 'choices' : [{ 'name' : value , 'checked' : value != 'dontcare' } for value in db . get_slot_values ( answers [ 'table' ], slot )] } for slot in answers [ 'informable' ] ] values = prompt ( questions , style = custom_style_2 ) # merge informable slot values with informable slots answers [ 'informable' ] = { slot : values [ slot ] for slot in answers [ 'informable' ] if slot in values } # get binary slots questions = [ { 'type' : 'checkbox' , 'qmark' : '>>>' , 'name' : 'binary' , 'message' : 'Select binary slots' , 'choices' : [{ 'name' : slot , 'checked' : set ( db . get_slot_values ( answers [ 'table' ], slot )) == { 'true' , 'false' }} for slot in list ( answers [ 'informable' ] . keys ()) + answers [ 'requestable' ] + answers [ 'system_requestable' ]] } ] answers_ = prompt ( questions , style = custom_style_2 ) # check whether there are answers (e.g. if the user cancels the prompt using Ctrl+c) if not answers_ : exit () answers . update ( answers_ ) return answers","title":"run_questions()"},{"location":"api/tools/#adviser.tools.espnet_minimal","text":"","title":"espnet_minimal"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr","text":"","title":"asr"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.asr_utils","text":"","title":"asr_utils"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.asr_utils.add_gradient_noise","text":"Adds noise from a standard normal distribution to the gradients. The standard deviation ( sigma ) is controlled by the three hyper-parameters below. sigma goes to zero (no noise) with more iterations. Parameters: Name Type Description Default model torch.nn.model Model. required iteration int Number of iterations. required duration int) {100, 1000} Number of durations to control the interval of the sigma change. 100 eta float) {0.01, 0.3, 1.0} The magnitude of sigma . 1.0 scale_factor float) {0.55} The scale of sigma . 0.55 Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def add_gradient_noise ( model , iteration , duration = 100 , eta = 1.0 , scale_factor = 0.55 ): \"\"\"Adds noise from a standard normal distribution to the gradients. The standard deviation (`sigma`) is controlled by the three hyper-parameters below. `sigma` goes to zero (no noise) with more iterations. Args: model (torch.nn.model): Model. iteration (int): Number of iterations. duration (int) {100, 1000}: Number of durations to control the interval of the `sigma` change. eta (float) {0.01, 0.3, 1.0}: The magnitude of `sigma`. scale_factor (float) {0.55}: The scale of `sigma`. \"\"\" interval = ( iteration // duration ) + 1 sigma = eta / interval ** scale_factor for param in model . parameters (): if param . grad is not None : _shape = param . grad . size () noise = sigma * torch . randn ( _shape ) . to ( param . device ) param . grad += noise","title":"add_gradient_noise()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.asr_utils.get_model_conf","text":"Get model config information by reading a model config file (model.json). Parameters: Name Type Description Default model_path str Model path. required conf_path str Optional model config path. None Returns: Type Description list[int, int, dict[str, Any]] Config information loaded from json file. Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_model_conf ( model_path , conf_path = None ): \"\"\"Get model config information by reading a model config file (model.json). Args: model_path (str): Model path. conf_path (str): Optional model config path. Returns: list[int, int, dict[str, Any]]: Config information loaded from json file. \"\"\" if conf_path is None : model_conf = os . path . dirname ( model_path ) + '/model.json' else : model_conf = conf_path with open ( model_conf , \"rb\" ) as f : logging . info ( 'reading a config file from ' + model_conf ) confs = json . load ( f ) if isinstance ( confs , dict ): # for lm args = confs return argparse . Namespace ( ** args ) else : # for asr, tts, mt idim , odim , args = confs return idim , odim , argparse . Namespace ( ** args )","title":"get_model_conf()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.asr_utils.torch_load","text":"Load torch model states. Parameters: Name Type Description Default path str Model path or snapshot file path to be loaded. required model torch.nn.Module Torch model. required Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def torch_load ( path , model ): \"\"\"Load torch model states. Args: path (str): Model path or snapshot file path to be loaded. model (torch.nn.Module): Torch model. \"\"\" if 'snapshot' in path : model_state_dict = torch . load ( path , map_location = lambda storage , loc : storage )[ 'model' ] else : model_state_dict = torch . load ( path , map_location = lambda storage , loc : storage ) # debugging: # print(model_state_dict) if hasattr ( model , 'module' ): model . module . load_state_dict ( model_state_dict ) else : model . load_state_dict ( model_state_dict ) del model_state_dict","title":"torch_load()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.asr_utils.torch_save","text":"Save torch model states. Parameters: Name Type Description Default path str Model path to be saved. required model torch.nn.Module Torch model. required Source code in adviser/tools/espnet_minimal/asr/asr_utils.py 70 71 72 73 74 75 76 77 78 79 80 81 def torch_save ( path , model ): \"\"\"Save torch model states. Args: path (str): Model path to be saved. model (torch.nn.Module): Torch model. \"\"\" if hasattr ( model , 'module' ): torch . save ( model . module . state_dict (), path ) else : torch . save ( model . state_dict (), path )","title":"torch_save()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend","text":"","title":"pytorch_backend"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init","text":"Finetuning methods.","title":"asr_init"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.filter_modules","text":"Filter non-matched modules in module_state_dict. Parameters: Name Type Description Default model_state_dict OrderedDict trained model state_dict required modules list specified module list for transfer required Returns: Type Description new_mods (list) the update module list Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def filter_modules ( model_state_dict , modules ): \"\"\"Filter non-matched modules in module_state_dict. Args: model_state_dict (OrderedDict): trained model state_dict modules (list): specified module list for transfer Return: new_mods (list): the update module list \"\"\" new_mods = [] incorrect_mods = [] mods_model = list ( model_state_dict . keys ()) for mod in modules : if any ( key . startswith ( mod ) for key in mods_model ): new_mods += [ mod ] else : incorrect_mods += [ mod ] if incorrect_mods : logging . warning ( \"module(s) %s don \\' t match or (partially match) \" \"available modules in model.\" , incorrect_mods ) logging . warning ( 'for information, the existing modules in model are:' ) logging . warning ( ' %s ' , mods_model ) return new_mods","title":"filter_modules()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.get_partial_asr_mt_state_dict","text":"Create state_dict with specified modules matching input model modules. Parameters: Name Type Description Default model_state_dict OrderedDict trained model state_dict required modules list specified module list for transfer required Returns: Type Description new_state_dict (OrderedDict) the updated state_dict Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def get_partial_asr_mt_state_dict ( model_state_dict , modules ): \"\"\"Create state_dict with specified modules matching input model modules. Args: model_state_dict (OrderedDict): trained model state_dict modules (list): specified module list for transfer Return: new_state_dict (OrderedDict): the updated state_dict \"\"\" new_state_dict = OrderedDict () for key , value in model_state_dict . items (): if any ( key . startswith ( m ) for m in modules ): new_state_dict [ key ] = value return new_state_dict","title":"get_partial_asr_mt_state_dict()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.get_partial_lm_state_dict","text":"Create compatible ASR state_dict from model_state_dict (LM). The keys for specified modules are modified to match ASR decoder modules keys. Parameters: Name Type Description Default model_state_dict OrderedDict trained model state_dict required modules list specified module list for transfer required Returns: Type Description new_state_dict (OrderedDict) the updated state_dict new_mods (list): the updated module list Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def get_partial_lm_state_dict ( model_state_dict , modules ): \"\"\"Create compatible ASR state_dict from model_state_dict (LM). The keys for specified modules are modified to match ASR decoder modules keys. Args: model_state_dict (OrderedDict): trained model state_dict modules (list): specified module list for transfer Return: new_state_dict (OrderedDict): the updated state_dict new_mods (list): the updated module list \"\"\" new_state_dict = OrderedDict () new_modules = [] for key , value in list ( model_state_dict . items ()): if key == \"predictor.embed.weight\" and \"predictor.embed.\" in modules : new_key = \"dec.embed.weight\" new_state_dict [ new_key ] = value new_modules += [ new_key ] elif \"predictor.rnn.\" in key and \"predictor.rnn.\" in modules : new_key = \"dec.decoder.\" + key . split ( \"predictor.rnn.\" , 1 )[ 1 ] new_state_dict [ new_key ] = value new_modules += [ new_key ] return new_state_dict , new_modules","title":"get_partial_lm_state_dict()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.get_root_dir","text":"Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 16 17 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ))))))","title":"get_root_dir()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.get_trained_model_state_dict","text":"Extract the trained model state dict for pre-initialization. Parameters: Name Type Description Default model_path str Path to model.***.best required Returns: Type Description model.state_dict() (OrderedDict) the loaded model state_dict (str): Type of model. Either ASR/MT or LM. Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def get_trained_model_state_dict ( model_path ): \"\"\"Extract the trained model state dict for pre-initialization. Args: model_path (str): Path to model.***.best Return: model.state_dict() (OrderedDict): the loaded model state_dict (str): Type of model. Either ASR/MT or LM. \"\"\" conf_path = os . path . join ( os . path . dirname ( model_path ), 'model.json' ) if 'rnnlm' in model_path : logging . warning ( 'reading model parameters from %s ' , model_path ) return torch . load ( model_path ), 'lm' idim , odim , args = get_model_conf ( model_path , conf_path ) logging . warning ( 'reading model parameters from ' + model_path ) if hasattr ( args , \"model_module\" ): model_module = args . model_module else : model_module = \"services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_asr:E2E\" model_class = dynamic_import ( model_module ) model = model_class ( idim , odim , args ) torch_load ( model_path , model ) assert isinstance ( model , MTInterface ) or isinstance ( model , ASRInterface ) return model . state_dict (), 'asr-mt'","title":"get_trained_model_state_dict()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.load_trained_model","text":"Load the trained model for recognition. Parameters: Name Type Description Default model_path str Path to model.***.best required Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def load_trained_model ( model_path ): \"\"\"Load the trained model for recognition. Args: model_path (str): Path to model.***.best \"\"\" idim , odim , train_args = get_model_conf ( model_path , os . path . join ( get_root_dir (), os . path . dirname ( model_path ), 'model.json' )) # logging.warning('reading model parameters from ' + model_path) if hasattr ( train_args , \"model_module\" ): model_module = train_args . model_module else : model_module = \"services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_asr:E2E\" model_class = dynamic_import ( model_module ) model = model_class ( idim , odim , train_args ) torch_load ( model_path , model ) return model , train_args","title":"load_trained_model()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.load_trained_modules","text":"Load model encoder or/and decoder modules with ESPNET pre-trained model(s). Parameters: Name Type Description Default idim int initial input dimension. required odim int initial output dimension. required args Namespace The initial model arguments. required interface Interface ASRInterface or STInterface <class 'tools.espnet_minimal.nets.asr_interface.ASRInterface'> Returns: Type Description model (torch.nn.Module) The model with pretrained modules. Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def load_trained_modules ( idim , odim , args , interface = ASRInterface ): \"\"\"Load model encoder or/and decoder modules with ESPNET pre-trained model(s). Args: idim (int): initial input dimension. odim (int): initial output dimension. args (Namespace): The initial model arguments. interface (Interface): ASRInterface or STInterface Return: model (torch.nn.Module): The model with pretrained modules. \"\"\" enc_model_path = args . enc_init dec_model_path = args . dec_init enc_modules = args . enc_init_mods dec_modules = args . dec_init_mods model_class = dynamic_import ( args . model_module ) main_model = model_class ( idim , odim , args ) assert isinstance ( main_model , interface ) main_state_dict = main_model . state_dict () logging . warning ( 'model(s) found for pre-initialization' ) for model_path , modules in [( enc_model_path , enc_modules ), ( dec_model_path , dec_modules )]: if model_path is not None : if os . path . isfile ( model_path ): model_state_dict , mode = get_trained_model_state_dict ( model_path ) modules = filter_modules ( model_state_dict , modules ) if mode == 'lm' : partial_state_dict , modules = get_partial_lm_state_dict ( model_state_dict , modules ) else : partial_state_dict = get_partial_asr_mt_state_dict ( model_state_dict , modules ) if partial_state_dict : if transfer_verification ( main_state_dict , partial_state_dict , modules ): logging . warning ( 'loading %s from model: %s ' , modules , model_path ) for k in partial_state_dict . keys (): logging . warning ( 'override %s ' % k ) main_state_dict . update ( partial_state_dict ) else : logging . warning ( 'modules %s in model %s don \\' t match your training config' , modules , model_path ) else : logging . warning ( 'model was not found : %s ' , model_path ) main_model . load_state_dict ( main_state_dict ) return main_model","title":"load_trained_modules()"},{"location":"api/tools/#adviser.tools.espnet_minimal.asr.pytorch_backend.asr_init.transfer_verification","text":"Verify tuples (key, shape) for input model modules match specified modules. Parameters: Name Type Description Default model_state_dict OrderedDict the initial model state_dict required partial_state_dict OrderedDict the trained model state_dict required modules list specified module list for transfer required Returns: Type Description (boolean) allow transfer Source code in adviser/tools/espnet_minimal/asr/pytorch_backend/asr_init.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def transfer_verification ( model_state_dict , partial_state_dict , modules ): \"\"\"Verify tuples (key, shape) for input model modules match specified modules. Args: model_state_dict (OrderedDict): the initial model state_dict partial_state_dict (OrderedDict): the trained model state_dict modules (list): specified module list for transfer Return: (boolean): allow transfer \"\"\" partial_modules = [] for key_p , value_p in partial_state_dict . items (): if any ( key_p . startswith ( m ) for m in modules ): if value_p . shape == model_state_dict [ key_p ] . shape : partial_modules += [( key_p , value_p . shape )] return len ( partial_modules ) > 0","title":"transfer_verification()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin","text":"","title":"bin"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.asr_recog","text":"End-to-end speech recognition model decoding script.","title":"asr_recog"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.asr_recog.get_parser","text":"Get default arguments. Source code in adviser/tools/espnet_minimal/bin/asr_recog.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def get_parser (): \"\"\"Get default arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Transcribe text from speech using a speech recognition model on one CPU or GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'Config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'Second config file path that overwrites the settings in `--config`' ) parser . add ( '--config3' , is_config_file = True , help = 'Third config file path that overwrites the settings in `--config` and `--config2`' ) parser . add_argument ( '--ngpu' , type = int , default = 0 , help = 'Number of GPUs' ) parser . add_argument ( '--dtype' , choices = ( \"float16\" , \"float32\" , \"float64\" ), default = \"float32\" , help = 'Float precision (only available in --api v2)' ) parser . add_argument ( '--backend' , type = str , default = 'chainer' , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , type = int , default = 1 , help = 'Debugmode' ) parser . add_argument ( '--seed' , type = int , default = 1 , help = 'Random seed' ) parser . add_argument ( '--verbose' , '-V' , type = int , default = 1 , help = 'Verbose option' ) parser . add_argument ( '--batchsize' , type = int , default = 1 , help = 'Batch size for beam search (0: means no batch processing)' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) parser . add_argument ( '--api' , default = \"v1\" , choices = [ \"v1\" , \"v2\" ], help = '''Beam search APIs v1: Default API. It only supports the ASRInterface.recognize method and DefaultRNNLM. v2: Experimental API. It supports any models that implements ScorerInterface.''' ) # task related parser . add_argument ( '--recog-json' , type = str , help = 'Filename of recognition data (json)' ) parser . add_argument ( '--result-label' , type = str , required = True , help = 'Filename of result label data (json)' ) # model (parameter) related parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) parser . add_argument ( '--model-conf' , type = str , default = None , help = 'Model config file' ) parser . add_argument ( '--num-spkrs' , type = int , default = 1 , choices = [ 1 , 2 ], help = 'Number of speakers in the speech' ) parser . add_argument ( '--num-encs' , default = 1 , type = int , help = 'Number of encoders in the model.' ) # search related parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 1 , help = 'Beam size' ) parser . add_argument ( '--penalty' , type = float , default = 0.0 , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , type = float , default = 0.0 , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , type = float , default = 0.0 , help = 'Input length ratio to obtain min output length' ) parser . add_argument ( '--ctc-weight' , type = float , default = 0.0 , help = 'CTC weight in joint decoding' ) parser . add_argument ( '--weights-ctc-dec' , type = float , action = 'append' , help = 'ctc weight assigned to each encoder during decoding.[in multi-encoder mode only]' ) parser . add_argument ( '--ctc-window-margin' , type = int , default = 0 , help = \"\"\"Use CTC window with margin parameter to accelerate CTC/attention decoding especially on GPU. Smaller magin makes decoding faster, but may increase search errors. If margin=0 (default), this function is disabled\"\"\" ) # transducer related parser . add_argument ( '--score-norm-transducer' , type = strtobool , nargs = '?' , default = True , help = 'Normalize transducer scores by length' ) # rnnlm related parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--word-rnnlm' , type = str , default = None , help = 'Word RNNLM model file to read' ) parser . add_argument ( '--word-rnnlm-conf' , type = str , default = None , help = 'Word RNNLM model config file to read' ) parser . add_argument ( '--word-dict' , type = str , default = None , help = 'Word list to read' ) parser . add_argument ( '--lm-weight' , type = float , default = 0.1 , help = 'RNNLM weight' ) # streaming related parser . add_argument ( '--streaming-mode' , type = str , default = None , choices = [ 'window' , 'segment' ], help = \"\"\"Use streaming recognizer for inference. `--batchsize` must be set to 0 to enable this mode\"\"\" ) parser . add_argument ( '--streaming-window' , type = int , default = 10 , help = 'Window size' ) parser . add_argument ( '--streaming-min-blank-dur' , type = int , default = 10 , help = 'Minimum blank duration threshold' ) parser . add_argument ( '--streaming-onset-margin' , type = int , default = 1 , help = 'Onset margin' ) parser . add_argument ( '--streaming-offset-margin' , type = int , default = 1 , help = 'Offset margin' ) return parser","title":"get_parser()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.asr_recog.main","text":"Run the main decoding function. Source code in adviser/tools/espnet_minimal/bin/asr_recog.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def main ( args ): \"\"\"Run the main decoding function.\"\"\" parser = get_parser () args = parser . parse_args ( args ) if args . ngpu == 0 and args . dtype == \"float16\" : raise ValueError ( f \"--dtype { args . dtype } does not support the CPU backend.\" ) # logging info if args . verbose == 1 : logging . basicConfig ( level = logging . INFO , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) elif args . verbose == 2 : logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) else : logging . basicConfig ( level = logging . WARN , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) logging . warning ( \"Skip DEBUG/INFO messages\" ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # TODO(mn5k): support of multiple GPUs if args . ngpu > 1 : logging . error ( \"The program only supports ngpu=1.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting random . seed ( args . seed ) np . random . seed ( args . seed ) logging . info ( 'set random seed = %d ' % args . seed ) # validate rnn options if args . rnnlm is not None and args . word_rnnlm is not None : logging . error ( \"It seems that both --rnnlm and --word-rnnlm are specified. Please use either option.\" ) sys . exit ( 1 ) # recog logging . info ( 'backend = ' + args . backend ) if args . num_spkrs == 1 : if args . backend == \"chainer\" : from tools.espnet_minimal.asr.chainer_backend.asr import recog recog ( args ) elif args . backend == \"pytorch\" : if args . num_encs == 1 : # Experimental API that supports custom LMs if args . api == \"v2\" : from tools.espnet_minimal.asr.pytorch_backend.recog import recog_v2 recog_v2 ( args ) else : from tools.espnet_minimal.asr.pytorch_backend.asr import recog if args . dtype != \"float32\" : raise NotImplementedError ( f \"`--dtype { args . dtype } ` is only available with `--api v2`\" ) recog ( args ) else : if args . api == \"v2\" : raise NotImplementedError ( f \"--num-encs { args . num_encs } > 1 is not supported in --api v2\" ) else : from tools.espnet_minimal.asr.pytorch_backend.asr import recog recog ( args ) else : raise ValueError ( \"Only chainer and pytorch are supported.\" ) elif args . num_spkrs == 2 : if args . backend == \"pytorch\" : from tools.espnet_minimal.asr.pytorch_backend.asr_mix import recog recog ( args ) else : raise ValueError ( \"Only pytorch is supported.\" )","title":"main()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.asr_train","text":"Automatic speech recognition model training script.","title":"asr_train"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.asr_train.get_parser","text":"Get default arguments. Source code in adviser/tools/espnet_minimal/bin/asr_train.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def get_parser ( parser = None , required = True ): \"\"\"Get default arguments.\"\"\" if parser is None : parser = configargparse . ArgumentParser ( description = \"Train an automatic speech recognition (ASR) model on one CPU, one or multiple GPUs\" , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'second config file path that overwrites the settings in `--config`.' ) parser . add ( '--config3' , is_config_file = True , help = 'third config file path that overwrites the settings in `--config` and `--config2`.' ) parser . add_argument ( '--ngpu' , default = None , type = int , help = 'Number of GPUs. If not given, use all visible devices' ) parser . add_argument ( '--train-dtype' , default = \"float32\" , choices = [ \"float16\" , \"float32\" , \"float64\" , \"O0\" , \"O1\" , \"O2\" , \"O3\" ], help = 'Data type for training (only pytorch backend). ' 'O0,O1,.. flags require apex. See https://nvidia.github.io/apex/amp.html#opt-levels' ) parser . add_argument ( '--backend' , default = 'chainer' , type = str , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--outdir' , type = str , required = required , help = 'Output directory' ) parser . add_argument ( '--debugmode' , default = 1 , type = int , help = 'Debugmode' ) parser . add_argument ( '--dict' , required = required , help = 'Dictionary' ) parser . add_argument ( '--seed' , default = 1 , type = int , help = 'Random seed' ) parser . add_argument ( '--debugdir' , type = str , help = 'Output directory for debugging' ) parser . add_argument ( '--resume' , '-r' , default = '' , nargs = '?' , help = 'Resume the training from snapshot' ) parser . add_argument ( '--minibatches' , '-N' , type = int , default = '-1' , help = 'Process only N minibatches (for debug)' ) parser . add_argument ( '--verbose' , '-V' , default = 0 , type = int , help = 'Verbose option' ) parser . add_argument ( '--tensorboard-dir' , default = None , type = str , nargs = '?' , help = \"Tensorboard log dir path\" ) parser . add_argument ( '--report-interval-iters' , default = 100 , type = int , help = \"Report interval iterations\" ) parser . add_argument ( '--save-interval-iters' , default = 0 , type = int , help = \"Save snapshot interval iterations\" ) # task related parser . add_argument ( '--train-json' , type = str , default = None , help = 'Filename of train label data (json)' ) parser . add_argument ( '--valid-json' , type = str , default = None , help = 'Filename of validation label data (json)' ) # network architecture parser . add_argument ( '--model-module' , type = str , default = None , help = 'model defined module (default: services.hci.speech.espnet_minimal.nets.xxx_backend.e2e_asr:E2E)' ) # encoder parser . add_argument ( '--num-encs' , default = 1 , type = int , help = 'Number of encoders in the model.' ) # loss related parser . add_argument ( '--ctc_type' , default = 'warpctc' , type = str , choices = [ 'builtin' , 'warpctc' ], help = 'Type of CTC implementation to calculate loss.' ) parser . add_argument ( '--mtlalpha' , default = 0.5 , type = float , help = 'Multitask learning coefficient, alpha: alpha*ctc_loss + (1-alpha)*att_loss ' ) parser . add_argument ( '--lsm-weight' , default = 0.0 , type = float , help = 'Label smoothing weight' ) # recognition options to compute CER/WER parser . add_argument ( '--report-cer' , default = False , action = 'store_true' , help = 'Compute CER on development set' ) parser . add_argument ( '--report-wer' , default = False , action = 'store_true' , help = 'Compute WER on development set' ) parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 4 , help = 'Beam size' ) parser . add_argument ( '--penalty' , default = 0.0 , type = float , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , default = 0.0 , type = float , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , default = 0.0 , type = float , help = 'Input length ratio to obtain min output length' ) parser . add_argument ( '--ctc-weight' , default = 0.3 , type = float , help = 'CTC weight in joint decoding' ) parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--lm-weight' , default = 0.1 , type = float , help = 'RNNLM weight.' ) parser . add_argument ( '--sym-space' , default = '<space>' , type = str , help = 'Space symbol' ) parser . add_argument ( '--sym-blank' , default = '<blank>' , type = str , help = 'Blank symbol' ) # minibatch related parser . add_argument ( '--sortagrad' , default = 0 , type = int , nargs = '?' , help = \"How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs\" ) parser . add_argument ( '--batch-count' , default = 'auto' , choices = BATCH_COUNT_CHOICES , help = 'How to count batch_size. The default (auto) will find how to count by args.' ) parser . add_argument ( '--batch-size' , '--batch-seqs' , '-b' , default = 0 , type = int , help = 'Maximum seqs in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-bins' , default = 0 , type = int , help = 'Maximum bins in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-frames-in' , default = 0 , type = int , help = 'Maximum input frames in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-frames-out' , default = 0 , type = int , help = 'Maximum output frames in a minibatch (0 to disable)' ) parser . add_argument ( '--batch-frames-inout' , default = 0 , type = int , help = 'Maximum input+output frames in a minibatch (0 to disable)' ) parser . add_argument ( '--maxlen-in' , '--batch-seq-maxlen-in' , default = 800 , type = int , metavar = 'ML' , help = 'When --batch-count=seq, batch size is reduced if the input sequence length > ML.' ) parser . add_argument ( '--maxlen-out' , '--batch-seq-maxlen-out' , default = 150 , type = int , metavar = 'ML' , help = 'When --batch-count=seq, batch size is reduced if the output sequence length > ML' ) parser . add_argument ( '--n-iter-processes' , default = 0 , type = int , help = 'Number of processes of iterator' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , nargs = '?' , help = 'The configuration file for the pre-processing' ) # optimization related parser . add_argument ( '--opt' , default = 'adadelta' , type = str , choices = [ 'adadelta' , 'adam' , 'noam' ], help = 'Optimizer' ) parser . add_argument ( '--accum-grad' , default = 1 , type = int , help = 'Number of gradient accumuration' ) parser . add_argument ( '--eps' , default = 1e-8 , type = float , help = 'Epsilon constant for optimizer' ) parser . add_argument ( '--eps-decay' , default = 0.01 , type = float , help = 'Decaying ratio of epsilon' ) parser . add_argument ( '--weight-decay' , default = 0.0 , type = float , help = 'Weight decay ratio' ) parser . add_argument ( '--criterion' , default = 'acc' , type = str , choices = [ 'loss' , 'acc' ], help = 'Criterion to perform epsilon decay' ) parser . add_argument ( '--threshold' , default = 1e-4 , type = float , help = 'Threshold to stop iteration' ) parser . add_argument ( '--epochs' , '-e' , default = 30 , type = int , help = 'Maximum number of epochs' ) parser . add_argument ( '--early-stop-criterion' , default = 'validation/main/acc' , type = str , nargs = '?' , help = \"Value to monitor to trigger an early stopping of the training\" ) parser . add_argument ( '--patience' , default = 3 , type = int , nargs = '?' , help = \"Number of epochs to wait without improvement before stopping the training\" ) parser . add_argument ( '--grad-clip' , default = 5 , type = float , help = 'Gradient norm threshold to clip' ) parser . add_argument ( '--num-save-attention' , default = 3 , type = int , help = 'Number of samples of attention to be saved' ) parser . add_argument ( '--grad-noise' , type = strtobool , default = False , help = 'The flag to switch to use noise injection to gradients during training' ) # asr_mix related parser . add_argument ( '--num-spkrs' , default = 1 , type = int , choices = [ 1 , 2 ], help = 'Number of speakers in the speech.' ) # decoder related parser . add_argument ( '--context-residual' , default = False , type = strtobool , nargs = '?' , help = 'The flag to switch to use context vector residual in the decoder network' ) # finetuning related parser . add_argument ( '--enc-init' , default = None , type = str , help = 'Pre-trained ASR model to initialize encoder.' ) parser . add_argument ( '--enc-init-mods' , default = 'enc.enc.' , type = lambda s : [ str ( mod ) for mod in s . split ( ',' ) if s != '' ], help = 'List of encoder modules to initialize, separated by a comma.' ) parser . add_argument ( '--dec-init' , default = None , type = str , help = 'Pre-trained ASR, MT or LM model to initialize decoder.' ) parser . add_argument ( '--dec-init-mods' , default = 'att., dec.' , type = lambda s : [ str ( mod ) for mod in s . split ( ',' ) if s != '' ], help = 'List of decoder modules to initialize, separated by a comma.' ) # front end related parser . add_argument ( '--use-frontend' , type = strtobool , default = False , help = 'The flag to switch to use frontend system.' ) # WPE related parser . add_argument ( '--use-wpe' , type = strtobool , default = False , help = 'Apply Weighted Prediction Error' ) parser . add_argument ( '--wtype' , default = 'blstmp' , type = str , choices = [ 'lstm' , 'blstm' , 'lstmp' , 'blstmp' , 'vgglstmp' , 'vggblstmp' , 'vgglstm' , 'vggblstm' , 'gru' , 'bgru' , 'grup' , 'bgrup' , 'vgggrup' , 'vggbgrup' , 'vgggru' , 'vggbgru' ], help = 'Type of encoder network architecture ' 'of the mask estimator for WPE. ' '' ) parser . add_argument ( '--wlayers' , type = int , default = 2 , help = '' ) parser . add_argument ( '--wunits' , type = int , default = 300 , help = '' ) parser . add_argument ( '--wprojs' , type = int , default = 300 , help = '' ) parser . add_argument ( '--wdropout-rate' , type = float , default = 0.0 , help = '' ) parser . add_argument ( '--wpe-taps' , type = int , default = 5 , help = '' ) parser . add_argument ( '--wpe-delay' , type = int , default = 3 , help = '' ) parser . add_argument ( '--use-dnn-mask-for-wpe' , type = strtobool , default = False , help = 'Use DNN to estimate the power spectrogram. ' 'This option is experimental.' ) # Beamformer related parser . add_argument ( '--use-beamformer' , type = strtobool , default = True , help = '' ) parser . add_argument ( '--btype' , default = 'blstmp' , type = str , choices = [ 'lstm' , 'blstm' , 'lstmp' , 'blstmp' , 'vgglstmp' , 'vggblstmp' , 'vgglstm' , 'vggblstm' , 'gru' , 'bgru' , 'grup' , 'bgrup' , 'vgggrup' , 'vggbgrup' , 'vgggru' , 'vggbgru' ], help = 'Type of encoder network architecture ' 'of the mask estimator for Beamformer.' ) parser . add_argument ( '--blayers' , type = int , default = 2 , help = '' ) parser . add_argument ( '--bunits' , type = int , default = 300 , help = '' ) parser . add_argument ( '--bprojs' , type = int , default = 300 , help = '' ) parser . add_argument ( '--badim' , type = int , default = 320 , help = '' ) parser . add_argument ( '--bnmask' , type = int , default = 2 , help = 'Number of beamforming masks, ' 'default is 2 for [speech, noise].' ) parser . add_argument ( '--ref-channel' , type = int , default =- 1 , help = 'The reference channel used for beamformer. ' 'By default, the channel is estimated by DNN.' ) parser . add_argument ( '--bdropout-rate' , type = float , default = 0.0 , help = '' ) # Feature transform: Normalization parser . add_argument ( '--stats-file' , type = str , default = None , help = 'The stats file for the feature normalization' ) parser . add_argument ( '--apply-uttmvn' , type = strtobool , default = True , help = 'Apply utterance level mean ' 'variance normalization.' ) parser . add_argument ( '--uttmvn-norm-means' , type = strtobool , default = True , help = '' ) parser . add_argument ( '--uttmvn-norm-vars' , type = strtobool , default = False , help = '' ) # Feature transform: Fbank parser . add_argument ( '--fbank-fs' , type = int , default = 16000 , help = 'The sample frequency used for ' 'the mel-fbank creation.' ) parser . add_argument ( '--n-mels' , type = int , default = 80 , help = 'The number of mel-frequency bins.' ) parser . add_argument ( '--fbank-fmin' , type = float , default = 0. , help = '' ) parser . add_argument ( '--fbank-fmax' , type = float , default = None , help = '' ) return parser","title":"get_parser()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.asr_train.main","text":"Run the main training function. Source code in adviser/tools/espnet_minimal/bin/asr_train.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def main ( cmd_args ): \"\"\"Run the main training function.\"\"\" parser = get_parser () args , _ = parser . parse_known_args ( cmd_args ) if args . backend == \"chainer\" and args . train_dtype != \"float32\" : raise NotImplementedError ( f \"chainer backend does not support --train-dtype { args . train_dtype } .\" \"Use --dtype float32.\" ) if args . ngpu == 0 and args . train_dtype in ( \"O0\" , \"O1\" , \"O2\" , \"O3\" , \"float16\" ): raise ValueError ( f \"--train-dtype { args . train_dtype } does not support the CPU backend.\" ) from tools.espnet_minimal import dynamic_import if args . model_module is None : model_module = \"services.hci.speech.espnet_minimal.nets.\" + args . backend + \"_backend.e2e_asr:E2E\" else : model_module = args . model_module model_class = dynamic_import ( model_module ) model_class . add_arguments ( parser ) args = parser . parse_args ( cmd_args ) args . model_module = model_module if 'chainer_backend' in args . model_module : args . backend = 'chainer' if 'pytorch_backend' in args . model_module : args . backend = 'pytorch' # logging info if args . verbose > 0 : logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) else : logging . basicConfig ( level = logging . WARN , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) logging . warning ( 'Skip DEBUG/INFO messages' ) # If --ngpu is not given, # 1. if CUDA_VISIBLE_DEVICES is set, all visible devices # 2. if nvidia-smi exists, use all devices # 3. else ngpu=0 if args . ngpu is None : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is not None : ngpu = len ( cvd . split ( ',' )) else : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) try : p = subprocess . run ([ 'nvidia-smi' , '-L' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except ( subprocess . CalledProcessError , FileNotFoundError ): ngpu = 0 else : ngpu = len ( p . stderr . decode () . split ( ' \\n ' )) - 1 else : if is_torch_1_2_plus : assert args . ngpu == 1 , \"There are some bugs with multi-GPU processing in PyTorch 1.2+\" \\ \" (see https://github.com/pytorch/pytorch/issues/21108)\" ngpu = args . ngpu logging . info ( f \"ngpu: { ngpu } \" ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # set random seed logging . info ( 'random seed = %d ' % args . seed ) random . seed ( args . seed ) np . random . seed ( args . seed ) # load dictionary for debug log if args . dict is not None : with open ( args . dict , 'rb' ) as f : dictionary = f . readlines () char_list = [ entry . decode ( 'utf-8' ) . split ( ' ' )[ 0 ] for entry in dictionary ] char_list . insert ( 0 , '<blank>' ) char_list . append ( '<eos>' ) args . char_list = char_list else : args . char_list = None # train logging . info ( 'backend = ' + args . backend ) if args . num_spkrs == 1 : if args . backend == \"chainer\" : from tools.espnet_minimal.asr.chainer_backend.asr import train train ( args ) elif args . backend == \"pytorch\" : from tools.espnet_minimal.asr.pytorch_backend.asr import train train ( args ) else : raise ValueError ( \"Only chainer and pytorch are supported.\" ) else : # FIXME(kamo): Support --model-module if args . backend == \"pytorch\" : from tools.espnet_minimal.asr.pytorch_backend.asr_mix import train train ( args ) else : raise ValueError ( \"Only pytorch is supported.\" )","title":"main()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.lm_train","text":"Language model training script.","title":"lm_train"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.lm_train.get_parser","text":"Get parser. Source code in adviser/tools/espnet_minimal/bin/lm_train.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def get_parser (): \"\"\"Get parser.\"\"\" parser = configargparse . ArgumentParser ( description = 'Train a new language model on one CPU or one GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'second config file path that overwrites the settings in `--config`.' ) parser . add ( '--config3' , is_config_file = True , help = 'third config file path that overwrites the settings in `--config` and `--config2`.' ) parser . add_argument ( '--ngpu' , default = None , type = int , help = 'Number of GPUs. If not given, use all visible devices' ) parser . add_argument ( '--train-dtype' , default = \"float32\" , choices = [ \"float16\" , \"float32\" , \"float64\" , \"O0\" , \"O1\" , \"O2\" , \"O3\" ], help = 'Data type for training (only pytorch backend). ' 'O0,O1,.. flags require apex. See https://nvidia.github.io/apex/amp.html#opt-levels' ) parser . add_argument ( '--backend' , default = 'chainer' , type = str , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--outdir' , type = str , required = True , help = 'Output directory' ) parser . add_argument ( '--debugmode' , default = 1 , type = int , help = 'Debugmode' ) parser . add_argument ( '--dict' , type = str , required = True , help = 'Dictionary' ) parser . add_argument ( '--seed' , default = 1 , type = int , help = 'Random seed' ) parser . add_argument ( '--resume' , '-r' , default = '' , nargs = '?' , help = 'Resume the training from snapshot' ) parser . add_argument ( '--verbose' , '-V' , default = 0 , type = int , help = 'Verbose option' ) parser . add_argument ( '--tensorboard-dir' , default = None , type = str , nargs = '?' , help = \"Tensorboard log dir path\" ) parser . add_argument ( '--report-interval-iters' , default = 100 , type = int , help = \"Report interval iterations\" ) # task related parser . add_argument ( '--train-label' , type = str , required = True , help = 'Filename of train label data' ) parser . add_argument ( '--valid-label' , type = str , required = True , help = 'Filename of validation label data' ) parser . add_argument ( '--test-label' , type = str , help = 'Filename of test label data' ) parser . add_argument ( '--dump-hdf5-path' , type = str , default = None , help = 'Path to dump a preprocessed dataset as hdf5' ) # training configuration parser . add_argument ( '--opt' , default = 'sgd' , type = str , choices = [ 'sgd' , 'adam' ], help = 'Optimizer' ) parser . add_argument ( '--sortagrad' , default = 0 , type = int , nargs = '?' , help = \"How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs\" ) parser . add_argument ( '--batchsize' , '-b' , type = int , default = 300 , help = 'Number of examples in each mini-batch' ) parser . add_argument ( '--epoch' , '-e' , type = int , default = 20 , help = 'Number of sweeps over the dataset to train' ) parser . add_argument ( '--early-stop-criterion' , default = 'validation/main/loss' , type = str , nargs = '?' , help = \"Value to monitor to trigger an early stopping of the training\" ) parser . add_argument ( '--patience' , default = 3 , type = int , nargs = '?' , help = \"Number of epochs to wait without improvement before stopping the training\" ) parser . add_argument ( '--gradclip' , '-c' , type = float , default = 5 , help = 'Gradient norm threshold to clip' ) parser . add_argument ( '--maxlen' , type = int , default = 40 , help = 'Batch size is reduced if the input sequence > ML' ) parser . add_argument ( '--model-module' , type = str , default = 'default' , help = 'model defined module (default: services.hci.speech.espnet_minimal.nets.xxx_backend.lm.default:DefaultRNNLM)' ) return parser","title":"get_parser()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.lm_train.main","text":"Train LM. Source code in adviser/tools/espnet_minimal/bin/lm_train.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def main ( cmd_args ): \"\"\"Train LM.\"\"\" parser = get_parser () args , _ = parser . parse_known_args ( cmd_args ) if args . backend == \"chainer\" and args . train_dtype != \"float32\" : raise NotImplementedError ( f \"chainer backend does not support --train-dtype { args . train_dtype } .\" \"Use --dtype float32.\" ) if args . ngpu == 0 and args . train_dtype in ( \"O0\" , \"O1\" , \"O2\" , \"O3\" , \"float16\" ): raise ValueError ( f \"--train-dtype { args . train_dtype } does not support the CPU backend.\" ) # parse model-specific arguments dynamically model_class = dynamic_import_lm ( args . model_module , args . backend ) model_class . add_arguments ( parser ) args = parser . parse_args ( cmd_args ) # logging info if args . verbose > 0 : logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) else : logging . basicConfig ( level = logging . WARN , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) logging . warning ( 'Skip DEBUG/INFO messages' ) # If --ngpu is not given, # 1. if CUDA_VISIBLE_DEVICES is set, all visible devices # 2. if nvidia-smi exists, use all devices # 3. else ngpu=0 if args . ngpu is None : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is not None : ngpu = len ( cvd . split ( ',' )) else : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) try : p = subprocess . run ([ 'nvidia-smi' , '-L' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except ( subprocess . CalledProcessError , FileNotFoundError ): ngpu = 0 else : ngpu = len ( p . stderr . decode () . split ( ' \\n ' )) - 1 else : ngpu = args . ngpu logging . info ( f \"ngpu: { ngpu } \" ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting nseed = args . seed random . seed ( nseed ) np . random . seed ( nseed ) # load dictionary with open ( args . dict , 'rb' ) as f : dictionary = f . readlines () char_list = [ entry . decode ( 'utf-8' ) . split ( ' ' )[ 0 ] for entry in dictionary ] char_list . insert ( 0 , '<blank>' ) char_list . append ( '<eos>' ) args . char_list_dict = { x : i for i , x in enumerate ( char_list )} args . n_vocab = len ( char_list ) # train logging . info ( 'backend = ' + args . backend ) if args . backend == \"chainer\" : from tools.espnet_minimal import train train ( args ) elif args . backend == \"pytorch\" : from tools.espnet_minimal.lm.pytorch_backend.lm import train train ( args ) else : raise ValueError ( \"Only chainer and pytorch are supported.\" )","title":"main()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.mt_trans","text":"Neural machine translation model decoding script.","title":"mt_trans"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.mt_trans.get_parser","text":"Get default arguments. Source code in adviser/tools/espnet_minimal/bin/mt_trans.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def get_parser (): \"\"\"Get default arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Translate text from speech using a speech translation model on one CPU or GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'Config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'Second config file path that overwrites the settings in `--config`' ) parser . add ( '--config3' , is_config_file = True , help = 'Third config file path that overwrites the settings in `--config` and `--config2`' ) parser . add_argument ( '--ngpu' , type = int , default = 0 , help = 'Number of GPUs' ) parser . add_argument ( '--dtype' , choices = ( \"float16\" , \"float32\" , \"float64\" ), default = \"float32\" , help = 'Float precision (only available in --api v2)' ) parser . add_argument ( '--backend' , type = str , default = 'chainer' , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , type = int , default = 1 , help = 'Debugmode' ) parser . add_argument ( '--seed' , type = int , default = 1 , help = 'Random seed' ) parser . add_argument ( '--verbose' , '-V' , type = int , default = 1 , help = 'Verbose option' ) parser . add_argument ( '--batchsize' , type = int , default = 1 , help = 'Batch size for beam search (0: means no batch processing)' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) parser . add_argument ( '--api' , default = \"v1\" , choices = [ \"v1\" , \"v2\" ], help = '''Beam search APIs v1: Default API. It only supports the ASRInterface.recognize method and DefaultRNNLM. v2: Experimental API. It supports any models that implements ScorerInterface.''' ) # task related parser . add_argument ( '--trans-json' , type = str , help = 'Filename of translation data (json)' ) parser . add_argument ( '--result-label' , type = str , required = True , help = 'Filename of result label data (json)' ) # model (parameter) related parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) parser . add_argument ( '--model-conf' , type = str , default = None , help = 'Model config file' ) # search related parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 1 , help = 'Beam size' ) parser . add_argument ( '--penalty' , type = float , default = 0.1 , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , type = float , default = 3.0 , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , type = float , default = 0.0 , help = 'Input length ratio to obtain min output length' ) # rnnlm related parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--lm-weight' , type = float , default = 0.0 , help = 'RNNLM weight' ) # multilingual related parser . add_argument ( '--tgt-lang' , default = False , type = str , help = 'target language ID (e.g., <en>, <de>, and <fr> etc.)' ) return parser","title":"get_parser()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.mt_trans.main","text":"Run the main decoding function. Source code in adviser/tools/espnet_minimal/bin/mt_trans.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def main ( args ): \"\"\"Run the main decoding function.\"\"\" parser = get_parser () args = parser . parse_args ( args ) # logging info if args . verbose == 1 : logging . basicConfig ( level = logging . INFO , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) elif args . verbose == 2 : logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) else : logging . basicConfig ( level = logging . WARN , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) logging . warning ( \"Skip DEBUG/INFO messages\" ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # TODO(mn5k): support of multiple GPUs if args . ngpu > 1 : logging . error ( \"The program only supports ngpu=1.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting random . seed ( args . seed ) np . random . seed ( args . seed ) logging . info ( 'set random seed = %d ' % args . seed ) # trans logging . info ( 'backend = ' + args . backend ) if args . backend == \"pytorch\" : # Experimental API that supports custom LMs from tools.espnet_minimal.mt.pytorch_backend.mt import trans if args . dtype != \"float32\" : raise NotImplementedError ( f \"`--dtype { args . dtype } ` is only available with `--api v2`\" ) trans ( args ) else : raise ValueError ( \"Only pytorch are supported.\" )","title":"main()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.st_trans","text":"End-to-end speech translation model decoding script.","title":"st_trans"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.st_trans.get_parser","text":"Get default arguments. Source code in adviser/tools/espnet_minimal/bin/st_trans.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def get_parser (): \"\"\"Get default arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Translate text from speech using a speech translation model on one CPU or GPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'Config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'Second config file path that overwrites the settings in `--config`' ) parser . add ( '--config3' , is_config_file = True , help = 'Third config file path that overwrites the settings in `--config` and `--config2`' ) parser . add_argument ( '--ngpu' , type = int , default = 0 , help = 'Number of GPUs' ) parser . add_argument ( '--dtype' , choices = ( \"float16\" , \"float32\" , \"float64\" ), default = \"float32\" , help = 'Float precision (only available in --api v2)' ) parser . add_argument ( '--backend' , type = str , default = 'chainer' , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , type = int , default = 1 , help = 'Debugmode' ) parser . add_argument ( '--seed' , type = int , default = 1 , help = 'Random seed' ) parser . add_argument ( '--verbose' , '-V' , type = int , default = 1 , help = 'Verbose option' ) parser . add_argument ( '--batchsize' , type = int , default = 1 , help = 'Batch size for beam search (0: means no batch processing)' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) parser . add_argument ( '--api' , default = \"v1\" , choices = [ \"v1\" , \"v2\" ], help = '''Beam search APIs v1: Default API. It only supports the ASRInterface.recognize method and DefaultRNNLM. v2: Experimental API. It supports any models that implements ScorerInterface.''' ) # task related parser . add_argument ( '--trans-json' , type = str , help = 'Filename of translation data (json)' ) parser . add_argument ( '--result-label' , type = str , required = True , help = 'Filename of result label data (json)' ) # model (parameter) related parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) # search related parser . add_argument ( '--nbest' , type = int , default = 1 , help = 'Output N-best hypotheses' ) parser . add_argument ( '--beam-size' , type = int , default = 1 , help = 'Beam size' ) parser . add_argument ( '--penalty' , type = float , default = 0.0 , help = 'Incertion penalty' ) parser . add_argument ( '--maxlenratio' , type = float , default = 0.0 , help = \"\"\"Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths\"\"\" ) parser . add_argument ( '--minlenratio' , type = float , default = 0.0 , help = 'Input length ratio to obtain min output length' ) # rnnlm related parser . add_argument ( '--rnnlm' , type = str , default = None , help = 'RNNLM model file to read' ) parser . add_argument ( '--rnnlm-conf' , type = str , default = None , help = 'RNNLM model config file to read' ) parser . add_argument ( '--word-rnnlm' , type = str , default = None , help = 'Word RNNLM model file to read' ) parser . add_argument ( '--word-rnnlm-conf' , type = str , default = None , help = 'Word RNNLM model config file to read' ) parser . add_argument ( '--word-dict' , type = str , default = None , help = 'Word list to read' ) parser . add_argument ( '--lm-weight' , type = float , default = 0.1 , help = 'RNNLM weight' ) # multilingual related parser . add_argument ( '--tgt-lang' , default = False , type = str , help = 'target language ID (e.g., <en>, <de>, and <fr> etc.)' ) return parser","title":"get_parser()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.st_trans.main","text":"Run the main decoding function. Source code in adviser/tools/espnet_minimal/bin/st_trans.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def main ( args ): \"\"\"Run the main decoding function.\"\"\" parser = get_parser () args = parser . parse_args ( args ) # logging info if args . verbose == 1 : logging . basicConfig ( level = logging . INFO , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) elif args . verbose == 2 : logging . basicConfig ( level = logging . DEBUG , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) else : logging . basicConfig ( level = logging . WARN , format = \" %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s \" ) logging . warning ( \"Skip DEBUG/INFO messages\" ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # TODO(mn5k): support of multiple GPUs if args . ngpu > 1 : logging . error ( \"The program only supports ngpu=1.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # seed setting random . seed ( args . seed ) np . random . seed ( args . seed ) logging . info ( 'set random seed = %d ' % args . seed ) # validate rnn options if args . rnnlm is not None and args . word_rnnlm is not None : logging . error ( \"It seems that both --rnnlm and --word-rnnlm are specified. Please use either option.\" ) sys . exit ( 1 ) # trans logging . info ( 'backend = ' + args . backend ) if args . backend == \"pytorch\" : # Experimental API that supports custom LMs from tools.espnet_minimal import trans if args . dtype != \"float32\" : raise NotImplementedError ( f \"`--dtype { args . dtype } ` is only available with `--api v2`\" ) trans ( args ) else : raise ValueError ( \"Only pytorch are supported.\" )","title":"main()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.tts_decode","text":"TTS decoding script.","title":"tts_decode"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.tts_decode.get_parser","text":"Get parser of decoding arguments. Source code in adviser/tools/espnet_minimal/bin/tts_decode.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_parser (): \"\"\"Get parser of decoding arguments.\"\"\" parser = configargparse . ArgumentParser ( description = 'Synthesize speech from text using a TTS model on one CPU' , config_file_parser_class = configargparse . YAMLConfigFileParser , formatter_class = configargparse . ArgumentDefaultsHelpFormatter ) # general configuration parser . add ( '--config' , is_config_file = True , help = 'config file path' ) parser . add ( '--config2' , is_config_file = True , help = 'second config file path that overwrites the settings in `--config`.' ) parser . add ( '--config3' , is_config_file = True , help = 'third config file path that overwrites the settings in `--config` and `--config2`.' ) parser . add_argument ( '--ngpu' , default = 0 , type = int , help = 'Number of GPUs' ) parser . add_argument ( '--backend' , default = 'pytorch' , type = str , choices = [ 'chainer' , 'pytorch' ], help = 'Backend library' ) parser . add_argument ( '--debugmode' , default = 1 , type = int , help = 'Debugmode' ) parser . add_argument ( '--seed' , default = 1 , type = int , help = 'Random seed' ) parser . add_argument ( '--out' , type = str , required = True , help = 'Output filename' ) parser . add_argument ( '--verbose' , '-V' , default = 0 , type = int , help = 'Verbose option' ) parser . add_argument ( '--preprocess-conf' , type = str , default = None , help = 'The configuration file for the pre-processing' ) # task related parser . add_argument ( '--json' , type = str , required = True , help = 'Filename of train label data (json)' ) parser . add_argument ( '--model' , type = str , required = True , help = 'Model file parameters to read' ) parser . add_argument ( '--model-conf' , type = str , default = None , help = 'Model config file' ) # decoding related parser . add_argument ( '--maxlenratio' , type = float , default = 5 , help = 'Maximum length ratio in decoding' ) parser . add_argument ( '--minlenratio' , type = float , default = 0 , help = 'Minimum length ratio in decoding' ) parser . add_argument ( '--threshold' , type = float , default = 0.5 , help = 'Threshold value in decoding' ) parser . add_argument ( '--use-att-constraint' , type = strtobool , default = False , help = 'Whether to use the attention constraint' ) parser . add_argument ( '--backward-window' , type = int , default = 1 , help = 'Backward window size in the attention constraint' ) parser . add_argument ( '--forward-window' , type = int , default = 3 , help = 'Forward window size in the attention constraint' ) # save related parser . add_argument ( '--save-durations' , default = False , type = strtobool , help = 'Whether to save durations converted from attentions' ) parser . add_argument ( '--save-focus-rates' , default = False , type = strtobool , help = 'Whether to save focus rates of attentions' ) return parser","title":"get_parser()"},{"location":"api/tools/#adviser.tools.espnet_minimal.bin.tts_decode.main","text":"Run deocding. Source code in adviser/tools/espnet_minimal/bin/tts_decode.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def main ( args ): \"\"\"Run deocding.\"\"\" parser = get_parser () args = parser . parse_args ( args ) # logging info if args . verbose > 0 : logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) else : logging . basicConfig ( level = logging . WARN , format = ' %(asctime)s ( %(module)s : %(lineno)d ) %(levelname)s : %(message)s ' ) logging . warning ( 'Skip DEBUG/INFO messages' ) # check CUDA_VISIBLE_DEVICES if args . ngpu > 0 : # python 2 case if platform . python_version_tuple ()[ 0 ] == '2' : if \"clsp.jhu.edu\" in subprocess . check_output ([ \"hostname\" , \"-f\" ]): cvd = subprocess . check_output ([ \"/usr/local/bin/free-gpu\" , \"-n\" , str ( args . ngpu )]) . strip () logging . info ( 'CLSP: use gpu' + cvd ) os . environ [ 'CUDA_VISIBLE_DEVICES' ] = cvd # python 3 case else : if \"clsp.jhu.edu\" in subprocess . check_output ([ \"hostname\" , \"-f\" ]) . decode (): cvd = subprocess . check_output ([ \"/usr/local/bin/free-gpu\" , \"-n\" , str ( args . ngpu )]) . decode () . strip () logging . info ( 'CLSP: use gpu' + cvd ) os . environ [ 'CUDA_VISIBLE_DEVICES' ] = cvd cvd = os . environ . get ( \"CUDA_VISIBLE_DEVICES\" ) if cvd is None : logging . warning ( \"CUDA_VISIBLE_DEVICES is not set.\" ) elif args . ngpu != len ( cvd . split ( \",\" )): logging . error ( \"#gpus is not matched with CUDA_VISIBLE_DEVICES.\" ) sys . exit ( 1 ) # display PYTHONPATH logging . info ( 'python path = ' + os . environ . get ( 'PYTHONPATH' , '(None)' )) # extract logging . info ( 'backend = ' + args . backend ) if args . backend == \"pytorch\" : from tools.espnet_minimal.tts.pytorch_backend.tts import decode decode ( args ) else : raise NotImplementedError ( \"Only pytorch is supported.\" )","title":"main()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets","text":"","title":"nets"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface","text":"ASR Interface module.","title":"asr_interface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface","text":"ASR Interface for ESPnet model implementation.","title":"ASRInterface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.attention_plot_class","text":"Get attention plot class.","title":"attention_plot_class"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.add_arguments","text":"Add arguments to parser. Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 12 13 14 15 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments to parser.\"\"\" return parser","title":"add_arguments()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.build","text":"Initialize this class with python-level args. Parameters: Name Type Description Default idim int The number of an input feature dim. required odim int The number of output vocab. required Returns: Type Description ASRinterface A new instance of ASRInterface. Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @classmethod def build ( cls , idim : int , odim : int , ** kwargs ): \"\"\"Initialize this class with python-level args. Args: idim (int): The number of an input feature dim. odim (int): The number of output vocab. Returns: ASRinterface: A new instance of ASRInterface. \"\"\" def wrap ( parser ): return get_parser ( parser , required = False ) args = argparse . Namespace ( ** kwargs ) args = fill_missing_args ( args , wrap ) args = fill_missing_args ( args , cls . add_arguments ) return cls ( idim , odim , args )","title":"build()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.calculate_all_attentions","text":"Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 79 80 81 82 83 84 85 86 87 88 def calculate_all_attentions ( self , xs , ilens , ys ): \"\"\"Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray \"\"\" raise NotImplementedError ( \"calculate_all_attentions method is not implemented\" )","title":"calculate_all_attentions()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.encode","text":"Encode feature in beam_search (optional). Parameters: Name Type Description Default x numpy.ndarray input feature (T, D) required Returns: Type Description torch.Tensor for pytorch, chainer.Variable for chainer encoded feature (T, D) Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 96 97 98 99 100 101 102 103 104 105 106 def encode ( self , feat ): \"\"\"Encode feature in `beam_search` (optional). Args: x (numpy.ndarray): input feature (T, D) Returns: torch.Tensor for pytorch, chainer.Variable for chainer: encoded feature (T, D) \"\"\" raise NotImplementedError ( \"encode method is not implemented\" )","title":"encode()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.forward","text":"Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , xs , ilens , ys ): \"\"\"Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer \"\"\" raise NotImplementedError ( \"forward method is not implemented\" )","title":"forward()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.recognize","text":"Recognize x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace recog_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 55 56 57 58 59 60 61 62 63 64 65 def recognize ( self , x , recog_args , char_list = None , rnnlm = None ): \"\"\"Recognize x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace recog_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"recognize method is not implemented\" )","title":"recognize()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.recognize_batch","text":"Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace recog_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 67 68 69 70 71 72 73 74 75 76 77 def recognize_batch ( self , x , recog_args , char_list = None , rnnlm = None ): \"\"\"Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace recog_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"Batch decoding is not supported yet.\" )","title":"recognize_batch()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.ASRInterface.scorers","text":"Get scorers for beam_search (optional). Returns: Type Description dict[str, ScorerInterface] dict of ScorerInterface objects Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 108 109 110 111 112 113 114 115 def scorers ( self ): \"\"\"Get scorers for `beam_search` (optional). Returns: dict[str, ScorerInterface]: dict of `ScorerInterface` objects \"\"\" raise NotImplementedError ( \"decoders method is not implemented\" )","title":"scorers()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.asr_interface.dynamic_import_asr","text":"Import ASR models dynamically. Parameters: Name Type Description Default module str module_name:class_name or alias in predefined_asr required backend str NN backend. e.g., pytorch, chainer required Returns: Type Description type ASR class Source code in adviser/tools/espnet_minimal/nets/asr_interface.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def dynamic_import_asr ( module , backend ): \"\"\"Import ASR models dynamically. Args: module (str): module_name:class_name or alias in `predefined_asr` backend (str): NN backend. e.g., pytorch, chainer Returns: type: ASR class \"\"\" model_class = dynamic_import ( module , predefined_asr . get ( backend , dict ())) assert issubclass ( model_class , ASRInterface ), f \" { module } does not implement ASRInterface\" return model_class","title":"dynamic_import_asr()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search","text":"Parallel beam search module.","title":"batch_beam_search"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchBeamSearch","text":"Batch beam search implementation.","title":"BatchBeamSearch"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchBeamSearch.batch_beam","text":"Batch-compute topk full token ids and partial token ids. Parameters: Name Type Description Default weighted_scores Tensor The weighted sum scores for each tokens. Its shape is (n_beam, self.vocab_size) . required ids Tensor The partial token ids to compute topk. Its shape is (n_beam, self.pre_beam_size) . required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all (self.beam_size,) Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def batch_beam ( self , weighted_scores : torch . Tensor , ids : torch . Tensor ) \\ -> Tuple [ torch . Tensor , torch . Tensor , torch . Tensor , torch . Tensor ]: \"\"\"Batch-compute topk full token ids and partial token ids. Args: weighted_scores (torch.Tensor): The weighted sum scores for each tokens. Its shape is `(n_beam, self.vocab_size)`. ids (torch.Tensor): The partial token ids to compute topk. Its shape is `(n_beam, self.pre_beam_size)`. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The topk full (prev_hyp, new_token) ids and partial (prev_hyp, new_token) ids. Their shapes are all `(self.beam_size,)` \"\"\" if not self . do_pre_beam : top_ids = weighted_scores . view ( - 1 ) . topk ( self . beam_size )[ 1 ] # Because of the flatten above, `top_ids` is organized as: # [hyp1 * V + token1, hyp2 * V + token2, ..., hypK * V + tokenK], # where V is `self.n_vocab` and K is `self.beam_size` prev_hyp_ids = top_ids // self . n_vocab new_token_ids = top_ids % self . n_vocab return prev_hyp_ids , new_token_ids , prev_hyp_ids , new_token_ids raise NotImplementedError ( \"batch decoding with PartialScorer is not supported yet.\" )","title":"batch_beam()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchBeamSearch.batchfy","text":"Convert list to batch. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 33 34 35 36 37 38 39 40 41 42 43 def batchfy ( self , hyps : List [ Hypothesis ]) -> BatchHypothesis : \"\"\"Convert list to batch.\"\"\" if len ( hyps ) == 0 : return BatchHypothesis () return BatchHypothesis ( yseq = pad_sequence ([ h . yseq for h in hyps ], batch_first = True , padding_value = self . eos ), length = torch . tensor ([ len ( h . yseq ) for h in hyps ], dtype = torch . int64 ), score = torch . tensor ([ h . score for h in hyps ]), scores = { k : torch . tensor ([ h . scores [ k ] for h in hyps ]) for k in self . scorers }, states = { k : [ h . states [ k ] for h in hyps ] for k in self . scorers } )","title":"batchfy()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchBeamSearch.init_hyp","text":"Get an initial hypothesis data. Parameters: Name Type Description Default x Tensor The encoder output feature required Returns: Type Description BatchHypothesis Hypothesis: The initial hypothesis. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def init_hyp ( self , x : torch . Tensor ) -> BatchHypothesis : \"\"\"Get an initial hypothesis data. Args: x (torch.Tensor): The encoder output feature Returns: Hypothesis: The initial hypothesis. \"\"\" init_states = dict () init_scores = dict () for k , d in self . scorers . items (): init_states [ k ] = d . init_state ( x ) init_scores [ k ] = 0.0 return self . batchfy ( super () . init_hyp ( x ))","title":"init_hyp()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchBeamSearch.post_process","text":"Perform post-processing of beam search iterations. Parameters: Name Type Description Default i int The length of hypothesis tokens. required maxlen int The maximum length of tokens in beam search. required maxlenratio float The maximum length ratio in beam search. required running_hyps BatchHypothesis The running hypotheses in beam search. required ended_hyps List[tools.espnet_minimal.nets.beam_search.Hypothesis] The ended hypotheses in beam search. required Returns: Type Description BatchHypothesis BatchHypothesis: The new running hypotheses. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def post_process ( self , i : int , maxlen : int , maxlenratio : float , running_hyps : BatchHypothesis , ended_hyps : List [ Hypothesis ]) -> BatchHypothesis : \"\"\"Perform post-processing of beam search iterations. Args: i (int): The length of hypothesis tokens. maxlen (int): The maximum length of tokens in beam search. maxlenratio (int): The maximum length ratio in beam search. running_hyps (BatchHypothesis): The running hypotheses in beam search. ended_hyps (List[Hypothesis]): The ended hypotheses in beam search. Returns: BatchHypothesis: The new running hypotheses. \"\"\" n_batch , maxlen = running_hyps . yseq . shape logging . debug ( f 'the number of running hypothes: { n_batch } ' ) if self . token_list is not None : logging . debug ( \"best hypo: \" + \"\" . join ( [ self . token_list [ x ] for x in running_hyps . yseq [ 0 , 1 : running_hyps . length [ 0 ]]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( \"adding <eos> in the last position in the loop\" ) running_hyps . yseq . resize_ ( n_batch , maxlen + 1 ) running_hyps . yseq [:, - 1 ] = self . eos running_hyps . yseq . index_fill_ ( 1 , running_hyps . length , self . eos ) # add ended hypotheses to a final list, and removed them from current hypotheses # (this will be a probmlem, number of hyps < beam) is_eos = running_hyps . yseq [ torch . arange ( n_batch ), running_hyps . length - 1 ] == self . eos for b in torch . nonzero ( is_eos ) . view ( - 1 ): hyp = self . _select ( running_hyps , b ) ended_hyps . append ( hyp ) remained_ids = torch . nonzero ( is_eos == 0 ) . view ( - 1 ) return self . _batch_select ( running_hyps , remained_ids )","title":"post_process()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchBeamSearch.search","text":"Search new tokens for running hypotheses and encoded speech x. Parameters: Name Type Description Default running_hyps BatchHypothesis Running hypotheses on beam required x Tensor Encoded speech feature (T, D) required Returns: Type Description BatchHypothesis BatchHypothesis: Best sorted hypotheses Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def search ( self , running_hyps : BatchHypothesis , x : torch . Tensor ) -> BatchHypothesis : \"\"\"Search new tokens for running hypotheses and encoded speech x. Args: running_hyps (BatchHypothesis): Running hypotheses on beam x (torch.Tensor): Encoded speech feature (T, D) Returns: BatchHypothesis: Best sorted hypotheses \"\"\" n_batch = len ( running_hyps ) # batch scoring scores , states = self . score_full ( running_hyps , x . expand ( n_batch , * x . shape )) if self . do_pre_beam : part_ids = torch . topk ( scores [ self . pre_beam_score_key ], self . pre_beam_size , dim =- 1 )[ 1 ] else : part_ids = torch . arange ( self . n_vocab , device = x . device ) . expand ( n_batch , self . n_vocab ) part_scores , part_states = self . score_partial ( running_hyps , part_ids , x ) # weighted sum scores weighted_scores = torch . zeros ( n_batch , self . n_vocab , dtype = x . dtype , device = x . device ) for k in self . full_scorers : weighted_scores += self . weights [ k ] * scores [ k ] for k in self . part_scorers : weighted_scores [ part_ids ] += self . weights [ k ] * part_scores [ k ] weighted_scores += running_hyps . score . unsqueeze ( 1 ) # TODO(karita): do not use list. use batch instead # update hyps best_hyps = [] prev_hyps = self . unbatchfy ( running_hyps ) for full_prev_hyp_id , full_new_token_id , part_prev_hyp_id , part_new_token_id in zip ( * self . batch_beam ( weighted_scores , part_ids )): prev_hyp = prev_hyps [ full_prev_hyp_id ] best_hyps . append ( Hypothesis ( score = weighted_scores [ full_prev_hyp_id , full_new_token_id ], yseq = self . append_token ( prev_hyp . yseq , full_new_token_id ), scores = self . merge_scores ( prev_hyp . scores , { k : v [ full_prev_hyp_id ] for k , v in scores . items ()}, full_new_token_id , { k : v [ part_prev_hyp_id ] for k , v in part_scores . items ()}, part_new_token_id ), states = self . merge_states ( { k : self . full_scorers [ k ] . select_state ( v , full_prev_hyp_id ) for k , v in states . items ()}, { k : self . part_scorers [ k ] . select_state ( v , part_prev_hyp_id ) for k , v in part_states . items ()}, part_new_token_id ) )) return self . batchfy ( best_hyps )","title":"search()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchBeamSearch.unbatchfy","text":"Revert batch to list. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 63 64 65 66 67 68 69 70 71 72 def unbatchfy ( self , batch_hyps : BatchHypothesis ) -> List [ Hypothesis ]: \"\"\"Revert batch to list.\"\"\" return [ Hypothesis ( yseq = batch_hyps . yseq [ i ][: batch_hyps . length [ i ]], score = batch_hyps . score [ i ], scores = { k : batch_hyps . scores [ k ][ i ] for k in self . scorers }, states = { k : v . select_state ( batch_hyps . states [ k ], i ) for k , v in self . scorers . items ()} ) for i in range ( len ( batch_hyps . length ))]","title":"unbatchfy()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchHypothesis","text":"Batchfied/Vectorized hypothesis data type.","title":"BatchHypothesis"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchHypothesis.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 427 428 429 def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchHypothesis.__len__","text":"Return a batch size. Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 25 26 27 def __len__ ( self ) -> int : \"\"\"Return a batch size.\"\"\" return len ( self . length )","title":"__len__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchHypothesis.__new__","text":"Create new instance of BatchHypothesis(yseq, score, length, scores, states)","title":"__new__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.batch_beam_search.BatchHypothesis.__repr__","text":"Return a nicely formatted representation string Source code in adviser/tools/espnet_minimal/nets/batch_beam_search.py 419 420 421 def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search","text":"Beam search module.","title":"beam_search"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch","text":"Beam search implementation.","title":"BeamSearch"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.__init__","text":"Initialize beam search. Parameters: Name Type Description Default scorers Dict[str, tools.espnet_minimal.nets.scorer_interface.ScorerInterface] Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None required weights Dict[str, float] Dict of weights for each scorers The scorer will be ignored if its weight is 0 required beam_size int The number of hypotheses kept during search required vocab_size int The number of vocabulary required sos int Start of sequence id required eos int End of sequence id required token_list List[str] List of tokens for debug log None pre_beam_score_key str key of scores to perform pre-beam search None pre_beam_ratio float beam size in the pre-beam search will be int(pre_beam_ratio * beam_size) 1.5 Source code in adviser/tools/espnet_minimal/nets/beam_search.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , scorers : Dict [ str , ScorerInterface ], weights : Dict [ str , float ], beam_size : int , vocab_size : int , sos : int , eos : int , token_list : List [ str ] = None , pre_beam_ratio : float = 1.5 , pre_beam_score_key : str = None ): \"\"\"Initialize beam search. Args: scorers (dict[str, ScorerInterface]): Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is `None` weights (dict[str, float]): Dict of weights for each scorers The scorer will be ignored if its weight is 0 beam_size (int): The number of hypotheses kept during search vocab_size (int): The number of vocabulary sos (int): Start of sequence id eos (int): End of sequence id token_list (list[str]): List of tokens for debug log pre_beam_score_key (str): key of scores to perform pre-beam search pre_beam_ratio (float): beam size in the pre-beam search will be `int(pre_beam_ratio * beam_size)` \"\"\" super () . __init__ () # set scorers self . weights = weights self . scorers = dict () self . full_scorers = dict () self . part_scorers = dict () # this module dict is required for recursive cast `self.to(device, dtype)` in `recog.py` self . nn_dict = torch . nn . ModuleDict () for k , v in scorers . items (): w = weights . get ( k , 0 ) if w == 0 or v is None : continue assert isinstance ( v , ScorerInterface ), f \" { k } ( { type ( v ) } ) does not implement ScorerInterface\" self . scorers [ k ] = v if isinstance ( v , PartialScorerInterface ): self . part_scorers [ k ] = v else : self . full_scorers [ k ] = v if isinstance ( v , torch . nn . Module ): self . nn_dict [ k ] = v # set configurations self . sos = sos self . eos = eos self . token_list = token_list self . pre_beam_size = int ( pre_beam_ratio * beam_size ) self . beam_size = beam_size self . n_vocab = vocab_size if pre_beam_score_key is not None and pre_beam_score_key not in self . full_scorers : raise KeyError ( f \" { pre_beam_score_key } is not found in { self . full_scorers } \" ) self . pre_beam_score_key = pre_beam_score_key self . do_pre_beam = self . pre_beam_score_key is not None and \\ self . pre_beam_size < self . n_vocab and len ( self . part_scorers ) > 0","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.append_token","text":"Append new token to prefix tokens. Parameters: Name Type Description Default xs Tensor The prefix token required x int The new token to append required Returns: Type Description Tensor torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device Source code in adviser/tools/espnet_minimal/nets/beam_search.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @staticmethod def append_token ( xs : torch . Tensor , x : int ) -> torch . Tensor : \"\"\"Append new token to prefix tokens. Args: xs (torch.Tensor): The prefix token x (int): The new token to append Returns: torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device \"\"\" x = torch . tensor ([ x ], dtype = xs . dtype , device = xs . device ) return torch . cat (( xs , x ))","title":"append_token()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.beam","text":"Compute topk full token ids and partial token ids. Parameters: Name Type Description Default weighted_scores Tensor The weighted sum scores for each tokens. Its shape is (self.n_vocab,) . required ids Tensor The partial token ids to compute topk required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: The topk full token ids and partial token ids. Their shapes are (self.beam_size,) Source code in adviser/tools/espnet_minimal/nets/beam_search.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def beam ( self , weighted_scores : torch . Tensor , ids : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Compute topk full token ids and partial token ids. Args: weighted_scores (torch.Tensor): The weighted sum scores for each tokens. Its shape is `(self.n_vocab,)`. ids (torch.Tensor): The partial token ids to compute topk Returns: Tuple[torch.Tensor, torch.Tensor]: The topk full token ids and partial token ids. Their shapes are `(self.beam_size,)` \"\"\" # no pre beam performed if weighted_scores . size ( 0 ) == ids . size ( 0 ): top_ids = weighted_scores . topk ( self . beam_size )[ 1 ] return top_ids , top_ids # mask pruned in pre-beam not to select in topk tmp = weighted_scores [ ids ] weighted_scores [:] = - float ( \"inf\" ) weighted_scores [ ids ] = tmp top_ids = weighted_scores . topk ( self . beam_size )[ 1 ] local_ids = weighted_scores [ ids ] . topk ( self . beam_size )[ 1 ] return top_ids , local_ids","title":"beam()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.forward","text":"Perform beam search. Parameters: Name Type Description Default x Tensor Encoded speech feature (T, D) required maxlenratio float Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths 0.0 minlenratio float Input length ratio to obtain min output length. 0.0 Returns: Type Description List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] list[Hypothesis]: N-best decoding results Source code in adviser/tools/espnet_minimal/nets/beam_search.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def forward ( self , x : torch . Tensor , maxlenratio : float = 0.0 , minlenratio : float = 0.0 ) -> List [ Hypothesis ]: \"\"\"Perform beam search. Args: x (torch.Tensor): Encoded speech feature (T, D) maxlenratio (float): Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths minlenratio (float): Input length ratio to obtain min output length. Returns: list[Hypothesis]: N-best decoding results \"\"\" # set length bounds if maxlenratio == 0 : maxlen = x . shape [ 0 ] else : maxlen = max ( 1 , int ( maxlenratio * x . size ( 0 ))) minlen = int ( minlenratio * x . size ( 0 )) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # main loop of prefix search running_hyps = self . init_hyp ( x ) ended_hyps = [] for i in range ( maxlen ): logging . debug ( 'position ' + str ( i )) best = self . search ( running_hyps , x ) # post process of one iteration running_hyps = self . post_process ( i , maxlen , maxlenratio , best , ended_hyps ) # end detection if maxlenratio == 0.0 and end_detect ([ h . asdict () for h in ended_hyps ], i ): logging . info ( f 'end detected at { i } ' ) break if len ( running_hyps ) == 0 : logging . info ( 'no hypothesis. Finish decoding.' ) break else : logging . debug ( f 'remeined hypothes: { len ( running_hyps ) } ' ) nbest_hyps = sorted ( ended_hyps , key = lambda x : x . score , reverse = True ) # check number of hypotheis if len ( nbest_hyps ) == 0 : logging . warning ( 'there is no N-best results, perform recognition again with smaller minlenratio.' ) return [] if minlenratio < 0.1 else self . forward ( x , maxlenratio , max ( 0.0 , minlenratio - 0.1 )) # report the best result best = nbest_hyps [ 0 ] logging . info ( f 'total log probability: { best . score } ' ) logging . info ( f 'normalized log probability: { best . score / len ( best . yseq ) } ' ) return nbest_hyps","title":"forward()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.init_hyp","text":"Get an initial hypothesis data. Parameters: Name Type Description Default x Tensor The encoder output feature required Returns: Type Description Hypothesis Hypothesis: The initial hypothesis. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def init_hyp ( self , x : torch . Tensor ) -> Hypothesis : \"\"\"Get an initial hypothesis data. Args: x (torch.Tensor): The encoder output feature Returns: Hypothesis: The initial hypothesis. \"\"\" init_states = dict () init_scores = dict () for k , d in self . scorers . items (): init_states [ k ] = d . init_state ( x ) init_scores [ k ] = 0.0 return [ Hypothesis ( score = 0.0 , scores = init_scores , states = init_states , yseq = torch . tensor ([ self . sos ], device = x . device ))]","title":"init_hyp()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.merge_scores","text":"Merge scores for new hypothesis. Parameters: Name Type Description Default prev_scores Dict[str, float] The previous hypothesis scores by self.scorers required next_full_scores Dict[str, torch.Tensor] scores by self.full_scorers required full_idx int The next token id for next_full_scores required next_part_scores Dict[str, torch.Tensor] scores of partial tokens by self.part_scorers required part_idx int The new token id for next_part_scores required Returns: Type Description Dict[str, torch.Tensor] Dict[str, torch.Tensor]: The new score dict. Its keys are names of self.full_scorers and self.part_scorers . Its values are scalar tensors by the scorers. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 @staticmethod def merge_scores ( prev_scores : Dict [ str , float ], next_full_scores : Dict [ str , torch . Tensor ], full_idx : int , next_part_scores : Dict [ str , torch . Tensor ], part_idx : int ) -> Dict [ str , torch . Tensor ]: \"\"\"Merge scores for new hypothesis. Args: prev_scores (Dict[str, float]): The previous hypothesis scores by `self.scorers` next_full_scores (Dict[str, torch.Tensor]): scores by `self.full_scorers` full_idx (int): The next token id for `next_full_scores` next_part_scores (Dict[str, torch.Tensor]): scores of partial tokens by `self.part_scorers` part_idx (int): The new token id for `next_part_scores` Returns: Dict[str, torch.Tensor]: The new score dict. Its keys are names of `self.full_scorers` and `self.part_scorers`. Its values are scalar tensors by the scorers. \"\"\" new_scores = dict () for k , v in next_full_scores . items (): new_scores [ k ] = prev_scores [ k ] + v [ full_idx ] for k , v in next_part_scores . items (): new_scores [ k ] = v [ part_idx ] return new_scores","title":"merge_scores()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.merge_states","text":"Merge states for new hypothesis. Parameters: Name Type Description Default states Any states of self.full_scorers required part_states Any states of self.part_scorers required part_idx int The new token id for part_scores required Returns: Type Description Any Dict[str, torch.Tensor]: The new score dict. Its keys are names of self.full_scorers and self.part_scorers . Its values are states of the scorers. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def merge_states ( self , states : Any , part_states : Any , part_idx : int ) -> Any : \"\"\"Merge states for new hypothesis. Args: states: states of `self.full_scorers` part_states: states of `self.part_scorers` part_idx (int): The new token id for `part_scores` Returns: Dict[str, torch.Tensor]: The new score dict. Its keys are names of `self.full_scorers` and `self.part_scorers`. Its values are states of the scorers. \"\"\" new_states = dict () for k , v in states . items (): new_states [ k ] = v for k , d in self . part_scorers . items (): new_states [ k ] = d . select_state ( part_states [ k ], part_idx ) return new_states","title":"merge_states()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.post_process","text":"Perform post-processing of beam search iterations. Parameters: Name Type Description Default i int The length of hypothesis tokens. required maxlen int The maximum length of tokens in beam search. required maxlenratio float The maximum length ratio in beam search. required running_hyps List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] The running hypotheses in beam search. required ended_hyps List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] The ended hypotheses in beam search. required Returns: Type Description List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] List[Hypothesis]: The new running hypotheses. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def post_process ( self , i : int , maxlen : int , maxlenratio : float , running_hyps : List [ Hypothesis ], ended_hyps : List [ Hypothesis ]) -> List [ Hypothesis ]: \"\"\"Perform post-processing of beam search iterations. Args: i (int): The length of hypothesis tokens. maxlen (int): The maximum length of tokens in beam search. maxlenratio (int): The maximum length ratio in beam search. running_hyps (List[Hypothesis]): The running hypotheses in beam search. ended_hyps (List[Hypothesis]): The ended hypotheses in beam search. Returns: List[Hypothesis]: The new running hypotheses. \"\"\" logging . debug ( f 'the number of running hypothes: { len ( running_hyps ) } ' ) if self . token_list is not None : logging . debug ( \"best hypo: \" + \"\" . join ([ self . token_list [ x ] for x in running_hyps [ 0 ] . yseq [ 1 :]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( \"adding <eos> in the last position in the loop\" ) running_hyps = [ h . _replace ( yseq = self . append_token ( h . yseq , self . eos )) for h in running_hyps ] # add ended hypotheses to a final list, and removed them from current hypotheses # (this will be a probmlem, number of hyps < beam) remained_hyps = [] for hyp in running_hyps : if hyp . yseq [ - 1 ] == self . eos : # e.g., Word LM needs to add final <eos> score for k , d in chain ( self . full_scorers . items (), self . part_scorers . items ()): s = d . final_score ( hyp . states [ k ]) hyp . scores [ k ] += s hyp = hyp . _replace ( score = hyp . score + self . weights [ k ] * s ) ended_hyps . append ( hyp ) else : remained_hyps . append ( hyp ) return remained_hyps","title":"post_process()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.score_full","text":"Score new hypothesis by self.full_scorers . Parameters: Name Type Description Default hyp Hypothesis Hypothesis with prefix tokens to score required x Tensor Corresponding input feature required Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, Any]] Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of hyp that has string keys of self.full_scorers and tensor score values of shape: (self.n_vocab,) , and state dict that has string keys and state values of self.full_scorers Source code in adviser/tools/espnet_minimal/nets/beam_search.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def score_full ( self , hyp : Hypothesis , x : torch . Tensor ) -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , Any ]]: \"\"\"Score new hypothesis by `self.full_scorers`. Args: hyp (Hypothesis): Hypothesis with prefix tokens to score x (torch.Tensor): Corresponding input feature Returns: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of `hyp` that has string keys of `self.full_scorers` and tensor score values of shape: `(self.n_vocab,)`, and state dict that has string keys and state values of `self.full_scorers` \"\"\" scores = dict () states = dict () for k , d in self . full_scorers . items (): scores [ k ], states [ k ] = d . score ( hyp . yseq , hyp . states [ k ], x ) return scores , states","title":"score_full()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.score_partial","text":"Score new hypothesis by self.part_scorers . Parameters: Name Type Description Default hyp Hypothesis Hypothesis with prefix tokens to score required ids Tensor 1D tensor of new partial tokens to score required x Tensor Corresponding input feature required Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, Any]] Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of hyp that has string keys of self.part_scorers and tensor score values of shape: (len(ids),) , and state dict that has string keys and state values of self.part_scorers Source code in adviser/tools/espnet_minimal/nets/beam_search.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def score_partial ( self , hyp : Hypothesis , ids : torch . Tensor , x : torch . Tensor ) \\ -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , Any ]]: \"\"\"Score new hypothesis by `self.part_scorers`. Args: hyp (Hypothesis): Hypothesis with prefix tokens to score ids (torch.Tensor): 1D tensor of new partial tokens to score x (torch.Tensor): Corresponding input feature Returns: Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of score dict of `hyp` that has string keys of `self.part_scorers` and tensor score values of shape: `(len(ids),)`, and state dict that has string keys and state values of `self.part_scorers` \"\"\" scores = dict () states = dict () for k , d in self . part_scorers . items (): scores [ k ], states [ k ] = d . score_partial ( hyp . yseq , ids , hyp . states [ k ], x ) return scores , states","title":"score_partial()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.BeamSearch.search","text":"Search new tokens for running hypotheses and encoded speech x. Parameters: Name Type Description Default running_hyps List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] Running hypotheses on beam required x Tensor Encoded speech feature (T, D) required Returns: Type Description List[adviser.tools.espnet_minimal.nets.beam_search.Hypothesis] List[Hypotheses]: Best sorted hypotheses Source code in adviser/tools/espnet_minimal/nets/beam_search.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def search ( self , running_hyps : List [ Hypothesis ], x : torch . Tensor ) -> List [ Hypothesis ]: \"\"\"Search new tokens for running hypotheses and encoded speech x. Args: running_hyps (List[Hypothesis]): Running hypotheses on beam x (torch.Tensor): Encoded speech feature (T, D) Returns: List[Hypotheses]: Best sorted hypotheses \"\"\" best_hyps = [] part_ids = torch . arange ( self . n_vocab , device = x . device ) # no pre-beam for hyp in running_hyps : # scoring scores , states = self . score_full ( hyp , x ) if self . do_pre_beam : part_ids = torch . topk ( scores [ self . pre_beam_score_key ], self . pre_beam_size )[ 1 ] part_scores , part_states = self . score_partial ( hyp , part_ids , x ) # weighted sum scores weighted_scores = torch . zeros ( self . n_vocab , dtype = x . dtype , device = x . device ) for k in self . full_scorers : weighted_scores += self . weights [ k ] * scores [ k ] for k in self . part_scorers : weighted_scores [ part_ids ] += self . weights [ k ] * part_scores [ k ] weighted_scores += hyp . score # update hyps for j , part_j in zip ( * self . beam ( weighted_scores , part_ids )): # will be (2 x beam at most) best_hyps . append ( Hypothesis ( score = weighted_scores [ j ], yseq = self . append_token ( hyp . yseq , j ), scores = self . merge_scores ( hyp . scores , scores , j , part_scores , part_j ), states = self . merge_states ( states , part_states , part_j ))) # sort and prune 2 x beam -> beam best_hyps = sorted ( best_hyps , key = lambda x : x . score , reverse = True )[: min ( len ( best_hyps ), self . beam_size )] return best_hyps","title":"search()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.Hypothesis","text":"Hypothesis data type.","title":"Hypothesis"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.Hypothesis.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 427 428 429 def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.Hypothesis.__new__","text":"Create new instance of Hypothesis(yseq, score, scores, states)","title":"__new__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.Hypothesis.__repr__","text":"Return a nicely formatted representation string Source code in adviser/tools/espnet_minimal/nets/beam_search.py 419 420 421 def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.Hypothesis.asdict","text":"Convert data to JSON-friendly dict. Source code in adviser/tools/espnet_minimal/nets/beam_search.py 26 27 28 29 30 31 32 def asdict ( self ) -> dict : \"\"\"Convert data to JSON-friendly dict.\"\"\" return self . _replace ( yseq = self . yseq . tolist (), score = float ( self . score ), scores = { k : float ( v ) for k , v in self . scores . items ()} ) . _asdict ()","title":"asdict()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.beam_search.beam_search","text":"Perform beam search with scorers. Parameters: Name Type Description Default x Tensor Encoded speech feature (T, D) required sos int Start of sequence id required eos int End of sequence id required beam_size int The number of hypotheses kept during search required vocab_size int The number of vocabulary required scorers Dict[str, tools.espnet_minimal.nets.scorer_interface.ScorerInterface] Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is None required weights Dict[str, float] Dict of weights for each scorers The scorer will be ignored if its weight is 0 required token_list List[str] List of tokens for debug log None maxlenratio float Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths 0.0 minlenratio float Input length ratio to obtain min output length. 0.0 pre_beam_score_key str key of scores to perform pre-beam search 'decoder' pre_beam_ratio float beam size in the pre-beam search will be int(pre_beam_ratio * beam_size) 1.5 Returns: Type Description list list: N-best decoding results Source code in adviser/tools/espnet_minimal/nets/beam_search.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def beam_search ( x : torch . Tensor , sos : int , eos : int , beam_size : int , vocab_size : int , scorers : Dict [ str , ScorerInterface ], weights : Dict [ str , float ], token_list : List [ str ] = None , maxlenratio : float = 0.0 , minlenratio : float = 0.0 , pre_beam_ratio : float = 1.5 , pre_beam_score_key : str = \"decoder\" ) -> list : \"\"\"Perform beam search with scorers. Args: x (torch.Tensor): Encoded speech feature (T, D) sos (int): Start of sequence id eos (int): End of sequence id beam_size (int): The number of hypotheses kept during search vocab_size (int): The number of vocabulary scorers (dict[str, ScorerInterface]): Dict of decoder modules e.g., Decoder, CTCPrefixScorer, LM The scorer will be ignored if it is `None` weights (dict[str, float]): Dict of weights for each scorers The scorer will be ignored if its weight is 0 token_list (list[str]): List of tokens for debug log maxlenratio (float): Input length ratio to obtain max output length. If maxlenratio=0.0 (default), it uses a end-detect function to automatically find maximum hypothesis lengths minlenratio (float): Input length ratio to obtain min output length. pre_beam_score_key (str): key of scores to perform pre-beam search pre_beam_ratio (float): beam size in the pre-beam search will be `int(pre_beam_ratio * beam_size)` Returns: list: N-best decoding results \"\"\" ret = BeamSearch ( scorers , weights , beam_size = beam_size , vocab_size = vocab_size , pre_beam_ratio = pre_beam_ratio , pre_beam_score_key = pre_beam_score_key , sos = sos , eos = eos , token_list = token_list , ) . forward ( x = x , maxlenratio = maxlenratio , minlenratio = minlenratio ) return [ h . asdict () for h in ret ]","title":"beam_search()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score","text":"","title":"ctc_prefix_score"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScore","text":"Compute CTC label sequence scores which is based on Algorithm 2 in WATANABE et al. \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\" but extended to efficiently compute the probablities of multiple labels simultaneously","title":"CTCPrefixScore"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScore.__call__","text":"Compute CTC prefix scores for next labels :param y : prefix label sequence :param cs : array of next labels :param r_prev: previous CTC state :return ctc_scores, ctc_states Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __call__ ( self , y , cs , r_prev ): \"\"\"Compute CTC prefix scores for next labels :param y : prefix label sequence :param cs : array of next labels :param r_prev: previous CTC state :return ctc_scores, ctc_states \"\"\" # initialize CTC states output_length = len ( y ) - 1 # ignore sos # new CTC states are prepared as a frame x (n or b) x n_labels tensor # that corresponds to r_t^n(h) and r_t^b(h). r = self . xp . ndarray (( self . input_length , 2 , len ( cs )), dtype = np . float32 ) xs = self . x [:, cs ] if output_length == 0 : r [ 0 , 0 ] = xs [ 0 ] r [ 0 , 1 ] = self . logzero else : r [ output_length - 1 ] = self . logzero # prepare forward probabilities for the last label r_sum = self . xp . logaddexp ( r_prev [:, 0 ], r_prev [:, 1 ]) # log(r_t^n(g) + r_t^b(g)) last = y [ - 1 ] if output_length > 0 and last in cs : log_phi = self . xp . ndarray (( self . input_length , len ( cs )), dtype = np . float32 ) for i in six . moves . range ( len ( cs )): log_phi [:, i ] = r_sum if cs [ i ] != last else r_prev [:, 1 ] else : log_phi = r_sum # compute forward probabilities log(r_t^n(h)), log(r_t^b(h)), # and log prefix probabilites log(psi) start = max ( output_length , 1 ) log_psi = r [ start - 1 , 0 ] for t in six . moves . range ( start , self . input_length ): r [ t , 0 ] = self . xp . logaddexp ( r [ t - 1 , 0 ], log_phi [ t - 1 ]) + xs [ t ] r [ t , 1 ] = self . xp . logaddexp ( r [ t - 1 , 0 ], r [ t - 1 , 1 ]) + self . x [ t , self . blank ] log_psi = self . xp . logaddexp ( log_psi , log_phi [ t - 1 ] + xs [ t ]) # get P(...eos|X) that ends with the prefix itself eos_pos = self . xp . where ( cs == self . eos )[ 0 ] if len ( eos_pos ) > 0 : log_psi [ eos_pos ] = r_sum [ - 1 ] # log(r_T^n(g) + r_T^b(g)) # return the log prefix probability and CTC states, where the label axis # of the CTC states is moved to the first axis to slice it easily return log_psi , self . xp . rollaxis ( r , 2 )","title":"__call__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScore.__init__","text":"Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 202 203 204 205 206 207 208 def __init__ ( self , x , blank , eos , xp ): self . xp = xp self . logzero = - 10000000000.0 self . blank = blank self . eos = eos self . input_length = len ( x ) self . x = x","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScore.initial_state","text":"Obtain an initial CTC state :return: CTC state Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 210 211 212 213 214 215 216 217 218 219 220 221 222 def initial_state ( self ): \"\"\"Obtain an initial CTC state :return: CTC state \"\"\" # initial CTC state is made of a frame x 2 tensor that corresponds to # r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent # superscripts n and b (non-blank and blank), respectively. r = self . xp . full (( self . input_length , 2 ), self . logzero , dtype = np . float32 ) r [ 0 , 1 ] = self . x [ 0 , self . blank ] for i in six . moves . range ( 1 , self . input_length ): r [ i , 1 ] = r [ i - 1 , 1 ] + self . x [ i , self . blank ] return r","title":"initial_state()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScoreTH","text":"Batch processing of CTCPrefixScore which is based on Algorithm 2 in WATANABE et al. \"HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,\" but extended to efficiently compute the probablities of multiple labels simultaneously","title":"CTCPrefixScoreTH"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScoreTH.__call__","text":"Compute CTC prefix scores for next labels :param list y: prefix label sequences :param tuple state: previous CTC state :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O) :param torch.Tensor att_w: attention weights to decide CTC window :return new_state, ctc_local_scores (BW, O) Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __call__ ( self , y , state , pre_scores = None , att_w = None ): \"\"\"Compute CTC prefix scores for next labels :param list y: prefix label sequences :param tuple state: previous CTC state :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O) :param torch.Tensor att_w: attention weights to decide CTC window :return new_state, ctc_local_scores (BW, O) \"\"\" output_length = len ( y [ 0 ]) - 1 # ignore sos last_ids = [ yi [ - 1 ] for yi in y ] # last output label ids # prepare state info if state is None : if self . scoring_num > 0 : r_prev = torch . full (( self . input_length , 2 , self . batch , self . beam ), self . logzero , dtype = torch . float32 , device = self . device ) r_prev [:, 1 ] = torch . cumsum ( self . x [ 0 , :, :, self . blank ], 0 ) . unsqueeze ( 2 ) r_prev = r_prev . view ( - 1 , 2 , self . n_bb ) else : r_prev = torch . full (( self . input_length , 2 , self . n_bb ), self . logzero , dtype = torch . float32 , device = self . device ) r_prev [:, 1 ] = torch . cumsum ( self . x [ 0 , :, :, self . blank ], 0 ) s_prev = 0.0 f_min_prev = 0 f_max_prev = 1 else : r_prev , s_prev , f_min_prev , f_max_prev = state # select input dimensions for scoring if self . scoring_num > 0 and pre_scores is not None : pre_scores [:, self . blank ] = self . logzero # ignore blank from pre-selection scoring_ids = torch . topk ( pre_scores , self . scoring_num , 1 )[ 1 ] scoring_idmap = torch . full (( self . n_bb , self . odim ), - 1 , dtype = torch . long , device = self . device ) snum = scoring_ids . size ( 1 ) scoring_idmap [ self . bb_idx , scoring_ids ] = torch . arange ( snum , device = self . device ) scoring_idx = ( scoring_ids + self . pad_o ) . view ( - 1 ) x_ = torch . index_select ( self . x . view ( 2 , - 1 , self . batch * self . odim ), 2 , scoring_idx ) . view ( 2 , - 1 , self . n_bb , snum ) else : scoring_ids = None scoring_idmap = None snum = self . odim x_ = self . x # new CTC forward probs are prepared as a (T x 2 x BW x S) tensor # that corresponds to r_t^n(h) and r_t^b(h) in a batch. r = torch . full (( self . input_length , 2 , self . n_bb , snum ), self . logzero , dtype = torch . float32 , device = self . device ) if output_length == 0 : r [ 0 , 0 ] = x_ [ 0 , 0 ] r_sum = torch . logsumexp ( r_prev , 1 ) log_phi = r_sum . unsqueeze ( 2 ) . repeat ( 1 , 1 , snum ) if scoring_ids is not None : for idx in range ( self . n_bb ): pos = scoring_idmap [ idx , last_ids [ idx ]] if pos >= 0 : log_phi [:, idx , pos ] = r_prev [:, 1 , idx ] else : for idx in range ( self . n_bb ): log_phi [:, idx , last_ids [ idx ]] = r_prev [:, 1 , idx ] # decide start and end frames based on attention weights if att_w is not None and self . margin > 0 : f_arg = torch . matmul ( att_w , self . frame_ids ) f_min = max ( int ( f_arg . min () . cpu ()), f_min_prev ) f_max = max ( int ( f_arg . max () . cpu ()), f_max_prev ) start = min ( f_max_prev , max ( f_min - self . margin , output_length , 1 )) end = min ( f_max + self . margin , self . input_length ) else : f_min = f_max = 0 start = max ( output_length , 1 ) end = self . input_length # compute forward probabilities log(r_t^n(h)) and log(r_t^b(h)) for t in range ( start , end ): rp = r [ t - 1 ] rr = torch . stack ([ rp [ 0 ], log_phi [ t - 1 ], rp [ 0 ], rp [ 1 ]]) . view ( 2 , 2 , self . n_bb , snum ) r [ t ] = torch . logsumexp ( rr , 1 ) + x_ [:, t ] # compute log prefix probabilites log(psi) log_phi_x = torch . cat (( log_phi [ 0 ] . unsqueeze ( 0 ), log_phi [: - 1 ]), dim = 0 ) + x_ [ 0 ] if scoring_ids is not None : log_psi = torch . full (( self . n_bb , self . odim ), self . logzero , device = self . device ) log_psi_ = torch . logsumexp ( torch . cat (( log_phi_x [ start : end ], r [ start - 1 , 0 ] . unsqueeze ( 0 )), dim = 0 ), dim = 0 ) for si in range ( self . n_bb ): log_psi [ si , scoring_ids [ si ]] = log_psi_ [ si ] else : log_psi = torch . logsumexp ( torch . cat (( log_phi_x [ start : end ], r [ start - 1 , 0 ] . unsqueeze ( 0 )), dim = 0 ), dim = 0 ) for si in range ( self . n_bb ): log_psi [ si , self . eos ] = r_sum [ self . end_frames [ si ], si ] return ( r , log_psi , f_min , f_max , scoring_idmap ), log_psi - s_prev","title":"__call__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScoreTH.__init__","text":"Construct CTC prefix scorer :param torch.Tensor x: input label posterior sequences (B, T, O) :param torch.Tensor xlens: input lengths (B,) :param int blank: blank label id :param int eos: end-of-sequence id :param int beam: beam size :param float scoring_ratio: ratio of #scored hypos to beam size :param int margin: margin parameter for windowing (0 means no windowing) Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , x , xlens , blank , eos , beam , scoring_ratio = 1.5 , margin = 0 ): \"\"\"Construct CTC prefix scorer :param torch.Tensor x: input label posterior sequences (B, T, O) :param torch.Tensor xlens: input lengths (B,) :param int blank: blank label id :param int eos: end-of-sequence id :param int beam: beam size :param float scoring_ratio: ratio of #scored hypos to beam size :param int margin: margin parameter for windowing (0 means no windowing) \"\"\" # In the comment lines, we assume T: input_length, B: batch size, W: beam width, O: output dim. self . logzero = - 10000000000.0 self . blank = blank self . eos = eos self . batch = x . size ( 0 ) self . input_length = x . size ( 1 ) self . odim = x . size ( 2 ) self . beam = beam self . n_bb = self . batch * beam self . device = torch . device ( 'cuda: %d ' % x . get_device ()) if x . is_cuda else torch . device ( 'cpu' ) # Pad the rest of posteriors in the batch # TODO(takaaki-hori): need a better way without for-loops for i , l in enumerate ( xlens ): if l < self . input_length : x [ i , l :, :] = self . logzero x [ i , l :, blank ] = 0 # Set the number of scoring hypotheses (scoring_num=0 means all) self . scoring_num = int ( beam * scoring_ratio ) if self . scoring_num >= self . odim : self . scoring_num = 0 # Expand input posteriors for fast computation if self . scoring_num == 0 : xn = x . transpose ( 0 , 1 ) . unsqueeze ( 2 ) . repeat ( 1 , 1 , beam , 1 ) . view ( - 1 , self . n_bb , self . odim ) else : xn = x . transpose ( 0 , 1 ) xb = xn [:, :, self . blank ] . unsqueeze ( 2 ) . expand ( - 1 , - 1 , self . odim ) self . x = torch . stack ([ xn , xb ]) # (2, T, B, O) or (2, T, BW, O) # Setup CTC windowing self . margin = margin if margin > 0 : self . frame_ids = torch . arange ( self . input_length , dtype = torch . float32 , device = self . device ) # Precompute end frames (BW,) self . end_frames = ( torch . as_tensor ( xlens ) - 1 ) . view ( self . batch , 1 ) . repeat ( 1 , beam ) . view ( - 1 ) # Precompute base indices to convert label ids to corresponding element indices self . pad_b = ( torch . arange ( self . batch , device = self . device ) * beam ) . view ( - 1 , 1 ) self . pad_bo = ( torch . arange ( self . batch , device = self . device ) * ( beam * self . odim )) . view ( - 1 , 1 ) self . pad_o = ( torch . arange ( self . batch , device = self . device ) * self . odim ) . unsqueeze ( 1 ) . repeat ( 1 , beam ) . view ( - 1 , 1 ) self . bb_idx = torch . arange ( self . n_bb , device = self . device ) . view ( - 1 , 1 )","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.ctc_prefix_score.CTCPrefixScoreTH.index_select_state","text":"Select CTC states according to best ids :param state : CTC state :param best_ids : index numbers selected by beam pruning (B, W) :return selected_state Source code in adviser/tools/espnet_minimal/nets/ctc_prefix_score.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def index_select_state ( self , state , best_ids ): \"\"\"Select CTC states according to best ids :param state : CTC state :param best_ids : index numbers selected by beam pruning (B, W) :return selected_state \"\"\" r , s , f_min , f_max , scoring_idmap = state # convert ids to BWO space vidx = ( best_ids + self . pad_bo ) . view ( - 1 ) # select hypothesis scores s_new = torch . index_select ( s . view ( - 1 ), 0 , vidx ) s_new = s_new . view ( - 1 , 1 ) . repeat ( 1 , self . odim ) . view ( self . n_bb , self . odim ) # convert ids to BWS space (S: scoring_num) if scoring_idmap is not None : snum = self . scoring_num beam_idx = ( torch . div ( best_ids , self . odim ) + self . pad_b ) . view ( - 1 ) label_ids = torch . fmod ( best_ids , self . odim ) . view ( - 1 ) score_idx = scoring_idmap [ beam_idx , label_ids ] score_idx [ score_idx == - 1 ] = 0 vidx = score_idx + beam_idx * snum else : snum = self . odim # select forward probabilities r_new = torch . index_select ( r . view ( - 1 , 2 , self . n_bb * snum ), 2 , vidx ) . view ( - 1 , 2 , self . n_bb ) return r_new , s_new , f_min , f_max","title":"index_select_state()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common","text":"","title":"e2e_asr_common"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.ErrorCalculator","text":"Calculate CER and WER for E2E_ASR and CTC models during training :param y_hats: numpy array with predicted text :param y_pads: numpy array with true (target) text :param char_list: :param sym_space: :param sym_blank: :return:","title":"ErrorCalculator"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.ErrorCalculator.__call__","text":"Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __call__ ( self , ys_hat , ys_pad , is_ctc = False ): cer , wer = None , None if is_ctc : return self . calculate_cer_ctc ( ys_hat , ys_pad ) elif not self . report_cer and not self . report_wer : return cer , wer seqs_hat , seqs_true = self . convert_to_char ( ys_hat , ys_pad ) if self . report_cer : cer = self . calculate_cer ( seqs_hat , seqs_true ) if self . report_wer : wer = self . calculate_wer ( seqs_hat , seqs_true ) return cer , wer","title":"__call__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.ErrorCalculator.__init__","text":"Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , char_list , sym_space , sym_blank , report_cer = False , report_wer = False ): super ( ErrorCalculator , self ) . __init__ () self . char_list = char_list self . space = sym_space self . blank = sym_blank self . report_cer = report_cer self . report_wer = report_wer self . idx_blank = self . char_list . index ( self . blank ) if self . space in self . char_list : self . idx_space = self . char_list . index ( self . space ) else : self . idx_space = None","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.ErrorCalculator.calculate_cer","text":"Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 167 168 169 170 171 172 173 174 def calculate_cer ( self , seqs_hat , seqs_true ): char_eds , char_ref_lens = [], [] for i , seq_hat_text in enumerate ( seqs_hat ): seq_true_text = seqs_true [ i ] hyp_chars = seq_hat_text . replace ( ' ' , '' ) ref_chars = seq_true_text . replace ( ' ' , '' ) char_ref_lens . append ( len ( ref_chars )) return float ( sum ( char_eds )) / sum ( char_ref_lens )","title":"calculate_cer()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.ErrorCalculator.calculate_cer_ctc","text":"Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def calculate_cer_ctc ( self , ys_hat , ys_pad ): cers , char_ref_lens = [], [] for i , y in enumerate ( ys_hat ): y_hat = [ x [ 0 ] for x in groupby ( y )] y_true = ys_pad [ i ] seq_hat , seq_true = [], [] for idx in y_hat : idx = int ( idx ) if idx != - 1 and idx != self . idx_blank and idx != self . idx_space : seq_hat . append ( self . char_list [ int ( idx )]) for idx in y_true : idx = int ( idx ) if idx != - 1 and idx != self . idx_blank and idx != self . idx_space : seq_true . append ( self . char_list [ int ( idx )]) hyp_chars = \"\" . join ( seq_hat ) ref_chars = \"\" . join ( seq_true ) cer_ctc = float ( sum ( cers )) / sum ( char_ref_lens ) if cers else None return cer_ctc","title":"calculate_cer_ctc()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.ErrorCalculator.calculate_wer","text":"Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 176 177 178 179 180 181 182 183 def calculate_wer ( self , seqs_hat , seqs_true ): word_eds , word_ref_lens = [], [] for i , seq_hat_text in enumerate ( seqs_hat ): seq_true_text = seqs_true [ i ] hyp_words = seq_hat_text . split () ref_words = seq_true_text . split () word_ref_lens . append ( len ( ref_words )) return float ( sum ( word_eds )) / sum ( word_ref_lens )","title":"calculate_wer()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.ErrorCalculator.convert_to_char","text":"Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def convert_to_char ( self , ys_hat , ys_pad ): seqs_hat , seqs_true = [], [] for i , y_hat in enumerate ( ys_hat ): y_true = ys_pad [ i ] eos_true = np . where ( y_true == - 1 )[ 0 ] eos_true = eos_true [ 0 ] if len ( eos_true ) > 0 else len ( y_true ) # To avoid wrong higger WER than the one obtained from the decoding # eos from y_true is used to mark the eos in y_hat # because of that y_hats has not padded outs with -1. seq_hat = [ self . char_list [ int ( idx )] for idx in y_hat [: eos_true ]] seq_true = [ self . char_list [ int ( idx )] for idx in y_true if int ( idx ) != - 1 ] seq_hat_text = \"\" . join ( seq_hat ) . replace ( self . space , ' ' ) seq_hat_text = seq_hat_text . replace ( self . blank , '' ) seq_true_text = \"\" . join ( seq_true ) . replace ( self . space , ' ' ) seqs_hat . append ( seq_hat_text ) seqs_true . append ( seq_true_text ) return seqs_hat , seqs_true","title":"convert_to_char()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.end_detect","text":"End detection desribed in Eq. (50) of S. Watanabe et al \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\" :param ended_hyps: :param i: :param M: :param D_end: :return: Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def end_detect ( ended_hyps , i , M = 3 , D_end = np . log ( 1 * np . exp ( - 10 ))): \"\"\"End detection desribed in Eq. (50) of S. Watanabe et al \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\" :param ended_hyps: :param i: :param M: :param D_end: :return: \"\"\" if len ( ended_hyps ) == 0 : return False count = 0 best_hyp = sorted ( ended_hyps , key = lambda x : x [ 'score' ], reverse = True )[ 0 ] for m in six . moves . range ( M ): # get ended_hyps with their length is i - m hyp_length = i - m hyps_same_length = [ x for x in ended_hyps if len ( x [ 'yseq' ]) == hyp_length ] if len ( hyps_same_length ) > 0 : best_hyp_same_length = sorted ( hyps_same_length , key = lambda x : x [ 'score' ], reverse = True )[ 0 ] if best_hyp_same_length [ 'score' ] - best_hyp [ 'score' ] < D_end : count += 1 if count == M : return True else : return False","title":"end_detect()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.get_vgg2l_odim","text":"Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 80 81 82 83 84 def get_vgg2l_odim ( idim , in_channel = 3 , out_channel = 128 ): idim = idim / in_channel idim = np . ceil ( np . array ( idim , dtype = np . float32 ) / 2 ) # 1st max pooling idim = np . ceil ( np . array ( idim , dtype = np . float32 ) / 2 ) # 2nd max pooling return int ( idim ) * out_channel # numer of channels","title":"get_vgg2l_odim()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.e2e_asr_common.label_smoothing_dist","text":"Obtain label distribution for loss smoothing :param odim: :param lsm_type: :param blank: :param transcript: :return: Source code in adviser/tools/espnet_minimal/nets/e2e_asr_common.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def label_smoothing_dist ( odim , lsm_type , transcript = None , blank = 0 ): \"\"\"Obtain label distribution for loss smoothing :param odim: :param lsm_type: :param blank: :param transcript: :return: \"\"\" if transcript is not None : with open ( transcript , 'rb' ) as f : trans_json = json . load ( f )[ 'utts' ] if lsm_type == 'unigram' : assert transcript is not None , 'transcript is required for %s label smoothing' % lsm_type labelcount = np . zeros ( odim ) for k , v in trans_json . items (): ids = np . array ([ int ( n ) for n in v [ 'output' ][ 0 ][ 'tokenid' ] . split ()]) # to avoid an error when there is no text in an uttrance if len ( ids ) > 0 : labelcount [ ids ] += 1 labelcount [ odim - 1 ] = len ( transcript ) # count <eos> labelcount [ labelcount == 0 ] = 1 # flooring labelcount [ blank ] = 0 # remove counts for blank labeldist = labelcount . astype ( np . float32 ) / np . sum ( labelcount ) else : logging . error ( \"Error: unexpected label smoothing type: %s \" % lsm_type ) sys . exit () return labeldist","title":"label_smoothing_dist()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface","text":"MT Interface module.","title":"mt_interface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface","text":"MT Interface for ESPnet model implementation.","title":"MTInterface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface.attention_plot_class","text":"Get attention plot class.","title":"attention_plot_class"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface.add_arguments","text":"Add arguments to parser. Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 11 12 13 14 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments to parser.\"\"\" return parser","title":"add_arguments()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface.build","text":"Initialize this class with python-level args. Parameters: Name Type Description Default idim int The number of an input feature dim. required odim int The number of output vocab. required Returns: Type Description ASRinterface A new instance of ASRInterface. Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @classmethod def build ( cls , idim : int , odim : int , ** kwargs ): \"\"\"Initialize this class with python-level args. Args: idim (int): The number of an input feature dim. odim (int): The number of output vocab. Returns: ASRinterface: A new instance of ASRInterface. \"\"\" def wrap ( parser ): return get_parser ( parser , required = False ) args = argparse . Namespace ( ** kwargs ) args = fill_missing_args ( args , wrap ) args = fill_missing_args ( args , cls . add_arguments ) return cls ( idim , odim , args )","title":"build()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface.calculate_all_attentions","text":"Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 78 79 80 81 82 83 84 85 86 87 def calculate_all_attentions ( self , xs , ilens , ys ): \"\"\"Caluculate attention. :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...] :param ndarray ilens: batch of lengths of input sequences (B) :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...] :return: attention weights (B, Lmax, Tmax) :rtype: float ndarray \"\"\" raise NotImplementedError ( \"calculate_all_attentions method is not implemented\" )","title":"calculate_all_attentions()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface.forward","text":"Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def forward ( self , xs , ilens , ys ): \"\"\"Compute loss for training. :param xs: For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim) For chainer, list of source sequences chainer.Variable :param ilens: batch of lengths of source sequences (B) For pytorch, torch.Tensor For chainer, list of int :param ys: For pytorch, batch of padded source sequences torch.Tensor (B, Lmax) For chainer, list of source sequences chainer.Variable :return: loss value :rtype: torch.Tensor for pytorch, chainer.Variable for chainer \"\"\" raise NotImplementedError ( \"forward method is not implemented\" )","title":"forward()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface.translate","text":"Translate x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace trans_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 54 55 56 57 58 59 60 61 62 63 64 def translate ( self , x , trans_args , char_list = None , rnnlm = None ): \"\"\"Translate x for evaluation. :param ndarray x: input acouctic feature (B, T, D) or (T, D) :param namespace trans_args: argment namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"translate method is not implemented\" )","title":"translate()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.mt_interface.MTInterface.translate_batch","text":"Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace trans_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/mt_interface.py 66 67 68 69 70 71 72 73 74 75 76 def translate_batch ( self , x , trans_args , char_list = None , rnnlm = None ): \"\"\"Beam search implementation for batch. :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc) :param namespace trans_args: argument namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" raise NotImplementedError ( \"Batch decoding is not supported yet.\" )","title":"translate_batch()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend","text":"","title":"pytorch_backend"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.ctc","text":"","title":"ctc"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.ctc.CTC","text":"CTC module :param int odim: dimension of outputs :param int eprojs: number of encoder projection units :param float dropout_rate: dropout rate (0.0 ~ 1.0) :param str ctc_type: builtin or warpctc :param bool reduce: reduce the CTC loss into a scalar __init__ ( self , odim , eprojs , dropout_rate , ctc_type = 'warpctc' , reduce = True ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , odim , eprojs , dropout_rate , ctc_type = 'warpctc' , reduce = True ): super () . __init__ () self . dropout_rate = dropout_rate self . loss = None self . ctc_lo = torch . nn . Linear ( eprojs , odim ) self . ctc_type = ctc_type if self . ctc_type == 'builtin' : reduction_type = 'sum' if reduce else 'none' self . ctc_loss = torch . nn . CTCLoss ( reduction = reduction_type ) elif self . ctc_type == 'warpctc' : import warpctc_pytorch as warp_ctc self . ctc_loss = warp_ctc . CTCLoss ( size_average = True , reduce = reduce ) else : raise ValueError ( 'ctc_type must be \"builtin\" or \"warpctc\": {} ' . format ( self . ctc_type )) self . ignore_id = - 1 self . reduce = reduce argmax ( self , hs_pad ) argmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: argmax applied 2d tensor (B, Tmax) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 110 111 112 113 114 115 116 117 def argmax ( self , hs_pad ): \"\"\"argmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: argmax applied 2d tensor (B, Tmax) :rtype: torch.Tensor \"\"\" return torch . argmax ( self . ctc_lo ( hs_pad ), dim = 2 ) forward ( self , hs_pad , hlens , ys_pad ) CTC forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :return: ctc loss value :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , hs_pad , hlens , ys_pad ): \"\"\"CTC forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :return: ctc loss value :rtype: torch.Tensor \"\"\" # TODO(kan-bayashi): need to make more smart way ys = [ y [ y != self . ignore_id ] for y in ys_pad ] # parse padded ys self . loss = None hlens = torch . from_numpy ( np . fromiter ( hlens , dtype = np . int32 )) olens = torch . from_numpy ( np . fromiter ( ( x . size ( 0 ) for x in ys ), dtype = np . int32 )) # zero padding for hs ys_hat = self . ctc_lo ( F . dropout ( hs_pad , p = self . dropout_rate )) # zero padding for ys ys_true = torch . cat ( ys ) . cpu () . int () # batch x olen # get length info logging . info ( self . __class__ . __name__ + ' input lengths: ' + '' . join ( str ( hlens ) . split ( ' \\n ' ))) logging . info ( self . __class__ . __name__ + ' output lengths: ' + '' . join ( str ( olens ) . split ( ' \\n ' ))) # get ctc loss # expected shape of seqLength x batchSize x alphabet_size dtype = ys_hat . dtype ys_hat = ys_hat . transpose ( 0 , 1 ) if self . ctc_type == \"warpctc\" : # warpctc only supports float32 ys_hat = ys_hat . to ( dtype = torch . float32 ) else : # use GPU when using the cuDNN implementation ys_true = to_device ( self , ys_true ) self . loss = to_device ( self , self . loss_fn ( ys_hat , ys_true , hlens , olens )) . to ( dtype = dtype ) if self . reduce : # NOTE: sum() is needed to keep consistency since warpctc return as tensor w/ shape (1,) # but builtin return as tensor w/o shape (scalar). self . loss = self . loss . sum () logging . info ( 'ctc loss:' + str ( float ( self . loss ))) return self . loss log_softmax ( self , hs_pad ) log_softmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: log softmax applied 3d tensor (B, Tmax, odim) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 101 102 103 104 105 106 107 108 def log_softmax ( self , hs_pad ): \"\"\"log_softmax of frame activations :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs) :return: log softmax applied 3d tensor (B, Tmax, odim) :rtype: torch.Tensor \"\"\" return F . log_softmax ( self . ctc_lo ( hs_pad ), dim = 2 ) loss_fn ( self , th_pred , th_target , th_ilen , th_olen ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def loss_fn ( self , th_pred , th_target , th_ilen , th_olen ): if self . ctc_type == 'builtin' : th_pred = th_pred . log_softmax ( 2 ) # Use the deterministic CuDNN implementation of CTC loss to avoid # [issue#17798](https://github.com/pytorch/pytorch/issues/17798) with torch . backends . cudnn . flags ( deterministic = True ): loss = self . ctc_loss ( th_pred , th_target , th_ilen , th_olen ) # Batch-size average loss = loss / th_pred . size ( 1 ) return loss elif self . ctc_type == 'warpctc' : return self . ctc_loss ( th_pred , th_target , th_ilen , th_olen ) else : raise NotImplementedError","title":"CTC"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.ctc.ctc_for","text":"Returns the CTC module for the given args and output dimension :param Namespace args: the program args :param int odim : The output dimension :param bool reduce : return the CTC loss in a scalar :return: the corresponding CTC module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/ctc.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def ctc_for ( args , odim , reduce = True ): \"\"\"Returns the CTC module for the given args and output dimension :param Namespace args: the program args :param int odim : The output dimension :param bool reduce : return the CTC loss in a scalar :return: the corresponding CTC module \"\"\" num_encs = getattr ( args , \"num_encs\" , 1 ) # use getattr to keep compatibility if num_encs == 1 : # compatible with single encoder asr mode return CTC ( odim , args . eprojs , args . dropout_rate , ctc_type = 'builtin' , reduce = reduce ) # changed this to use builtin ctc rather # than warpctc, so we have nothing to # install and it's just about the loss anyways. elif num_encs >= 1 : ctcs_list = torch . nn . ModuleList () if args . share_ctc : # use dropout_rate of the first encoder ctc = CTC ( odim , args . eprojs , args . dropout_rate [ 0 ], ctc_type = args . ctc_type , reduce = reduce ) ctcs_list . append ( ctc ) else : for idx in range ( num_encs ): ctc = CTC ( odim , args . eprojs , args . dropout_rate [ idx ], ctc_type = args . ctc_type , reduce = reduce ) ctcs_list . append ( ctc ) return ctcs_list else : raise ValueError ( \"Number of encoders needs to be more than one. {} \" . format ( num_encs ))","title":"ctc_for()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_asr","text":"RNN sequence-to-sequence speech recognition model (pytorch).","title":"e2e_asr"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_asr.E2E","text":"E2E module. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options __init__ ( self , idim , odim , args ) special Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def __init__ ( self , idim , odim , args ): \"\"\"Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options \"\"\" super ( E2E , self ) . __init__ () # This loads default arguments, # but not calling this yields the same error, so it's not why things break. torch . nn . Module . __init__ ( self ) self . mtlalpha = args . mtlalpha assert 0.0 <= self . mtlalpha <= 1.0 , \"mtlalpha should be [0.0, 1.0]\" self . etype = args . etype self . verbose = args . verbose # NOTE: for self.build method args . char_list = getattr ( args , \"char_list\" , None ) self . char_list = args . char_list self . outdir = args . outdir self . space = args . sym_space self . blank = args . sym_blank # below means the last number becomes eos/sos ID # note that sos/eos IDs are identical self . sos = odim - 1 self . eos = odim - 1 # subsample info # +1 means input (+1) and layers outputs (args.elayer) subsample = np . ones ( args . elayers + 1 , dtype = np . int ) if args . etype . endswith ( \"p\" ) and not args . etype . startswith ( \"vgg\" ): ss = args . subsample . split ( \"_\" ) for j in range ( min ( args . elayers + 1 , len ( ss ))): subsample [ j ] = int ( ss [ j ]) else : logging . warning ( 'Subsampling is not performed for vgg*. It is performed in max pooling layers at CNN.' ) logging . info ( 'subsample: ' + ' ' . join ([ str ( x ) for x in subsample ])) self . subsample = subsample # label smoothing info if args . lsm_type and os . path . isfile ( args . train_json ): logging . info ( \"Use label smoothing with \" + args . lsm_type ) labeldist = label_smoothing_dist ( odim , args . lsm_type , transcript = args . train_json ) else : labeldist = None if getattr ( args , \"use_frontend\" , False ): # use getattr to keep compatibility # Relative importing because of using python3 syntax from tools.espnet_minimal.nets.pytorch_backend.frontends.feature_transform \\ import feature_transform_for from tools.espnet_minimal.nets.pytorch_backend.frontends.frontend \\ import frontend_for self . frontend = frontend_for ( args , idim ) self . feature_transform = feature_transform_for ( args , ( idim - 1 ) * 2 ) idim = args . n_mels else : self . frontend = None # encoder self . enc = encoder_for ( args , idim , self . subsample ) # ctc # self.ctc = ctc_for(args, odim) <-- if this is executed, the shapes don't match. # The missing/unexpected arguments are not fixed by removing this however. # attention self . att = att_for ( args ) # decoder self . dec = decoder_for ( args , odim , self . sos , self . eos , self . att , labeldist ) # weight initialization self . init_like_chainer () self . report_cer = False self . report_wer = False self . rnnlm = None self . logzero = - 10000000000.0 self . loss = None self . acc = None add_arguments ( parser ) staticmethod Add arguments. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 44 45 46 47 48 49 50 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments.\"\"\" E2E . encoder_add_arguments ( parser ) E2E . attention_add_arguments ( parser ) E2E . decoder_add_arguments ( parser ) return parser attention_add_arguments ( parser ) staticmethod Add arguments for the attention. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 @staticmethod def attention_add_arguments ( parser ): \"\"\"Add arguments for the attention.\"\"\" group = parser . add_argument_group ( \"E2E attention setting\" ) # attention group . add_argument ( '--atype' , default = 'dot' , type = str , choices = [ 'noatt' , 'dot' , 'add' , 'location' , 'coverage' , 'coverage_location' , 'location2d' , 'location_recurrent' , 'multi_head_dot' , 'multi_head_add' , 'multi_head_loc' , 'multi_head_multi_res_loc' ], help = 'Type of attention architecture' ) group . add_argument ( '--adim' , default = 320 , type = int , help = 'Number of attention transformation dimensions' ) group . add_argument ( '--awin' , default = 5 , type = int , help = 'Window size for location2d attention' ) group . add_argument ( '--aheads' , default = 4 , type = int , help = 'Number of heads for multi head attention' ) group . add_argument ( '--aconv-chans' , default =- 1 , type = int , help = 'Number of attention convolution channels \\ (negative value indicates no location-aware attention)' ) group . add_argument ( '--aconv-filts' , default = 100 , type = int , help = 'Number of attention convolution filters \\ (negative value indicates no location-aware attention)' ) group . add_argument ( '--dropout-rate' , default = 0.0 , type = float , help = 'Dropout rate for the encoder' ) return parser calculate_all_attentions ( self , xs_pad , ilens , ys_pad ) E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 def calculate_all_attentions ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray \"\"\" with torch . no_grad (): # 0. Frontend if self . frontend is not None : hs_pad , hlens , mask = self . frontend ( to_torch_tensor ( xs_pad ), ilens ) hs_pad , hlens = self . feature_transform ( hs_pad , hlens ) else : hs_pad , hlens = xs_pad , ilens # 1. Encoder hpad , hlens , _ = self . enc ( hs_pad , hlens ) # 2. Decoder att_ws = self . dec . calculate_all_attentions ( hpad , hlens , ys_pad ) return att_ws decoder_add_arguments ( parser ) staticmethod Add arguments for the decoder. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @staticmethod def decoder_add_arguments ( parser ): \"\"\"Add arguments for the decoder.\"\"\" group = parser . add_argument_group ( \"E2E encoder setting\" ) group . add_argument ( '--dtype' , default = 'lstm' , type = str , choices = [ 'lstm' , 'gru' ], help = 'Type of decoder network architecture' ) group . add_argument ( '--dlayers' , default = 1 , type = int , help = 'Number of decoder layers' ) group . add_argument ( '--dunits' , default = 320 , type = int , help = 'Number of decoder hidden units' ) group . add_argument ( '--dropout-rate-decoder' , default = 0.0 , type = float , help = 'Dropout rate for the decoder' ) group . add_argument ( '--sampling-probability' , default = 0.0 , type = float , help = 'Ratio of predicted labels fed back to decoder' ) group . add_argument ( '--lsm-type' , const = '' , default = '' , type = str , nargs = '?' , choices = [ '' , 'unigram' ], help = 'Apply label smoothing with a specified distribution type' ) return parser encode ( self , x ) Encode acoustic features. :param ndarray x: input acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def encode ( self , x ): \"\"\"Encode acoustic features. :param ndarray x: input acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor \"\"\" self . eval () ilens = [ x . shape [ 0 ]] # subsample frame x = x [:: self . subsample [ 0 ], :] p = next ( self . parameters ()) h = torch . as_tensor ( x , device = p . device , dtype = p . dtype ) # make a utt list (1) to use the same interface for encoder hs = h . contiguous () . unsqueeze ( 0 ) # 0. Frontend if self . frontend is not None : enhanced , hlens , mask = self . frontend ( hs , ilens ) hs , hlens = self . feature_transform ( enhanced , hlens ) else : hs , hlens = hs , ilens # 1. encoder hs , _ , _ = self . enc ( hs , hlens ) return hs . squeeze ( 0 ) encoder_add_arguments ( parser ) staticmethod Add arguments for the encoder. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @staticmethod def encoder_add_arguments ( parser ): \"\"\"Add arguments for the encoder.\"\"\" group = parser . add_argument_group ( \"E2E encoder setting\" ) # encoder group . add_argument ( '--etype' , default = 'blstmp' , type = str , choices = [ 'lstm' , 'blstm' , 'lstmp' , 'blstmp' , 'vgglstmp' , 'vggblstmp' , 'vgglstm' , 'vggblstm' , 'gru' , 'bgru' , 'grup' , 'bgrup' , 'vgggrup' , 'vggbgrup' , 'vgggru' , 'vggbgru' ], help = 'Type of encoder network architecture' ) group . add_argument ( '--elayers' , default = 4 , type = int , help = 'Number of encoder layers (for shared recognition part in multi-speaker asr mode)' ) group . add_argument ( '--eunits' , '-u' , default = 300 , type = int , help = 'Number of encoder hidden units' ) group . add_argument ( '--eprojs' , default = 320 , type = int , help = 'Number of encoder projection units' ) group . add_argument ( '--subsample' , default = \"1\" , type = str , help = 'Subsample input frames x_y_z means subsample every x frame at 1st layer, ' 'every y frame at 2nd layer etc.' ) return parser enhance ( self , xs ) Forward only in the frontend stage. :param ndarray xs: input acoustic feature (T, C, F) :return: enhaned feature :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def enhance ( self , xs ): \"\"\"Forward only in the frontend stage. :param ndarray xs: input acoustic feature (T, C, F) :return: enhaned feature :rtype: torch.Tensor \"\"\" if self . frontend is None : raise RuntimeError ( 'Frontend does \\' t exist' ) prev = self . training self . eval () ilens = np . fromiter (( xx . shape [ 0 ] for xx in xs ), dtype = np . int64 ) # subsample frame xs = [ xx [:: self . subsample [ 0 ], :] for xx in xs ] xs = [ to_device ( self , to_torch_tensor ( xx ) . float ()) for xx in xs ] xs_pad = pad_list ( xs , 0.0 ) enhanced , hlensm , mask = self . frontend ( xs_pad , ilens ) if prev : self . train () return enhanced . cpu () . numpy (), mask . cpu () . numpy (), ilens forward ( self , xs_pad , ilens , ys_pad ) E2E forward. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: loss value :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def forward ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E forward. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: loss value :rtype: torch.Tensor \"\"\" # 0. Frontend if self . frontend is not None : hs_pad , hlens , mask = self . frontend ( to_torch_tensor ( xs_pad ), ilens ) hs_pad , hlens = self . feature_transform ( hs_pad , hlens ) else : hs_pad , hlens = xs_pad , ilens # 1. Encoder hs_pad , hlens , _ = self . enc ( hs_pad , hlens ) # 2. CTC loss if self . mtlalpha == 0 : self . loss_ctc = None else : self . loss_ctc = self . ctc ( hs_pad , hlens , ys_pad ) # 3. attention loss if self . mtlalpha == 1 : self . loss_att , acc = None , None else : self . loss_att , acc , _ = self . dec ( hs_pad , hlens , ys_pad ) self . acc = acc # 4. compute cer without beam search if self . mtlalpha == 0 or self . char_list is None : cer_ctc = None else : cers = [] y_hats = self . ctc . argmax ( hs_pad ) . data for i , y in enumerate ( y_hats ): y_hat = [ x [ 0 ] for x in groupby ( y )] y_true = ys_pad [ i ] seq_hat = [ self . char_list [ int ( idx )] for idx in y_hat if int ( idx ) != - 1 ] seq_true = [ self . char_list [ int ( idx )] for idx in y_true if int ( idx ) != - 1 ] seq_hat_text = \"\" . join ( seq_hat ) . replace ( self . space , ' ' ) seq_hat_text = seq_hat_text . replace ( self . blank , '' ) seq_true_text = \"\" . join ( seq_true ) . replace ( self . space , ' ' ) hyp_chars = seq_hat_text . replace ( ' ' , '' ) ref_chars = seq_true_text . replace ( ' ' , '' ) cer_ctc = sum ( cers ) / len ( cers ) if cers else None # 5. compute cer/wer if self . training or not ( self . report_cer or self . report_wer ): cer , wer = 0.0 , 0.0 # oracle_cer, oracle_wer = 0.0, 0.0 else : if self . recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( hs_pad ) . data else : lpz = None word_eds , word_ref_lens , char_eds , char_ref_lens = [], [], [], [] nbest_hyps = self . dec . recognize_beam_batch ( hs_pad , torch . tensor ( hlens ), lpz , self . recog_args , self . char_list , self . rnnlm ) # remove <sos> and <eos> y_hats = [ nbest_hyp [ 0 ][ 'yseq' ][ 1 : - 1 ] for nbest_hyp in nbest_hyps ] for i , y_hat in enumerate ( y_hats ): y_true = ys_pad [ i ] seq_hat = [ self . char_list [ int ( idx )] for idx in y_hat if int ( idx ) != - 1 ] seq_true = [ self . char_list [ int ( idx )] for idx in y_true if int ( idx ) != - 1 ] seq_hat_text = \"\" . join ( seq_hat ) . replace ( self . recog_args . space , ' ' ) seq_hat_text = seq_hat_text . replace ( self . recog_args . blank , '' ) seq_true_text = \"\" . join ( seq_true ) . replace ( self . recog_args . space , ' ' ) hyp_words = seq_hat_text . split () ref_words = seq_true_text . split () word_ref_lens . append ( len ( ref_words )) hyp_chars = seq_hat_text . replace ( ' ' , '' ) ref_chars = seq_true_text . replace ( ' ' , '' ) char_ref_lens . append ( len ( ref_chars )) wer = 0.0 if not self . report_wer else float ( sum ( word_eds )) / sum ( word_ref_lens ) cer = 0.0 if not self . report_cer else float ( sum ( char_eds )) / sum ( char_ref_lens ) alpha = self . mtlalpha if alpha == 0 : self . loss = self . loss_att loss_att_data = float ( self . loss_att ) loss_ctc_data = None elif alpha == 1 : self . loss = self . loss_ctc loss_att_data = None loss_ctc_data = float ( self . loss_ctc ) else : self . loss = alpha * self . loss_ctc + ( 1 - alpha ) * self . loss_att loss_att_data = float ( self . loss_att ) loss_ctc_data = float ( self . loss_ctc ) loss_data = float ( self . loss ) if loss_data < CTC_LOSS_THRESHOLD and not math . isnan ( loss_data ): self . reporter . report ( loss_ctc_data , loss_att_data , acc , cer_ctc , cer , wer , loss_data ) else : logging . warning ( 'loss (= %f ) is not correct' , loss_data ) return self . loss init_like_chainer ( self ) Initialize weight like chainer. chainer basically uses LeCun way: W ~ Normal(0, fan_in -0.5), b = 0 pytorch basically uses W, b ~ Uniform(-fan_in -0.5, fan_in**-0.5) however, there are two exceptions as far as I know. - EmbedID.W ~ Normal(0, 1) - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def init_like_chainer ( self ): \"\"\"Initialize weight like chainer. chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0 pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5) however, there are two exceptions as far as I know. - EmbedID.W ~ Normal(0, 1) - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM) \"\"\" lecun_normal_init_parameters ( self ) # exceptions # embed weight ~ Normal(0, 1) self . dec . embed . weight . data . normal_ ( 0 , 1 ) # forget-bias = 1.0 # https://discuss.pytorch.org/t/set-forget-gate-bias-of-lstm/1745 for l in six . moves . range ( len ( self . dec . decoder )): set_forget_bias_to_one ( self . dec . decoder [ l ] . bias_ih ) recognize ( self , x , recog_args , char_list , rnnlm = None ) E2E beam search. :param ndarray x: input acoustic feature (T, D) :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def recognize ( self , x , recog_args , char_list , rnnlm = None ): \"\"\"E2E beam search. :param ndarray x: input acoustic feature (T, D) :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" hs = self . encode ( x ) . unsqueeze ( 0 ) # calculate log P(z_t|X) for CTC scores if recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( hs )[ 0 ] else : lpz = None # 2. Decoder # decode the first utterance y = self . dec . recognize_beam ( hs [ 0 ], lpz , recog_args , char_list , rnnlm ) return y recognize_batch ( self , xs , recog_args , char_list , rnnlm = None ) E2E beam search. :param list xs: list of input acoustic feature arrays [(T_1, D), (T_2, D), ...] :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def recognize_batch ( self , xs , recog_args , char_list , rnnlm = None ): \"\"\"E2E beam search. :param list xs: list of input acoustic feature arrays [(T_1, D), (T_2, D), ...] :param Namespace recog_args: argument Namespace containing options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" prev = self . training self . eval () ilens = np . fromiter (( xx . shape [ 0 ] for xx in xs ), dtype = np . int64 ) # subsample frame xs = [ xx [:: self . subsample [ 0 ], :] for xx in xs ] xs = [ to_device ( self , to_torch_tensor ( xx ) . float ()) for xx in xs ] xs_pad = pad_list ( xs , 0.0 ) # 0. Frontend if self . frontend is not None : enhanced , hlens , mask = self . frontend ( xs_pad , ilens ) hs_pad , hlens = self . feature_transform ( enhanced , hlens ) else : hs_pad , hlens = xs_pad , ilens # 1. Encoder hs_pad , hlens , _ = self . enc ( hs_pad , hlens ) # calculate log P(z_t|X) for CTC scores if recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( hs_pad ) normalize_score = False else : lpz = None normalize_score = True # 2. Decoder hlens = torch . tensor ( list ( map ( int , hlens ))) # make sure hlens is tensor y = self . dec . recognize_beam_batch ( hs_pad , hlens , lpz , recog_args , char_list , rnnlm , normalize_score = normalize_score ) if prev : self . train () return y scorers ( self ) Scorers. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 328 329 330 def scorers ( self ): \"\"\"Scorers.\"\"\" return dict ( decoder = self . dec , ctc = CTCPrefixScorer ( self . ctc , self . eos )) subsample_frames ( self , x ) Subsample speeh frames in the encoder. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr.py 477 478 479 480 481 482 483 484 485 def subsample_frames ( self , x ): \"\"\"Subsample speeh frames in the encoder.\"\"\" # subsample frame x = x [:: self . subsample [ 0 ], :] ilen = [ x . shape [ 0 ]] h = to_device ( self , torch . from_numpy ( np . array ( x , dtype = np . float32 ))) h . contiguous () return h , ilen","title":"E2E"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_asr_transformer","text":"Transformer speech recognition model (pytorch).","title":"e2e_asr_transformer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_asr_transformer.E2E","text":"E2E module. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options attention_plot_class property readonly Return PlotAttentionReport. __init__ ( self , idim , odim , args , ignore_id =- 1 ) special Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , idim , odim , args , ignore_id =- 1 ): \"\"\"Construct an E2E object. :param int idim: dimension of inputs :param int odim: dimension of outputs :param Namespace args: argument Namespace containing options \"\"\" torch . nn . Module . __init__ ( self ) if args . transformer_attn_dropout_rate is None : args . transformer_attn_dropout_rate = args . dropout_rate self . encoder = Encoder ( idim = idim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . eunits , num_blocks = args . elayers , input_layer = args . transformer_input_layer , dropout_rate = args . dropout_rate , positional_dropout_rate = args . dropout_rate , attention_dropout_rate = args . transformer_attn_dropout_rate ) self . decoder = Decoder ( odim = odim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . dunits , num_blocks = args . dlayers , dropout_rate = args . dropout_rate , positional_dropout_rate = args . dropout_rate , self_attention_dropout_rate = args . transformer_attn_dropout_rate , src_attention_dropout_rate = args . transformer_attn_dropout_rate ) self . sos = odim - 1 self . eos = odim - 1 self . odim = odim self . ignore_id = ignore_id self . subsample = [ 1 ] # self.lsm_weight = a self . criterion = LabelSmoothingLoss ( self . odim , self . ignore_id , args . lsm_weight , args . transformer_length_normalized_loss ) # self.verbose = args.verbose self . reset_parameters ( args ) self . adim = args . adim self . mtlalpha = args . mtlalpha if args . mtlalpha > 0.0 : self . ctc = CTC ( odim , args . adim , args . dropout_rate , ctc_type = args . ctc_type , reduce = True ) else : self . ctc = None if args . report_cer or args . report_wer : from tools.espnet_minimal import ErrorCalculator self . error_calculator = ErrorCalculator ( args . char_list , args . sym_space , args . sym_blank , args . report_cer , args . report_wer ) else : self . error_calculator = None self . rnnlm = None add_arguments ( parser ) staticmethod Add arguments. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @staticmethod def add_arguments ( parser ): \"\"\"Add arguments.\"\"\" group = parser . add_argument_group ( \"transformer model setting\" ) group . add_argument ( \"--transformer-init\" , type = str , default = \"pytorch\" , choices = [ \"pytorch\" , \"xavier_uniform\" , \"xavier_normal\" , \"kaiming_uniform\" , \"kaiming_normal\" ], help = 'how to initialize transformer parameters' ) group . add_argument ( \"--transformer-input-layer\" , type = str , default = \"conv2d\" , choices = [ \"conv2d\" , \"linear\" , \"embed\" ], help = 'transformer input layer type' ) group . add_argument ( '--transformer-attn-dropout-rate' , default = None , type = float , help = 'dropout in transformer attention. use --dropout-rate if None is set' ) group . add_argument ( '--transformer-lr' , default = 10.0 , type = float , help = 'Initial value of learning rate' ) group . add_argument ( '--transformer-warmup-steps' , default = 25000 , type = int , help = 'optimizer warmup steps' ) group . add_argument ( '--transformer-length-normalized-loss' , default = True , type = strtobool , help = 'normalize loss by length' ) group . add_argument ( '--dropout-rate' , default = 0.0 , type = float , help = 'Dropout rate for the encoder' ) # Encoder group . add_argument ( '--elayers' , default = 4 , type = int , help = 'Number of encoder layers (for shared recognition part in multi-speaker asr mode)' ) group . add_argument ( '--eunits' , '-u' , default = 300 , type = int , help = 'Number of encoder hidden units' ) # Attention group . add_argument ( '--adim' , default = 320 , type = int , help = 'Number of attention transformation dimensions' ) group . add_argument ( '--aheads' , default = 4 , type = int , help = 'Number of heads for multi head attention' ) # Decoder group . add_argument ( '--dlayers' , default = 1 , type = int , help = 'Number of decoder layers' ) group . add_argument ( '--dunits' , default = 320 , type = int , help = 'Number of decoder hidden units' ) return parser calculate_all_attentions ( self , xs_pad , ilens , ys_pad ) E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def calculate_all_attentions ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E attention calculation. :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray \"\"\" with torch . no_grad (): self . forward ( xs_pad , ilens , ys_pad ) ret = dict () for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): ret [ name ] = m . attn . cpu () . numpy () return ret encode ( self , x ) Encode acoustic features. :param ndarray x: source acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 223 224 225 226 227 228 229 230 231 232 233 def encode ( self , x ): \"\"\"Encode acoustic features. :param ndarray x: source acoustic feature (T, D) :return: encoder outputs :rtype: torch.Tensor \"\"\" self . eval () x = torch . as_tensor ( x ) . unsqueeze ( 0 ) enc_output , _ = self . encoder ( x , None ) return enc_output . squeeze ( 0 ) forward ( self , xs_pad , ilens , ys_pad ) E2E forward. :param torch.Tensor xs_pad: batch of padded source sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of source sequences (B) :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :return: ctc loass value :rtype: torch.Tensor :return: attention loss value :rtype: torch.Tensor :return: accuracy in attention decoder :rtype: float Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def forward ( self , xs_pad , ilens , ys_pad ): \"\"\"E2E forward. :param torch.Tensor xs_pad: batch of padded source sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of source sequences (B) :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :return: ctc loass value :rtype: torch.Tensor :return: attention loss value :rtype: torch.Tensor :return: accuracy in attention decoder :rtype: float \"\"\" # 1. forward encoder xs_pad = xs_pad [:, : max ( ilens )] # for data parallel src_mask = ( ~ make_pad_mask ( ilens . tolist ())) . to ( xs_pad . device ) . unsqueeze ( - 2 ) hs_pad , hs_mask = self . encoder ( xs_pad , src_mask ) self . hs_pad = hs_pad # 2. forward decoder ys_in_pad , ys_out_pad = add_sos_eos ( ys_pad , self . sos , self . eos , self . ignore_id ) ys_mask = target_mask ( ys_in_pad , self . ignore_id ) pred_pad , pred_mask = self . decoder ( ys_in_pad , ys_mask , hs_pad , hs_mask ) self . pred_pad = pred_pad # 3. compute attention loss loss_att = self . criterion ( pred_pad , ys_out_pad ) self . acc = th_accuracy ( pred_pad . view ( - 1 , self . odim ), ys_out_pad , ignore_label = self . ignore_id ) # TODO(karita) show predicted text # TODO(karita) calculate these stats cer_ctc = None if self . mtlalpha == 0.0 : loss_ctc = None else : batch_size = xs_pad . size ( 0 ) hs_len = hs_mask . view ( batch_size , - 1 ) . sum ( 1 ) loss_ctc = self . ctc ( hs_pad . view ( batch_size , - 1 , self . adim ), hs_len , ys_pad ) if self . error_calculator is not None : ys_hat = self . ctc . argmax ( hs_pad . view ( batch_size , - 1 , self . adim )) . data cer_ctc = self . error_calculator ( ys_hat . cpu (), ys_pad . cpu (), is_ctc = True ) # 5. compute cer/wer if self . training or self . error_calculator is None : cer , wer = None , None else : ys_hat = pred_pad . argmax ( dim =- 1 ) cer , wer = self . error_calculator ( ys_hat . cpu (), ys_pad . cpu ()) # copyied from e2e_asr alpha = self . mtlalpha if alpha == 0 : self . loss = loss_att loss_att_data = float ( loss_att ) loss_ctc_data = None elif alpha == 1 : self . loss = loss_ctc loss_att_data = None loss_ctc_data = float ( loss_ctc ) else : self . loss = alpha * loss_ctc + ( 1 - alpha ) * loss_att loss_att_data = float ( loss_att ) loss_ctc_data = float ( loss_ctc ) loss_data = float ( self . loss ) if loss_data < CTC_LOSS_THRESHOLD and not math . isnan ( loss_data ): self . reporter . report ( loss_ctc_data , loss_att_data , self . acc , cer_ctc , cer , wer , loss_data ) else : logging . warning ( 'loss (= %f ) is not correct' , loss_data ) return self . loss recognize ( self , x , recog_args , char_list = None , rnnlm = None , use_jit = False ) Recognize input speech. :param ndnarray x: input acoustic feature (B, T, D) or (T, D) :param Namespace recog_args: argment Namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 def recognize ( self , x , recog_args , char_list = None , rnnlm = None , use_jit = False ): \"\"\"Recognize input speech. :param ndnarray x: input acoustic feature (B, T, D) or (T, D) :param Namespace recog_args: argment Namespace contraining options :param list char_list: list of characters :param torch.nn.Module rnnlm: language model module :return: N-best decoding results :rtype: list \"\"\" enc_output = self . encode ( x ) . unsqueeze ( 0 ) if recog_args . ctc_weight > 0.0 : lpz = self . ctc . log_softmax ( enc_output ) lpz = lpz . squeeze ( 0 ) else : lpz = None h = enc_output . squeeze ( 0 ) logging . info ( 'input lengths: ' + str ( h . size ( 0 ))) # search parms beam = recog_args . beam_size penalty = recog_args . penalty ctc_weight = recog_args . ctc_weight # preprare sos y = self . sos vy = h . new_zeros ( 1 ) . long () if recog_args . maxlenratio == 0 : maxlen = h . shape [ 0 ] else : # maxlen >= 1 maxlen = max ( 1 , int ( recog_args . maxlenratio * h . size ( 0 ))) minlen = int ( recog_args . minlenratio * h . size ( 0 )) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # initialize hypothesis if rnnlm : hyp = { 'score' : 0.0 , 'yseq' : [ y ], 'rnnlm_prev' : None } else : hyp = { 'score' : 0.0 , 'yseq' : [ y ]} if lpz is not None : import numpy from tools.espnet_minimal.nets.ctc_prefix_score import CTCPrefixScore ctc_prefix_score = CTCPrefixScore ( lpz . detach () . numpy (), 0 , self . eos , numpy ) hyp [ 'ctc_state_prev' ] = ctc_prefix_score . initial_state () hyp [ 'ctc_score_prev' ] = 0.0 if ctc_weight != 1.0 : # pre-pruning based on attention scores from tools.espnet_minimal.nets.pytorch_backend.rnn.decoders import \\ CTC_SCORING_RATIO ctc_beam = min ( lpz . shape [ - 1 ], int ( beam * CTC_SCORING_RATIO )) else : ctc_beam = lpz . shape [ - 1 ] hyps = [ hyp ] ended_hyps = [] import six traced_decoder = None for i in six . moves . range ( maxlen ): logging . debug ( 'position ' + str ( i )) hyps_best_kept = [] for hyp in hyps : vy . unsqueeze ( 1 ) vy [ 0 ] = hyp [ 'yseq' ][ i ] # get nbest local scores and their ids ys_mask = subsequent_mask ( i + 1 ) . unsqueeze ( 0 ) ys = torch . tensor ( hyp [ 'yseq' ]) . unsqueeze ( 0 ) # FIXME: jit does not match non-jit result if use_jit : if traced_decoder is None : traced_decoder = torch . jit . trace ( self . decoder . forward_one_step , ( ys , ys_mask , enc_output )) local_att_scores = traced_decoder ( ys , ys_mask , enc_output )[ 0 ] else : local_att_scores = self . decoder . forward_one_step ( ys , ys_mask , enc_output )[ 0 ] if rnnlm : rnnlm_state , local_lm_scores = rnnlm . predict ( hyp [ 'rnnlm_prev' ], vy ) local_scores = local_att_scores + recog_args . lm_weight * local_lm_scores else : local_scores = local_att_scores if lpz is not None : local_best_scores , local_best_ids = torch . topk ( local_att_scores , ctc_beam , dim = 1 ) ctc_scores , ctc_states = ctc_prefix_score ( hyp [ 'yseq' ], local_best_ids [ 0 ], hyp [ 'ctc_state_prev' ]) local_scores = \\ ( 1.0 - ctc_weight ) * local_att_scores [:, local_best_ids [ 0 ]] \\ + ctc_weight * torch . from_numpy ( ctc_scores - hyp [ 'ctc_score_prev' ]) if rnnlm : local_scores += recog_args . lm_weight * local_lm_scores [:, local_best_ids [ 0 ]] local_best_scores , joint_best_ids = torch . topk ( local_scores , beam , dim = 1 ) local_best_ids = local_best_ids [:, joint_best_ids [ 0 ]] else : local_best_scores , local_best_ids = torch . topk ( local_scores , beam , dim = 1 ) for j in six . moves . range ( beam ): new_hyp = {} new_hyp [ 'score' ] = hyp [ 'score' ] + float ( local_best_scores [ 0 , j ]) new_hyp [ 'yseq' ] = [ 0 ] * ( 1 + len ( hyp [ 'yseq' ])) new_hyp [ 'yseq' ][: len ( hyp [ 'yseq' ])] = hyp [ 'yseq' ] new_hyp [ 'yseq' ][ len ( hyp [ 'yseq' ])] = int ( local_best_ids [ 0 , j ]) if rnnlm : new_hyp [ 'rnnlm_prev' ] = rnnlm_state if lpz is not None : new_hyp [ 'ctc_state_prev' ] = ctc_states [ joint_best_ids [ 0 , j ]] new_hyp [ 'ctc_score_prev' ] = ctc_scores [ joint_best_ids [ 0 , j ]] # will be (2 x beam) hyps at most hyps_best_kept . append ( new_hyp ) hyps_best_kept = sorted ( hyps_best_kept , key = lambda x : x [ 'score' ], reverse = True )[: beam ] # sort and get nbest hyps = hyps_best_kept logging . debug ( 'number of pruned hypothes: ' + str ( len ( hyps ))) if char_list is not None : logging . debug ( 'best hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyps [ 0 ][ 'yseq' ][ 1 :]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( 'adding <eos> in the last postion in the loop' ) for hyp in hyps : hyp [ 'yseq' ] . append ( self . eos ) # add ended hypothes to a final list, and removed them from current hypothes # (this will be a probmlem, number of hyps < beam) remained_hyps = [] for hyp in hyps : if hyp [ 'yseq' ][ - 1 ] == self . eos : # only store the sequence that has more than minlen outputs # also add penalty if len ( hyp [ 'yseq' ]) > minlen : hyp [ 'score' ] += ( i + 1 ) * penalty if rnnlm : # Word LM needs to add final <eos> score hyp [ 'score' ] += recog_args . lm_weight * rnnlm . final ( hyp [ 'rnnlm_prev' ]) ended_hyps . append ( hyp ) else : remained_hyps . append ( hyp ) # end detection from tools.espnet_minimal.nets.e2e_asr_common import end_detect if end_detect ( ended_hyps , i ) and recog_args . maxlenratio == 0.0 : logging . info ( 'end detected at %d ' , i ) break hyps = remained_hyps if len ( hyps ) > 0 : logging . debug ( 'remeined hypothes: ' + str ( len ( hyps ))) else : logging . info ( 'no hypothesis. Finish decoding.' ) break if char_list is not None : for hyp in hyps : logging . debug ( 'hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyp [ 'yseq' ][ 1 :]])) logging . debug ( 'number of ended hypothes: ' + str ( len ( ended_hyps ))) nbest_hyps = sorted ( ended_hyps , key = lambda x : x [ 'score' ], reverse = True )[: min ( len ( ended_hyps ), recog_args . nbest )] # check number of hypotheis if len ( nbest_hyps ) == 0 : logging . warning ( 'there is no N-best results, perform recognition again with smaller minlenratio.' ) # should copy becasuse Namespace will be overwritten globally recog_args = Namespace ( ** vars ( recog_args )) recog_args . minlenratio = max ( 0.0 , recog_args . minlenratio - 0.1 ) return self . recognize ( x , recog_args , char_list , rnnlm ) logging . info ( 'total log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ])) logging . info ( 'normalized log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ] / len ( nbest_hyps [ 0 ][ 'yseq' ]))) return nbest_hyps reset_parameters ( self , args ) Initialize parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 142 143 144 145 def reset_parameters ( self , args ): \"\"\"Initialize parameters.\"\"\" # initialize parameters initialize ( self , args . transformer_init ) scorers ( self ) Scorers. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_asr_transformer.py 219 220 221 def scorers ( self ): \"\"\"Scorers.\"\"\" return dict ( decoder = self . decoder , ctc = CTCPrefixScorer ( self . ctc , self . eos ))","title":"E2E"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_fastspeech","text":"FastSpeech related modules.","title":"e2e_tts_fastspeech"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer","text":"Feed Forward Transformer for TTS a.k.a. FastSpeech. This is a module of FastSpeech , feed - forward Transformer with duration predictor described in ` FastSpeech : Fast , Robust and Controllable Text to Speech ` _ , which does not require any auto - regressive processing during inference , resulting in fast decoding compared with auto - regressive Transformer . .. _ ` FastSpeech : Fast , Robust and Controllable Text to Speech ` : https : // arxiv . org / pdf / 1905 . 09263 . pdf attention_plot_class property readonly Return plot class for attention weight plot. base_plot_keys property readonly Return base key names to plot during training. keys should match what chainer.reporter reports. If you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list List of strings which are base keys to plot during training. __init__ ( self , idim , odim , args = None ) special Initialize feed-forward Transformer module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required args Namespace elayers (int): Number of encoder layers. eunits (int): Number of encoder hidden units. adim (int): Number of attention transformation dimensions. aheads (int): Number of heads for multi head attention. dlayers (int): Number of decoder layers. dunits (int): Number of decoder hidden units. use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. duration_predictor_layers (int): Number of duration predictor layers. duration_predictor_chans (int): Number of duration predictor channels. duration_predictor_kernel_size (int): Kernel size of duration predictor. spk_embed_dim (int): Number of speaker embedding dimenstions. spk_embed_integration_type: How to integrate speaker embedding. teacher_model (str): Teacher auto-regressive transformer model path. reduction_factor (int): Reduction factor. transformer_init (float): How to initialize transformer parameters. transformer_lr (float): Initial value of learning rate. transformer_warmup_steps (int): Optimizer warmup steps. transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. transfer_encoder_from_teacher: Whether to transfer encoder using teacher encoder parameters. transferred_encoder_module: Encoder module to be initialized using teacher parameters. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def __init__ ( self , idim , odim , args = None ): \"\"\"Initialize feed-forward Transformer module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. args (Namespace, optional): - elayers (int): Number of encoder layers. - eunits (int): Number of encoder hidden units. - adim (int): Number of attention transformation dimensions. - aheads (int): Number of heads for multi head attention. - dlayers (int): Number of decoder layers. - dunits (int): Number of decoder hidden units. - use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. - encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. - decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. - encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. - decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. - duration_predictor_layers (int): Number of duration predictor layers. - duration_predictor_chans (int): Number of duration predictor channels. - duration_predictor_kernel_size (int): Kernel size of duration predictor. - spk_embed_dim (int): Number of speaker embedding dimenstions. - spk_embed_integration_type: How to integrate speaker embedding. - teacher_model (str): Teacher auto-regressive transformer model path. - reduction_factor (int): Reduction factor. - transformer_init (float): How to initialize transformer parameters. - transformer_lr (float): Initial value of learning rate. - transformer_warmup_steps (int): Optimizer warmup steps. - transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. - transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. - transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. - transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. - transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. - transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. - transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. - use_masking (bool): Whether to apply masking for padded part in loss calculation. - use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. - transfer_encoder_from_teacher: Whether to transfer encoder using teacher encoder parameters. - transferred_encoder_module: Encoder module to be initialized using teacher parameters. \"\"\" # initialize base classes TTSInterface . __init__ ( self ) torch . nn . Module . __init__ ( self ) # fill missing arguments args = fill_missing_args ( args , self . add_arguments ) # store hyperparameters self . idim = idim self . odim = odim self . reduction_factor = args . reduction_factor self . use_scaled_pos_enc = args . use_scaled_pos_enc self . spk_embed_dim = args . spk_embed_dim if self . spk_embed_dim is not None : self . spk_embed_integration_type = args . spk_embed_integration_type # use idx 0 as padding idx padding_idx = 0 # get positional encoding class pos_enc_class = ScaledPositionalEncoding if self . use_scaled_pos_enc else PositionalEncoding # define encoder encoder_input_layer = torch . nn . Embedding ( num_embeddings = idim , embedding_dim = args . adim , padding_idx = padding_idx ) self . encoder = Encoder ( idim = idim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . eunits , num_blocks = args . elayers , input_layer = encoder_input_layer , dropout_rate = args . transformer_enc_dropout_rate , positional_dropout_rate = args . transformer_enc_positional_dropout_rate , attention_dropout_rate = args . transformer_enc_attn_dropout_rate , pos_enc_class = pos_enc_class , normalize_before = args . encoder_normalize_before , concat_after = args . encoder_concat_after , positionwise_layer_type = args . positionwise_layer_type , positionwise_conv_kernel_size = args . positionwise_conv_kernel_size ) # define additional projection for speaker embedding if self . spk_embed_dim is not None : if self . spk_embed_integration_type == \"add\" : self . projection = torch . nn . Linear ( self . spk_embed_dim , args . adim ) else : self . projection = torch . nn . Linear ( args . adim + self . spk_embed_dim , args . adim ) # define duration predictor self . duration_predictor = DurationPredictor ( idim = args . adim , n_layers = args . duration_predictor_layers , n_chans = args . duration_predictor_chans , kernel_size = args . duration_predictor_kernel_size , dropout_rate = args . duration_predictor_dropout_rate , ) # define length regulator self . length_regulator = LengthRegulator () # define decoder # NOTE: we use encoder as decoder because fastspeech's decoder is the same as encoder self . decoder = Encoder ( idim = 0 , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . dunits , num_blocks = args . dlayers , input_layer = None , dropout_rate = args . transformer_dec_dropout_rate , positional_dropout_rate = args . transformer_dec_positional_dropout_rate , attention_dropout_rate = args . transformer_dec_attn_dropout_rate , pos_enc_class = pos_enc_class , normalize_before = args . decoder_normalize_before , concat_after = args . decoder_concat_after , positionwise_layer_type = args . positionwise_layer_type , positionwise_conv_kernel_size = args . positionwise_conv_kernel_size ) # define final projection self . feat_out = torch . nn . Linear ( args . adim , odim * args . reduction_factor ) # define postnet self . postnet = None if args . postnet_layers == 0 else Postnet ( idim = idim , odim = odim , n_layers = args . postnet_layers , n_chans = args . postnet_chans , n_filts = args . postnet_filts , use_batch_norm = args . use_batch_norm , dropout_rate = args . postnet_dropout_rate ) # initialize parameters self . _reset_parameters ( init_type = args . transformer_init , init_enc_alpha = args . initial_encoder_alpha , init_dec_alpha = args . initial_decoder_alpha ) # define teacher model if args . teacher_model is not None : self . teacher = self . _load_teacher_model ( args . teacher_model ) else : self . teacher = None # define duration calculator if self . teacher is not None : self . duration_calculator = DurationCalculator ( self . teacher ) else : self . duration_calculator = None # transfer teacher parameters if self . teacher is not None and args . transfer_encoder_from_teacher : self . _transfer_from_teacher ( args . transferred_encoder_module ) # define criterions self . criterion = FeedForwardTransformerLoss ( use_masking = args . use_masking , use_weighted_masking = args . use_weighted_masking ) add_arguments ( parser ) staticmethod Add model-specific arguments to the parser. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 @staticmethod def add_arguments ( parser ): \"\"\"Add model-specific arguments to the parser.\"\"\" group = parser . add_argument_group ( \"feed-forward transformer model setting\" ) # network structure related group . add_argument ( \"--adim\" , default = 384 , type = int , help = \"Number of attention transformation dimensions\" ) group . add_argument ( \"--aheads\" , default = 4 , type = int , help = \"Number of heads for multi head attention\" ) group . add_argument ( \"--elayers\" , default = 6 , type = int , help = \"Number of encoder layers\" ) group . add_argument ( \"--eunits\" , default = 1536 , type = int , help = \"Number of encoder hidden units\" ) group . add_argument ( \"--dlayers\" , default = 6 , type = int , help = \"Number of decoder layers\" ) group . add_argument ( \"--dunits\" , default = 1536 , type = int , help = \"Number of decoder hidden units\" ) group . add_argument ( \"--positionwise-layer-type\" , default = \"linear\" , type = str , choices = [ \"linear\" , \"conv1d\" , \"conv1d-linear\" ], help = \"Positionwise layer type.\" ) group . add_argument ( \"--positionwise-conv-kernel-size\" , default = 3 , type = int , help = \"Kernel size of positionwise conv1d layer\" ) group . add_argument ( \"--postnet-layers\" , default = 0 , type = int , help = \"Number of postnet layers\" ) group . add_argument ( \"--postnet-chans\" , default = 256 , type = int , help = \"Number of postnet channels\" ) group . add_argument ( \"--postnet-filts\" , default = 5 , type = int , help = \"Filter size of postnet\" ) group . add_argument ( \"--use-batch-norm\" , default = True , type = strtobool , help = \"Whether to use batch normalization\" ) group . add_argument ( \"--use-scaled-pos-enc\" , default = True , type = strtobool , help = \"Use trainable scaled positional encoding instead of the fixed scale one\" ) group . add_argument ( \"--encoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before encoder block\" ) group . add_argument ( \"--decoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before decoder block\" ) group . add_argument ( \"--encoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in encoder\" ) group . add_argument ( \"--decoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in decoder\" ) group . add_argument ( \"--duration-predictor-layers\" , default = 2 , type = int , help = \"Number of layers in duration predictor\" ) group . add_argument ( \"--duration-predictor-chans\" , default = 384 , type = int , help = \"Number of channels in duration predictor\" ) group . add_argument ( \"--duration-predictor-kernel-size\" , default = 3 , type = int , help = \"Kernel size in duration predictor\" ) group . add_argument ( \"--teacher-model\" , default = None , type = str , nargs = \"?\" , help = \"Teacher model file path\" ) group . add_argument ( \"--reduction-factor\" , default = 1 , type = int , help = \"Reduction factor\" ) group . add_argument ( \"--spk-embed-dim\" , default = None , type = int , help = \"Number of speaker embedding dimensions\" ) group . add_argument ( \"--spk-embed-integration-type\" , type = str , default = \"add\" , choices = [ \"add\" , \"concat\" ], help = \"How to integrate speaker embedding\" ) # training related group . add_argument ( \"--transformer-init\" , type = str , default = \"pytorch\" , choices = [ \"pytorch\" , \"xavier_uniform\" , \"xavier_normal\" , \"kaiming_uniform\" , \"kaiming_normal\" ], help = \"How to initialize transformer parameters\" ) group . add_argument ( \"--initial-encoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in encoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--initial-decoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in decoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--transformer-lr\" , default = 1.0 , type = float , help = \"Initial value of learning rate\" ) group . add_argument ( \"--transformer-warmup-steps\" , default = 4000 , type = int , help = \"Optimizer warmup steps\" ) group . add_argument ( \"--transformer-enc-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder except for attention\" ) group . add_argument ( \"--transformer-enc-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder positional encoding\" ) group . add_argument ( \"--transformer-enc-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder self-attention\" ) group . add_argument ( \"--transformer-dec-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder except for attention and pos encoding\" ) group . add_argument ( \"--transformer-dec-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder positional encoding\" ) group . add_argument ( \"--transformer-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder self-attention\" ) group . add_argument ( \"--transformer-enc-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder-decoder attention\" ) group . add_argument ( \"--duration-predictor-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for duration predictor\" ) group . add_argument ( \"--postnet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in postnet\" ) group . add_argument ( \"--transfer-encoder-from-teacher\" , default = True , type = strtobool , help = \"Whether to transfer teacher's parameters\" ) group . add_argument ( \"--transferred-encoder-module\" , default = \"all\" , type = str , choices = [ \"all\" , \"embed\" ], help = \"Encoder modeules to be trasferred from teacher\" ) # loss related group . add_argument ( \"--use-masking\" , default = True , type = strtobool , help = \"Whether to use masking in calculation of loss\" ) group . add_argument ( \"--use-weighted-masking\" , default = False , type = strtobool , help = \"Whether to use weighted masking in calculation of loss\" ) return parser calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ) Calculate all of the attention weights. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None extras Tensor Batch of precalculated durations (B, Tmax, 1). None Returns: Type Description dict Dict of attention weights and outputs. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ): \"\"\"Calculate all of the attention weights. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). extras (Tensor, optional): Batch of precalculated durations (B, Tmax, 1). Returns: dict: Dict of attention weights and outputs. \"\"\" with torch . no_grad (): # remove unnecessary padded part (for multi-gpus) xs = xs [:, : max ( ilens )] ys = ys [:, : max ( olens )] if extras is not None : extras = extras [:, : max ( ilens )] . squeeze ( - 1 ) # forward propagation outs = self . _forward ( xs , ilens , ys , olens , spembs = spembs , ds = extras , is_inference = False )[ 1 ] att_ws_dict = dict () for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): attn = m . attn . cpu () . numpy () if \"encoder\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , ilens . tolist ())] elif \"decoder\" in name : if \"src\" in name : attn = [ a [:, : ol , : il ] for a , il , ol in zip ( attn , ilens . tolist (), olens . tolist ())] elif \"self\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , olens . tolist ())] else : logging . warning ( \"unknown attention module: \" + name ) else : logging . warning ( \"unknown attention module: \" + name ) att_ws_dict [ name ] = attn att_ws_dict [ \"predicted_fbank\" ] = [ m [: l ] . T for m , l in zip ( outs . cpu () . numpy (), olens . tolist ())] return att_ws_dict forward ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None extras Tensor Batch of precalculated durations (B, Tmax, 1). None Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def forward ( self , xs , ilens , ys , olens , spembs = None , extras = None , * args , ** kwargs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). extras (Tensor, optional): Batch of precalculated durations (B, Tmax, 1). Returns: Tensor: Loss value. \"\"\" # remove unnecessary padded part (for multi-gpus) xs = xs [:, : max ( ilens )] ys = ys [:, : max ( olens )] if extras is not None : extras = extras [:, : max ( ilens )] . squeeze ( - 1 ) # forward propagation before_outs , after_outs , ds , d_outs = self . _forward ( xs , ilens , ys , olens , spembs = spembs , ds = extras , is_inference = False ) # modifiy mod part of groundtruth if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) max_olen = max ( olens ) ys = ys [:, : max_olen ] # calculate loss if self . postnet is None : l1_loss , duration_loss = self . criterion ( None , before_outs , d_outs , ys , ds , ilens , olens ) else : l1_loss , duration_loss = self . criterion ( after_outs , before_outs , d_outs , ys , ds , ilens , olens ) loss = l1_loss + duration_loss report_keys = [ { \"l1_loss\" : l1_loss . item ()}, { \"duration_loss\" : duration_loss . item ()}, { \"loss\" : loss . item ()}, ] # report extra information if self . use_scaled_pos_enc : report_keys += [ { \"encoder_alpha\" : self . encoder . embed [ - 1 ] . alpha . data . item ()}, { \"decoder_alpha\" : self . decoder . embed [ - 1 ] . alpha . data . item ()}, ] self . reporter . report ( report_keys ) return loss inference ( self , x , inference_args , spemb = None , * args , ** kwargs ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default x Tensor Input sequence of characters (T,). required inference_args Namespace Dummy for compatibility. required spemb Tensor Speaker embedding vector (spk_embed_dim). None Returns: Type Description Tensor Output sequence of features (L, odim). None: Dummy for compatibility. None: Dummy for compatibility. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 def inference ( self , x , inference_args , spemb = None , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Args: x (Tensor): Input sequence of characters (T,). inference_args (Namespace): Dummy for compatibility. spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim). Returns: Tensor: Output sequence of features (L, odim). None: Dummy for compatibility. None: Dummy for compatibility. \"\"\" # setup batch axis ilens = torch . tensor ([ x . shape [ 0 ]], dtype = torch . long , device = x . device ) xs = x . unsqueeze ( 0 ) if spemb is not None : spembs = spemb . unsqueeze ( 0 ) else : spembs = None # inference _ , outs , _ = self . _forward ( xs , ilens , spembs = spembs , is_inference = True ) # (1, L, odim) return outs [ 0 ], None , None","title":"FeedForwardTransformer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformerLoss","text":"Loss function module for feed-forward Transformer. __init__ ( self , use_masking = True , use_weighted_masking = False ) special Initialize feed-forward Transformer loss module. Parameters: Name Type Description Default use_masking bool Whether to apply masking for padded part in loss calculation. True use_weighted_masking bool Whether to weighted masking in loss calculation. False Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , use_masking = True , use_weighted_masking = False ): \"\"\"Initialize feed-forward Transformer loss module. Args: use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to weighted masking in loss calculation. \"\"\" super ( FeedForwardTransformerLoss , self ) . __init__ () assert ( use_masking != use_weighted_masking ) or not use_masking self . use_masking = use_masking self . use_weighted_masking = use_weighted_masking # define criterions reduction = \"none\" if self . use_weighted_masking else \"mean\" self . l1_criterion = torch . nn . L1Loss ( reduction = reduction ) self . duration_criterion = DurationPredictorLoss ( reduction = reduction ) forward ( self , after_outs , before_outs , d_outs , ys , ds , ilens , olens ) Calculate forward propagation. Parameters: Name Type Description Default after_outs Tensor Batch of outputs after postnets (B, Lmax, odim). required before_outs Tensor Batch of outputs before postnets (B, Lmax, odim). required d_outs Tensor Batch of outputs of duration predictor (B, Tmax). required ys Tensor Batch of target features (B, Lmax, odim). required ds Tensor Batch of durations (B, Tmax). required ilens LongTensor Batch of the lengths of each input (B,). required olens LongTensor Batch of the lengths of each target (B,). required Returns: Type Description Tensor L1 loss value. Tensor: Duration predictor loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_fastspeech.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , after_outs , before_outs , d_outs , ys , ds , ilens , olens ): \"\"\"Calculate forward propagation. Args: after_outs (Tensor): Batch of outputs after postnets (B, Lmax, odim). before_outs (Tensor): Batch of outputs before postnets (B, Lmax, odim). d_outs (Tensor): Batch of outputs of duration predictor (B, Tmax). ys (Tensor): Batch of target features (B, Lmax, odim). ds (Tensor): Batch of durations (B, Tmax). ilens (LongTensor): Batch of the lengths of each input (B,). olens (LongTensor): Batch of the lengths of each target (B,). Returns: Tensor: L1 loss value. Tensor: Duration predictor loss value. \"\"\" # apply mask to remove padded part if self . use_masking : duration_masks = make_non_pad_mask ( ilens ) . to ( ys . device ) d_outs = d_outs . masked_select ( duration_masks ) ds = ds . masked_select ( duration_masks ) out_masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) before_outs = before_outs . masked_select ( out_masks ) after_outs = after_outs . masked_select ( out_masks ) if after_outs is not None else None ys = ys . masked_select ( out_masks ) # calculate loss l1_loss = self . l1_criterion ( before_outs , ys ) if after_outs is not None : l1_loss += self . l1_criterion ( after_outs , ys ) duration_loss = self . duration_criterion ( d_outs , ds ) # make weighted mask and apply it if self . use_weighted_masking : out_masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) out_weights = out_masks . float () / out_masks . sum ( dim = 1 , keepdim = True ) . float () out_weights /= ys . size ( 0 ) * ys . size ( 2 ) duration_masks = make_non_pad_mask ( ilens ) . to ( ys . device ) duration_weights = duration_masks . float () / duration_masks . sum ( dim = 1 , keepdim = True ) . float () duration_weights /= ds . size ( 0 ) # apply weight l1_loss = l1_loss . mul ( out_weights ) . masked_select ( out_masks ) . sum () duration_loss = duration_loss . mul ( duration_weights ) . masked_select ( duration_masks ) . sum () return l1_loss , duration_loss","title":"FeedForwardTransformerLoss"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2","text":"Tacotron 2 related modules.","title":"e2e_tts_tacotron2"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss","text":"Guided attention loss function module. This module calculates the guided attention loss described in ` Efficiently Trainable Text - to - Speech System Based on Deep Convolutional Networks with Guided Attention ` _ , which forces the attention to be diagonal . .. _ ` Efficiently Trainable Text - to - Speech System Based on Deep Convolutional Networks with Guided Attention ` : https : // arxiv . org / abs / 1710 . 08969 __init__ ( self , sigma = 0.4 , alpha = 1.0 , reset_always = True ) special Initialize guided attention loss module. Parameters: Name Type Description Default sigma float Standard deviation to control how close attention to a diagonal. 0.4 alpha float Scaling coefficient (lambda). 1.0 reset_always bool Whether to always reset masks. True Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , sigma = 0.4 , alpha = 1.0 , reset_always = True ): \"\"\"Initialize guided attention loss module. Args: sigma (float, optional): Standard deviation to control how close attention to a diagonal. alpha (float, optional): Scaling coefficient (lambda). reset_always (bool, optional): Whether to always reset masks. \"\"\" super ( GuidedAttentionLoss , self ) . __init__ () self . sigma = sigma self . alpha = alpha self . reset_always = reset_always self . guided_attn_masks = None self . masks = None forward ( self , att_ws , ilens , olens ) Calculate forward propagation. Parameters: Name Type Description Default att_ws Tensor Batch of attention weights (B, T_max_out, T_max_in). required ilens LongTensor Batch of input lenghts (B,). required olens LongTensor Batch of output lenghts (B,). required Returns: Type Description Tensor Guided attention loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def forward ( self , att_ws , ilens , olens ): \"\"\"Calculate forward propagation. Args: att_ws (Tensor): Batch of attention weights (B, T_max_out, T_max_in). ilens (LongTensor): Batch of input lenghts (B,). olens (LongTensor): Batch of output lenghts (B,). Returns: Tensor: Guided attention loss value. \"\"\" if self . guided_attn_masks is None : self . guided_attn_masks = self . _make_guided_attention_masks ( ilens , olens ) . to ( att_ws . device ) if self . masks is None : self . masks = self . _make_masks ( ilens , olens ) . to ( att_ws . device ) losses = self . guided_attn_masks * att_ws loss = torch . mean ( losses . masked_select ( self . masks )) if self . reset_always : self . _reset_masks () return self . alpha * loss","title":"GuidedAttentionLoss"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2","text":"Tacotron2 module for end-to-end text-to-speech (E2E-TTS). This is a module of Spectrogram prediction network in Tacotron2 described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ , which converts the sequence of characters into the sequence of Mel - filterbanks . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 base_plot_keys property readonly Return base key names to plot during training. keys should match what chainer.reporter reports. If you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list List of strings which are base keys to plot during training. __init__ ( self , idim , odim , args = None ) special Initialize Tacotron2 module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required args Namespace spk_embed_dim (int): Dimension of the speaker embedding. embed_dim (int): Dimension of character embedding. elayers (int): The number of encoder blstm layers. eunits (int): The number of encoder blstm units. econv_layers (int): The number of encoder conv layers. econv_filts (int): The number of encoder conv filter size. econv_chans (int): The number of encoder conv filter channels. dlayers (int): The number of decoder lstm layers. dunits (int): The number of decoder lstm units. prenet_layers (int): The number of prenet layers. prenet_units (int): The number of prenet units. postnet_layers (int): The number of postnet layers. postnet_filts (int): The number of postnet filter size. postnet_chans (int): The number of postnet filter channels. output_activation (int): The name of activation function for outputs. adim (int): The number of dimension of mlp in attention. aconv_chans (int): The number of attention conv filter channels. aconv_filts (int): The number of attention conv filter size. cumulate_att_w (bool): Whether to cumulate previous attention weight. use_batch_norm (bool): Whether to use batch normalization. use_concate (int): Whether to concatenate encoder embedding with decoder lstm outputs. dropout_rate (float): Dropout rate. zoneout_rate (float): Zoneout rate. reduction_factor (int): Reduction factor. spk_embed_dim (int): Number of speaker embedding dimenstions. spc_dim (int): Number of spectrogram embedding dimenstions (only for use_cbhg=True). use_cbhg (bool): Whether to use CBHG module. cbhg_conv_bank_layers (int): The number of convoluional banks in CBHG. cbhg_conv_bank_chans (int): The number of channels of convolutional bank in CBHG. cbhg_proj_filts (int): The number of filter size of projection layeri in CBHG. cbhg_proj_chans (int): The number of channels of projection layer in CBHG. cbhg_highway_layers (int): The number of layers of highway network in CBHG. cbhg_highway_units (int): The number of units of highway network in CBHG. cbhg_gru_units (int): The number of units of GRU in CBHG. use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. bce_pos_weight (float): Weight of positive sample of stop token (only for use_masking=True). use-guided-attn-loss (bool): Whether to use guided attention loss. guided-attn-loss-sigma (float) Sigma in guided attention loss. guided-attn-loss-lamdba (float): Lambda in guided attention loss. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def __init__ ( self , idim , odim , args = None ): \"\"\"Initialize Tacotron2 module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. args (Namespace, optional): - spk_embed_dim (int): Dimension of the speaker embedding. - embed_dim (int): Dimension of character embedding. - elayers (int): The number of encoder blstm layers. - eunits (int): The number of encoder blstm units. - econv_layers (int): The number of encoder conv layers. - econv_filts (int): The number of encoder conv filter size. - econv_chans (int): The number of encoder conv filter channels. - dlayers (int): The number of decoder lstm layers. - dunits (int): The number of decoder lstm units. - prenet_layers (int): The number of prenet layers. - prenet_units (int): The number of prenet units. - postnet_layers (int): The number of postnet layers. - postnet_filts (int): The number of postnet filter size. - postnet_chans (int): The number of postnet filter channels. - output_activation (int): The name of activation function for outputs. - adim (int): The number of dimension of mlp in attention. - aconv_chans (int): The number of attention conv filter channels. - aconv_filts (int): The number of attention conv filter size. - cumulate_att_w (bool): Whether to cumulate previous attention weight. - use_batch_norm (bool): Whether to use batch normalization. - use_concate (int): Whether to concatenate encoder embedding with decoder lstm outputs. - dropout_rate (float): Dropout rate. - zoneout_rate (float): Zoneout rate. - reduction_factor (int): Reduction factor. - spk_embed_dim (int): Number of speaker embedding dimenstions. - spc_dim (int): Number of spectrogram embedding dimenstions (only for use_cbhg=True). - use_cbhg (bool): Whether to use CBHG module. - cbhg_conv_bank_layers (int): The number of convoluional banks in CBHG. - cbhg_conv_bank_chans (int): The number of channels of convolutional bank in CBHG. - cbhg_proj_filts (int): The number of filter size of projection layeri in CBHG. - cbhg_proj_chans (int): The number of channels of projection layer in CBHG. - cbhg_highway_layers (int): The number of layers of highway network in CBHG. - cbhg_highway_units (int): The number of units of highway network in CBHG. - cbhg_gru_units (int): The number of units of GRU in CBHG. - use_masking (bool): Whether to apply masking for padded part in loss calculation. - use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. - bce_pos_weight (float): Weight of positive sample of stop token (only for use_masking=True). - use-guided-attn-loss (bool): Whether to use guided attention loss. - guided-attn-loss-sigma (float) Sigma in guided attention loss. - guided-attn-loss-lamdba (float): Lambda in guided attention loss. \"\"\" # initialize base classes TTSInterface . __init__ ( self ) torch . nn . Module . __init__ ( self ) # fill missing arguments args = fill_missing_args ( args , self . add_arguments ) # store hyperparameters self . idim = idim self . odim = odim self . spk_embed_dim = args . spk_embed_dim self . cumulate_att_w = args . cumulate_att_w self . reduction_factor = args . reduction_factor self . use_cbhg = args . use_cbhg self . use_guided_attn_loss = args . use_guided_attn_loss # define activation function for the final output if args . output_activation is None : self . output_activation_fn = None elif hasattr ( F , args . output_activation ): self . output_activation_fn = getattr ( F , args . output_activation ) else : raise ValueError ( 'there is no such an activation function. ( %s )' % args . output_activation ) # set padding idx padding_idx = 0 # define network modules self . enc = Encoder ( idim = idim , embed_dim = args . embed_dim , elayers = args . elayers , eunits = args . eunits , econv_layers = args . econv_layers , econv_chans = args . econv_chans , econv_filts = args . econv_filts , use_batch_norm = args . use_batch_norm , use_residual = args . use_residual , dropout_rate = args . dropout_rate , padding_idx = padding_idx ) dec_idim = args . eunits if args . spk_embed_dim is None else args . eunits + args . spk_embed_dim if args . atype == \"location\" : att = AttLoc ( dec_idim , args . dunits , args . adim , args . aconv_chans , args . aconv_filts ) elif args . atype == \"forward\" : att = AttForward ( dec_idim , args . dunits , args . adim , args . aconv_chans , args . aconv_filts ) if self . cumulate_att_w : logging . warning ( \"cumulation of attention weights is disabled in forward attention.\" ) self . cumulate_att_w = False elif args . atype == \"forward_ta\" : att = AttForwardTA ( dec_idim , args . dunits , args . adim , args . aconv_chans , args . aconv_filts , odim ) if self . cumulate_att_w : logging . warning ( \"cumulation of attention weights is disabled in forward attention.\" ) self . cumulate_att_w = False else : raise NotImplementedError ( \"Support only location or forward\" ) self . dec = Decoder ( idim = dec_idim , odim = odim , att = att , dlayers = args . dlayers , dunits = args . dunits , prenet_layers = args . prenet_layers , prenet_units = args . prenet_units , postnet_layers = args . postnet_layers , postnet_chans = args . postnet_chans , postnet_filts = args . postnet_filts , output_activation_fn = self . output_activation_fn , cumulate_att_w = self . cumulate_att_w , use_batch_norm = args . use_batch_norm , use_concate = args . use_concate , dropout_rate = args . dropout_rate , zoneout_rate = args . zoneout_rate , reduction_factor = args . reduction_factor ) self . taco2_loss = Tacotron2Loss ( use_masking = args . use_masking , use_weighted_masking = args . use_weighted_masking , bce_pos_weight = args . bce_pos_weight ) if self . use_guided_attn_loss : self . attn_loss = GuidedAttentionLoss ( sigma = args . guided_attn_loss_sigma , alpha = args . guided_attn_loss_lambda , ) if self . use_cbhg : self . cbhg = CBHG ( idim = odim , odim = args . spc_dim , conv_bank_layers = args . cbhg_conv_bank_layers , conv_bank_chans = args . cbhg_conv_bank_chans , conv_proj_filts = args . cbhg_conv_proj_filts , conv_proj_chans = args . cbhg_conv_proj_chans , highway_layers = args . cbhg_highway_layers , highway_units = args . cbhg_highway_units , gru_units = args . cbhg_gru_units ) self . cbhg_loss = CBHGLoss ( use_masking = args . use_masking ) # load pretrained model if args . pretrained_model is not None : self . load_pretrained_model ( args . pretrained_model ) add_arguments ( parser ) staticmethod Add model-specific arguments to the parser. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 @staticmethod def add_arguments ( parser ): \"\"\"Add model-specific arguments to the parser.\"\"\" group = parser . add_argument_group ( \"tacotron 2 model setting\" ) # encoder group . add_argument ( '--embed-dim' , default = 512 , type = int , help = 'Number of dimension of embedding' ) group . add_argument ( '--elayers' , default = 1 , type = int , help = 'Number of encoder layers' ) group . add_argument ( '--eunits' , '-u' , default = 512 , type = int , help = 'Number of encoder hidden units' ) group . add_argument ( '--econv-layers' , default = 3 , type = int , help = 'Number of encoder convolution layers' ) group . add_argument ( '--econv-chans' , default = 512 , type = int , help = 'Number of encoder convolution channels' ) group . add_argument ( '--econv-filts' , default = 5 , type = int , help = 'Filter size of encoder convolution' ) # attention group . add_argument ( '--atype' , default = \"location\" , type = str , choices = [ \"forward_ta\" , \"forward\" , \"location\" ], help = 'Type of attention mechanism' ) group . add_argument ( '--adim' , default = 512 , type = int , help = 'Number of attention transformation dimensions' ) group . add_argument ( '--aconv-chans' , default = 32 , type = int , help = 'Number of attention convolution channels' ) group . add_argument ( '--aconv-filts' , default = 15 , type = int , help = 'Filter size of attention convolution' ) group . add_argument ( '--cumulate-att-w' , default = True , type = strtobool , help = \"Whether or not to cumulate attention weights\" ) # decoder group . add_argument ( '--dlayers' , default = 2 , type = int , help = 'Number of decoder layers' ) group . add_argument ( '--dunits' , default = 1024 , type = int , help = 'Number of decoder hidden units' ) group . add_argument ( '--prenet-layers' , default = 2 , type = int , help = 'Number of prenet layers' ) group . add_argument ( '--prenet-units' , default = 256 , type = int , help = 'Number of prenet hidden units' ) group . add_argument ( '--postnet-layers' , default = 5 , type = int , help = 'Number of postnet layers' ) group . add_argument ( '--postnet-chans' , default = 512 , type = int , help = 'Number of postnet channels' ) group . add_argument ( '--postnet-filts' , default = 5 , type = int , help = 'Filter size of postnet' ) group . add_argument ( '--output-activation' , default = None , type = str , nargs = '?' , help = 'Output activation function' ) # cbhg group . add_argument ( '--use-cbhg' , default = False , type = strtobool , help = 'Whether to use CBHG module' ) group . add_argument ( '--cbhg-conv-bank-layers' , default = 8 , type = int , help = 'Number of convoluional bank layers in CBHG' ) group . add_argument ( '--cbhg-conv-bank-chans' , default = 128 , type = int , help = 'Number of convoluional bank channles in CBHG' ) group . add_argument ( '--cbhg-conv-proj-filts' , default = 3 , type = int , help = 'Filter size of convoluional projection layer in CBHG' ) group . add_argument ( '--cbhg-conv-proj-chans' , default = 256 , type = int , help = 'Number of convoluional projection channels in CBHG' ) group . add_argument ( '--cbhg-highway-layers' , default = 4 , type = int , help = 'Number of highway layers in CBHG' ) group . add_argument ( '--cbhg-highway-units' , default = 128 , type = int , help = 'Number of highway units in CBHG' ) group . add_argument ( '--cbhg-gru-units' , default = 256 , type = int , help = 'Number of GRU units in CBHG' ) # model (parameter) related group . add_argument ( '--use-batch-norm' , default = True , type = strtobool , help = 'Whether to use batch normalization' ) group . add_argument ( '--use-concate' , default = True , type = strtobool , help = 'Whether to concatenate encoder embedding with decoder outputs' ) group . add_argument ( '--use-residual' , default = True , type = strtobool , help = 'Whether to use residual connection in conv layer' ) group . add_argument ( '--dropout-rate' , default = 0.5 , type = float , help = 'Dropout rate' ) group . add_argument ( '--zoneout-rate' , default = 0.1 , type = float , help = 'Zoneout rate' ) group . add_argument ( '--reduction-factor' , default = 1 , type = int , help = 'Reduction factor' ) group . add_argument ( \"--spk-embed-dim\" , default = None , type = int , help = \"Number of speaker embedding dimensions\" ) group . add_argument ( \"--spc-dim\" , default = None , type = int , help = \"Number of spectrogram dimensions\" ) group . add_argument ( \"--pretrained-model\" , default = None , type = str , help = \"Pretrained model path\" ) # loss related group . add_argument ( '--use-masking' , default = False , type = strtobool , help = 'Whether to use masking in calculation of loss' ) group . add_argument ( '--use-weighted-masking' , default = False , type = strtobool , help = 'Whether to use weighted masking in calculation of loss' ) group . add_argument ( '--bce-pos-weight' , default = 20.0 , type = float , help = 'Positive sample weight in BCE calculation (only for use-masking=True)' ) group . add_argument ( \"--use-guided-attn-loss\" , default = False , type = strtobool , help = \"Whether to use guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-sigma\" , default = 0.4 , type = float , help = \"Sigma in guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-lambda\" , default = 1.0 , type = float , help = \"Lambda in guided attention loss\" ) return parser calculate_all_attentions ( self , xs , ilens , ys , spembs = None , keep_tensor = False , * args , ** kwargs ) Calculate all of the attention weights. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None keep_tensor bool Whether to keep original tensor. False Returns: Type Description Union[ndarray, Tensor] Batch of attention weights (B, Lmax, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 def calculate_all_attentions ( self , xs , ilens , ys , spembs = None , keep_tensor = False , * args , ** kwargs ): \"\"\"Calculate all of the attention weights. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). keep_tensor (bool, optional): Whether to keep original tensor. Returns: Union[ndarray, Tensor]: Batch of attention weights (B, Lmax, Tmax). \"\"\" # check ilens type (should be list of int) if isinstance ( ilens , torch . Tensor ) or isinstance ( ilens , np . ndarray ): ilens = list ( map ( int , ilens )) self . eval () with torch . no_grad (): hs , hlens = self . enc ( xs , ilens ) if self . spk_embed_dim is not None : spembs = F . normalize ( spembs ) . unsqueeze ( 1 ) . expand ( - 1 , hs . size ( 1 ), - 1 ) hs = torch . cat ([ hs , spembs ], dim =- 1 ) att_ws = self . dec . calculate_all_attentions ( hs , hlens , ys ) self . train () if keep_tensor : return att_ws else : return att_ws . cpu () . numpy () forward ( self , xs , ilens , ys , labels , olens , spembs = None , extras = None , * args , ** kwargs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None extras Tensor Batch of groundtruth spectrograms (B, Lmax, spc_dim). None Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def forward ( self , xs , ilens , ys , labels , olens , spembs = None , extras = None , * args , ** kwargs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). extras (Tensor, optional): Batch of groundtruth spectrograms (B, Lmax, spc_dim). Returns: Tensor: Loss value. \"\"\" # remove unnecessary padded part (for multi-gpus) max_in = max ( ilens ) max_out = max ( olens ) if max_in != xs . shape [ 1 ]: xs = xs [:, : max_in ] if max_out != ys . shape [ 1 ]: ys = ys [:, : max_out ] labels = labels [:, : max_out ] # calculate tacotron2 outputs hs , hlens = self . enc ( xs , ilens ) if self . spk_embed_dim is not None : spembs = F . normalize ( spembs ) . unsqueeze ( 1 ) . expand ( - 1 , hs . size ( 1 ), - 1 ) hs = torch . cat ([ hs , spembs ], dim =- 1 ) after_outs , before_outs , logits , att_ws = self . dec ( hs , hlens , ys ) # modifiy mod part of groundtruth if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) max_out = max ( olens ) ys = ys [:, : max_out ] labels = labels [:, : max_out ] labels [:, - 1 ] = 1.0 # make sure at least one frame has 1 # caluculate taco2 loss l1_loss , mse_loss , bce_loss = self . taco2_loss ( after_outs , before_outs , logits , ys , labels , olens ) loss = l1_loss + mse_loss + bce_loss report_keys = [ { 'l1_loss' : l1_loss . item ()}, { 'mse_loss' : mse_loss . item ()}, { 'bce_loss' : bce_loss . item ()}, ] # caluculate attention loss if self . use_guided_attn_loss : # NOTE(kan-bayashi): length of output for auto-regressive input will be changed when r > 1 if self . reduction_factor > 1 : olens_in = olens . new ([ olen // self . reduction_factor for olen in olens ]) else : olens_in = olens attn_loss = self . attn_loss ( att_ws , ilens , olens_in ) loss = loss + attn_loss report_keys += [ { 'attn_loss' : attn_loss . item ()}, ] # caluculate cbhg loss if self . use_cbhg : # remove unnecessary padded part (for multi-gpus) if max_out != extras . shape [ 1 ]: extras = extras [:, : max_out ] # caluculate cbhg outputs & loss and report them cbhg_outs , _ = self . cbhg ( after_outs , olens ) cbhg_l1_loss , cbhg_mse_loss = self . cbhg_loss ( cbhg_outs , extras , olens ) loss = loss + cbhg_l1_loss + cbhg_mse_loss report_keys += [ { 'cbhg_l1_loss' : cbhg_l1_loss . item ()}, { 'cbhg_mse_loss' : cbhg_mse_loss . item ()}, ] report_keys += [{ 'loss' : loss . item ()}] self . reporter . report ( report_keys ) return loss inference ( self , x , inference_args , spemb = None , * args , ** kwargs ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default x Tensor Input sequence of characters (T,). required inference_args Namespace threshold (float): Threshold in inference. minlenratio (float): Minimum length ratio in inference. maxlenratio (float): Maximum length ratio in inference. required spemb Tensor Speaker embedding vector (spk_embed_dim). None Returns: Type Description Tensor Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 def inference ( self , x , inference_args , spemb = None , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Args: x (Tensor): Input sequence of characters (T,). inference_args (Namespace): - threshold (float): Threshold in inference. - minlenratio (float): Minimum length ratio in inference. - maxlenratio (float): Maximum length ratio in inference. spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim). Returns: Tensor: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). \"\"\" # get options threshold = inference_args . threshold minlenratio = inference_args . minlenratio maxlenratio = inference_args . maxlenratio use_att_constraint = getattr ( inference_args , \"use_att_constraint\" , False ) # keep compatibility backward_window = inference_args . backward_window if use_att_constraint else 0 forward_window = inference_args . forward_window if use_att_constraint else 0 # inference h = self . enc . inference ( x ) if self . spk_embed_dim is not None : spemb = F . normalize ( spemb , dim = 0 ) . unsqueeze ( 0 ) . expand ( h . size ( 0 ), - 1 ) h = torch . cat ([ h , spemb ], dim =- 1 ) outs , probs , att_ws = self . dec . inference ( h , threshold , minlenratio , maxlenratio , use_att_constraint = use_att_constraint , backward_window = backward_window , forward_window = forward_window ) if self . use_cbhg : cbhg_outs = self . cbhg . inference ( outs ) return cbhg_outs , probs , att_ws else : return outs , probs , att_ws","title":"Tacotron2"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss","text":"Loss function module for Tacotron2. __init__ ( self , use_masking = True , use_weighted_masking = False , bce_pos_weight = 20.0 ) special Initialize Tactoron2 loss module. Parameters: Name Type Description Default use_masking bool Whether to apply masking for padded part in loss calculation. True use_weighted_masking bool Whether to apply weighted masking in loss calculation. False bce_pos_weight float Weight of positive sample of stop token. 20.0 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __init__ ( self , use_masking = True , use_weighted_masking = False , bce_pos_weight = 20.0 ): \"\"\"Initialize Tactoron2 loss module. Args: use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. bce_pos_weight (float): Weight of positive sample of stop token. \"\"\" super ( Tacotron2Loss , self ) . __init__ () assert ( use_masking != use_weighted_masking ) or not use_masking self . use_masking = use_masking self . use_weighted_masking = use_weighted_masking # define criterions reduction = \"none\" if self . use_weighted_masking else \"mean\" self . l1_criterion = torch . nn . L1Loss ( reduction = reduction ) self . mse_criterion = torch . nn . MSELoss ( reduction = reduction ) self . bce_criterion = torch . nn . BCEWithLogitsLoss ( reduction = reduction , pos_weight = torch . tensor ( bce_pos_weight )) # NOTE(kan-bayashi): register pre hook function for the compatibility self . _register_load_state_dict_pre_hook ( self . _load_state_dict_pre_hook ) forward ( self , after_outs , before_outs , logits , ys , labels , olens ) Calculate forward propagation. Parameters: Name Type Description Default after_outs Tensor Batch of outputs after postnets (B, Lmax, odim). required before_outs Tensor Batch of outputs before postnets (B, Lmax, odim). required logits Tensor Batch of stop logits (B, Lmax). required ys Tensor Batch of padded target features (B, Lmax, odim). required labels LongTensor Batch of the sequences of stop token labels (B, Lmax). required olens LongTensor Batch of the lengths of each target (B,). required Returns: Type Description Tensor L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_tacotron2.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def forward ( self , after_outs , before_outs , logits , ys , labels , olens ): \"\"\"Calculate forward propagation. Args: after_outs (Tensor): Batch of outputs after postnets (B, Lmax, odim). before_outs (Tensor): Batch of outputs before postnets (B, Lmax, odim). logits (Tensor): Batch of stop logits (B, Lmax). ys (Tensor): Batch of padded target features (B, Lmax, odim). labels (LongTensor): Batch of the sequences of stop token labels (B, Lmax). olens (LongTensor): Batch of the lengths of each target (B,). Returns: Tensor: L1 loss value. Tensor: Mean square error loss value. Tensor: Binary cross entropy loss value. \"\"\" # make mask and apply it if self . use_masking : masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) ys = ys . masked_select ( masks ) after_outs = after_outs . masked_select ( masks ) before_outs = before_outs . masked_select ( masks ) labels = labels . masked_select ( masks [:, :, 0 ]) logits = logits . masked_select ( masks [:, :, 0 ]) # calculate loss l1_loss = self . l1_criterion ( after_outs , ys ) + self . l1_criterion ( before_outs , ys ) mse_loss = self . mse_criterion ( after_outs , ys ) + self . mse_criterion ( before_outs , ys ) bce_loss = self . bce_criterion ( logits , labels ) # make weighted mask and apply it if self . use_weighted_masking : masks = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( ys . device ) weights = masks . float () / masks . sum ( dim = 1 , keepdim = True ) . float () out_weights = weights . div ( ys . size ( 0 ) * ys . size ( 2 )) logit_weights = weights . div ( ys . size ( 0 )) # apply weight l1_loss = l1_loss . mul ( out_weights ) . masked_select ( masks ) . sum () mse_loss = mse_loss . mul ( out_weights ) . masked_select ( masks ) . sum () bce_loss = bce_loss . mul ( logit_weights . squeeze ( - 1 )) . masked_select ( masks . squeeze ( - 1 )) . sum () return l1_loss , mse_loss , bce_loss","title":"Tacotron2Loss"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_transformer","text":"TTS-Transformer related modules.","title":"e2e_tts_transformer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss","text":"Guided attention loss function module for multi head attention. !!! args sigma (float, optional): Standard deviation to control how close attention to a diagonal. alpha (float, optional): Scaling coefficient (lambda). reset_always (bool, optional): Whether to always reset masks. forward ( self , att_ws , ilens , olens ) Calculate forward propagation. Parameters: Name Type Description Default att_ws Tensor Batch of multi head attention weights (B, H, T_max_out, T_max_in). required ilens LongTensor Batch of input lenghts (B,). required olens LongTensor Batch of output lenghts (B,). required Returns: Type Description Tensor Guided attention loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def forward ( self , att_ws , ilens , olens ): \"\"\"Calculate forward propagation. Args: att_ws (Tensor): Batch of multi head attention weights (B, H, T_max_out, T_max_in). ilens (LongTensor): Batch of input lenghts (B,). olens (LongTensor): Batch of output lenghts (B,). Returns: Tensor: Guided attention loss value. \"\"\" if self . guided_attn_masks is None : self . guided_attn_masks = self . _make_guided_attention_masks ( ilens , olens ) . to ( att_ws . device ) . unsqueeze ( 1 ) if self . masks is None : self . masks = self . _make_masks ( ilens , olens ) . to ( att_ws . device ) . unsqueeze ( 1 ) losses = self . guided_attn_masks * att_ws loss = torch . mean ( losses . masked_select ( self . masks )) if self . reset_always : self . _reset_masks () return self . alpha * loss","title":"GuidedMultiHeadAttentionLoss"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_transformer.Transformer","text":"Text-to-Speech Transformer module. This is a module of text - to - speech Transformer described in ` Neural Speech Synthesis with Transformer Network ` _ , which convert the sequence of characters or phonemes into the sequence of Mel - filterbanks . .. _ ` Neural Speech Synthesis with Transformer Network ` : https : // arxiv . org / pdf / 1809 . 08895 . pdf attention_plot_class property readonly Return plot class for attention weight plot. base_plot_keys property readonly Return base key names to plot during training. keys should match what chainer.reporter reports. If you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list List of strings which are base keys to plot during training. __init__ ( self , idim , odim , args = None ) special Initialize TTS-Transformer module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required args Namespace embed_dim (int): Dimension of character embedding. eprenet_conv_layers (int): Number of encoder prenet convolution layers. eprenet_conv_chans (int): Number of encoder prenet convolution channels. eprenet_conv_filts (int): Filter size of encoder prenet convolution. dprenet_layers (int): Number of decoder prenet layers. dprenet_units (int): Number of decoder prenet hidden units. elayers (int): Number of encoder layers. eunits (int): Number of encoder hidden units. adim (int): Number of attention transformation dimensions. aheads (int): Number of heads for multi head attention. dlayers (int): Number of decoder layers. dunits (int): Number of decoder hidden units. postnet_layers (int): Number of postnet layers. postnet_chans (int): Number of postnet channels. postnet_filts (int): Filter size of postnet. use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. use_batch_norm (bool): Whether to use batch normalization in encoder prenet. encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. reduction_factor (int): Reduction factor. spk_embed_dim (int): Number of speaker embedding dimenstions. spk_embed_integration_type: How to integrate speaker embedding. transformer_init (float): How to initialize transformer parameters. transformer_lr (float): Initial value of learning rate. transformer_warmup_steps (int): Optimizer warmup steps. transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. eprenet_dropout_rate (float): Dropout rate in encoder prenet. dprenet_dropout_rate (float): Dropout rate in decoder prenet. postnet_dropout_rate (float): Dropout rate in postnet. use_masking (bool): Whether to apply masking for padded part in loss calculation. use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. bce_pos_weight (float): Positive sample weight in bce calculation (only for use_masking=true). loss_type (str): How to calculate loss. use_guided_attn_loss (bool): Whether to use guided attention loss. num_heads_applied_guided_attn (int): Number of heads in each layer to apply guided attention loss. num_layers_applied_guided_attn (int): Number of layers to apply guided attention loss. modules_applied_guided_attn (list): List of module names to apply guided attention loss. guided-attn-loss-sigma (float) Sigma in guided attention loss. guided-attn-loss-lambda (float): Lambda in guided attention loss. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def __init__ ( self , idim , odim , args = None ): \"\"\"Initialize TTS-Transformer module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. args (Namespace, optional): - embed_dim (int): Dimension of character embedding. - eprenet_conv_layers (int): Number of encoder prenet convolution layers. - eprenet_conv_chans (int): Number of encoder prenet convolution channels. - eprenet_conv_filts (int): Filter size of encoder prenet convolution. - dprenet_layers (int): Number of decoder prenet layers. - dprenet_units (int): Number of decoder prenet hidden units. - elayers (int): Number of encoder layers. - eunits (int): Number of encoder hidden units. - adim (int): Number of attention transformation dimensions. - aheads (int): Number of heads for multi head attention. - dlayers (int): Number of decoder layers. - dunits (int): Number of decoder hidden units. - postnet_layers (int): Number of postnet layers. - postnet_chans (int): Number of postnet channels. - postnet_filts (int): Filter size of postnet. - use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding. - use_batch_norm (bool): Whether to use batch normalization in encoder prenet. - encoder_normalize_before (bool): Whether to perform layer normalization before encoder block. - decoder_normalize_before (bool): Whether to perform layer normalization before decoder block. - encoder_concat_after (bool): Whether to concatenate attention layer's input and output in encoder. - decoder_concat_after (bool): Whether to concatenate attention layer's input and output in decoder. - reduction_factor (int): Reduction factor. - spk_embed_dim (int): Number of speaker embedding dimenstions. - spk_embed_integration_type: How to integrate speaker embedding. - transformer_init (float): How to initialize transformer parameters. - transformer_lr (float): Initial value of learning rate. - transformer_warmup_steps (int): Optimizer warmup steps. - transformer_enc_dropout_rate (float): Dropout rate in encoder except attention & positional encoding. - transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding. - transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module. - transformer_dec_dropout_rate (float): Dropout rate in decoder except attention & positional encoding. - transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding. - transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module. - transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module. - eprenet_dropout_rate (float): Dropout rate in encoder prenet. - dprenet_dropout_rate (float): Dropout rate in decoder prenet. - postnet_dropout_rate (float): Dropout rate in postnet. - use_masking (bool): Whether to apply masking for padded part in loss calculation. - use_weighted_masking (bool): Whether to apply weighted masking in loss calculation. - bce_pos_weight (float): Positive sample weight in bce calculation (only for use_masking=true). - loss_type (str): How to calculate loss. - use_guided_attn_loss (bool): Whether to use guided attention loss. - num_heads_applied_guided_attn (int): Number of heads in each layer to apply guided attention loss. - num_layers_applied_guided_attn (int): Number of layers to apply guided attention loss. - modules_applied_guided_attn (list): List of module names to apply guided attention loss. - guided-attn-loss-sigma (float) Sigma in guided attention loss. - guided-attn-loss-lambda (float): Lambda in guided attention loss. \"\"\" # initialize base classes TTSInterface . __init__ ( self ) torch . nn . Module . __init__ ( self ) # fill missing arguments args = fill_missing_args ( args , self . add_arguments ) # store hyperparameters self . idim = idim self . odim = odim self . spk_embed_dim = args . spk_embed_dim if self . spk_embed_dim is not None : self . spk_embed_integration_type = args . spk_embed_integration_type self . use_scaled_pos_enc = args . use_scaled_pos_enc self . reduction_factor = args . reduction_factor self . loss_type = args . loss_type self . use_guided_attn_loss = args . use_guided_attn_loss if self . use_guided_attn_loss : if args . num_layers_applied_guided_attn == - 1 : self . num_layers_applied_guided_attn = args . elayers else : self . num_layers_applied_guided_attn = args . num_layers_applied_guided_attn if args . num_heads_applied_guided_attn == - 1 : self . num_heads_applied_guided_attn = args . aheads else : self . num_heads_applied_guided_attn = args . num_heads_applied_guided_attn self . modules_applied_guided_attn = args . modules_applied_guided_attn # use idx 0 as padding idx padding_idx = 0 # get positional encoding class pos_enc_class = ScaledPositionalEncoding if self . use_scaled_pos_enc else PositionalEncoding # define transformer encoder if args . eprenet_conv_layers != 0 : # encoder prenet encoder_input_layer = torch . nn . Sequential ( EncoderPrenet ( idim = idim , embed_dim = args . embed_dim , elayers = 0 , econv_layers = args . eprenet_conv_layers , econv_chans = args . eprenet_conv_chans , econv_filts = args . eprenet_conv_filts , use_batch_norm = args . use_batch_norm , dropout_rate = args . eprenet_dropout_rate , padding_idx = padding_idx ), torch . nn . Linear ( args . eprenet_conv_chans , args . adim ) ) else : encoder_input_layer = torch . nn . Embedding ( num_embeddings = idim , embedding_dim = args . adim , padding_idx = padding_idx ) self . encoder = Encoder ( idim = idim , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . eunits , num_blocks = args . elayers , input_layer = encoder_input_layer , dropout_rate = args . transformer_enc_dropout_rate , positional_dropout_rate = args . transformer_enc_positional_dropout_rate , attention_dropout_rate = args . transformer_enc_attn_dropout_rate , pos_enc_class = pos_enc_class , normalize_before = args . encoder_normalize_before , concat_after = args . encoder_concat_after , positionwise_layer_type = args . positionwise_layer_type , positionwise_conv_kernel_size = args . positionwise_conv_kernel_size , ) # define projection layer if self . spk_embed_dim is not None : if self . spk_embed_integration_type == \"add\" : self . projection = torch . nn . Linear ( self . spk_embed_dim , args . adim ) else : self . projection = torch . nn . Linear ( args . adim + self . spk_embed_dim , args . adim ) # define transformer decoder if args . dprenet_layers != 0 : # decoder prenet decoder_input_layer = torch . nn . Sequential ( DecoderPrenet ( idim = odim , n_layers = args . dprenet_layers , n_units = args . dprenet_units , dropout_rate = args . dprenet_dropout_rate ), torch . nn . Linear ( args . dprenet_units , args . adim ) ) else : decoder_input_layer = \"linear\" self . decoder = Decoder ( odim =- 1 , attention_dim = args . adim , attention_heads = args . aheads , linear_units = args . dunits , num_blocks = args . dlayers , dropout_rate = args . transformer_dec_dropout_rate , positional_dropout_rate = args . transformer_dec_positional_dropout_rate , self_attention_dropout_rate = args . transformer_dec_attn_dropout_rate , src_attention_dropout_rate = args . transformer_enc_dec_attn_dropout_rate , input_layer = decoder_input_layer , use_output_layer = False , pos_enc_class = pos_enc_class , normalize_before = args . decoder_normalize_before , concat_after = args . decoder_concat_after ) # define final projection self . feat_out = torch . nn . Linear ( args . adim , odim * args . reduction_factor ) self . prob_out = torch . nn . Linear ( args . adim , args . reduction_factor ) # define postnet self . postnet = None if args . postnet_layers == 0 else Postnet ( idim = idim , odim = odim , n_layers = args . postnet_layers , n_chans = args . postnet_chans , n_filts = args . postnet_filts , use_batch_norm = args . use_batch_norm , dropout_rate = args . postnet_dropout_rate ) # define loss function self . criterion = TransformerLoss ( use_masking = args . use_masking , use_weighted_masking = args . use_weighted_masking , bce_pos_weight = args . bce_pos_weight ) if self . use_guided_attn_loss : self . attn_criterion = GuidedMultiHeadAttentionLoss ( sigma = args . guided_attn_loss_sigma , alpha = args . guided_attn_loss_lambda , ) # initialize parameters self . _reset_parameters ( init_type = args . transformer_init , init_enc_alpha = args . initial_encoder_alpha , init_dec_alpha = args . initial_decoder_alpha ) # load pretrained model if args . pretrained_model is not None : self . load_pretrained_model ( args . pretrained_model ) add_arguments ( parser ) staticmethod Add model-specific arguments to the parser. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 @staticmethod def add_arguments ( parser ): \"\"\"Add model-specific arguments to the parser.\"\"\" group = parser . add_argument_group ( \"transformer model setting\" ) # network structure related group . add_argument ( \"--embed-dim\" , default = 512 , type = int , help = \"Dimension of character embedding in encoder prenet\" ) group . add_argument ( \"--eprenet-conv-layers\" , default = 3 , type = int , help = \"Number of encoder prenet convolution layers\" ) group . add_argument ( \"--eprenet-conv-chans\" , default = 256 , type = int , help = \"Number of encoder prenet convolution channels\" ) group . add_argument ( \"--eprenet-conv-filts\" , default = 5 , type = int , help = \"Filter size of encoder prenet convolution\" ) group . add_argument ( \"--dprenet-layers\" , default = 2 , type = int , help = \"Number of decoder prenet layers\" ) group . add_argument ( \"--dprenet-units\" , default = 256 , type = int , help = \"Number of decoder prenet hidden units\" ) group . add_argument ( \"--elayers\" , default = 3 , type = int , help = \"Number of encoder layers\" ) group . add_argument ( \"--eunits\" , default = 1536 , type = int , help = \"Number of encoder hidden units\" ) group . add_argument ( \"--adim\" , default = 384 , type = int , help = \"Number of attention transformation dimensions\" ) group . add_argument ( \"--aheads\" , default = 4 , type = int , help = \"Number of heads for multi head attention\" ) group . add_argument ( \"--dlayers\" , default = 3 , type = int , help = \"Number of decoder layers\" ) group . add_argument ( \"--dunits\" , default = 1536 , type = int , help = \"Number of decoder hidden units\" ) group . add_argument ( \"--positionwise-layer-type\" , default = \"linear\" , type = str , choices = [ \"linear\" , \"conv1d\" , \"conv1d-linear\" ], help = \"Positionwise layer type.\" ) group . add_argument ( \"--positionwise-conv-kernel-size\" , default = 1 , type = int , help = \"Kernel size of positionwise conv1d layer\" ) group . add_argument ( \"--postnet-layers\" , default = 5 , type = int , help = \"Number of postnet layers\" ) group . add_argument ( \"--postnet-chans\" , default = 256 , type = int , help = \"Number of postnet channels\" ) group . add_argument ( \"--postnet-filts\" , default = 5 , type = int , help = \"Filter size of postnet\" ) group . add_argument ( \"--use-scaled-pos-enc\" , default = True , type = strtobool , help = \"Use trainable scaled positional encoding instead of the fixed scale one.\" ) group . add_argument ( \"--use-batch-norm\" , default = True , type = strtobool , help = \"Whether to use batch normalization\" ) group . add_argument ( \"--encoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before encoder block\" ) group . add_argument ( \"--decoder-normalize-before\" , default = False , type = strtobool , help = \"Whether to apply layer norm before decoder block\" ) group . add_argument ( \"--encoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in encoder\" ) group . add_argument ( \"--decoder-concat-after\" , default = False , type = strtobool , help = \"Whether to concatenate attention layer's input and output in decoder\" ) group . add_argument ( \"--reduction-factor\" , default = 1 , type = int , help = \"Reduction factor\" ) group . add_argument ( \"--spk-embed-dim\" , default = None , type = int , help = \"Number of speaker embedding dimensions\" ) group . add_argument ( \"--spk-embed-integration-type\" , type = str , default = \"add\" , choices = [ \"add\" , \"concat\" ], help = \"How to integrate speaker embedding\" ) # training related group . add_argument ( \"--transformer-init\" , type = str , default = \"pytorch\" , choices = [ \"pytorch\" , \"xavier_uniform\" , \"xavier_normal\" , \"kaiming_uniform\" , \"kaiming_normal\" ], help = \"How to initialize transformer parameters\" ) group . add_argument ( \"--initial-encoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in encoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--initial-decoder-alpha\" , type = float , default = 1.0 , help = \"Initial alpha value in decoder's ScaledPositionalEncoding\" ) group . add_argument ( \"--transformer-lr\" , default = 1.0 , type = float , help = \"Initial value of learning rate\" ) group . add_argument ( \"--transformer-warmup-steps\" , default = 4000 , type = int , help = \"Optimizer warmup steps\" ) group . add_argument ( \"--transformer-enc-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder except for attention\" ) group . add_argument ( \"--transformer-enc-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder positional encoding\" ) group . add_argument ( \"--transformer-enc-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder self-attention\" ) group . add_argument ( \"--transformer-dec-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder except for attention and pos encoding\" ) group . add_argument ( \"--transformer-dec-positional-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder positional encoding\" ) group . add_argument ( \"--transformer-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer decoder self-attention\" ) group . add_argument ( \"--transformer-enc-dec-attn-dropout-rate\" , default = 0.1 , type = float , help = \"Dropout rate for transformer encoder-decoder attention\" ) group . add_argument ( \"--eprenet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in encoder prenet\" ) group . add_argument ( \"--dprenet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in decoder prenet\" ) group . add_argument ( \"--postnet-dropout-rate\" , default = 0.5 , type = float , help = \"Dropout rate in postnet\" ) group . add_argument ( \"--pretrained-model\" , default = None , type = str , help = \"Pretrained model path\" ) # loss related group . add_argument ( \"--use-masking\" , default = True , type = strtobool , help = \"Whether to use masking in calculation of loss\" ) group . add_argument ( \"--use-weighted-masking\" , default = False , type = strtobool , help = \"Whether to use weighted masking in calculation of loss\" ) group . add_argument ( \"--loss-type\" , default = \"L1\" , choices = [ \"L1\" , \"L2\" , \"L1+L2\" ], help = \"How to calc loss\" ) group . add_argument ( \"--bce-pos-weight\" , default = 5.0 , type = float , help = \"Positive sample weight in BCE calculation (only for use-masking=True)\" ) group . add_argument ( \"--use-guided-attn-loss\" , default = False , type = strtobool , help = \"Whether to use guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-sigma\" , default = 0.4 , type = float , help = \"Sigma in guided attention loss\" ) group . add_argument ( \"--guided-attn-loss-lambda\" , default = 1.0 , type = float , help = \"Lambda in guided attention loss\" ) group . add_argument ( \"--num-heads-applied-guided-attn\" , default = 2 , type = int , help = \"Number of heads in each layer to be applied guided attention loss\" \"if set -1, all of the heads will be applied.\" ) group . add_argument ( \"--num-layers-applied-guided-attn\" , default = 2 , type = int , help = \"Number of layers to be applied guided attention loss\" \"if set -1, all of the layers will be applied.\" ) group . add_argument ( \"--modules-applied-guided-attn\" , type = str , nargs = \"+\" , default = [ \"encoder-decoder\" ], help = \"Module name list to be applied guided attention loss\" ) return parser calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , skip_output = False , keep_tensor = False , * args , ** kwargs ) Calculate all of the attention weights. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None skip_output bool Whether to skip calculate the final output. False keep_tensor bool Whether to keep original tensor. False Returns: Type Description dict Dict of attention weights and outputs. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def calculate_all_attentions ( self , xs , ilens , ys , olens , spembs = None , skip_output = False , keep_tensor = False , * args , ** kwargs ): \"\"\"Calculate all of the attention weights. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). skip_output (bool, optional): Whether to skip calculate the final output. keep_tensor (bool, optional): Whether to keep original tensor. Returns: dict: Dict of attention weights and outputs. \"\"\" with torch . no_grad (): # forward encoder x_masks = self . _source_mask ( ilens ) hs , _ = self . encoder ( xs , x_masks ) # integrate speaker embedding if self . spk_embed_dim is not None : hs = self . _integrate_with_spk_embed ( hs , spembs ) # thin out frames for reduction factor (B, Lmax, odim) -> (B, Lmax//r, odim) if self . reduction_factor > 1 : ys_in = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] olens_in = olens . new ([ olen // self . reduction_factor for olen in olens ]) else : ys_in , olens_in = ys , olens # add first zero frame and remove last frame for auto-regressive ys_in = self . _add_first_frame_and_remove_last_frame ( ys_in ) # forward decoder y_masks = self . _target_mask ( olens_in ) xy_masks = self . _source_to_target_mask ( ilens , olens_in ) zs , _ = self . decoder ( ys_in , y_masks , hs , xy_masks ) # calculate final outputs if not skip_output : before_outs = self . feat_out ( zs ) . view ( zs . size ( 0 ), - 1 , self . odim ) if self . postnet is None : after_outs = before_outs else : after_outs = before_outs + self . postnet ( before_outs . transpose ( 1 , 2 )) . transpose ( 1 , 2 ) # modifiy mod part of output lengths due to reduction factor > 1 if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) # store into dict att_ws_dict = dict () if keep_tensor : for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): att_ws_dict [ name ] = m . attn if not skip_output : att_ws_dict [ \"before_postnet_fbank\" ] = before_outs att_ws_dict [ \"after_postnet_fbank\" ] = after_outs else : for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ): attn = m . attn . cpu () . numpy () if \"encoder\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , ilens . tolist ())] elif \"decoder\" in name : if \"src\" in name : attn = [ a [:, : ol , : il ] for a , il , ol in zip ( attn , ilens . tolist (), olens_in . tolist ())] elif \"self\" in name : attn = [ a [:, : l , : l ] for a , l in zip ( attn , olens_in . tolist ())] else : logging . warning ( \"unknown attention module: \" + name ) else : logging . warning ( \"unknown attention module: \" + name ) att_ws_dict [ name ] = attn if not skip_output : before_outs = before_outs . cpu () . numpy () after_outs = after_outs . cpu () . numpy () att_ws_dict [ \"before_postnet_fbank\" ] = [ m [: l ] . T for m , l in zip ( before_outs , olens . tolist ())] att_ws_dict [ \"after_postnet_fbank\" ] = [ m [: l ] . T for m , l in zip ( after_outs , olens . tolist ())] return att_ws_dict forward ( self , xs , ilens , ys , labels , olens , spembs = None , * args , ** kwargs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of padded character ids (B, Tmax). required ilens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of padded target features (B, Lmax, odim). required olens LongTensor Batch of the lengths of each target (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def forward ( self , xs , ilens , ys , labels , olens , spembs = None , * args , ** kwargs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of padded character ids (B, Tmax). ilens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of padded target features (B, Lmax, odim). olens (LongTensor): Batch of the lengths of each target (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). Returns: Tensor: Loss value. \"\"\" # remove unnecessary padded part (for multi-gpus) max_ilen = max ( ilens ) max_olen = max ( olens ) if max_ilen != xs . shape [ 1 ]: xs = xs [:, : max_ilen ] if max_olen != ys . shape [ 1 ]: ys = ys [:, : max_olen ] labels = labels [:, : max_olen ] # forward encoder x_masks = self . _source_mask ( ilens ) hs , _ = self . encoder ( xs , x_masks ) # integrate speaker embedding if self . spk_embed_dim is not None : hs = self . _integrate_with_spk_embed ( hs , spembs ) # thin out frames for reduction factor (B, Lmax, odim) -> (B, Lmax//r, odim) if self . reduction_factor > 1 : ys_in = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] olens_in = olens . new ([ olen // self . reduction_factor for olen in olens ]) else : ys_in , olens_in = ys , olens # add first zero frame and remove last frame for auto-regressive ys_in = self . _add_first_frame_and_remove_last_frame ( ys_in ) # forward decoder y_masks = self . _target_mask ( olens_in ) xy_masks = self . _source_to_target_mask ( ilens , olens_in ) zs , _ = self . decoder ( ys_in , y_masks , hs , xy_masks ) # (B, Lmax//r, odim * r) -> (B, Lmax//r * r, odim) before_outs = self . feat_out ( zs ) . view ( zs . size ( 0 ), - 1 , self . odim ) # (B, Lmax//r, r) -> (B, Lmax//r * r) logits = self . prob_out ( zs ) . view ( zs . size ( 0 ), - 1 ) # postnet -> (B, Lmax//r * r, odim) if self . postnet is None : after_outs = before_outs else : after_outs = before_outs + self . postnet ( before_outs . transpose ( 1 , 2 )) . transpose ( 1 , 2 ) # modifiy mod part of groundtruth if self . reduction_factor > 1 : olens = olens . new ([ olen - olen % self . reduction_factor for olen in olens ]) max_olen = max ( olens ) ys = ys [:, : max_olen ] labels = labels [:, : max_olen ] labels [:, - 1 ] = 1.0 # make sure at least one frame has 1 # caluculate loss values l1_loss , l2_loss , bce_loss = self . criterion ( after_outs , before_outs , logits , ys , labels , olens ) if self . loss_type == \"L1\" : loss = l1_loss + bce_loss elif self . loss_type == \"L2\" : loss = l2_loss + bce_loss elif self . loss_type == \"L1+L2\" : loss = l1_loss + l2_loss + bce_loss else : raise ValueError ( \"unknown --loss-type \" + self . loss_type ) report_keys = [ { \"l1_loss\" : l1_loss . item ()}, { \"l2_loss\" : l2_loss . item ()}, { \"bce_loss\" : bce_loss . item ()}, { \"loss\" : loss . item ()}, ] # calculate guided attention loss if self . use_guided_attn_loss : # calculate for encoder if \"encoder\" in self . modules_applied_guided_attn : att_ws = [] for idx , layer_idx in enumerate ( reversed ( range ( len ( self . encoder . encoders )))): att_ws += [ self . encoder . encoders [ layer_idx ] . self_attn . attn [:, : self . num_heads_applied_guided_attn ]] if idx + 1 == self . num_layers_applied_guided_attn : break att_ws = torch . cat ( att_ws , dim = 1 ) # (B, H*L, T_in, T_in) enc_attn_loss = self . attn_criterion ( att_ws , ilens , ilens ) loss = loss + enc_attn_loss report_keys += [{ \"enc_attn_loss\" : enc_attn_loss . item ()}] # calculate for decoder if \"decoder\" in self . modules_applied_guided_attn : att_ws = [] for idx , layer_idx in enumerate ( reversed ( range ( len ( self . decoder . decoders )))): att_ws += [ self . decoder . decoders [ layer_idx ] . self_attn . attn [:, : self . num_heads_applied_guided_attn ]] if idx + 1 == self . num_layers_applied_guided_attn : break att_ws = torch . cat ( att_ws , dim = 1 ) # (B, H*L, T_out, T_out) dec_attn_loss = self . attn_criterion ( att_ws , olens_in , olens_in ) loss = loss + dec_attn_loss report_keys += [{ \"dec_attn_loss\" : dec_attn_loss . item ()}] # calculate for encoder-decoder if \"encoder-decoder\" in self . modules_applied_guided_attn : att_ws = [] for idx , layer_idx in enumerate ( reversed ( range ( len ( self . decoder . decoders )))): att_ws += [ self . decoder . decoders [ layer_idx ] . src_attn . attn [:, : self . num_heads_applied_guided_attn ]] if idx + 1 == self . num_layers_applied_guided_attn : break att_ws = torch . cat ( att_ws , dim = 1 ) # (B, H*L, T_out, T_in) enc_dec_attn_loss = self . attn_criterion ( att_ws , ilens , olens_in ) loss = loss + enc_dec_attn_loss report_keys += [{ \"enc_dec_attn_loss\" : enc_dec_attn_loss . item ()}] # report extra information if self . use_scaled_pos_enc : report_keys += [ { \"encoder_alpha\" : self . encoder . embed [ - 1 ] . alpha . data . item ()}, { \"decoder_alpha\" : self . decoder . embed [ - 1 ] . alpha . data . item ()}, ] self . reporter . report ( report_keys ) return loss inference ( self , x , inference_args , spemb = None , * args , ** kwargs ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default x Tensor Input sequence of characters (T,). required inference_args Namespace threshold (float): Threshold in inference. minlenratio (float): Minimum length ratio in inference. maxlenratio (float): Maximum length ratio in inference. required spemb Tensor Speaker embedding vector (spk_embed_dim). None Returns: Type Description Tensor Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Encoder-decoder (source) attention weights (#layers, #heads, L, T). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def inference ( self , x , inference_args , spemb = None , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Args: x (Tensor): Input sequence of characters (T,). inference_args (Namespace): - threshold (float): Threshold in inference. - minlenratio (float): Minimum length ratio in inference. - maxlenratio (float): Maximum length ratio in inference. spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim). Returns: Tensor: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Encoder-decoder (source) attention weights (#layers, #heads, L, T). \"\"\" # get options threshold = inference_args . threshold minlenratio = inference_args . minlenratio maxlenratio = inference_args . maxlenratio use_att_constraint = getattr ( inference_args , \"use_att_constraint\" , False ) # keep compatibility if use_att_constraint : logging . warning ( \"Attention constraint is not yet supported in Transformer. Not enabled.\" ) # forward encoder xs = x . unsqueeze ( 0 ) hs , _ = self . encoder ( xs , None ) # integrate speaker embedding if self . spk_embed_dim is not None : spembs = spemb . unsqueeze ( 0 ) hs = self . _integrate_with_spk_embed ( hs , spembs ) # set limits of length maxlen = int ( hs . size ( 1 ) * maxlenratio / self . reduction_factor ) minlen = int ( hs . size ( 1 ) * minlenratio / self . reduction_factor ) # initialize idx = 0 ys = hs . new_zeros ( 1 , 1 , self . odim ) outs , probs = [], [] # forward decoder step-by-step z_cache = self . decoder . init_state ( x ) while True : # update index idx += 1 # calculate output and stop prob at idx-th step y_masks = subsequent_mask ( idx ) . unsqueeze ( 0 ) . to ( x . device ) z , z_cache = self . decoder . forward_one_step ( ys , y_masks , hs , cache = z_cache ) # (B, adim) outs += [ self . feat_out ( z ) . view ( self . reduction_factor , self . odim )] # [(r, odim), ...] probs += [ torch . sigmoid ( self . prob_out ( z ))[ 0 ]] # [(r), ...] # update next inputs ys = torch . cat (( ys , outs [ - 1 ][ - 1 ] . view ( 1 , 1 , self . odim )), dim = 1 ) # (1, idx + 1, odim) # get attention weights att_ws_ = [] for name , m in self . named_modules (): if isinstance ( m , MultiHeadedAttention ) and \"src\" in name : att_ws_ += [ m . attn [ 0 , :, - 1 ] . unsqueeze ( 1 )] # [(#heads, 1, T),...] if idx == 1 : att_ws = att_ws_ else : # [(#heads, l, T), ...] att_ws = [ torch . cat ([ att_w , att_w_ ], dim = 1 ) for att_w , att_w_ in zip ( att_ws , att_ws_ )] # check whether to finish generation if int ( sum ( probs [ - 1 ] >= threshold )) > 0 or idx >= maxlen : # check mininum length if idx < minlen : continue outs = torch . cat ( outs , dim = 0 ) . unsqueeze ( 0 ) . transpose ( 1 , 2 ) # (L, odim) -> (1, L, odim) -> (1, odim, L) if self . postnet is not None : outs = outs + self . postnet ( outs ) # (1, odim, L) outs = outs . transpose ( 2 , 1 ) . squeeze ( 0 ) # (L, odim) probs = torch . cat ( probs , dim = 0 ) break # concatenate attention weights -> (#layers, #heads, L, T) att_ws = torch . stack ( att_ws , dim = 0 ) return outs , probs , att_ws","title":"Transformer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.e2e_tts_transformer.TTSPlot","text":"Attention plot module for TTS-Transformer. plotfn ( self , data , attn_dict , outdir , suffix = 'png' , savefn = None ) Plot multi head attentions. Parameters: Name Type Description Default data dict Utts info from json file. required attn_dict dict Multi head attention dict. Values should be numpy.ndarray (H, L, T) required outdir str Directory name to save figures. required suffix str Filename suffix including image type (e.g., png). 'png' savefn function Function to save figures. None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/e2e_tts_transformer.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def plotfn ( self , data , attn_dict , outdir , suffix = \"png\" , savefn = None ): \"\"\"Plot multi head attentions. Args: data (dict): Utts info from json file. attn_dict (dict): Multi head attention dict. Values should be numpy.ndarray (H, L, T) outdir (str): Directory name to save figures. suffix (str): Filename suffix including image type (e.g., png). savefn (function): Function to save figures. \"\"\" import matplotlib.pyplot as plt for name , att_ws in attn_dict . items (): for idx , att_w in enumerate ( att_ws ): filename = \" %s / %s . %s . %s \" % ( outdir , data [ idx ][ 0 ], name , suffix ) if \"fbank\" in name : fig = plt . Figure () ax = fig . subplots ( 1 , 1 ) ax . imshow ( att_w , aspect = \"auto\" ) ax . set_xlabel ( \"frames\" ) ax . set_ylabel ( \"fbank coeff\" ) fig . tight_layout () else : fig = _plot_and_save_attention ( att_w , filename ) savefn ( fig , filename )","title":"TTSPlot"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.fastspeech","text":"","title":"fastspeech"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.fastspeech.duration_calculator","text":"Duration calculator related modules. DurationCalculator Duration calculator module for FastSpeech. !!! todo * Fix the duplicated calculation of diagonal head decision __init__ ( self , teacher_model ) special Initialize duration calculator module. Parameters: Name Type Description Default teacher_model e2e_tts_transformer.Transformer Pretrained auto-regressive Transformer. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_calculator.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , teacher_model ): \"\"\"Initialize duration calculator module. Args: teacher_model (e2e_tts_transformer.Transformer): Pretrained auto-regressive Transformer. \"\"\" super ( DurationCalculator , self ) . __init__ () if isinstance ( teacher_model , Transformer ): self . register_buffer ( \"diag_head_idx\" , torch . tensor ( - 1 )) elif isinstance ( teacher_model , Tacotron2 ): pass else : raise ValueError ( \"teacher model should be the instance of e2e_tts_transformer.Transformer \" \"or e2e_tts_tacotron2.Tacotron2.\" ) self . teacher_model = teacher_model forward ( self , xs , ilens , ys , olens , spembs = None ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the padded sequences of character ids (B, Tmax). required ilens Tensor Batch of lengths of each input sequence (B,). required ys Tensor Batch of the padded sequence of target features (B, Lmax, odim). required olens Tensor Batch of lengths of each output sequence (B,). required spembs Tensor Batch of speaker embedding vectors (B, spk_embed_dim). None Returns: Type Description Tensor Batch of durations (B, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_calculator.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def forward ( self , xs , ilens , ys , olens , spembs = None ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the padded sequences of character ids (B, Tmax). ilens (Tensor): Batch of lengths of each input sequence (B,). ys (Tensor): Batch of the padded sequence of target features (B, Lmax, odim). olens (Tensor): Batch of lengths of each output sequence (B,). spembs (Tensor, optional): Batch of speaker embedding vectors (B, spk_embed_dim). Returns: Tensor: Batch of durations (B, Tmax). \"\"\" if isinstance ( self . teacher_model , Transformer ): att_ws = self . _calculate_encoder_decoder_attentions ( xs , ilens , ys , olens , spembs = spembs ) # TODO(kan-bayashi): fix this issue # this does not work in multi-gpu case. registered buffer is not saved. if int ( self . diag_head_idx ) == - 1 : self . _init_diagonal_head ( att_ws ) att_ws = att_ws [:, self . diag_head_idx ] else : # NOTE(kan-bayashi): Here we assume that the teacher is tacotron 2 att_ws = self . teacher_model . calculate_all_attentions ( xs , ilens , ys , spembs = spembs , keep_tensor = True ) durations = [ self . _calculate_duration ( att_w , ilen , olen ) for att_w , ilen , olen in zip ( att_ws , ilens , olens )] return pad_list ( durations , 0 )","title":"duration_calculator"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.fastspeech.duration_predictor","text":"Duration predictor related modules. DurationPredictor Duration predictor module. This is a module of duration predictor described in `FastSpeech: Fast, Robust and Controllable Text to Speech` _ . The duration predictor predicts a duration of each frame in log domain from the hidden embeddings of encoder . .. _ `FastSpeech: Fast, Robust and Controllable Text to Speech` : https : // arxiv . org / pdf / 1905 . 09263 . pdf !!! note The calculation domain of outputs is different between in `forward` and in `inference` . In `forward` , the outputs are calculated in log domain but in `inference` , those are calculated in linear domain . __init__ ( self , idim , n_layers = 2 , n_chans = 384 , kernel_size = 3 , dropout_rate = 0.1 , offset = 1.0 ) special Initilize duration predictor module. Parameters: Name Type Description Default idim int Input dimension. required n_layers int Number of convolutional layers. 2 n_chans int Number of channels of convolutional layers. 384 kernel_size int Kernel size of convolutional layers. 3 dropout_rate float Dropout rate. 0.1 offset float Offset value to avoid nan in log domain. 1.0 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , idim , n_layers = 2 , n_chans = 384 , kernel_size = 3 , dropout_rate = 0.1 , offset = 1.0 ): \"\"\"Initilize duration predictor module. Args: idim (int): Input dimension. n_layers (int, optional): Number of convolutional layers. n_chans (int, optional): Number of channels of convolutional layers. kernel_size (int, optional): Kernel size of convolutional layers. dropout_rate (float, optional): Dropout rate. offset (float, optional): Offset value to avoid nan in log domain. \"\"\" super ( DurationPredictor , self ) . __init__ () self . offset = offset self . conv = torch . nn . ModuleList () for idx in range ( n_layers ): in_chans = idim if idx == 0 else n_chans self . conv += [ torch . nn . Sequential ( torch . nn . Conv1d ( in_chans , n_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ), torch . nn . ReLU (), LayerNorm ( n_chans , dim = 1 ), torch . nn . Dropout ( dropout_rate ) )] self . linear = torch . nn . Linear ( n_chans , 1 ) forward ( self , xs , x_masks = None ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of input sequences (B, Tmax, idim). required x_masks ByteTensor Batch of masks indicating padded part (B, Tmax). None Returns: Type Description Tensor Batch of predicted durations in log domain (B, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 71 72 73 74 75 76 77 78 79 80 81 82 def forward ( self , xs , x_masks = None ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of input sequences (B, Tmax, idim). x_masks (ByteTensor, optional): Batch of masks indicating padded part (B, Tmax). Returns: Tensor: Batch of predicted durations in log domain (B, Tmax). \"\"\" return self . _forward ( xs , x_masks , False ) inference ( self , xs , x_masks = None ) Inference duration. Parameters: Name Type Description Default xs Tensor Batch of input sequences (B, Tmax, idim). required x_masks ByteTensor Batch of masks indicating padded part (B, Tmax). None Returns: Type Description LongTensor Batch of predicted durations in linear domain (B, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 84 85 86 87 88 89 90 91 92 93 94 95 def inference ( self , xs , x_masks = None ): \"\"\"Inference duration. Args: xs (Tensor): Batch of input sequences (B, Tmax, idim). x_masks (ByteTensor, optional): Batch of masks indicating padded part (B, Tmax). Returns: LongTensor: Batch of predicted durations in linear domain (B, Tmax). \"\"\" return self . _forward ( xs , x_masks , True ) DurationPredictorLoss Loss function module for duration predictor. The loss value is Calculated in log domain to make it Gaussian. __init__ ( self , offset = 1.0 , reduction = 'mean' ) special Initilize duration predictor loss module. Parameters: Name Type Description Default offset float Offset value to avoid nan in log domain. 1.0 reduction str Reduction type in loss calculation. 'mean' Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , offset = 1.0 , reduction = \"mean\" ): \"\"\"Initilize duration predictor loss module. Args: offset (float, optional): Offset value to avoid nan in log domain. reduction (str): Reduction type in loss calculation. \"\"\" super ( DurationPredictorLoss , self ) . __init__ () self . criterion = torch . nn . MSELoss ( reduction = reduction ) self . offset = offset forward ( self , outputs , targets ) Calculate forward propagation. Parameters: Name Type Description Default outputs Tensor Batch of prediction durations in log domain (B, T) required targets LongTensor Batch of groundtruth durations in linear domain (B, T) required Returns: Type Description Tensor Mean squared error loss value. Note outputs is in log domain but targets is in linear domain. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/duration_predictor.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def forward ( self , outputs , targets ): \"\"\"Calculate forward propagation. Args: outputs (Tensor): Batch of prediction durations in log domain (B, T) targets (LongTensor): Batch of groundtruth durations in linear domain (B, T) Returns: Tensor: Mean squared error loss value. Note: `outputs` is in log domain but `targets` is in linear domain. \"\"\" # NOTE: outputs is in log domain while targets in linear targets = torch . log ( targets . float () + self . offset ) loss = self . criterion ( outputs , targets ) return loss","title":"duration_predictor"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.fastspeech.length_regulator","text":"Length regulator related modules. LengthRegulator Length regulator module for feed-forward Transformer. This is a module of length regulator described in ` FastSpeech : Fast , Robust and Controllable Text to Speech ` _ . The length regulator expands char or phoneme - level embedding features to frame - level by repeating each feature based on the corresponding predicted durations . .. _ ` FastSpeech : Fast , Robust and Controllable Text to Speech ` : https : // arxiv . org / pdf / 1905 . 09263 . pdf __init__ ( self , pad_value = 0.0 ) special Initilize length regulator module. Parameters: Name Type Description Default pad_value float Value used for padding. 0.0 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/length_regulator.py 28 29 30 31 32 33 34 35 36 def __init__ ( self , pad_value = 0.0 ): \"\"\"Initilize length regulator module. Args: pad_value (float, optional): Value used for padding. \"\"\" super ( LengthRegulator , self ) . __init__ () self . pad_value = pad_value forward ( self , xs , ds , ilens , alpha = 1.0 ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of sequences of char or phoneme embeddings (B, Tmax, D). required ds LongTensor Batch of durations of each frame (B, T). required ilens LongTensor Batch of input lengths (B,). required alpha float Alpha value to control speed of speech. 1.0 Returns: Type Description Tensor replicated input tensor based on durations (B, T*, D). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/fastspeech/length_regulator.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def forward ( self , xs , ds , ilens , alpha = 1.0 ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of sequences of char or phoneme embeddings (B, Tmax, D). ds (LongTensor): Batch of durations of each frame (B, T). ilens (LongTensor): Batch of input lengths (B,). alpha (float, optional): Alpha value to control speed of speech. Returns: Tensor: replicated input tensor based on durations (B, T*, D). \"\"\" assert alpha > 0 if alpha != 1.0 : ds = torch . round ( ds . float () * alpha ) . long () xs = [ x [: ilen ] for x , ilen in zip ( xs , ilens )] ds = [ d [: ilen ] for d , ilen in zip ( ds , ilens )] xs = [ self . _repeat_one_sequence ( x , d ) for x , d in zip ( xs , ds )] return pad_list ( xs , self . pad_value )","title":"length_regulator"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.initialization","text":"Initialization functions for RNN sequence-to-sequence models.","title":"initialization"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.initialization.lecun_normal_init_parameters","text":"Initialize parameters in the LeCun's manner. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/initialization.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def lecun_normal_init_parameters ( module ): \"\"\"Initialize parameters in the LeCun's manner.\"\"\" for p in module . parameters (): data = p . data if data . dim () == 1 : # bias data . zero_ () elif data . dim () == 2 : # linear weight n = data . size ( 1 ) stdv = 1. / math . sqrt ( n ) data . normal_ ( 0 , stdv ) elif data . dim () in ( 3 , 4 ): # conv weight n = data . size ( 1 ) for k in data . size ()[ 2 :]: n *= k stdv = 1. / math . sqrt ( n ) data . normal_ ( 0 , stdv ) else : raise NotImplementedError","title":"lecun_normal_init_parameters()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.initialization.set_forget_bias_to_one","text":"Initialize a bias vector in the forget gate with one. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/initialization.py 51 52 53 54 55 def set_forget_bias_to_one ( bias ): \"\"\"Initialize a bias vector in the forget gate with one.\"\"\" n = bias . size ( 0 ) start , end = n // 4 , n // 2 bias . data [ start : end ] . fill_ ( 1. )","title":"set_forget_bias_to_one()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.initialization.uniform_init_parameters","text":"Initialize parameters with an uniform distribution. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/initialization.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def uniform_init_parameters ( module ): \"\"\"Initialize parameters with an uniform distribution.\"\"\" for p in module . parameters (): data = p . data if data . dim () == 1 : # bias data . uniform_ ( - 0.1 , 0.1 ) elif data . dim () == 2 : # linear weight data . uniform_ ( - 0.1 , 0.1 ) elif data . dim () in ( 3 , 4 ): # conv weight pass # use the pytorch default else : raise NotImplementedError","title":"uniform_init_parameters()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils","text":"Network related utility tools.","title":"nets_utils"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils.make_non_pad_mask","text":"Make mask tensor containing indices of non-padded part. Parameters: Name Type Description Default lengths LongTensor or List Batch of lengths (B,). required xs Tensor The reference tensor. If set, masks will be the same shape as this tensor. None length_dim int Dimension indicator of the above tensor. See the example. -1 Returns: Type Description ByteTensor mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples With only lengths. lengths = [5, 3, 2] make_non_pad_mask(lengths) masks = [[1, 1, 1, 1 ,1], [1, 1, 1, 0, 0], [1, 1, 0, 0, 0]] With the reference tensor. xs = torch.zeros((3, 2, 4)) make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 0], [1, 1, 1, 0]], [[1, 1, 0, 0], [1, 1, 0, 0]]], dtype=torch.uint8) xs = torch.zeros((3, 2, 6)) make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) With the reference tensor and dimension indicator. xs = torch.zeros((3, 6, 6)) make_non_pad_mask(lengths, xs, 1) tensor([[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) make_non_pad_mask(lengths, xs, 2) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def make_non_pad_mask ( lengths , xs = None , length_dim =- 1 ): \"\"\"Make mask tensor containing indices of non-padded part. Args: lengths (LongTensor or List): Batch of lengths (B,). xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor. length_dim (int, optional): Dimension indicator of the above tensor. See the example. Returns: ByteTensor: mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples: With only lengths. >>> lengths = [5, 3, 2] >>> make_non_pad_mask(lengths) masks = [[1, 1, 1, 1 ,1], [1, 1, 1, 0, 0], [1, 1, 0, 0, 0]] With the reference tensor. >>> xs = torch.zeros((3, 2, 4)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 0], [1, 1, 1, 0]], [[1, 1, 0, 0], [1, 1, 0, 0]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_non_pad_mask(lengths, xs) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) With the reference tensor and dimension indicator. >>> xs = torch.zeros((3, 6, 6)) >>> make_non_pad_mask(lengths, xs, 1) tensor([[[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8) >>> make_non_pad_mask(lengths, xs, 2) tensor([[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0]], [[1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]], [[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8) \"\"\" return ~ make_pad_mask ( lengths , xs , length_dim )","title":"make_non_pad_mask()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils.make_pad_mask","text":"Make mask tensor containing indices of padded part. Parameters: Name Type Description Default lengths LongTensor or List Batch of lengths (B,). required xs Tensor The reference tensor. If set, masks will be the same shape as this tensor. None length_dim int Dimension indicator of the above tensor. See the example. -1 Returns: Type Description Tensor Mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples With only lengths. lengths = [5, 3, 2] make_non_pad_mask(lengths) masks = [[0, 0, 0, 0 ,0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 1]] With the reference tensor. xs = torch.zeros((3, 2, 4)) make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 1], [0, 0, 0, 1]], [[0, 0, 1, 1], [0, 0, 1, 1]]], dtype=torch.uint8) xs = torch.zeros((3, 2, 6)) make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) With the reference tensor and dimension indicator. xs = torch.zeros((3, 6, 6)) make_pad_mask(lengths, xs, 1) tensor([[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) make_pad_mask(lengths, xs, 2) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def make_pad_mask ( lengths , xs = None , length_dim =- 1 ): \"\"\"Make mask tensor containing indices of padded part. Args: lengths (LongTensor or List): Batch of lengths (B,). xs (Tensor, optional): The reference tensor. If set, masks will be the same shape as this tensor. length_dim (int, optional): Dimension indicator of the above tensor. See the example. Returns: Tensor: Mask tensor containing indices of padded part. dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (including 1.2) Examples: With only lengths. >>> lengths = [5, 3, 2] >>> make_non_pad_mask(lengths) masks = [[0, 0, 0, 0 ,0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 1]] With the reference tensor. >>> xs = torch.zeros((3, 2, 4)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 1], [0, 0, 0, 1]], [[0, 0, 1, 1], [0, 0, 1, 1]]], dtype=torch.uint8) >>> xs = torch.zeros((3, 2, 6)) >>> make_pad_mask(lengths, xs) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) With the reference tensor and dimension indicator. >>> xs = torch.zeros((3, 6, 6)) >>> make_pad_mask(lengths, xs, 1) tensor([[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) >>> make_pad_mask(lengths, xs, 2) tensor([[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], [[0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8) \"\"\" if length_dim == 0 : raise ValueError ( 'length_dim cannot be 0: {} ' . format ( length_dim )) if not isinstance ( lengths , list ): lengths = lengths . tolist () bs = int ( len ( lengths )) if xs is None : maxlen = int ( max ( lengths )) else : maxlen = xs . size ( length_dim ) seq_range = torch . arange ( 0 , maxlen , dtype = torch . int64 ) seq_range_expand = seq_range . unsqueeze ( 0 ) . expand ( bs , maxlen ) seq_length_expand = seq_range_expand . new ( lengths ) . unsqueeze ( - 1 ) mask = seq_range_expand >= seq_length_expand if xs is not None : assert xs . size ( 0 ) == bs , ( xs . size ( 0 ), bs ) if length_dim < 0 : length_dim = xs . dim () + length_dim # ind = (:, None, ..., None, :, , None, ..., None) ind = tuple ( slice ( None ) if i in ( 0 , length_dim ) else None for i in range ( xs . dim ())) mask = mask [ ind ] . expand_as ( xs ) . to ( xs . device ) return mask","title":"make_pad_mask()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils.mask_by_length","text":"Mask tensor according to length. Parameters: Name Type Description Default xs Tensor Batch of input tensor (B, * ). required lengths LongTensor or List Batch of lengths (B,). required fill int or float Value to fill masked part. 0 Returns: Type Description Tensor Batch of masked input tensor (B, * ). Examples x = torch.arange(5).repeat(3, 1) + 1 x tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]) lengths = [5, 3, 2] mask_by_length(x, lengths) tensor([[1, 2, 3, 4, 5], [1, 2, 3, 0, 0], [1, 2, 0, 0, 0]]) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def mask_by_length ( xs , lengths , fill = 0 ): \"\"\"Mask tensor according to length. Args: xs (Tensor): Batch of input tensor (B, `*`). lengths (LongTensor or List): Batch of lengths (B,). fill (int or float): Value to fill masked part. Returns: Tensor: Batch of masked input tensor (B, `*`). Examples: >>> x = torch.arange(5).repeat(3, 1) + 1 >>> x tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]) >>> lengths = [5, 3, 2] >>> mask_by_length(x, lengths) tensor([[1, 2, 3, 4, 5], [1, 2, 3, 0, 0], [1, 2, 0, 0, 0]]) \"\"\" assert xs . size ( 0 ) == len ( lengths ) ret = xs . data . new ( * xs . size ()) . fill_ ( fill ) for i , l in enumerate ( lengths ): ret [ i , : l ] = xs [ i , : l ] return ret","title":"mask_by_length()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils.pad_list","text":"Perform padding for the list of tensors. Parameters: Name Type Description Default xs List List of Tensors [(T_1, * ), (T_2, * ), ..., (T_B, * )]. required pad_value float Value for padding. required Returns: Type Description Tensor Padded tensor (B, Tmax, * ). Examples x = [torch.ones(4), torch.ones(2), torch.ones(1)] x [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])] pad_list(x, 0) tensor([[1., 1., 1., 1.], [1., 1., 0., 0.], [1., 0., 0., 0.]]) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def pad_list ( xs , pad_value ): \"\"\"Perform padding for the list of tensors. Args: xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)]. pad_value (float): Value for padding. Returns: Tensor: Padded tensor (B, Tmax, `*`). Examples: >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)] >>> x [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])] >>> pad_list(x, 0) tensor([[1., 1., 1., 1.], [1., 1., 0., 0.], [1., 0., 0., 0.]]) \"\"\" n_batch = len ( xs ) max_len = max ( x . size ( 0 ) for x in xs ) pad = xs [ 0 ] . new ( n_batch , max_len , * xs [ 0 ] . size ()[ 1 :]) . fill_ ( pad_value ) for i in range ( n_batch ): pad [ i , : xs [ i ] . size ( 0 )] = xs [ i ] return pad","title":"pad_list()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils.th_accuracy","text":"Calculate accuracy. Parameters: Name Type Description Default pad_outputs Tensor Prediction tensors (B * Lmax, D). required pad_targets LongTensor Target label tensors (B, Lmax, D). required ignore_label int Ignore label id. required Returns: Type Description float Accuracy value (0.0 - 1.0). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def th_accuracy ( pad_outputs , pad_targets , ignore_label ): \"\"\"Calculate accuracy. Args: pad_outputs (Tensor): Prediction tensors (B * Lmax, D). pad_targets (LongTensor): Target label tensors (B, Lmax, D). ignore_label (int): Ignore label id. Returns: float: Accuracy value (0.0 - 1.0). \"\"\" pad_pred = pad_outputs . view ( pad_targets . size ( 0 ), pad_targets . size ( 1 ), pad_outputs . size ( 1 )) . argmax ( 2 ) mask = pad_targets != ignore_label numerator = torch . sum ( pad_pred . masked_select ( mask ) == pad_targets . masked_select ( mask )) denominator = torch . sum ( mask ) return float ( numerator ) / float ( denominator )","title":"th_accuracy()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils.to_device","text":"Send tensor into the device of the module. Parameters: Name Type Description Default m torch.nn.Module Torch module. required x Tensor Torch tensor. required Returns: Type Description Tensor Torch tensor located in the same place as torch module. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def to_device ( m , x ): \"\"\"Send tensor into the device of the module. Args: m (torch.nn.Module): Torch module. x (Tensor): Torch tensor. Returns: Tensor: Torch tensor located in the same place as torch module. \"\"\" assert isinstance ( m , torch . nn . Module ) device = next ( m . parameters ()) . device return x . to ( device )","title":"to_device()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.nets_utils.to_torch_tensor","text":"Change to torch.Tensor or ComplexTensor from numpy.ndarray. Parameters: Name Type Description Default x Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict. required Returns: Type Description Tensor or ComplexTensor Type converted inputs. Examples xs = np.ones(3, dtype=np.float32) xs = to_torch_tensor(xs) tensor([1., 1., 1.]) xs = torch.ones(3, 4, 5) assert to_torch_tensor(xs) is xs xs = {'real': xs, 'imag': xs} to_torch_tensor(xs) ComplexTensor( Real: tensor([1., 1., 1.]) Imag; tensor([1., 1., 1.]) ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/nets_utils.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def to_torch_tensor ( x ): \"\"\"Change to torch.Tensor or ComplexTensor from numpy.ndarray. Args: x: Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict. Returns: Tensor or ComplexTensor: Type converted inputs. Examples: >>> xs = np.ones(3, dtype=np.float32) >>> xs = to_torch_tensor(xs) tensor([1., 1., 1.]) >>> xs = torch.ones(3, 4, 5) >>> assert to_torch_tensor(xs) is xs >>> xs = {'real': xs, 'imag': xs} >>> to_torch_tensor(xs) ComplexTensor( Real: tensor([1., 1., 1.]) Imag; tensor([1., 1., 1.]) ) \"\"\" # If numpy, change to torch tensor if isinstance ( x , np . ndarray ): if x . dtype . kind == 'c' : # Dynamically importing because torch_complex requires python3 from torch_complex.tensor import ComplexTensor return ComplexTensor ( x ) else : return torch . from_numpy ( x ) # If {'real': ..., 'imag': ...}, convert to ComplexTensor elif isinstance ( x , dict ): # Dynamically importing because torch_complex requires python3 from torch_complex.tensor import ComplexTensor if 'real' not in x or 'imag' not in x : raise ValueError ( \"has 'real' and 'imag' keys: {} \" . format ( list ( x ))) # Relative importing because of using python3 syntax return ComplexTensor ( x [ 'real' ], x [ 'imag' ]) # If torch.Tensor, as it is elif isinstance ( x , torch . Tensor ): return x else : error = ( \"x must be numpy.ndarray, torch.Tensor or a dict like \" \"{{'real': torch.Tensor, 'imag': torch.Tensor}}, \" \"but got {} \" . format ( type ( x ))) try : from torch_complex.tensor import ComplexTensor except Exception : # If PY2 raise ValueError ( error ) else : # If PY3 if isinstance ( x , ComplexTensor ): return x else : raise ValueError ( error )","title":"to_torch_tensor()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.rnn","text":"","title":"rnn"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.rnn.attentions","text":"Attention modules for RNN. AttAdd Additive attention :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int att_dim: attention dimension :param bool han_mode: flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 171 172 173 174 175 176 177 178 179 180 181 182 183 def __init__ ( self , eprojs , dunits , att_dim , han_mode = False ): super ( AttAdd , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 185 186 187 188 189 190 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttCov Coverage mechanism attention Reference : Get To The Point : Summarization with Pointer - Generator Network ( https :// arxiv . org /abs/ 1704.04368 ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def __init__ ( self , eprojs , dunits , att_dim , han_mode = False ): super ( AttCov , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . wvec = torch . nn . Linear ( 1 , att_dim ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ) AttCov forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ): \"\"\"AttCov forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev_list is None : # if no bias, 0 0-pad goes 0 att_prev_list = to_device ( self , ( 1. - make_pad_mask ( enc_hs_len ) . float ())) att_prev_list = [ att_prev_list / att_prev_list . new ( enc_hs_len ) . unsqueeze ( - 1 )] # att_prev_list: L' * [B x T] => cov_vec B x T cov_vec = sum ( att_prev_list ) # cov_vec: B x T => B x T x 1 => B x T x att_dim cov_vec = self . wvec ( cov_vec . unsqueeze ( - 1 )) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( cov_vec + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) att_prev_list += [ w ] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , att_prev_list reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 376 377 378 379 380 381 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttCovLoc Coverage mechanism location aware attention This attention is a combination of coverage and location - aware attentions . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ): super ( AttCovLoc , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . aconv_chans = aconv_chans self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ) AttCovLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_list , scaling = 2.0 ): \"\"\"AttCovLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param list att_prev_list: list of previous attention weight :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weights :rtype: list \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev_list is None : # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev_list = [ to_device ( self , mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ))] # att_prev_list: L' * [B x T] => cov_vec B x T cov_vec = sum ( att_prev_list ) # cov_vec: B x T -> B x 1 x 1 x T -> B x C x 1 x T att_conv = self . loc_conv ( cov_vec . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) att_prev_list += [ w ] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , att_prev_list reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 696 697 698 699 700 701 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttDot Dot product attention :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int att_dim: attention dimension :param bool han_mode: flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , eprojs , dunits , att_dim , han_mode = False ): super ( AttDot , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weight (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weight (B x T_max) :rtype: torch.Tensor \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = torch . tanh ( self . mlp_enc ( self . enc_h )) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) e = torch . sum ( self . pre_compute_enc_h * torch . tanh ( self . mlp_dec ( dec_z )) . view ( batch , 1 , self . att_dim ), dim = 2 ) # utt x frame # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 112 113 114 115 116 117 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttForward Forward attention module. !!! reference \"Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\" ( https : // arxiv . org / pdf / 1807 . 06736 . pdf ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts ): super ( AttForward , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ) Calculate AttForward forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: attention weights of previous step :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ): \"\"\"Calculate AttForward forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: attention weights of previous step :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : # initial attention will be [1, 0, 0, ...] att_prev = enc_hs_pad . new_zeros ( * enc_hs_pad . size ()[: 2 ]) att_prev [:, 0 ] = 1.0 # att_prev: utt x frame -> utt x 1 x 1 x frame -> utt x att_conv_chans x 1 x frame att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . unsqueeze ( 1 ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( self . pre_compute_enc_h + dec_z_tiled + att_conv )) . squeeze ( 2 ) # NOTE: consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) # apply monotonic attention constraint (mainly for TTS) if last_attended_idx is not None : e = _apply_attention_constraint ( e , last_attended_idx , backward_window , forward_window ) w = F . softmax ( scaling * e , dim = 1 ) # forward attention att_prev_shift = F . pad ( att_prev , ( 1 , 0 ))[:, : - 1 ] w = ( att_prev + att_prev_shift ) * w # NOTE: clamp is needed to avoid nan gradient w = F . normalize ( torch . clamp ( w , 1e-6 ), p = 1 , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . unsqueeze ( - 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1260 1261 1262 1263 1264 1265 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttForwardTA Forward attention with transition agent module. !!! reference \"Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\" ( https : // arxiv . org / pdf / 1807 . 06736 . pdf ) : param int eunits : # units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param int odim : output dimension __init__ ( self , eunits , dunits , att_dim , aconv_chans , aconv_filts , odim ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 def __init__ ( self , eunits , dunits , att_dim , aconv_chans , aconv_filts , odim ): super ( AttForwardTA , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eunits , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_ta = torch . nn . Linear ( eunits + dunits + odim , 1 ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eunits = eunits self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . trans_agent_prob = 0.5 forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , out_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ) Calculate AttForwardTA forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, Tmax, eunits) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B, dunits) :param torch.Tensor att_prev: attention weights of previous step :param torch.Tensor out_prev: decoder outputs of previous step (B, odim) :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, dunits) :rtype: torch.Tensor :return: previous attention weights (B, Tmax) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , out_prev , scaling = 1.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ): \"\"\"Calculate AttForwardTA forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, Tmax, eunits) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B, dunits) :param torch.Tensor att_prev: attention weights of previous step :param torch.Tensor out_prev: decoder outputs of previous step (B, odim) :param float scaling: scaling parameter before applying softmax :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, dunits) :rtype: torch.Tensor :return: previous attention weights (B, Tmax) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : # initial attention will be [1, 0, 0, ...] att_prev = enc_hs_pad . new_zeros ( * enc_hs_pad . size ()[: 2 ]) att_prev [:, 0 ] = 1.0 # att_prev: utt x frame -> utt x 1 x 1 x frame -> utt x att_conv_chans x 1 x frame att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) # apply monotonic attention constraint (mainly for TTS) if last_attended_idx is not None : e = _apply_attention_constraint ( e , last_attended_idx , backward_window , forward_window ) w = F . softmax ( scaling * e , dim = 1 ) # forward attention att_prev_shift = F . pad ( att_prev , ( 1 , 0 ))[:, : - 1 ] w = ( self . trans_agent_prob * att_prev + ( 1 - self . trans_agent_prob ) * att_prev_shift ) * w # NOTE: clamp is needed to avoid nan gradient w = F . normalize ( torch . clamp ( w , 1e-6 ), p = 1 , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) # update transition agent prob self . trans_agent_prob = torch . sigmoid ( self . mlp_ta ( torch . cat ([ c , out_prev , dec_z ], dim = 1 ))) return c , w reset ( self ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1373 1374 1375 1376 1377 1378 def reset ( self ): self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . trans_agent_prob = 0.5 AttLoc location-aware attention module. !!! reference \"Attention-Based Models for Speech Recognition\" ( https : // arxiv . org / pdf / 1506 . 07503 . pdf ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ): super ( AttLoc , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ) Calcualte AttLoc forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x T_max) :param float scaling: scaling parameter before applying softmax :param torch.Tensor forward_window: forward window size when constraining attention :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 , last_attended_idx = None , backward_window = 1 , forward_window = 3 ): \"\"\"Calcualte AttLoc forward propagation. :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x T_max) :param float scaling: scaling parameter before applying softmax :param torch.Tensor forward_window: forward window size when constraining attention :param int last_attended_idx: index of the inputs of the last attended :param int backward_window: backward window size in attention constraint :param int forward_window: forward window size in attetion constraint :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev is None : # if no bias, 0 0-pad goes 0 att_prev = ( 1. - make_pad_mask ( enc_hs_len ) . to ( device = dec_z . device , dtype = dec_z . dtype )) att_prev = att_prev / att_prev . new ( enc_hs_len ) . unsqueeze ( - 1 ) # att_prev: utt x frame -> utt x 1 x 1 x frame -> utt x att_conv_chans x 1 x frame att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE: consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) # apply monotonic attention constraint (mainly for TTS) if last_attended_idx is not None : e = _apply_attention_constraint ( e , last_attended_idx , backward_window , forward_window ) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 272 273 274 275 276 277 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttLoc2D 2D location-aware attention This attention is an extended version of location aware attention . It take not only one frame before attention weights , but also earlier frames into account . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param int att_win : attention window size ( default = 5 ) : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , att_win , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def __init__ ( self , eprojs , dunits , att_dim , att_win , aconv_chans , aconv_filts , han_mode = False ): super ( AttLoc2D , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . mlp_att = torch . nn . Linear ( aconv_chans , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( att_win , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . aconv_chans = aconv_chans self . att_win = att_win self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttLoc2D forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x att_win x T_max) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x att_win x T_max) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttLoc2D forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: previous attention weight (B x att_win x T_max) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights (B x att_win x T_max) :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) # initialize attention weight with uniform dist. if att_prev is None : # B * [Li x att_win] # if no bias, 0 0-pad goes 0 att_prev = to_device ( self , ( 1. - make_pad_mask ( enc_hs_len ) . float ())) att_prev = att_prev / att_prev . new ( enc_hs_len ) . unsqueeze ( - 1 ) att_prev = att_prev . unsqueeze ( 1 ) . expand ( - 1 , self . att_win , - 1 ) # att_prev: B x att_win x Tmax -> B x 1 x att_win x Tmax -> B x C x 1 x Tmax att_conv = self . loc_conv ( att_prev . unsqueeze ( 1 )) # att_conv: B x C x 1 x Tmax -> B x Tmax x C att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim att_conv = self . mlp_att ( att_conv ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_conv + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) # update att_prev: B x att_win x Tmax -> B x att_win+1 x Tmax -> B x att_win x Tmax att_prev = torch . cat ([ att_prev , w . unsqueeze ( 1 )], dim = 1 ) att_prev = att_prev [:, 1 :] return c , att_prev reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 478 479 480 481 482 483 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttLocRec location-aware recurrent attention This attention is an extended version of location aware attention . With the use of RNN , it take the effect of the history of attention weights into account . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int att_dim : attention dimension : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_enc_h __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def __init__ ( self , eprojs , dunits , att_dim , aconv_chans , aconv_filts , han_mode = False ): super ( AttLocRec , self ) . __init__ () self . mlp_enc = torch . nn . Linear ( eprojs , att_dim ) self . mlp_dec = torch . nn . Linear ( dunits , att_dim , bias = False ) self . loc_conv = torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False ) self . att_lstm = torch . nn . LSTMCell ( aconv_chans , att_dim , bias = False ) self . gvec = torch . nn . Linear ( att_dim , 1 ) self . dunits = dunits self . eprojs = eprojs self . att_dim = att_dim self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_states , scaling = 2.0 ) AttLocRec forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param tuple att_prev_states: previous attention weight and lstm states ((B, T_max), ((B, att_dim), (B, att_dim))) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights and lstm states (w, (hx, cx)) ((B, T_max), ((B, att_dim), (B, att_dim))) :rtype: tuple Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev_states , scaling = 2.0 ): \"\"\"AttLocRec forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param tuple att_prev_states: previous attention weight and lstm states ((B, T_max), ((B, att_dim), (B, att_dim))) :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights and lstm states (w, (hx, cx)) ((B, T_max), ((B, att_dim), (B, att_dim))) :rtype: tuple \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_enc_h = self . mlp_enc ( self . enc_h ) if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev_states is None : # initialize attention weight with uniform dist. # if no bias, 0 0-pad goes 0 att_prev = to_device ( self , ( 1. - make_pad_mask ( enc_hs_len ) . float ())) att_prev = att_prev / att_prev . new ( enc_hs_len ) . unsqueeze ( - 1 ) # initialize lstm states att_h = enc_hs_pad . new_zeros ( batch , self . att_dim ) att_c = enc_hs_pad . new_zeros ( batch , self . att_dim ) att_states = ( att_h , att_c ) else : att_prev = att_prev_states [ 0 ] att_states = att_prev_states [ 1 ] # B x 1 x 1 x T -> B x C x 1 x T att_conv = self . loc_conv ( att_prev . view ( batch , 1 , 1 , self . h_length )) # apply non-linear att_conv = F . relu ( att_conv ) # B x C x 1 x T -> B x C x 1 x 1 -> B x C att_conv = F . max_pool2d ( att_conv , ( 1 , att_conv . size ( 3 ))) . view ( batch , - 1 ) att_h , att_c = self . att_lstm ( att_conv , att_states ) # dec_z_tiled: utt x frame x att_dim dec_z_tiled = self . mlp_dec ( dec_z ) . view ( batch , 1 , self . att_dim ) # dot with gvec # utt x frame x att_dim -> utt x frame e = self . gvec ( torch . tanh ( att_h . unsqueeze ( 1 ) + self . pre_compute_enc_h + dec_z_tiled )) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w = F . softmax ( scaling * e , dim = 1 ) # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c = torch . sum ( self . enc_h * w . view ( batch , self . h_length , 1 ), dim = 1 ) return c , ( w , ( att_h , att_c )) reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 584 585 586 587 588 589 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . mask = None AttMultiHeadAdd Multi head additive attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) This attention is multi head attention using additive attention for each head . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ): super ( AttMultiHeadAdd , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () self . gvec = torch . nn . ModuleList () for _ in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . gvec += [ torch . nn . Linear ( att_dim_k , 1 )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) AttMultiHeadAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"AttMultiHeadAdd forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ self . mlp_k [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) c = [] w = [] for h in six . moves . range ( self . aheads ): e = self . gvec [ h ]( torch . tanh ( self . pre_compute_k [ h ] + self . mlp_q [ h ]( dec_z ) . view ( batch , 1 , self . att_dim_k ))) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( self . scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 911 912 913 914 915 916 917 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None AttMultiHeadDot Multi head dot product attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , han_mode = False ): super ( AttMultiHeadDot , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () for _ in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) AttMultiHeadDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"AttMultiHeadDot forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ torch . tanh ( self . mlp_k [ h ]( self . enc_h )) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) c = [] w = [] for h in six . moves . range ( self . aheads ): e = torch . sum ( self . pre_compute_k [ h ] * torch . tanh ( self . mlp_q [ h ]( dec_z )) . view ( batch , 1 , self . att_dim_k ), dim = 2 ) # utt x frame # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( self . scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 805 806 807 808 809 810 811 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None AttMultiHeadLoc Multi head location based attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) This attention is multi head attention using location - aware attention for each head . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param int aconv_chans : # channels of attention convolution : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ): super ( AttMultiHeadLoc , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () self . gvec = torch . nn . ModuleList () self . loc_conv = torch . nn . ModuleList () self . mlp_att = torch . nn . ModuleList () for _ in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . gvec += [ torch . nn . Linear ( att_dim_k , 1 )] self . loc_conv += [ torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * aconv_filts + 1 ), padding = ( 0 , aconv_filts ), bias = False )] self . mlp_att += [ torch . nn . Linear ( aconv_chans , att_dim_k , bias = False )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ) AttMultiHeadLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev , scaling = 2.0 ): \"\"\"AttMultiHeadLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :param float scaling: scaling parameter before applying softmax :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ self . mlp_k [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : att_prev = [] for _ in six . moves . range ( self . aheads ): # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev += [ to_device ( self , mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ))] c = [] w = [] for h in six . moves . range ( self . aheads ): att_conv = self . loc_conv [ h ]( att_prev [ h ] . view ( batch , 1 , 1 , self . h_length )) att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) att_conv = self . mlp_att [ h ]( att_conv ) e = self . gvec [ h ]( torch . tanh ( self . pre_compute_k [ h ] + att_conv + self . mlp_q [ h ]( dec_z ) . view ( batch , 1 , self . att_dim_k ))) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1024 1025 1026 1027 1028 1029 1030 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None AttMultiHeadMultiResLoc Multi head multi resolution location based attention !!! reference \"Attention is all you need\" ( https : // arxiv . org / abs / 1706 . 03762 ) This attention is multi head attention using location - aware attention for each head . Furthermore , it uses different filter size for each head . : param int eprojs : # projection - units of encoder : param int dunits : # units of decoder : param int aheads : # heads of multi head attention : param int att_dim_k : dimension k in multi head attention : param int att_dim_v : dimension v in multi head attention : param int aconv_chans : maximum # channels of attention convolution each head use # ch = aconv_chans * ( head + 1 ) / aheads e . g . aheads = 4 , aconv_chans = 100 => filter size = 25 , 50 , 75 , 100 : param int aconv_filts : filter size of attention convolution : param bool han_mode : flag to swith on mode of hierarchical attention and not store pre_compute_k and pre_compute_v __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 def __init__ ( self , eprojs , dunits , aheads , att_dim_k , att_dim_v , aconv_chans , aconv_filts , han_mode = False ): super ( AttMultiHeadMultiResLoc , self ) . __init__ () self . mlp_q = torch . nn . ModuleList () self . mlp_k = torch . nn . ModuleList () self . mlp_v = torch . nn . ModuleList () self . gvec = torch . nn . ModuleList () self . loc_conv = torch . nn . ModuleList () self . mlp_att = torch . nn . ModuleList () for h in six . moves . range ( aheads ): self . mlp_q += [ torch . nn . Linear ( dunits , att_dim_k )] self . mlp_k += [ torch . nn . Linear ( eprojs , att_dim_k , bias = False )] self . mlp_v += [ torch . nn . Linear ( eprojs , att_dim_v , bias = False )] self . gvec += [ torch . nn . Linear ( att_dim_k , 1 )] afilts = aconv_filts * ( h + 1 ) // aheads self . loc_conv += [ torch . nn . Conv2d ( 1 , aconv_chans , ( 1 , 2 * afilts + 1 ), padding = ( 0 , afilts ), bias = False )] self . mlp_att += [ torch . nn . Linear ( aconv_chans , att_dim_k , bias = False )] self . mlp_o = torch . nn . Linear ( aheads * att_dim_v , eprojs , bias = False ) self . dunits = dunits self . eprojs = eprojs self . aheads = aheads self . att_dim_k = att_dim_k self . att_dim_v = att_dim_v self . scaling = 1.0 / math . sqrt ( att_dim_k ) self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None self . han_mode = han_mode forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) AttMultiHeadMultiResLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"AttMultiHeadMultiResLoc forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: decoder hidden state (B x D_dec) :param torch.Tensor att_prev: list of previous attention weight (B x T_max) * aheads :return: attention weighted encoder state (B x D_enc) :rtype: torch.Tensor :return: list of previous attention weight (B x T_max) * aheads :rtype: list \"\"\" batch = enc_hs_pad . size ( 0 ) # pre-compute all k and v outside the decoder loop if self . pre_compute_k is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_k = [ self . mlp_k [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if self . pre_compute_v is None or self . han_mode : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # utt x frame x att_dim self . pre_compute_v = [ self . mlp_v [ h ]( self . enc_h ) for h in six . moves . range ( self . aheads )] if dec_z is None : dec_z = enc_hs_pad . new_zeros ( batch , self . dunits ) else : dec_z = dec_z . view ( batch , self . dunits ) if att_prev is None : att_prev = [] for _ in six . moves . range ( self . aheads ): # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev += [ to_device ( self , mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ))] c = [] w = [] for h in six . moves . range ( self . aheads ): att_conv = self . loc_conv [ h ]( att_prev [ h ] . view ( batch , 1 , 1 , self . h_length )) att_conv = att_conv . squeeze ( 2 ) . transpose ( 1 , 2 ) att_conv = self . mlp_att [ h ]( att_conv ) e = self . gvec [ h ]( torch . tanh ( self . pre_compute_k [ h ] + att_conv + self . mlp_q [ h ]( dec_z ) . view ( batch , 1 , self . att_dim_k ))) . squeeze ( 2 ) # NOTE consider zero padding when compute w. if self . mask is None : self . mask = to_device ( self , make_pad_mask ( enc_hs_len )) e . masked_fill_ ( self . mask , - float ( 'inf' )) w += [ F . softmax ( self . scaling * e , dim = 1 )] # weighted sum over flames # utt x hdim # NOTE use bmm instead of sum(*) c += [ torch . sum ( self . pre_compute_v [ h ] * w [ h ] . view ( batch , self . h_length , 1 ), dim = 1 )] # concat all of c c = self . mlp_o ( torch . cat ( c , dim = 1 )) return c , w reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1154 1155 1156 1157 1158 1159 1160 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_k = None self . pre_compute_v = None self . mask = None NoAtt No attention __init__ ( self ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 46 47 48 49 50 51 def __init__ ( self ): super ( NoAtt , self ) . __init__ () self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . c = None forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ) NoAtt forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, T_max, D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def forward ( self , enc_hs_pad , enc_hs_len , dec_z , att_prev ): \"\"\"NoAtt forward :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, T_max, D_enc) :param list enc_hs_len: padded encoder hidden state length (B) :param torch.Tensor dec_z: dummy (does not use) :param torch.Tensor att_prev: dummy (does not use) :return: attention weighted encoder state (B, D_enc) :rtype: torch.Tensor :return: previous attention weights :rtype: torch.Tensor \"\"\" batch = len ( enc_hs_pad ) # pre-compute all h outside the decoder loop if self . pre_compute_enc_h is None : self . enc_h = enc_hs_pad # utt x frame x hdim self . h_length = self . enc_h . size ( 1 ) # initialize attention weight with uniform dist. if att_prev is None : # if no bias, 0 0-pad goes 0 mask = 1. - make_pad_mask ( enc_hs_len ) . float () att_prev = mask / mask . new ( enc_hs_len ) . unsqueeze ( - 1 ) att_prev = att_prev . to ( self . enc_h ) self . c = torch . sum ( self . enc_h * att_prev . view ( batch , self . h_length , 1 ), dim = 1 ) return self . c , att_prev reset ( self ) reset states Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 53 54 55 56 57 58 def reset ( self ): \"\"\"reset states\"\"\" self . h_length = None self . enc_h = None self . pre_compute_enc_h = None self . c = None att_for ( args , num_att = 1 , han_mode = False ) Instantiates an attention module given the program arguments :param Namespace args: The arguments :param int num_att: number of attention modules (in multi-speaker case, it can be 2 or more) :param bool han_mode: switch on/off mode of hierarchical attention network (HAN) :rtype torch.nn.Module :return: The attention module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 def att_for ( args , num_att = 1 , han_mode = False ): \"\"\"Instantiates an attention module given the program arguments :param Namespace args: The arguments :param int num_att: number of attention modules (in multi-speaker case, it can be 2 or more) :param bool han_mode: switch on/off mode of hierarchical attention network (HAN) :rtype torch.nn.Module :return: The attention module \"\"\" att_list = torch . nn . ModuleList () num_encs = getattr ( args , \"num_encs\" , 1 ) # use getattr to keep compatibility aheads = getattr ( args , 'aheads' , None ) awin = getattr ( args , 'awin' , None ) aconv_chans = getattr ( args , 'aconv_chans' , None ) aconv_filts = getattr ( args , 'aconv_filts' , None ) if num_encs == 1 : for i in range ( num_att ): att = initial_att ( args . atype , args . eprojs , args . dunits , aheads , args . adim , awin , aconv_chans , aconv_filts ) att_list . append ( att ) elif num_encs > 1 : # no multi-speaker mode if han_mode : att = initial_att ( args . han_type , args . eprojs , args . dunits , args . han_heads , args . han_dim , args . han_win , args . han_conv_chans , args . han_conv_filts , han_mode = True ) return att else : att_list = torch . nn . ModuleList () for idx in range ( num_encs ): att = initial_att ( args . atype [ idx ], args . eprojs , args . dunits , aheads [ idx ], args . adim [ idx ], awin [ idx ], aconv_chans [ idx ], aconv_filts [ idx ]) att_list . append ( att ) else : raise ValueError ( \"Number of encoders needs to be more than one. {} \" . format ( num_encs )) return att_list att_to_numpy ( att_ws , att ) Converts attention weights to a numpy array given the attention :param list att_ws: The attention weights :param torch.nn.Module att: The attention :rtype: np.ndarray :return: The numpy array of the attention weights Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 def att_to_numpy ( att_ws , att ): \"\"\"Converts attention weights to a numpy array given the attention :param list att_ws: The attention weights :param torch.nn.Module att: The attention :rtype: np.ndarray :return: The numpy array of the attention weights \"\"\" # convert to numpy array with the shape (B, Lmax, Tmax) if isinstance ( att , AttLoc2D ): # att_ws => list of previous concate attentions att_ws = torch . stack ([ aw [:, - 1 ] for aw in att_ws ], dim = 1 ) . cpu () . numpy () elif isinstance ( att , ( AttCov , AttCovLoc )): # att_ws => list of list of previous attentions att_ws = torch . stack ([ aw [ idx ] for idx , aw in enumerate ( att_ws )], dim = 1 ) . cpu () . numpy () elif isinstance ( att , AttLocRec ): # att_ws => list of tuple of attention and hidden states att_ws = torch . stack ([ aw [ 0 ] for aw in att_ws ], dim = 1 ) . cpu () . numpy () elif isinstance ( att , ( AttMultiHeadDot , AttMultiHeadAdd , AttMultiHeadLoc , AttMultiHeadMultiResLoc )): # att_ws => list of list of each head attention n_heads = len ( att_ws [ 0 ]) att_ws_sorted_by_head = [] for h in six . moves . range ( n_heads ): att_ws_head = torch . stack ([ aw [ h ] for aw in att_ws ], dim = 1 ) att_ws_sorted_by_head += [ att_ws_head ] att_ws = torch . stack ( att_ws_sorted_by_head , dim = 1 ) . cpu () . numpy () else : # att_ws => list of attentions att_ws = torch . stack ( att_ws , dim = 1 ) . cpu () . numpy () return att_ws initial_att ( atype , eprojs , dunits , aheads , adim , awin , aconv_chans , aconv_filts , han_mode = False ) Instantiates a single attention module :param str atype: attention type :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int aheads: # heads of multi head attention :param int adim: attention dimension :param int awin: attention window size :param int aconv_chans: # channels of attention convolution :param int aconv_filts: filter size of attention convolution :param bool han_mode: flag to swith on mode of hierarchical attention :return: The attention module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/attentions.py 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 def initial_att ( atype , eprojs , dunits , aheads , adim , awin , aconv_chans , aconv_filts , han_mode = False ): \"\"\"Instantiates a single attention module :param str atype: attention type :param int eprojs: # projection-units of encoder :param int dunits: # units of decoder :param int aheads: # heads of multi head attention :param int adim: attention dimension :param int awin: attention window size :param int aconv_chans: # channels of attention convolution :param int aconv_filts: filter size of attention convolution :param bool han_mode: flag to swith on mode of hierarchical attention :return: The attention module \"\"\" if atype == 'noatt' : att = NoAtt () elif atype == 'dot' : att = AttDot ( eprojs , dunits , adim , han_mode ) elif atype == 'add' : att = AttAdd ( eprojs , dunits , adim , han_mode ) elif atype == 'location' : att = AttLoc ( eprojs , dunits , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'location2d' : att = AttLoc2D ( eprojs , dunits , adim , awin , aconv_chans , aconv_filts , han_mode ) elif atype == 'location_recurrent' : att = AttLocRec ( eprojs , dunits , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'coverage' : att = AttCov ( eprojs , dunits , adim , han_mode ) elif atype == 'coverage_location' : att = AttCovLoc ( eprojs , dunits , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'multi_head_dot' : att = AttMultiHeadDot ( eprojs , dunits , aheads , adim , adim , han_mode ) elif atype == 'multi_head_add' : att = AttMultiHeadAdd ( eprojs , dunits , aheads , adim , adim , han_mode ) elif atype == 'multi_head_loc' : att = AttMultiHeadLoc ( eprojs , dunits , aheads , adim , adim , aconv_chans , aconv_filts , han_mode ) elif atype == 'multi_head_multi_res_loc' : att = AttMultiHeadMultiResLoc ( eprojs , dunits , aheads , adim , adim , aconv_chans , aconv_filts , han_mode ) return att","title":"attentions"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.rnn.decoders","text":"Decoder Decoder module :param int eprojs: encoder projection units :param int odim: dimension of outputs :param str dtype: gru or lstm :param int dlayers: decoder layers :param int dunits: decoder units :param int sos: start of sequence symbol id :param int eos: end of sequence symbol id :param torch.nn.Module att: attention module :param int verbose: verbose level :param list char_list: list of character strings :param ndarray labeldist: distribution of label smoothing :param float lsm_weight: label smoothing weight :param float sampling_probability: scheduled sampling probability :param float dropout: dropout rate :param float context_residual: if True, use context vector for token generation :param float replace_sos: use for multilingual (speech/text) translation __init__ ( self , eprojs , odim , dtype , dlayers , dunits , sos , eos , att , verbose = 0 , char_list = None , labeldist = None , lsm_weight = 0.0 , sampling_probability = 0.0 , dropout = 0.0 , context_residual = False , replace_sos = False , num_encs = 1 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , eprojs , odim , dtype , dlayers , dunits , sos , eos , att , verbose = 0 , char_list = None , labeldist = None , lsm_weight = 0. , sampling_probability = 0.0 , dropout = 0.0 , context_residual = False , replace_sos = False , num_encs = 1 ): torch . nn . Module . __init__ ( self ) self . dtype = dtype self . dunits = dunits self . dlayers = dlayers self . context_residual = context_residual self . embed = torch . nn . Embedding ( odim , dunits ) self . dropout_emb = torch . nn . Dropout ( p = dropout ) self . decoder = torch . nn . ModuleList () self . dropout_dec = torch . nn . ModuleList () self . decoder += [ torch . nn . LSTMCell ( dunits + eprojs , dunits ) if self . dtype == \"lstm\" else torch . nn . GRUCell ( dunits + eprojs , dunits )] self . dropout_dec += [ torch . nn . Dropout ( p = dropout )] for _ in six . moves . range ( 1 , self . dlayers ): self . decoder += [ torch . nn . LSTMCell ( dunits , dunits ) if self . dtype == \"lstm\" else torch . nn . GRUCell ( dunits , dunits )] self . dropout_dec += [ torch . nn . Dropout ( p = dropout )] # NOTE: dropout is applied only for the vertical connections # see https://arxiv.org/pdf/1409.2329.pdf self . ignore_id = - 1 if context_residual : self . output = torch . nn . Linear ( dunits + eprojs , odim ) else : self . output = torch . nn . Linear ( dunits , odim ) self . loss = None self . att = att self . dunits = dunits self . sos = sos self . eos = eos self . odim = odim self . verbose = verbose self . char_list = char_list # for label smoothing self . labeldist = labeldist self . vlabeldist = None self . lsm_weight = lsm_weight self . sampling_probability = sampling_probability self . dropout = dropout self . num_encs = num_encs # for multilingual E2E-ST self . replace_sos = replace_sos self . logzero = - 10000000000.0 calculate_all_attentions ( self , hs_pad , hlen , ys_pad , strm_idx = 0 , lang_ids = None ) Calculate all of attentions :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlen: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index for parallel speaker attention in multi-speaker case :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) multi-encoder case => [(B, Lmax, Tmax1), (B, Lmax, Tmax2), ..., (B, Lmax, NumEncs)] 3) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 def calculate_all_attentions ( self , hs_pad , hlen , ys_pad , strm_idx = 0 , lang_ids = None ): \"\"\"Calculate all of attentions :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlen: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index for parallel speaker attention in multi-speaker case :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention weights with the following shape, 1) multi-head case => attention weights (B, H, Lmax, Tmax), 2) multi-encoder case => [(B, Lmax, Tmax1), (B, Lmax, Tmax2), ..., (B, Lmax, NumEncs)] 3) other case => attention weights (B, Lmax, Tmax). :rtype: float ndarray \"\"\" # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : hs_pad = [ hs_pad ] hlen = [ hlen ] # TODO(kan-bayashi): need to make more smart way ys = [ y [ y != self . ignore_id ] for y in ys_pad ] # parse padded ys att_idx = min ( strm_idx , len ( self . att ) - 1 ) # hlen should be list of list of integer hlen = [ list ( map ( int , hlen [ idx ])) for idx in range ( self . num_encs )] self . loss = None # prepare input and output word sequences with sos/eos IDs eos = ys [ 0 ] . new ([ self . eos ]) sos = ys [ 0 ] . new ([ self . sos ]) if self . replace_sos : ys_in = [ torch . cat ([ idx , y ], dim = 0 ) for idx , y in zip ( lang_ids , ys )] else : ys_in = [ torch . cat ([ sos , y ], dim = 0 ) for y in ys ] ys_out = [ torch . cat ([ y , eos ], dim = 0 ) for y in ys ] # padding for ys with -1 # pys: utt x olen ys_in_pad = pad_list ( ys_in , self . eos ) ys_out_pad = pad_list ( ys_out , self . ignore_id ) # get length info olength = ys_out_pad . size ( 1 ) # initialization c_list = [ self . zero_state ( hs_pad [ 0 ])] z_list = [ self . zero_state ( hs_pad [ 0 ])] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( hs_pad [ 0 ])) z_list . append ( self . zero_state ( hs_pad [ 0 ])) att_ws = [] if self . num_encs == 1 : att_w = None self . att [ att_idx ] . reset () # reset pre-computation of h else : att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han # pre-computation of embedding eys = self . dropout_emb ( self . embed ( ys_in_pad )) # utt x olen x zdim # loop for an output sequence for i in six . moves . range ( olength ): if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( hs_pad [ 0 ], hlen [ 0 ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w ) att_ws . append ( att_w ) else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( hs_pad [ idx ], hlen [ idx ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ idx ]) hs_pad_han = torch . stack ( att_c_list , dim = 1 ) hlen_han = [ self . num_encs ] * len ( ys_in ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( hs_pad_han , hlen_han , self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ self . num_encs ]) att_ws . append ( att_w_list ) ey = torch . cat (( eys [:, i , :], att_c ), dim = 1 ) # utt x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , z_list , c_list ) if self . num_encs == 1 : # convert to numpy array with the shape (B, Lmax, Tmax) att_ws = att_to_numpy ( att_ws , self . att [ att_idx ]) else : _att_ws = [] for idx , ws in enumerate ( zip ( * att_ws )): ws = att_to_numpy ( ws , self . att [ idx ]) _att_ws . append ( ws ) att_ws = _att_ws return att_ws forward ( self , hs_pad , hlens , ys_pad , strm_idx = 0 , lang_ids = None ) Decoder forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index indicates the index of decoding stream. :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention loss value :rtype: torch.Tensor :return: accuracy :rtype: float Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def forward ( self , hs_pad , hlens , ys_pad , strm_idx = 0 , lang_ids = None ): \"\"\"Decoder forward :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D) [in multi-encoder case, list of torch.Tensor, [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ] :param torch.Tensor hlens: batch of lengths of hidden state sequences (B) [in multi-encoder case, list of torch.Tensor, [(B), (B), ..., ] :param torch.Tensor ys_pad: batch of padded character id sequence tensor (B, Lmax) :param int strm_idx: stream index indicates the index of decoding stream. :param torch.Tensor lang_ids: batch of target language id tensor (B, 1) :return: attention loss value :rtype: torch.Tensor :return: accuracy :rtype: float \"\"\" # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : hs_pad = [ hs_pad ] hlens = [ hlens ] # TODO(kan-bayashi): need to make more smart way ys = [ y [ y != self . ignore_id ] for y in ys_pad ] # parse padded ys # attention index for the attention module # in SPA (speaker parallel attention), att_idx is used to select attention module. In other cases, it is 0. att_idx = min ( strm_idx , len ( self . att ) - 1 ) # hlens should be list of list of integer hlens = [ list ( map ( int , hlens [ idx ])) for idx in range ( self . num_encs )] self . loss = None # prepare input and output word sequences with sos/eos IDs eos = ys [ 0 ] . new ([ self . eos ]) sos = ys [ 0 ] . new ([ self . sos ]) if self . replace_sos : ys_in = [ torch . cat ([ idx , y ], dim = 0 ) for idx , y in zip ( lang_ids , ys )] else : ys_in = [ torch . cat ([ sos , y ], dim = 0 ) for y in ys ] ys_out = [ torch . cat ([ y , eos ], dim = 0 ) for y in ys ] # padding for ys with -1 # pys: utt x olen ys_in_pad = pad_list ( ys_in , self . eos ) ys_out_pad = pad_list ( ys_out , self . ignore_id ) # get dim, length info batch = ys_out_pad . size ( 0 ) olength = ys_out_pad . size ( 1 ) for idx in range ( self . num_encs ): logging . info ( self . __class__ . __name__ + 'Number of Encoder: {} ; enc {} : input lengths: {} .' . format ( self . num_encs , idx + 1 , hlens [ idx ])) logging . info ( self . __class__ . __name__ + ' output lengths: ' + str ([ y . size ( 0 ) for y in ys_out ])) # initialization c_list = [ self . zero_state ( hs_pad [ 0 ])] z_list = [ self . zero_state ( hs_pad [ 0 ])] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( hs_pad [ 0 ])) z_list . append ( self . zero_state ( hs_pad [ 0 ])) z_all = [] if self . num_encs == 1 : att_w = None self . att [ att_idx ] . reset () # reset pre-computation of h else : att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han # pre-computation of embedding eys = self . dropout_emb ( self . embed ( ys_in_pad )) # utt x olen x zdim # loop for an output sequence for i in six . moves . range ( olength ): if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( hs_pad [ 0 ], hlens [ 0 ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w ) else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( hs_pad [ idx ], hlens [ idx ], self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ idx ]) hs_pad_han = torch . stack ( att_c_list , dim = 1 ) hlens_han = [ self . num_encs ] * len ( ys_in ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( hs_pad_han , hlens_han , self . dropout_dec [ 0 ]( z_list [ 0 ]), att_w_list [ self . num_encs ]) if i > 0 and random . random () < self . sampling_probability : logging . info ( ' scheduled sampling ' ) z_out = self . output ( z_all [ - 1 ]) z_out = np . argmax ( z_out . detach () . cpu (), axis = 1 ) z_out = self . dropout_emb ( self . embed ( to_device ( self , z_out ))) ey = torch . cat (( z_out , att_c ), dim = 1 ) # utt x (zdim + hdim) else : ey = torch . cat (( eys [:, i , :], att_c ), dim = 1 ) # utt x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , z_list , c_list ) if self . context_residual : z_all . append ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) # utt x (zdim + hdim) else : z_all . append ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) # utt x (zdim) z_all = torch . stack ( z_all , dim = 1 ) . view ( batch * olength , - 1 ) # compute loss y_all = self . output ( z_all ) if LooseVersion ( torch . __version__ ) < LooseVersion ( '1.0' ): reduction_str = 'elementwise_mean' else : reduction_str = 'mean' self . loss = F . cross_entropy ( y_all , ys_out_pad . view ( - 1 ), ignore_index = self . ignore_id , reduction = reduction_str ) # compute perplexity ppl = math . exp ( self . loss . item ()) # -1: eos, which is removed in the loss computation self . loss *= ( np . mean ([ len ( x ) for x in ys_in ]) - 1 ) acc = th_accuracy ( y_all , ys_out_pad , ignore_label = self . ignore_id ) logging . info ( 'att loss:' + '' . join ( str ( self . loss . item ()) . split ( ' \\n ' ))) # show predicted character sequence for debug if self . verbose > 0 and self . char_list is not None : ys_hat = y_all . view ( batch , olength , - 1 ) ys_true = ys_out_pad for ( i , y_hat ), y_true in zip ( enumerate ( ys_hat . detach () . cpu () . numpy ()), ys_true . detach () . cpu () . numpy ()): if i == MAX_DECODER_OUTPUT : break idx_hat = np . argmax ( y_hat [ y_true != self . ignore_id ], axis = 1 ) idx_true = y_true [ y_true != self . ignore_id ] seq_hat = [ self . char_list [ int ( idx )] for idx in idx_hat ] seq_true = [ self . char_list [ int ( idx )] for idx in idx_true ] seq_hat = \"\" . join ( seq_hat ) seq_true = \"\" . join ( seq_true ) logging . info ( \"groundtruth[ %d ]: \" % i + seq_true ) logging . info ( \"prediction [ %d ]: \" % i + seq_hat ) if self . labeldist is not None : if self . vlabeldist is None : self . vlabeldist = to_device ( self , torch . from_numpy ( self . labeldist )) loss_reg = - torch . sum (( F . log_softmax ( y_all , dim = 1 ) * self . vlabeldist ) . view ( - 1 ), dim = 0 ) / len ( ys_in ) self . loss = ( 1. - self . lsm_weight ) * self . loss + self . lsm_weight * loss_reg return self . loss , acc , ppl init_state ( self , x ) Get an initial state for decoding (optional). Parameters: Name Type Description Default x torch.Tensor The encoded feature tensor required Returns: initial state Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 def init_state ( self , x ): # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : x = [ x ] c_list = [ self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))] z_list = [ self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))) z_list . append ( self . zero_state ( x [ 0 ] . unsqueeze ( 0 ))) # TODO(karita): support strm_index for `asr_mix` strm_index = 0 att_idx = min ( strm_index , len ( self . att ) - 1 ) if self . num_encs == 1 : a = None self . att [ att_idx ] . reset () # reset pre-computation of h else : a = [ None ] * ( self . num_encs + 1 ) # atts + han for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han return dict ( c_prev = c_list [:], z_prev = z_list [:], a_prev = a , workspace = ( att_idx , z_list , c_list )) recognize_beam ( self , h , lpz , recog_args , char_list , rnnlm = None , strm_idx = 0 ) beam search implementation :param torch.Tensor h: encoder hidden state (T, eprojs) [in multi-encoder case, list of torch.Tensor, [(T1, eprojs), (T2, eprojs), ...] ] :param torch.Tensor lpz: ctc log softmax output (T, odim) [in multi-encoder case, list of torch.Tensor, [(T1, odim), (T2, odim), ...] ] :param Namespace recog_args: argument Namespace containing options :param char_list: list of character strings :param torch.nn.Module rnnlm: language module :param int strm_idx: stream index for speaker parallel attention in multi-speaker case :return: N-best decoding results :rtype: list of dicts Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def recognize_beam ( self , h , lpz , recog_args , char_list , rnnlm = None , strm_idx = 0 ): \"\"\"beam search implementation :param torch.Tensor h: encoder hidden state (T, eprojs) [in multi-encoder case, list of torch.Tensor, [(T1, eprojs), (T2, eprojs), ...] ] :param torch.Tensor lpz: ctc log softmax output (T, odim) [in multi-encoder case, list of torch.Tensor, [(T1, odim), (T2, odim), ...] ] :param Namespace recog_args: argument Namespace containing options :param char_list: list of character strings :param torch.nn.Module rnnlm: language module :param int strm_idx: stream index for speaker parallel attention in multi-speaker case :return: N-best decoding results :rtype: list of dicts \"\"\" # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : h = [ h ] lpz = [ lpz ] if self . num_encs > 1 and lpz is None : lpz = [ lpz ] * self . num_encs for idx in range ( self . num_encs ): logging . info ( 'Number of Encoder: {} ; enc {} : input lengths: {} .' . format ( self . num_encs , idx + 1 , h [ 0 ] . size ( 0 ))) att_idx = min ( strm_idx , len ( self . att ) - 1 ) # initialization c_list = [ self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))] z_list = [ self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))] for _ in six . moves . range ( 1 , self . dlayers ): c_list . append ( self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))) z_list . append ( self . zero_state ( h [ 0 ] . unsqueeze ( 0 ))) if self . num_encs == 1 : a = None self . att [ att_idx ] . reset () # reset pre-computation of h else : a = [ None ] * ( self . num_encs + 1 ) # atts + han att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han # search parms beam = recog_args . beam_size penalty = recog_args . penalty ctc_weight = getattr ( recog_args , \"ctc_weight\" , False ) # for NMT if lpz [ 0 ] is not None and self . num_encs > 1 : # weights-ctc, e.g. ctc_loss = w_1*ctc_1_loss + w_2 * ctc_2_loss + w_N * ctc_N_loss weights_ctc_dec = recog_args . weights_ctc_dec / np . sum ( recog_args . weights_ctc_dec ) # normalize logging . info ( 'ctc weights (decoding): ' + ' ' . join ([ str ( x ) for x in weights_ctc_dec ])) else : weights_ctc_dec = [ 1.0 ] # preprate sos if self . replace_sos and recog_args . tgt_lang : y = char_list . index ( recog_args . tgt_lang ) else : y = self . sos logging . info ( '<sos> index: ' + str ( y )) logging . info ( '<sos> mark: ' + char_list [ y ]) vy = h [ 0 ] . new_zeros ( 1 ) . long () maxlen = np . amin ([ h [ idx ] . size ( 0 ) for idx in range ( self . num_encs )]) if recog_args . maxlenratio != 0 : # maxlen >= 1 maxlen = max ( 1 , int ( recog_args . maxlenratio * maxlen )) minlen = int ( recog_args . minlenratio * maxlen ) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # initialize hypothesis if rnnlm : hyp = { 'score' : 0.0 , 'yseq' : [ y ], 'c_prev' : c_list , 'z_prev' : z_list , 'a_prev' : a , 'rnnlm_prev' : None } else : hyp = { 'score' : 0.0 , 'yseq' : [ y ], 'c_prev' : c_list , 'z_prev' : z_list , 'a_prev' : a } if lpz [ 0 ] is not None : ctc_prefix_score = [ CTCPrefixScore ( lpz [ idx ] . detach () . numpy (), 0 , self . eos , np ) for idx in range ( self . num_encs )] hyp [ 'ctc_state_prev' ] = [ ctc_prefix_score [ idx ] . initial_state () for idx in range ( self . num_encs )] hyp [ 'ctc_score_prev' ] = [ 0.0 ] * self . num_encs if ctc_weight != 1.0 : # pre-pruning based on attention scores ctc_beam = min ( lpz [ 0 ] . shape [ - 1 ], int ( beam * CTC_SCORING_RATIO )) else : ctc_beam = lpz [ 0 ] . shape [ - 1 ] hyps = [ hyp ] ended_hyps = [] for i in six . moves . range ( maxlen ): logging . debug ( 'position ' + str ( i )) hyps_best_kept = [] for hyp in hyps : vy . unsqueeze ( 1 ) vy [ 0 ] = hyp [ 'yseq' ][ i ] ey = self . dropout_emb ( self . embed ( vy )) # utt list (1) x zdim ey . unsqueeze ( 0 ) if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( h [ 0 ] . unsqueeze ( 0 ), [ h [ 0 ] . size ( 0 )], self . dropout_dec [ 0 ]( hyp [ 'z_prev' ][ 0 ]), hyp [ 'a_prev' ]) else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( h [ idx ] . unsqueeze ( 0 ), [ h [ idx ] . size ( 0 )], self . dropout_dec [ 0 ]( hyp [ 'z_prev' ][ 0 ]), hyp [ 'a_prev' ][ idx ]) h_han = torch . stack ( att_c_list , dim = 1 ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( h_han , [ self . num_encs ], self . dropout_dec [ 0 ]( hyp [ 'z_prev' ][ 0 ]), hyp [ 'a_prev' ][ self . num_encs ]) ey = torch . cat (( ey , att_c ), dim = 1 ) # utt(1) x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , hyp [ 'z_prev' ], hyp [ 'c_prev' ]) # get nbest local scores and their ids if self . context_residual : logits = self . output ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) else : logits = self . output ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) local_att_scores = F . log_softmax ( logits , dim = 1 ) if rnnlm : rnnlm_state , local_lm_scores = rnnlm . predict ( hyp [ 'rnnlm_prev' ], vy ) local_scores = local_att_scores + recog_args . lm_weight * local_lm_scores else : local_scores = local_att_scores if lpz [ 0 ] is not None : local_best_scores , local_best_ids = torch . topk ( local_att_scores , ctc_beam , dim = 1 ) ctc_scores , ctc_states = [ None ] * self . num_encs , [ None ] * self . num_encs for idx in range ( self . num_encs ): ctc_scores [ idx ], ctc_states [ idx ] = ctc_prefix_score [ idx ]( hyp [ 'yseq' ], local_best_ids [ 0 ], hyp [ 'ctc_state_prev' ][ idx ]) local_scores = \\ ( 1.0 - ctc_weight ) * local_att_scores [:, local_best_ids [ 0 ]] if self . num_encs == 1 : local_scores += ctc_weight * torch . from_numpy ( ctc_scores [ 0 ] - hyp [ 'ctc_score_prev' ][ 0 ]) else : for idx in range ( self . num_encs ): local_scores += ctc_weight * weights_ctc_dec [ idx ] * torch . from_numpy ( ctc_scores [ idx ] - hyp [ 'ctc_score_prev' ][ idx ]) if rnnlm : local_scores += recog_args . lm_weight * local_lm_scores [:, local_best_ids [ 0 ]] local_best_scores , joint_best_ids = torch . topk ( local_scores , beam , dim = 1 ) local_best_ids = local_best_ids [:, joint_best_ids [ 0 ]] else : local_best_scores , local_best_ids = torch . topk ( local_scores , beam , dim = 1 ) for j in six . moves . range ( beam ): new_hyp = {} # [:] is needed! new_hyp [ 'z_prev' ] = z_list [:] new_hyp [ 'c_prev' ] = c_list [:] if self . num_encs == 1 : new_hyp [ 'a_prev' ] = att_w [:] else : new_hyp [ 'a_prev' ] = [ att_w_list [ idx ][:] for idx in range ( self . num_encs + 1 )] new_hyp [ 'score' ] = hyp [ 'score' ] + local_best_scores [ 0 , j ] new_hyp [ 'yseq' ] = [ 0 ] * ( 1 + len ( hyp [ 'yseq' ])) new_hyp [ 'yseq' ][: len ( hyp [ 'yseq' ])] = hyp [ 'yseq' ] new_hyp [ 'yseq' ][ len ( hyp [ 'yseq' ])] = int ( local_best_ids [ 0 , j ]) if rnnlm : new_hyp [ 'rnnlm_prev' ] = rnnlm_state if lpz [ 0 ] is not None : new_hyp [ 'ctc_state_prev' ] = [ ctc_states [ idx ][ joint_best_ids [ 0 , j ]] for idx in range ( self . num_encs )] new_hyp [ 'ctc_score_prev' ] = [ ctc_scores [ idx ][ joint_best_ids [ 0 , j ]] for idx in range ( self . num_encs )] # will be (2 x beam) hyps at most hyps_best_kept . append ( new_hyp ) hyps_best_kept = sorted ( hyps_best_kept , key = lambda x : x [ 'score' ], reverse = True )[: beam ] # sort and get nbest hyps = hyps_best_kept logging . debug ( 'number of pruned hypotheses: ' + str ( len ( hyps ))) logging . debug ( 'best hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyps [ 0 ][ 'yseq' ][ 1 :]])) # add eos in the final loop to avoid that there are no ended hyps if i == maxlen - 1 : logging . info ( 'adding <eos> in the last position in the loop' ) for hyp in hyps : hyp [ 'yseq' ] . append ( self . eos ) # add ended hypotheses to a final list, and removed them from current hypotheses # (this will be a problem, number of hyps < beam) remained_hyps = [] for hyp in hyps : if hyp [ 'yseq' ][ - 1 ] == self . eos : # only store the sequence that has more than minlen outputs # also add penalty if len ( hyp [ 'yseq' ]) > minlen : hyp [ 'score' ] += ( i + 1 ) * penalty if rnnlm : # Word LM needs to add final <eos> score hyp [ 'score' ] += recog_args . lm_weight * rnnlm . final ( hyp [ 'rnnlm_prev' ]) ended_hyps . append ( hyp ) else : remained_hyps . append ( hyp ) # end detection if end_detect ( ended_hyps , i ) and recog_args . maxlenratio == 0.0 : logging . info ( 'end detected at %d ' , i ) break hyps = remained_hyps if len ( hyps ) > 0 : logging . debug ( 'remaining hypotheses: ' + str ( len ( hyps ))) else : logging . info ( 'no hypothesis. Finish decoding.' ) break for hyp in hyps : logging . debug ( 'hypo: ' + '' . join ([ char_list [ int ( x )] for x in hyp [ 'yseq' ][ 1 :]])) logging . debug ( 'number of ended hypotheses: ' + str ( len ( ended_hyps ))) nbest_hyps = sorted ( ended_hyps , key = lambda x : x [ 'score' ], reverse = True )[: min ( len ( ended_hyps ), recog_args . nbest )] # check number of hypotheses if len ( nbest_hyps ) == 0 : logging . warning ( 'there is no N-best results, perform recognition again with smaller minlenratio.' ) # should copy because Namespace will be overwritten globally recog_args = Namespace ( ** vars ( recog_args )) recog_args . minlenratio = max ( 0.0 , recog_args . minlenratio - 0.1 ) if self . num_encs == 1 : return self . recognize_beam ( h [ 0 ], lpz [ 0 ], recog_args , char_list , rnnlm ) else : return self . recognize_beam ( h , lpz , recog_args , char_list , rnnlm ) logging . info ( 'total log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ])) logging . info ( 'normalized log probability: ' + str ( nbest_hyps [ 0 ][ 'score' ] / len ( nbest_hyps [ 0 ][ 'yseq' ]))) # remove sos return nbest_hyps recognize_beam_batch ( self , h , hlens , lpz , recog_args , char_list , rnnlm = None , normalize_score = True , strm_idx = 0 , lang_ids = None ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 def recognize_beam_batch ( self , h , hlens , lpz , recog_args , char_list , rnnlm = None , normalize_score = True , strm_idx = 0 , lang_ids = None ): # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : h = [ h ] hlens = [ hlens ] lpz = [ lpz ] if self . num_encs > 1 and lpz is None : lpz = [ lpz ] * self . num_encs att_idx = min ( strm_idx , len ( self . att ) - 1 ) for idx in range ( self . num_encs ): logging . info ( 'Number of Encoder: {} ; enc {} : input lengths: {} .' . format ( self . num_encs , idx + 1 , h [ idx ] . size ( 1 ))) h [ idx ] = mask_by_length ( h [ idx ], hlens [ idx ], 0.0 ) # search params batch = len ( hlens [ 0 ]) beam = recog_args . beam_size penalty = recog_args . penalty ctc_weight = getattr ( recog_args , \"ctc_weight\" , 0 ) # for NMT att_weight = 1.0 - ctc_weight ctc_margin = getattr ( recog_args , \"ctc_window_margin\" , 0 ) # use getattr to keep compatibility # weights-ctc, e.g. ctc_loss = w_1*ctc_1_loss + w_2 * ctc_2_loss + w_N * ctc_N_loss if lpz [ 0 ] is not None and self . num_encs > 1 : weights_ctc_dec = recog_args . weights_ctc_dec / np . sum ( recog_args . weights_ctc_dec ) # normalize logging . info ( 'ctc weights (decoding): ' + ' ' . join ([ str ( x ) for x in weights_ctc_dec ])) else : weights_ctc_dec = [ 1.0 ] n_bb = batch * beam pad_b = to_device ( self , torch . arange ( batch ) * beam ) . view ( - 1 , 1 ) max_hlen = np . amin ([ max ( hlens [ idx ]) for idx in range ( self . num_encs )]) if recog_args . maxlenratio == 0 : maxlen = max_hlen else : maxlen = max ( 1 , int ( recog_args . maxlenratio * max_hlen )) minlen = int ( recog_args . minlenratio * max_hlen ) logging . info ( 'max output length: ' + str ( maxlen )) logging . info ( 'min output length: ' + str ( minlen )) # initialization c_prev = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] z_prev = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] c_list = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] z_list = [ to_device ( self , torch . zeros ( n_bb , self . dunits )) for _ in range ( self . dlayers )] vscores = to_device ( self , torch . zeros ( batch , beam )) rnnlm_state = None if self . num_encs == 1 : a_prev = [ None ] att_w_list , ctc_scorer , ctc_state = [ None ], [ None ], [ None ] self . att [ att_idx ] . reset () # reset pre-computation of h else : a_prev = [ None ] * ( self . num_encs + 1 ) # atts + han att_w_list = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts ctc_scorer , ctc_state = [ None ] * ( self . num_encs ), [ None ] * ( self . num_encs ) for idx in range ( self . num_encs + 1 ): self . att [ idx ] . reset () # reset pre-computation of h in atts and han if self . replace_sos and recog_args . tgt_lang : logging . info ( '<sos> index: ' + str ( char_list . index ( recog_args . tgt_lang ))) logging . info ( '<sos> mark: ' + recog_args . tgt_lang ) yseq = [[ char_list . index ( recog_args . tgt_lang )] for _ in six . moves . range ( n_bb )] elif lang_ids is not None : # NOTE: used for evaluation during training yseq = [[ lang_ids [ b // recog_args . beam_size ]] for b in six . moves . range ( n_bb )] else : logging . info ( '<sos> index: ' + str ( self . sos )) logging . info ( '<sos> mark: ' + char_list [ self . sos ]) yseq = [[ self . sos ] for _ in six . moves . range ( n_bb )] accum_odim_ids = [ self . sos for _ in six . moves . range ( n_bb )] stop_search = [ False for _ in six . moves . range ( batch )] nbest_hyps = [[] for _ in six . moves . range ( batch )] ended_hyps = [[] for _ in range ( batch )] exp_hlens = [ hlens [ idx ] . repeat ( beam ) . view ( beam , batch ) . transpose ( 0 , 1 ) . contiguous () for idx in range ( self . num_encs )] exp_hlens = [ exp_hlens [ idx ] . view ( - 1 ) . tolist () for idx in range ( self . num_encs )] exp_h = [ h [ idx ] . unsqueeze ( 1 ) . repeat ( 1 , beam , 1 , 1 ) . contiguous () for idx in range ( self . num_encs )] exp_h = [ exp_h [ idx ] . view ( n_bb , h [ idx ] . size ()[ 1 ], h [ idx ] . size ()[ 2 ]) for idx in range ( self . num_encs )] if lpz [ 0 ] is not None : scoring_ratio = CTC_SCORING_RATIO if att_weight > 0.0 and not lpz [ 0 ] . is_cuda else 0 ctc_scorer = [ CTCPrefixScoreTH ( lpz [ idx ], hlens [ idx ], 0 , self . eos , beam , scoring_ratio , margin = ctc_margin ) for idx in range ( self . num_encs )] for i in six . moves . range ( maxlen ): logging . debug ( 'position ' + str ( i )) vy = to_device ( self , torch . LongTensor ( self . _get_last_yseq ( yseq ))) ey = self . dropout_emb ( self . embed ( vy )) if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( exp_h [ 0 ], exp_hlens [ 0 ], self . dropout_dec [ 0 ]( z_prev [ 0 ]), a_prev [ 0 ]) att_w_list = [ att_w ] else : for idx in range ( self . num_encs ): att_c_list [ idx ], att_w_list [ idx ] = self . att [ idx ]( exp_h [ idx ], exp_hlens [ idx ], self . dropout_dec [ 0 ]( z_prev [ 0 ]), a_prev [ idx ]) exp_h_han = torch . stack ( att_c_list , dim = 1 ) att_c , att_w_list [ self . num_encs ] = self . att [ self . num_encs ]( exp_h_han , [ self . num_encs ] * n_bb , self . dropout_dec [ 0 ]( z_prev [ 0 ]), a_prev [ self . num_encs ]) ey = torch . cat (( ey , att_c ), dim = 1 ) # attention decoder z_list , c_list = self . rnn_forward ( ey , z_list , c_list , z_prev , c_prev ) if self . context_residual : logits = self . output ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) else : logits = self . output ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) local_scores = att_weight * F . log_softmax ( logits , dim = 1 ) # rnnlm if rnnlm : rnnlm_state , local_lm_scores = rnnlm . buff_predict ( rnnlm_state , vy , n_bb ) local_scores = local_scores + recog_args . lm_weight * local_lm_scores # ctc if ctc_scorer [ 0 ]: for idx in range ( self . num_encs ): att_w = att_w_list [ idx ] att_w_ = att_w if isinstance ( att_w , torch . Tensor ) else att_w [ 0 ] ctc_state [ idx ], local_ctc_scores = ctc_scorer [ idx ]( yseq , ctc_state [ idx ], local_scores , att_w_ ) local_scores = local_scores + ctc_weight * weights_ctc_dec [ idx ] * local_ctc_scores local_scores = local_scores . view ( batch , beam , self . odim ) if i == 0 : local_scores [:, 1 :, :] = self . logzero # accumulate scores eos_vscores = local_scores [:, :, self . eos ] + vscores vscores = vscores . view ( batch , beam , 1 ) . repeat ( 1 , 1 , self . odim ) vscores [:, :, self . eos ] = self . logzero vscores = ( vscores + local_scores ) . view ( batch , - 1 ) # global pruning accum_best_scores , accum_best_ids = torch . topk ( vscores , beam , 1 ) accum_odim_ids = torch . fmod ( accum_best_ids , self . odim ) . view ( - 1 ) . data . cpu () . tolist () accum_padded_beam_ids = ( torch . div ( accum_best_ids , self . odim ) + pad_b ) . view ( - 1 ) . data . cpu () . tolist () y_prev = yseq [:][:] yseq = self . _index_select_list ( yseq , accum_padded_beam_ids ) yseq = self . _append_ids ( yseq , accum_odim_ids ) vscores = accum_best_scores vidx = to_device ( self , torch . LongTensor ( accum_padded_beam_ids )) a_prev = [] num_atts = self . num_encs if self . num_encs == 1 else self . num_encs + 1 for idx in range ( num_atts ): if isinstance ( att_w_list [ idx ], torch . Tensor ): _a_prev = torch . index_select ( att_w_list [ idx ] . view ( n_bb , * att_w_list [ idx ] . shape [ 1 :]), 0 , vidx ) elif isinstance ( att_w_list [ idx ], list ): # handle the case of multi-head attention _a_prev = [ torch . index_select ( att_w_one . view ( n_bb , - 1 ), 0 , vidx ) for att_w_one in att_w_list [ idx ]] else : # handle the case of location_recurrent when return is a tuple _a_prev_ = torch . index_select ( att_w_list [ idx ][ 0 ] . view ( n_bb , - 1 ), 0 , vidx ) _h_prev_ = torch . index_select ( att_w_list [ idx ][ 1 ][ 0 ] . view ( n_bb , - 1 ), 0 , vidx ) _c_prev_ = torch . index_select ( att_w_list [ idx ][ 1 ][ 1 ] . view ( n_bb , - 1 ), 0 , vidx ) _a_prev = ( _a_prev_ , ( _h_prev_ , _c_prev_ )) a_prev . append ( _a_prev ) z_prev = [ torch . index_select ( z_list [ li ] . view ( n_bb , - 1 ), 0 , vidx ) for li in range ( self . dlayers )] c_prev = [ torch . index_select ( c_list [ li ] . view ( n_bb , - 1 ), 0 , vidx ) for li in range ( self . dlayers )] # pick ended hyps if i > minlen : k = 0 penalty_i = ( i + 1 ) * penalty thr = accum_best_scores [:, - 1 ] for samp_i in six . moves . range ( batch ): if stop_search [ samp_i ]: k = k + beam continue for beam_j in six . moves . range ( beam ): if eos_vscores [ samp_i , beam_j ] > thr [ samp_i ]: yk = y_prev [ k ][:] yk . append ( self . eos ) if len ( yk ) < min ( hlens [ idx ][ samp_i ] for idx in range ( self . num_encs )): _vscore = eos_vscores [ samp_i ][ beam_j ] + penalty_i if rnnlm : _vscore += recog_args . lm_weight * rnnlm . final ( rnnlm_state , index = k ) _score = _vscore . data . cpu () . numpy () ended_hyps [ samp_i ] . append ({ 'yseq' : yk , 'vscore' : _vscore , 'score' : _score }) k = k + 1 # end detection stop_search = [ stop_search [ samp_i ] or end_detect ( ended_hyps [ samp_i ], i ) for samp_i in six . moves . range ( batch )] stop_search_summary = list ( set ( stop_search )) if len ( stop_search_summary ) == 1 and stop_search_summary [ 0 ]: break if rnnlm : rnnlm_state = self . _index_select_lm_state ( rnnlm_state , 0 , vidx ) if ctc_scorer [ 0 ]: for idx in range ( self . num_encs ): ctc_state [ idx ] = ctc_scorer [ idx ] . index_select_state ( ctc_state [ idx ], accum_best_ids ) torch . cuda . empty_cache () dummy_hyps = [{ 'yseq' : [ self . sos , self . eos ], 'score' : np . array ([ - float ( 'inf' )])}] ended_hyps = [ ended_hyps [ samp_i ] if len ( ended_hyps [ samp_i ]) != 0 else dummy_hyps for samp_i in six . moves . range ( batch )] if normalize_score : for samp_i in six . moves . range ( batch ): for x in ended_hyps [ samp_i ]: x [ 'score' ] /= len ( x [ 'yseq' ]) nbest_hyps = [ sorted ( ended_hyps [ samp_i ], key = lambda x : x [ 'score' ], reverse = True )[: min ( len ( ended_hyps [ samp_i ]), recog_args . nbest )] for samp_i in six . moves . range ( batch )] return nbest_hyps rnn_forward ( self , ey , z_list , c_list , z_prev , c_prev ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 102 103 104 105 106 107 108 109 110 111 112 def rnn_forward ( self , ey , z_list , c_list , z_prev , c_prev ): if self . dtype == \"lstm\" : z_list [ 0 ], c_list [ 0 ] = self . decoder [ 0 ]( ey , ( z_prev [ 0 ], c_prev [ 0 ])) for l in six . moves . range ( 1 , self . dlayers ): z_list [ l ], c_list [ l ] = self . decoder [ l ]( self . dropout_dec [ l - 1 ]( z_list [ l - 1 ]), ( z_prev [ l ], c_prev [ l ])) else : z_list [ 0 ] = self . decoder [ 0 ]( ey , z_prev [ 0 ]) for l in six . moves . range ( 1 , self . dlayers ): z_list [ l ] = self . decoder [ l ]( self . dropout_dec [ l - 1 ]( z_list [ l - 1 ]), z_prev [ l ]) return z_list , c_list score ( self , yseq , state , x ) Score new token (required). Parameters: Name Type Description Default y torch.Tensor 1D torch.int64 prefix tokens. required state Scorer state for prefix tokens required x torch.Tensor The encoder feature that generates ys. required Returns: Type Description tuple[torch.Tensor, Any] Tuple of scores for next token that has a shape of (n_vocab) and next state for ys Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 def score ( self , yseq , state , x ): # to support mutiple encoder asr mode, in single encoder mode, convert torch.Tensor to List of torch.Tensor if self . num_encs == 1 : x = [ x ] att_idx , z_list , c_list = state [ \"workspace\" ] vy = yseq [ - 1 ] . unsqueeze ( 0 ) ey = self . dropout_emb ( self . embed ( vy )) # utt list (1) x zdim if self . num_encs == 1 : att_c , att_w = self . att [ att_idx ]( x [ 0 ] . unsqueeze ( 0 ), [ x [ 0 ] . size ( 0 )], self . dropout_dec [ 0 ]( state [ 'z_prev' ][ 0 ]), state [ 'a_prev' ]) else : att_w = [ None ] * ( self . num_encs + 1 ) # atts + han att_c_list = [ None ] * ( self . num_encs ) # atts for idx in range ( self . num_encs ): att_c_list [ idx ], att_w [ idx ] = self . att [ idx ]( x [ idx ] . unsqueeze ( 0 ), [ x [ idx ] . size ( 0 )], self . dropout_dec [ 0 ]( state [ 'z_prev' ][ 0 ]), state [ 'a_prev' ][ idx ]) h_han = torch . stack ( att_c_list , dim = 1 ) att_c , att_w [ self . num_encs ] = self . att [ self . num_encs ]( h_han , [ self . num_encs ], self . dropout_dec [ 0 ]( state [ 'z_prev' ][ 0 ]), state [ 'a_prev' ][ self . num_encs ]) ey = torch . cat (( ey , att_c ), dim = 1 ) # utt(1) x (zdim + hdim) z_list , c_list = self . rnn_forward ( ey , z_list , c_list , state [ 'z_prev' ], state [ 'c_prev' ]) if self . context_residual : logits = self . output ( torch . cat (( self . dropout_dec [ - 1 ]( z_list [ - 1 ]), att_c ), dim =- 1 )) else : logits = self . output ( self . dropout_dec [ - 1 ]( z_list [ - 1 ])) logp = F . log_softmax ( logits , dim = 1 ) . squeeze ( 0 ) return logp , dict ( c_prev = c_list [:], z_prev = z_list [:], a_prev = att_w , workspace = ( att_idx , z_list , c_list )) zero_state ( self , hs_pad ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 99 100 def zero_state ( self , hs_pad ): return hs_pad . new_zeros ( hs_pad . size ( 0 ), self . dunits ) decoder_for ( args , odim , sos , eos , att , labeldist ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/decoders.py 899 900 901 902 903 904 905 def decoder_for ( args , odim , sos , eos , att , labeldist ): return Decoder ( args . eprojs , odim , args . dtype , args . dlayers , args . dunits , sos , eos , att , args . verbose , args . char_list , labeldist , args . lsm_weight , args . sampling_probability , args . dropout_rate_decoder , getattr ( args , \"context_residual\" , False ), # use getattr to keep compatibility getattr ( args , \"replace_sos\" , False ), # use getattr to keep compatibility getattr ( args , \"num_encs\" , 1 )) # use getattr to keep compatibility","title":"decoders"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.rnn.encoders","text":"Encoder Encoder module :param str etype: type of encoder network :param int idim: number of dimensions of encoder network :param int elayers: number of layers of encoder network :param int eunits: number of lstm units of encoder network :param int eprojs: number of projection units of encoder network :param np.ndarray subsample: list of subsampling numbers :param float dropout: dropout rate :param int in_channel: number of input channels __init__ ( self , etype , idim , elayers , eunits , eprojs , subsample , dropout , in_channel = 1 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def __init__ ( self , etype , idim , elayers , eunits , eprojs , subsample , dropout , in_channel = 1 ): super ( Encoder , self ) . __init__ () typ = etype . lstrip ( \"vgg\" ) . rstrip ( \"p\" ) if typ not in [ 'lstm' , 'gru' , 'blstm' , 'bgru' ]: logging . error ( \"Error: need to specify an appropriate encoder architecture\" ) if etype . startswith ( \"vgg\" ): if etype [ - 1 ] == \"p\" : self . enc = torch . nn . ModuleList ([ VGG2L ( in_channel ), RNNP ( get_vgg2l_odim ( idim , in_channel = in_channel ), elayers , eunits , eprojs , subsample , dropout , typ = typ )]) logging . info ( 'Use CNN-VGG + ' + typ . upper () + 'P for encoder' ) else : self . enc = torch . nn . ModuleList ([ VGG2L ( in_channel ), RNN ( get_vgg2l_odim ( idim , in_channel = in_channel ), elayers , eunits , eprojs , dropout , typ = typ )]) logging . info ( 'Use CNN-VGG + ' + typ . upper () + ' for encoder' ) else : if etype [ - 1 ] == \"p\" : self . enc = torch . nn . ModuleList ( [ RNNP ( idim , elayers , eunits , eprojs , subsample , dropout , typ = typ )]) logging . info ( typ . upper () + ' with every-layer projection for encoder' ) else : self . enc = torch . nn . ModuleList ([ RNN ( idim , elayers , eunits , eprojs , dropout , typ = typ )]) logging . info ( typ . upper () + ' without projection for encoder' ) forward ( self , xs_pad , ilens , prev_states = None ) Encoder forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous encoder hidden states (?, ...) :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def forward ( self , xs_pad , ilens , prev_states = None ): \"\"\"Encoder forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous encoder hidden states (?, ...) :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor \"\"\" if prev_states is None : prev_states = [ None ] * len ( self . enc ) assert len ( prev_states ) == len ( self . enc ) current_states = [] for module , prev_state in zip ( self . enc , prev_states ): xs_pad , ilens , states = module ( xs_pad , ilens , prev_state = prev_state ) current_states . append ( states ) # make mask to remove bias value in padded part mask = to_device ( self , make_pad_mask ( ilens ) . unsqueeze ( - 1 )) return xs_pad . masked_fill ( mask , 0.0 ), ilens , current_states RNN RNN module :param int idim: dimension of inputs :param int elayers: number of encoder layers :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional) :param int hdim: number of final projection units :param float dropout: dropout rate :param str typ: The RNN type __init__ ( self , idim , elayers , cdim , hdim , dropout , typ = 'blstm' ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , idim , elayers , cdim , hdim , dropout , typ = \"blstm\" ): super ( RNN , self ) . __init__ () bidir = typ [ 0 ] == \"b\" self . nbrnn = torch . nn . LSTM ( idim , cdim , elayers , batch_first = True , dropout = dropout , bidirectional = bidir ) if \"lstm\" in typ \\ else torch . nn . GRU ( idim , cdim , elayers , batch_first = True , dropout = dropout , bidirectional = bidir ) if bidir : self . l_last = torch . nn . Linear ( cdim * 2 , hdim ) else : self . l_last = torch . nn . Linear ( cdim , hdim ) self . typ = typ forward ( self , xs_pad , ilens , prev_state = None ) RNN forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def forward ( self , xs_pad , ilens , prev_state = None ): \"\"\"RNN forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, eprojs) :rtype: torch.Tensor \"\"\" logging . debug ( self . __class__ . __name__ + ' input lengths: ' + str ( ilens )) xs_pack = pack_padded_sequence ( xs_pad , ilens , batch_first = True ) self . nbrnn . flatten_parameters () if prev_state is not None and self . nbrnn . bidirectional : # We assume that when previous state is passed, it means that we're streaming the input # and therefore cannot propagate backward BRNN state (otherwise it goes in the wrong direction) prev_state = reset_backward_rnn_state ( prev_state ) ys , states = self . nbrnn ( xs_pack , hx = prev_state ) # ys: utt list of frame x cdim x 2 (2: means bidirectional) ys_pad , ilens = pad_packed_sequence ( ys , batch_first = True ) # (sum _utt frame_utt) x dim projected = torch . tanh ( self . l_last ( ys_pad . contiguous () . view ( - 1 , ys_pad . size ( 2 )))) xs_pad = projected . view ( ys_pad . size ( 0 ), ys_pad . size ( 1 ), - 1 ) return xs_pad , ilens , states # x: utt list of frame x dim RNNP RNN with projection layer module :param int idim: dimension of inputs :param int elayers: number of encoder layers :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional) :param int hdim: number of projection units :param np.ndarray subsample: list of subsampling numbers :param float dropout: dropout rate :param str typ: The RNN type __init__ ( self , idim , elayers , cdim , hdim , subsample , dropout , typ = 'blstm' ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , idim , elayers , cdim , hdim , subsample , dropout , typ = \"blstm\" ): super ( RNNP , self ) . __init__ () bidir = typ [ 0 ] == \"b\" for i in six . moves . range ( elayers ): if i == 0 : inputdim = idim else : inputdim = hdim rnn = torch . nn . LSTM ( inputdim , cdim , dropout = dropout , num_layers = 1 , bidirectional = bidir , batch_first = True ) if \"lstm\" in typ \\ else torch . nn . GRU ( inputdim , cdim , dropout = dropout , num_layers = 1 , bidirectional = bidir , batch_first = True ) setattr ( self , \" %s%d \" % ( \"birnn\" if bidir else \"rnn\" , i ), rnn ) # bottleneck layer to merge if bidir : setattr ( self , \"bt %d \" % i , torch . nn . Linear ( 2 * cdim , hdim )) else : setattr ( self , \"bt %d \" % i , torch . nn . Linear ( cdim , hdim )) self . elayers = elayers self . cdim = cdim self . subsample = subsample self . typ = typ self . bidir = bidir forward ( self , xs_pad , ilens , prev_state = None ) RNNP forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, hdim) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def forward ( self , xs_pad , ilens , prev_state = None ): \"\"\"RNNP forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim) :param torch.Tensor ilens: batch of lengths of input sequences (B) :param torch.Tensor prev_state: batch of previous RNN states :return: batch of hidden state sequences (B, Tmax, hdim) :rtype: torch.Tensor \"\"\" logging . debug ( self . __class__ . __name__ + ' input lengths: ' + str ( ilens )) elayer_states = [] for layer in six . moves . range ( self . elayers ): xs_pack = pack_padded_sequence ( xs_pad , ilens , batch_first = True ) rnn = getattr ( self , ( \"birnn\" if self . bidir else \"rnn\" ) + str ( layer )) rnn . flatten_parameters () if prev_state is not None and rnn . bidirectional : prev_state = reset_backward_rnn_state ( prev_state ) ys , states = rnn ( xs_pack , hx = None if prev_state is None else prev_state [ layer ]) elayer_states . append ( states ) # ys: utt list of frame x cdim x 2 (2: means bidirectional) ys_pad , ilens = pad_packed_sequence ( ys , batch_first = True ) sub = self . subsample [ layer + 1 ] if sub > 1 : ys_pad = ys_pad [:, :: sub ] ilens = [ int ( i + 1 ) // sub for i in ilens ] # (sum _utt frame_utt) x dim projected = getattr ( self , 'bt' + str ( layer ) )( ys_pad . contiguous () . view ( - 1 , ys_pad . size ( 2 ))) if layer == self . elayers - 1 : xs_pad = projected . view ( ys_pad . size ( 0 ), ys_pad . size ( 1 ), - 1 ) else : xs_pad = torch . tanh ( projected . view ( ys_pad . size ( 0 ), ys_pad . size ( 1 ), - 1 )) return xs_pad , ilens , elayer_states # x: utt list of frame x dim VGG2L VGG-like module :param int in_channel: number of input channels __init__ ( self , in_channel = 1 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 153 154 155 156 157 158 159 160 161 def __init__ ( self , in_channel = 1 ): super ( VGG2L , self ) . __init__ () # CNN layer (VGG motivated) self . conv1_1 = torch . nn . Conv2d ( in_channel , 64 , 3 , stride = 1 , padding = 1 ) self . conv1_2 = torch . nn . Conv2d ( 64 , 64 , 3 , stride = 1 , padding = 1 ) self . conv2_1 = torch . nn . Conv2d ( 64 , 128 , 3 , stride = 1 , padding = 1 ) self . conv2_2 = torch . nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ) self . in_channel = in_channel forward ( self , xs_pad , ilens , ** kwargs ) VGG2L forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :return: batch of padded hidden state sequences (B, Tmax // 4, 128 * D // 4) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def forward ( self , xs_pad , ilens , ** kwargs ): \"\"\"VGG2L forward :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D) :param torch.Tensor ilens: batch of lengths of input sequences (B) :return: batch of padded hidden state sequences (B, Tmax // 4, 128 * D // 4) :rtype: torch.Tensor \"\"\" logging . debug ( self . __class__ . __name__ + ' input lengths: ' + str ( ilens )) # x: utt x frame x dim # xs_pad = F.pad_sequence(xs_pad) # x: utt x 1 (input channel num) x frame x dim xs_pad = xs_pad . view ( xs_pad . size ( 0 ), xs_pad . size ( 1 ), self . in_channel , xs_pad . size ( 2 ) // self . in_channel ) . transpose ( 1 , 2 ) # NOTE: max_pool1d ? xs_pad = F . relu ( self . conv1_1 ( xs_pad )) xs_pad = F . relu ( self . conv1_2 ( xs_pad )) xs_pad = F . max_pool2d ( xs_pad , 2 , stride = 2 , ceil_mode = True ) xs_pad = F . relu ( self . conv2_1 ( xs_pad )) xs_pad = F . relu ( self . conv2_2 ( xs_pad )) xs_pad = F . max_pool2d ( xs_pad , 2 , stride = 2 , ceil_mode = True ) if torch . is_tensor ( ilens ): ilens = ilens . cpu () . numpy () else : ilens = np . array ( ilens , dtype = np . float32 ) ilens = np . array ( np . ceil ( ilens / 2 ), dtype = np . int64 ) ilens = np . array ( np . ceil ( np . array ( ilens , dtype = np . float32 ) / 2 ), dtype = np . int64 ) . tolist () # x: utt_list of frame (remove zeropaded frames) x (input channel num x dim) xs_pad = xs_pad . transpose ( 1 , 2 ) xs_pad = xs_pad . contiguous () . view ( xs_pad . size ( 0 ), xs_pad . size ( 1 ), xs_pad . size ( 2 ) * xs_pad . size ( 3 )) return xs_pad , ilens , None # no state in this layer encoder_for ( args , idim , subsample ) Instantiates an encoder module given the program arguments :param Namespace args: The arguments :param int or List of integer idim: dimension of input, e.g. 83, or List of dimensions of inputs, e.g. [83,83] :param List or List of List subsample: subsample factors, e.g. [1,2,2,1,1], or List of subsample factors of each encoder. e.g. [[1,2,2,1,1], [1,2,2,1,1]] :rtype torch.nn.Module :return: The encoder module Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def encoder_for ( args , idim , subsample ): \"\"\"Instantiates an encoder module given the program arguments :param Namespace args: The arguments :param int or List of integer idim: dimension of input, e.g. 83, or List of dimensions of inputs, e.g. [83,83] :param List or List of List subsample: subsample factors, e.g. [1,2,2,1,1], or List of subsample factors of each encoder. e.g. [[1,2,2,1,1], [1,2,2,1,1]] :rtype torch.nn.Module :return: The encoder module \"\"\" num_encs = getattr ( args , \"num_encs\" , 1 ) # use getattr to keep compatibility if num_encs == 1 : # compatible with single encoder asr mode return Encoder ( args . etype , idim , args . elayers , args . eunits , args . eprojs , subsample , args . dropout_rate ) elif num_encs >= 1 : enc_list = torch . nn . ModuleList () for idx in range ( num_encs ): enc = Encoder ( args . etype [ idx ], idim [ idx ], args . elayers [ idx ], args . eunits [ idx ], args . eprojs , subsample [ idx ], args . dropout_rate [ idx ]) enc_list . append ( enc ) return enc_list else : raise ValueError ( \"Number of encoders needs to be more than one. {} \" . format ( num_encs )) reset_backward_rnn_state ( states ) Sets backward BRNN states to zeroes - useful in processing of sliding windows over the inputs Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/rnn/encoders.py 137 138 139 140 141 142 143 144 def reset_backward_rnn_state ( states ): \"\"\"Sets backward BRNN states to zeroes - useful in processing of sliding windows over the inputs\"\"\" if isinstance ( states , ( list , tuple )): for state in states : state [ 1 :: 2 ] = 0. else : states [ 1 :: 2 ] = 0. return states","title":"encoders"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.streaming","text":"","title":"streaming"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.streaming.segment","text":"SegmentStreamingE2E SegmentStreamingE2E constructor. :param E2E e2e: E2E ASR object :param recog_args: arguments for \"recognize\" method of E2E __init__ ( self , e2e , recog_args , rnnlm = None ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/segment.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , e2e , recog_args , rnnlm = None ): self . _e2e = e2e self . _recog_args = recog_args self . _char_list = e2e . char_list self . _rnnlm = rnnlm self . _e2e . eval () self . _blank_idx_in_char_list = - 1 for idx in range ( len ( self . _char_list )): if self . _char_list [ idx ] == self . _e2e . blank : self . _blank_idx_in_char_list = idx break self . _subsampling_factor = np . prod ( e2e . subsample ) self . _activates = 0 self . _blank_dur = 0 self . _previous_input = [] self . _previous_encoder_recurrent_state = None self . _encoder_states = [] self . _ctc_posteriors = [] assert self . _recog_args . batchsize <= 1 , \\ \"SegmentStreamingE2E works only with batch size <= 1\" assert \"b\" not in self . _e2e . etype , \\ \"SegmentStreamingE2E works only with uni-directional encoders\" accept_input ( self , x ) Call this method each time a new batch of input is available. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/segment.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def accept_input ( self , x ): \"\"\"Call this method each time a new batch of input is available.\"\"\" self . _previous_input . extend ( x ) h , ilen = self . _e2e . subsample_frames ( x ) # Run encoder and apply greedy search on CTC softmax output h , _ , self . _previous_encoder_recurrent_state = self . _e2e . enc ( h . unsqueeze ( 0 ), ilen , self . _previous_encoder_recurrent_state ) z = self . _e2e . ctc . argmax ( h ) . squeeze ( 0 ) if self . _activates == 0 and z [ 0 ] != self . _blank_idx_in_char_list : self . _activates = 1 # Rerun encoder with zero state at onset of detection tail_len = self . _subsampling_factor * ( self . _recog_args . streaming_onset_margin + 1 ) h , ilen = self . _e2e . subsample_frames ( np . reshape ( self . _previous_input [ - tail_len :], [ - 1 , len ( self . _previous_input [ 0 ])])) h , _ , self . _previous_encoder_recurrent_state = self . _e2e . enc ( h . unsqueeze ( 0 ), ilen , None ) hyp = None if self . _activates == 1 : self . _encoder_states . extend ( h . squeeze ( 0 )) self . _ctc_posteriors . extend ( self . _e2e . ctc . log_softmax ( h ) . squeeze ( 0 )) if z [ 0 ] == self . _blank_idx_in_char_list : self . _blank_dur += 1 else : self . _blank_dur = 0 if self . _blank_dur >= self . _recog_args . streaming_min_blank_dur : seg_len = len ( self . _encoder_states ) - self . _blank_dur + self . _recog_args . streaming_offset_margin if seg_len > 0 : # Run decoder with a detected segment h = torch . cat ( self . _encoder_states [: seg_len ], dim = 0 ) . view ( - 1 , self . _encoder_states [ 0 ] . size ( 0 )) if self . _recog_args . ctc_weight > 0.0 : lpz = torch . cat ( self . _ctc_posteriors [: seg_len ], dim = 0 ) . view ( - 1 , self . _ctc_posteriors [ 0 ] . size ( 0 )) if self . _recog_args . batchsize > 0 : lpz = lpz . unsqueeze ( 0 ) normalize_score = False else : lpz = None normalize_score = True if self . _recog_args . batchsize == 0 : hyp = self . _e2e . dec . recognize_beam ( h , lpz , self . _recog_args , self . _char_list , self . _rnnlm ) else : hlens = torch . tensor ([ h . shape [ 0 ]]) hyp = self . _e2e . dec . recognize_beam_batch ( h . unsqueeze ( 0 ), hlens , lpz , self . _recog_args , self . _char_list , self . _rnnlm , normalize_score = normalize_score )[ 0 ] self . _activates = 0 self . _blank_dur = 0 tail_len = self . _subsampling_factor * self . _recog_args . streaming_onset_margin self . _previous_input = self . _previous_input [ - tail_len :] self . _encoder_states = [] self . _ctc_posteriors = [] return hyp","title":"segment"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.streaming.window","text":"WindowStreamingE2E WindowStreamingE2E constructor. :param E2E e2e: E2E ASR object :param recog_args: arguments for \"recognize\" method of E2E __init__ ( self , e2e , recog_args , rnnlm = None ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/window.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , e2e , recog_args , rnnlm = None ): self . _e2e = e2e self . _recog_args = recog_args self . _char_list = e2e . char_list self . _rnnlm = rnnlm self . _e2e . eval () self . _offset = 0 self . _previous_encoder_recurrent_state = None self . _encoder_states = [] self . _ctc_posteriors = [] self . _last_recognition = None assert self . _recog_args . ctc_weight > 0.0 , \\ \"WindowStreamingE2E works only with combined CTC and attention decoders.\" accept_input ( self , x ) Call this method each time a new batch of input is available. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/window.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def accept_input ( self , x ): \"\"\"Call this method each time a new batch of input is available.\"\"\" h , ilen = self . _e2e . subsample_frames ( x ) # Streaming encoder h , _ , self . _previous_encoder_recurrent_state = self . _e2e . enc ( h . unsqueeze ( 0 ), ilen , self . _previous_encoder_recurrent_state ) self . _encoder_states . append ( h . squeeze ( 0 )) # CTC posteriors for the incoming audio self . _ctc_posteriors . append ( self . _e2e . ctc . log_softmax ( h ) . squeeze ( 0 )) decode_with_attention_offline ( self ) Run the attention decoder offline. Works even if the previous layers (encoder and CTC decoder) were being run in the online mode. This method should be run after all the audio has been consumed. This is used mostly to compare the results between offline and online implementation of the previous layers. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/streaming/window.py 65 66 67 68 69 70 71 72 73 74 def decode_with_attention_offline ( self ): \"\"\"Run the attention decoder offline. Works even if the previous layers (encoder and CTC decoder) were being run in the online mode. This method should be run after all the audio has been consumed. This is used mostly to compare the results between offline and online implementation of the previous layers. \"\"\" h , lpz = self . _input_window_for_decoder ( use_all = True ) return self . _e2e . dec . recognize_beam ( h , lpz , self . _recog_args , self . _char_list , self . _rnnlm )","title":"window"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.tacotron2","text":"","title":"tacotron2"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.tacotron2.cbhg","text":"CBHG related modules. CBHG CBHG module to convert log Mel-filterbanks to linear spectrogram. This is a module of CBHG introduced in ` Tacotron : Towards End - to - End Speech Synthesis ` _ . The CBHG converts the sequence of log Mel - filterbanks into linear spectrogram . .. _ ` Tacotron : Towards End - to - End Speech Synthesis ` : https : // arxiv . org / abs / 1703 . 10135 __init__ ( self , idim , odim , conv_bank_layers = 8 , conv_bank_chans = 128 , conv_proj_filts = 3 , conv_proj_chans = 256 , highway_layers = 4 , highway_units = 128 , gru_units = 256 ) special Initialize CBHG module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required conv_bank_layers int The number of convolution bank layers. 8 conv_bank_chans int The number of channels in convolution bank. 128 conv_proj_filts int Kernel size of convolutional projection layer. 3 conv_proj_chans int The number of channels in convolutional projection layer. 256 highway_layers int The number of highway network layers. 4 highway_units int The number of highway network units. 128 gru_units int The number of GRU units (for both directions). 256 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , idim , odim , conv_bank_layers = 8 , conv_bank_chans = 128 , conv_proj_filts = 3 , conv_proj_chans = 256 , highway_layers = 4 , highway_units = 128 , gru_units = 256 ): \"\"\"Initialize CBHG module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. conv_bank_layers (int, optional): The number of convolution bank layers. conv_bank_chans (int, optional): The number of channels in convolution bank. conv_proj_filts (int, optional): Kernel size of convolutional projection layer. conv_proj_chans (int, optional): The number of channels in convolutional projection layer. highway_layers (int, optional): The number of highway network layers. highway_units (int, optional): The number of highway network units. gru_units (int, optional): The number of GRU units (for both directions). \"\"\" super ( CBHG , self ) . __init__ () self . idim = idim self . odim = odim self . conv_bank_layers = conv_bank_layers self . conv_bank_chans = conv_bank_chans self . conv_proj_filts = conv_proj_filts self . conv_proj_chans = conv_proj_chans self . highway_layers = highway_layers self . highway_units = highway_units self . gru_units = gru_units # define 1d convolution bank self . conv_bank = torch . nn . ModuleList () for k in range ( 1 , self . conv_bank_layers + 1 ): if k % 2 != 0 : padding = ( k - 1 ) // 2 else : padding = (( k - 1 ) // 2 , ( k - 1 ) // 2 + 1 ) self . conv_bank += [ torch . nn . Sequential ( torch . nn . ConstantPad1d ( padding , 0.0 ), torch . nn . Conv1d ( idim , self . conv_bank_chans , k , stride = 1 , padding = 0 , bias = True ), torch . nn . BatchNorm1d ( self . conv_bank_chans ), torch . nn . ReLU ())] # define max pooling (need padding for one-side to keep same length) self . max_pool = torch . nn . Sequential ( torch . nn . ConstantPad1d (( 0 , 1 ), 0.0 ), torch . nn . MaxPool1d ( 2 , stride = 1 )) # define 1d convolution projection self . projections = torch . nn . Sequential ( torch . nn . Conv1d ( self . conv_bank_chans * self . conv_bank_layers , self . conv_proj_chans , self . conv_proj_filts , stride = 1 , padding = ( self . conv_proj_filts - 1 ) // 2 , bias = True ), torch . nn . BatchNorm1d ( self . conv_proj_chans ), torch . nn . ReLU (), torch . nn . Conv1d ( self . conv_proj_chans , self . idim , self . conv_proj_filts , stride = 1 , padding = ( self . conv_proj_filts - 1 ) // 2 , bias = True ), torch . nn . BatchNorm1d ( self . idim ), ) # define highway network self . highways = torch . nn . ModuleList () self . highways += [ torch . nn . Linear ( idim , self . highway_units )] for _ in range ( self . highway_layers ): self . highways += [ HighwayNet ( self . highway_units )] # define bidirectional GRU self . gru = torch . nn . GRU ( self . highway_units , gru_units // 2 , num_layers = 1 , batch_first = True , bidirectional = True ) # define final projection self . output = torch . nn . Linear ( gru_units , odim , bias = True ) forward ( self , xs , ilens ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the padded sequences of inputs (B, Tmax, idim). required ilens LongTensor Batch of lengths of each input sequence (B,). required Returns: Type Description Tensor Batch of the padded sequence of outputs (B, Tmax, odim). LongTensor: Batch of lengths of each output sequence (B,). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def forward ( self , xs , ilens ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the padded sequences of inputs (B, Tmax, idim). ilens (LongTensor): Batch of lengths of each input sequence (B,). Return: Tensor: Batch of the padded sequence of outputs (B, Tmax, odim). LongTensor: Batch of lengths of each output sequence (B,). \"\"\" xs = xs . transpose ( 1 , 2 ) # (B, idim, Tmax) convs = [] for k in range ( self . conv_bank_layers ): convs += [ self . conv_bank [ k ]( xs )] convs = torch . cat ( convs , dim = 1 ) # (B, #CH * #BANK, Tmax) convs = self . max_pool ( convs ) convs = self . projections ( convs ) . transpose ( 1 , 2 ) # (B, Tmax, idim) xs = xs . transpose ( 1 , 2 ) + convs # + 1 for dimension adjustment layer for l in range ( self . highway_layers + 1 ): xs = self . highways [ l ]( xs ) # sort by length xs , ilens , sort_idx = self . _sort_by_length ( xs , ilens ) # total_length needs for DataParallel # (see https://github.com/pytorch/pytorch/pull/6327) total_length = xs . size ( 1 ) xs = pack_padded_sequence ( xs , ilens , batch_first = True ) self . gru . flatten_parameters () xs , _ = self . gru ( xs ) xs , ilens = pad_packed_sequence ( xs , batch_first = True , total_length = total_length ) # revert sorting by length xs , ilens = self . _revert_sort_by_length ( xs , ilens , sort_idx ) xs = self . output ( xs ) # (B, Tmax, odim) return xs , ilens inference ( self , x ) Inference. Parameters: Name Type Description Default x Tensor The sequences of inputs (T, idim). required Returns: Type Description Tensor The sequence of outputs (T, odim). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def inference ( self , x ): \"\"\"Inference. Args: x (Tensor): The sequences of inputs (T, idim). Return: Tensor: The sequence of outputs (T, odim). \"\"\" assert len ( x . size ()) == 2 xs = x . unsqueeze ( 0 ) ilens = x . new ([ x . size ( 0 )]) . long () return self . forward ( xs , ilens )[ 0 ][ 0 ] CBHGLoss Loss function module for CBHG. __init__ ( self , use_masking = True ) special Initialize CBHG loss module. Parameters: Name Type Description Default use_masking bool Whether to mask padded part in loss calculation. True Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 20 21 22 23 24 25 26 27 28 def __init__ ( self , use_masking = True ): \"\"\"Initialize CBHG loss module. Args: use_masking (bool): Whether to mask padded part in loss calculation. \"\"\" super ( CBHGLoss , self ) . __init__ () self . use_masking = use_masking forward ( self , cbhg_outs , spcs , olens ) Calculate forward propagation. Parameters: Name Type Description Default cbhg_outs Tensor Batch of CBHG outputs (B, Lmax, spc_dim). required spcs Tensor Batch of groundtruth of spectrogram (B, Lmax, spc_dim). required olens LongTensor Batch of the lengths of each sequence (B,). required Returns: Type Description Tensor L1 loss value Tensor: Mean square error loss value. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , cbhg_outs , spcs , olens ): \"\"\"Calculate forward propagation. Args: cbhg_outs (Tensor): Batch of CBHG outputs (B, Lmax, spc_dim). spcs (Tensor): Batch of groundtruth of spectrogram (B, Lmax, spc_dim). olens (LongTensor): Batch of the lengths of each sequence (B,). Returns: Tensor: L1 loss value Tensor: Mean square error loss value. \"\"\" # perform masking for padded values if self . use_masking : mask = make_non_pad_mask ( olens ) . unsqueeze ( - 1 ) . to ( spcs . device ) spcs = spcs . masked_select ( mask ) cbhg_outs = cbhg_outs . masked_select ( mask ) # calculate loss cbhg_l1_loss = F . l1_loss ( cbhg_outs , spcs ) cbhg_mse_loss = F . mse_loss ( cbhg_outs , spcs ) return cbhg_l1_loss , cbhg_mse_loss HighwayNet Highway Network module. This is a module of Highway Network introduced in ` Highway Networks ` _ . .. _ ` Highway Networks ` : https : // arxiv . org / abs / 1505 . 00387 __init__ ( self , idim ) special Initialize Highway Network module. Parameters: Name Type Description Default idim int Dimension of the inputs. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def __init__ ( self , idim ): \"\"\"Initialize Highway Network module. Args: idim (int): Dimension of the inputs. \"\"\" super ( HighwayNet , self ) . __init__ () self . idim = idim self . projection = torch . nn . Sequential ( torch . nn . Linear ( idim , idim ), torch . nn . ReLU ()) self . gate = torch . nn . Sequential ( torch . nn . Linear ( idim , idim ), torch . nn . Sigmoid ()) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of inputs (B, ..., idim). required Returns: Type Description Tensor Batch of outputs, which are the same shape as inputs (B, ..., idim). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/cbhg.py 238 239 240 241 242 243 244 245 246 247 248 249 250 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of inputs (B, ..., idim). Returns: Tensor: Batch of outputs, which are the same shape as inputs (B, ..., idim). \"\"\" proj = self . projection ( x ) gate = self . gate ( x ) return proj * gate + x * ( 1.0 - gate )","title":"cbhg"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.tacotron2.decoder","text":"Tacotron2 decoder related modules. Decoder Decoder module of Spectrogram prediction network. This is a module of decoder of Spectrogram prediction network in Tacotron2 , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . The decoder generates the sequence of features from the sequence of the hidden states . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , odim , att , dlayers = 2 , dunits = 1024 , prenet_layers = 2 , prenet_units = 256 , postnet_layers = 5 , postnet_chans = 512 , postnet_filts = 5 , output_activation_fn = None , cumulate_att_w = True , use_batch_norm = True , use_concate = True , dropout_rate = 0.5 , zoneout_rate = 0.1 , reduction_factor = 1 ) special Initialize Tacotron2 decoder module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required att torch.nn.Module Instance of attention class. required dlayers int The number of decoder lstm layers. 2 dunits int The number of decoder lstm units. 1024 prenet_layers int The number of prenet layers. 2 prenet_units int The number of prenet units. 256 postnet_layers int The number of postnet layers. 5 postnet_filts int The number of postnet filter size. 5 postnet_chans int The number of postnet filter channels. 512 output_activation_fn torch.nn.Module Activation function for outputs. None cumulate_att_w bool Whether to cumulate previous attention weight. True use_batch_norm bool Whether to use batch normalization. True use_concate bool Whether to concatenate encoder embedding with decoder lstm outputs. True dropout_rate float Dropout rate. 0.5 zoneout_rate float Zoneout rate. 0.1 reduction_factor int Reduction factor. 1 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __init__ ( self , idim , odim , att , dlayers = 2 , dunits = 1024 , prenet_layers = 2 , prenet_units = 256 , postnet_layers = 5 , postnet_chans = 512 , postnet_filts = 5 , output_activation_fn = None , cumulate_att_w = True , use_batch_norm = True , use_concate = True , dropout_rate = 0.5 , zoneout_rate = 0.1 , reduction_factor = 1 ): \"\"\"Initialize Tacotron2 decoder module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. att (torch.nn.Module): Instance of attention class. dlayers (int, optional): The number of decoder lstm layers. dunits (int, optional): The number of decoder lstm units. prenet_layers (int, optional): The number of prenet layers. prenet_units (int, optional): The number of prenet units. postnet_layers (int, optional): The number of postnet layers. postnet_filts (int, optional): The number of postnet filter size. postnet_chans (int, optional): The number of postnet filter channels. output_activation_fn (torch.nn.Module, optional): Activation function for outputs. cumulate_att_w (bool, optional): Whether to cumulate previous attention weight. use_batch_norm (bool, optional): Whether to use batch normalization. use_concate (bool, optional): Whether to concatenate encoder embedding with decoder lstm outputs. dropout_rate (float, optional): Dropout rate. zoneout_rate (float, optional): Zoneout rate. reduction_factor (int, optional): Reduction factor. \"\"\" super ( Decoder , self ) . __init__ () # store the hyperparameters self . idim = idim self . odim = odim self . att = att self . output_activation_fn = output_activation_fn self . cumulate_att_w = cumulate_att_w self . use_concate = use_concate self . reduction_factor = reduction_factor # check attention type if isinstance ( self . att , AttForwardTA ): self . use_att_extra_inputs = True else : self . use_att_extra_inputs = False # define lstm network prenet_units = prenet_units if prenet_layers != 0 else odim self . lstm = torch . nn . ModuleList () for layer in six . moves . range ( dlayers ): iunits = idim + prenet_units if layer == 0 else dunits lstm = torch . nn . LSTMCell ( iunits , dunits ) if zoneout_rate > 0.0 : lstm = ZoneOutCell ( lstm , zoneout_rate ) self . lstm += [ lstm ] # define prenet if prenet_layers > 0 : self . prenet = Prenet ( idim = odim , n_layers = prenet_layers , n_units = prenet_units , dropout_rate = dropout_rate ) else : self . prenet = None # define postnet if postnet_layers > 0 : self . postnet = Postnet ( idim = idim , odim = odim , n_layers = postnet_layers , n_chans = postnet_chans , n_filts = postnet_filts , use_batch_norm = use_batch_norm , dropout_rate = dropout_rate ) else : self . postnet = None # define projection layers iunits = idim + dunits if use_concate else dunits self . feat_out = torch . nn . Linear ( iunits , odim * reduction_factor , bias = False ) self . prob_out = torch . nn . Linear ( iunits , reduction_factor ) # initialize self . apply ( decoder_init ) calculate_all_attentions ( self , hs , hlens , ys ) Calculate all of the attention weights. Parameters: Name Type Description Default hs Tensor Batch of the sequences of padded hidden states (B, Tmax, idim). required hlens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of the sequences of padded target features (B, Lmax, odim). required Returns: Type Description numpy.ndarray Batch of attention weights (B, Lmax, Tmax). Note This computation is performed in teacher-forcing manner. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 def calculate_all_attentions ( self , hs , hlens , ys ): \"\"\"Calculate all of the attention weights. Args: hs (Tensor): Batch of the sequences of padded hidden states (B, Tmax, idim). hlens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of the sequences of padded target features (B, Lmax, odim). Returns: numpy.ndarray: Batch of attention weights (B, Lmax, Tmax). Note: This computation is performed in teacher-forcing manner. \"\"\" # thin out frames (B, Lmax, odim) -> (B, Lmax/r, odim) if self . reduction_factor > 1 : ys = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] # length list should be list of int hlens = list ( map ( int , hlens )) # initialize hidden states of decoder c_list = [ self . _zero_state ( hs )] z_list = [ self . _zero_state ( hs )] for _ in six . moves . range ( 1 , len ( self . lstm )): c_list += [ self . _zero_state ( hs )] z_list += [ self . _zero_state ( hs )] prev_out = hs . new_zeros ( hs . size ( 0 ), self . odim ) # initialize attention prev_att_w = None self . att . reset () # loop for an output sequence att_ws = [] for y in ys . transpose ( 0 , 1 ): if self . use_att_extra_inputs : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w , prev_out ) else : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w ) att_ws += [ att_w ] prenet_out = self . prenet ( prev_out ) if self . prenet is not None else prev_out xs = torch . cat ([ att_c , prenet_out ], dim = 1 ) z_list [ 0 ], c_list [ 0 ] = self . lstm [ 0 ]( xs , ( z_list [ 0 ], c_list [ 0 ])) for l in six . moves . range ( 1 , len ( self . lstm )): z_list [ l ], c_list [ l ] = self . lstm [ l ]( z_list [ l - 1 ], ( z_list [ l ], c_list [ l ])) prev_out = y # teacher forcing if self . cumulate_att_w and prev_att_w is not None : prev_att_w = prev_att_w + att_w # Note: error when use += else : prev_att_w = att_w att_ws = torch . stack ( att_ws , dim = 1 ) # (B, Lmax, Tmax) return att_ws forward ( self , hs , hlens , ys ) Calculate forward propagation. Parameters: Name Type Description Default hs Tensor Batch of the sequences of padded hidden states (B, Tmax, idim). required hlens LongTensor Batch of lengths of each input batch (B,). required ys Tensor Batch of the sequences of padded target features (B, Lmax, odim). required Returns: Type Description Tensor Batch of output tensors after postnet (B, Lmax, odim). Tensor: Batch of output tensors before postnet (B, Lmax, odim). Tensor: Batch of logits of stop prediction (B, Lmax). Tensor: Batch of attention weights (B, Lmax, Tmax). Note This computation is performed in teacher-forcing manner. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 def forward ( self , hs , hlens , ys ): \"\"\"Calculate forward propagation. Args: hs (Tensor): Batch of the sequences of padded hidden states (B, Tmax, idim). hlens (LongTensor): Batch of lengths of each input batch (B,). ys (Tensor): Batch of the sequences of padded target features (B, Lmax, odim). Returns: Tensor: Batch of output tensors after postnet (B, Lmax, odim). Tensor: Batch of output tensors before postnet (B, Lmax, odim). Tensor: Batch of logits of stop prediction (B, Lmax). Tensor: Batch of attention weights (B, Lmax, Tmax). Note: This computation is performed in teacher-forcing manner. \"\"\" # thin out frames (B, Lmax, odim) -> (B, Lmax/r, odim) if self . reduction_factor > 1 : ys = ys [:, self . reduction_factor - 1 :: self . reduction_factor ] # length list should be list of int hlens = list ( map ( int , hlens )) # initialize hidden states of decoder c_list = [ self . _zero_state ( hs )] z_list = [ self . _zero_state ( hs )] for _ in six . moves . range ( 1 , len ( self . lstm )): c_list += [ self . _zero_state ( hs )] z_list += [ self . _zero_state ( hs )] prev_out = hs . new_zeros ( hs . size ( 0 ), self . odim ) # initialize attention prev_att_w = None self . att . reset () # loop for an output sequence outs , logits , att_ws = [], [], [] for y in ys . transpose ( 0 , 1 ): if self . use_att_extra_inputs : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w , prev_out ) else : att_c , att_w = self . att ( hs , hlens , z_list [ 0 ], prev_att_w ) prenet_out = self . prenet ( prev_out ) if self . prenet is not None else prev_out xs = torch . cat ([ att_c , prenet_out ], dim = 1 ) z_list [ 0 ], c_list [ 0 ] = self . lstm [ 0 ]( xs , ( z_list [ 0 ], c_list [ 0 ])) for l in six . moves . range ( 1 , len ( self . lstm )): z_list [ l ], c_list [ l ] = self . lstm [ l ]( z_list [ l - 1 ], ( z_list [ l ], c_list [ l ])) zcs = torch . cat ([ z_list [ - 1 ], att_c ], dim = 1 ) if self . use_concate else z_list [ - 1 ] outs += [ self . feat_out ( zcs ) . view ( hs . size ( 0 ), self . odim , - 1 )] logits += [ self . prob_out ( zcs )] att_ws += [ att_w ] prev_out = y # teacher forcing if self . cumulate_att_w and prev_att_w is not None : prev_att_w = prev_att_w + att_w # Note: error when use += else : prev_att_w = att_w logits = torch . cat ( logits , dim = 1 ) # (B, Lmax) before_outs = torch . cat ( outs , dim = 2 ) # (B, odim, Lmax) att_ws = torch . stack ( att_ws , dim = 1 ) # (B, Lmax, Tmax) if self . reduction_factor > 1 : before_outs = before_outs . view ( before_outs . size ( 0 ), self . odim , - 1 ) # (B, odim, Lmax) if self . postnet is not None : after_outs = before_outs + self . postnet ( before_outs ) # (B, odim, Lmax) else : after_outs = before_outs before_outs = before_outs . transpose ( 2 , 1 ) # (B, Lmax, odim) after_outs = after_outs . transpose ( 2 , 1 ) # (B, Lmax, odim) logits = logits # apply activation function for scaling if self . output_activation_fn is not None : before_outs = self . output_activation_fn ( before_outs ) after_outs = self . output_activation_fn ( after_outs ) return after_outs , before_outs , logits , att_ws inference ( self , h , threshold = 0.5 , minlenratio = 0.0 , maxlenratio = 10.0 , use_att_constraint = False , backward_window = None , forward_window = None ) Generate the sequence of features given the sequences of characters. Parameters: Name Type Description Default h Tensor Input sequence of encoder hidden states (T, C). required threshold float Threshold to stop generation. 0.5 minlenratio float Minimum length ratio. If set to 1.0 and the length of input is 10, the minimum length of outputs will be 10 * 1 = 10. 0.0 minlenratio float Minimum length ratio. If set to 10 and the length of input is 10, the maximum length of outputs will be 10 * 10 = 100. 0.0 use_att_constraint bool Whether to apply attention constraint introduced in Deep Voice 3 _. False backward_window int Backward window size in attention constraint. None forward_window int Forward window size in attention constraint. None Returns: Type Description Tensor Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). Note This computation is performed in auto-regressive manner. .. _ Deep Voice 3 : https://arxiv.org/abs/1710.07654 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def inference ( self , h , threshold = 0.5 , minlenratio = 0.0 , maxlenratio = 10.0 , use_att_constraint = False , backward_window = None , forward_window = None ): \"\"\"Generate the sequence of features given the sequences of characters. Args: h (Tensor): Input sequence of encoder hidden states (T, C). threshold (float, optional): Threshold to stop generation. minlenratio (float, optional): Minimum length ratio. If set to 1.0 and the length of input is 10, the minimum length of outputs will be 10 * 1 = 10. minlenratio (float, optional): Minimum length ratio. If set to 10 and the length of input is 10, the maximum length of outputs will be 10 * 10 = 100. use_att_constraint (bool): Whether to apply attention constraint introduced in `Deep Voice 3`_. backward_window (int): Backward window size in attention constraint. forward_window (int): Forward window size in attention constraint. Returns: Tensor: Output sequence of features (L, odim). Tensor: Output sequence of stop probabilities (L,). Tensor: Attention weights (L, T). Note: This computation is performed in auto-regressive manner. .. _`Deep Voice 3`: https://arxiv.org/abs/1710.07654 \"\"\" # setup assert len ( h . size ()) == 2 hs = h . unsqueeze ( 0 ) ilens = [ h . size ( 0 )] maxlen = int ( h . size ( 0 ) * maxlenratio ) minlen = int ( h . size ( 0 ) * minlenratio ) # initialize hidden states of decoder c_list = [ self . _zero_state ( hs )] z_list = [ self . _zero_state ( hs )] for _ in six . moves . range ( 1 , len ( self . lstm )): c_list += [ self . _zero_state ( hs )] z_list += [ self . _zero_state ( hs )] prev_out = hs . new_zeros ( 1 , self . odim ) # initialize attention prev_att_w = None self . att . reset () # setup for attention constraint if use_att_constraint : last_attended_idx = 0 else : last_attended_idx = None # loop for an output sequence idx = 0 outs , att_ws , probs = [], [], [] while True : # updated index idx += self . reduction_factor # decoder calculation if self . use_att_extra_inputs : att_c , att_w = self . att ( hs , ilens , z_list [ 0 ], prev_att_w , prev_out , last_attended_idx = last_attended_idx , backward_window = backward_window , forward_window = forward_window ) else : att_c , att_w = self . att ( hs , ilens , z_list [ 0 ], prev_att_w , last_attended_idx = last_attended_idx , backward_window = backward_window , forward_window = forward_window ) att_ws += [ att_w ] prenet_out = self . prenet ( prev_out ) if self . prenet is not None else prev_out xs = torch . cat ([ att_c , prenet_out ], dim = 1 ) z_list [ 0 ], c_list [ 0 ] = self . lstm [ 0 ]( xs , ( z_list [ 0 ], c_list [ 0 ])) for l in six . moves . range ( 1 , len ( self . lstm )): z_list [ l ], c_list [ l ] = self . lstm [ l ]( z_list [ l - 1 ], ( z_list [ l ], c_list [ l ])) zcs = torch . cat ([ z_list [ - 1 ], att_c ], dim = 1 ) if self . use_concate else z_list [ - 1 ] outs += [ self . feat_out ( zcs ) . view ( 1 , self . odim , - 1 )] # [(1, odim, r), ...] probs += [ torch . sigmoid ( self . prob_out ( zcs ))[ 0 ]] # [(r), ...] if self . output_activation_fn is not None : prev_out = self . output_activation_fn ( outs [ - 1 ][:, :, - 1 ]) # (1, odim) else : prev_out = outs [ - 1 ][:, :, - 1 ] # (1, odim) if self . cumulate_att_w and prev_att_w is not None : prev_att_w = prev_att_w + att_w # Note: error when use += else : prev_att_w = att_w if use_att_constraint : last_attended_idx = int ( att_w . argmax ()) # check whether to finish generation if int ( sum ( probs [ - 1 ] >= threshold )) > 0 or idx >= maxlen : # check mininum length if idx < minlen : continue outs = torch . cat ( outs , dim = 2 ) # (1, odim, L) if self . postnet is not None : outs = outs + self . postnet ( outs ) # (1, odim, L) outs = outs . transpose ( 2 , 1 ) . squeeze ( 0 ) # (L, odim) probs = torch . cat ( probs , dim = 0 ) att_ws = torch . cat ( att_ws , dim = 0 ) break if self . output_activation_fn is not None : outs = self . output_activation_fn ( outs ) return outs , probs , att_ws Postnet Postnet module for Spectrogram prediction network. This is a module of Postnet in Spectrogram prediction network , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . The Postnet predicts refines the predicted Mel - filterbank of the decoder , which helps to compensate the detail sturcture of spectrogram . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , odim , n_layers = 5 , n_chans = 512 , n_filts = 5 , dropout_rate = 0.5 , use_batch_norm = True ) special Initialize postnet module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required n_layers int The number of layers. 5 n_filts int The number of filter size. 5 n_units int The number of filter channels. required use_batch_norm bool Whether to use batch normalization.. True dropout_rate float Dropout rate.. 0.5 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __init__ ( self , idim , odim , n_layers = 5 , n_chans = 512 , n_filts = 5 , dropout_rate = 0.5 , use_batch_norm = True ): \"\"\"Initialize postnet module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. n_layers (int, optional): The number of layers. n_filts (int, optional): The number of filter size. n_units (int, optional): The number of filter channels. use_batch_norm (bool, optional): Whether to use batch normalization.. dropout_rate (float, optional): Dropout rate.. \"\"\" super ( Postnet , self ) . __init__ () self . postnet = torch . nn . ModuleList () for layer in six . moves . range ( n_layers - 1 ): ichans = odim if layer == 0 else n_chans ochans = odim if layer == n_layers - 1 else n_chans if use_batch_norm : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , ochans , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . BatchNorm1d ( ochans ), torch . nn . Tanh (), torch . nn . Dropout ( dropout_rate ))] else : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , ochans , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . Tanh (), torch . nn . Dropout ( dropout_rate ))] ichans = n_chans if n_layers != 1 else odim if use_batch_norm : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , odim , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . BatchNorm1d ( odim ), torch . nn . Dropout ( dropout_rate ))] else : self . postnet += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , odim , n_filts , stride = 1 , padding = ( n_filts - 1 ) // 2 , bias = False ), torch . nn . Dropout ( dropout_rate ))] forward ( self , xs ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the sequences of padded input tensors (B, idim, Tmax). required Returns: Type Description Tensor Batch of padded output tensor. (B, odim, Tmax). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def forward ( self , xs ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the sequences of padded input tensors (B, idim, Tmax). Returns: Tensor: Batch of padded output tensor. (B, odim, Tmax). \"\"\" for l in six . moves . range ( len ( self . postnet )): xs = self . postnet [ l ]( xs ) return xs Prenet Prenet module for decoder of Spectrogram prediction network. This is a module of Prenet in the decoder of Spectrogram prediction network , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . The Prenet preforms nonlinear conversion of inputs before input to auto - regressive lstm , which helps to learn diagonal attentions . !!! note This module alway applies dropout even in evaluation . See the detail in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , n_layers = 2 , n_units = 256 , dropout_rate = 0.5 ) special Initialize prenet module. Parameters: Name Type Description Default idim int Dimension of the inputs. required odim int Dimension of the outputs. required n_layers int The number of prenet layers. 2 n_units int The number of prenet units. 256 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , idim , n_layers = 2 , n_units = 256 , dropout_rate = 0.5 ): \"\"\"Initialize prenet module. Args: idim (int): Dimension of the inputs. odim (int): Dimension of the outputs. n_layers (int, optional): The number of prenet layers. n_units (int, optional): The number of prenet units. \"\"\" super ( Prenet , self ) . __init__ () self . dropout_rate = dropout_rate self . prenet = torch . nn . ModuleList () for layer in six . moves . range ( n_layers ): n_inputs = idim if layer == 0 else n_units self . prenet += [ torch . nn . Sequential ( torch . nn . Linear ( n_inputs , n_units ), torch . nn . ReLU ())] forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of input tensors (B, ..., idim). required Returns: Type Description Tensor Batch of output tensors (B, ..., odim). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 124 125 126 127 128 129 130 131 132 133 134 135 136 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of input tensors (B, ..., idim). Returns: Tensor: Batch of output tensors (B, ..., odim). \"\"\" for l in six . moves . range ( len ( self . prenet )): x = F . dropout ( self . prenet [ l ]( x ), self . dropout_rate ) return x ZoneOutCell ZoneOut Cell module. This is a module of zoneout described in ` Zoneout : Regularizing RNNs by Randomly Preserving Hidden Activations ` _ . This code is modified from ` eladhoffer / seq2seq . pytorch ` _ . !!! examples >>> lstm = torch . nn . LSTMCell ( 16 , 32 ) >>> lstm = ZoneOutCell ( lstm , 0 . 5 ) .. _ ` Zoneout : Regularizing RNNs by Randomly Preserving Hidden Activations ` : https : // arxiv . org / abs / 1606 . 01305 .. _ ` eladhoffer / seq2seq . pytorch ` : https : // github . com / eladhoffer / seq2seq . pytorch __init__ ( self , cell , zoneout_rate = 0.1 ) special Initialize zone out cell module. Parameters: Name Type Description Default cell torch.nn.Module Pytorch recurrent cell module e.g. torch.nn.Module.LSTMCell . required zoneout_rate float Probability of zoneout from 0.0 to 1.0. 0.1 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , cell , zoneout_rate = 0.1 ): \"\"\"Initialize zone out cell module. Args: cell (torch.nn.Module): Pytorch recurrent cell module e.g. `torch.nn.Module.LSTMCell`. zoneout_rate (float, optional): Probability of zoneout from 0.0 to 1.0. \"\"\" super ( ZoneOutCell , self ) . __init__ () self . cell = cell self . hidden_size = cell . hidden_size self . zoneout_rate = zoneout_rate if zoneout_rate > 1.0 or zoneout_rate < 0.0 : raise ValueError ( \"zoneout probability must be in the range from 0.0 to 1.0.\" ) forward ( self , inputs , hidden ) Calculate forward propagation. Parameters: Name Type Description Default inputs Tensor Batch of input tensor (B, input_size). required hidden tuple Tensor: Batch of initial hidden states (B, hidden_size). Tensor: Batch of initial cell states (B, hidden_size). required Returns: Type Description tuple Tensor: Batch of next hidden states (B, hidden_size). Tensor: Batch of next cell states (B, hidden_size). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def forward ( self , inputs , hidden ): \"\"\"Calculate forward propagation. Args: inputs (Tensor): Batch of input tensor (B, input_size). hidden (tuple): - Tensor: Batch of initial hidden states (B, hidden_size). - Tensor: Batch of initial cell states (B, hidden_size). Returns: tuple: - Tensor: Batch of next hidden states (B, hidden_size). - Tensor: Batch of next cell states (B, hidden_size). \"\"\" next_hidden = self . cell ( inputs , hidden ) next_hidden = self . _zoneout ( hidden , next_hidden , self . zoneout_rate ) return next_hidden decoder_init ( m ) Initialize decoder parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/decoder.py 16 17 18 19 def decoder_init ( m ): \"\"\"Initialize decoder parameters.\"\"\" if isinstance ( m , torch . nn . Conv1d ): torch . nn . init . xavier_uniform_ ( m . weight , torch . nn . init . calculate_gain ( 'tanh' ))","title":"decoder"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.tacotron2.encoder","text":"Tacotron2 encoder related modules. Encoder Encoder module of Spectrogram prediction network. This is a module of encoder of Spectrogram prediction network in Tacotron2 , which described in ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` _ . This is the encoder which converts the sequence of characters into the sequence of hidden states . .. _ ` Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ` : https : // arxiv . org / abs / 1712 . 05884 __init__ ( self , idim , embed_dim = 512 , elayers = 1 , eunits = 512 , econv_layers = 3 , econv_chans = 512 , econv_filts = 5 , use_batch_norm = True , use_residual = False , dropout_rate = 0.5 , padding_idx = 0 ) special Initialize Tacotron2 encoder module. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , idim , embed_dim = 512 , elayers = 1 , eunits = 512 , econv_layers = 3 , econv_chans = 512 , econv_filts = 5 , use_batch_norm = True , use_residual = False , dropout_rate = 0.5 , padding_idx = 0 ): \"\"\"Initialize Tacotron2 encoder module. Args: idim (int) Dimension of the inputs. embed_dim (int, optional) Dimension of character embedding. elayers (int, optional) The number of encoder blstm layers. eunits (int, optional) The number of encoder blstm units. econv_layers (int, optional) The number of encoder conv layers. econv_filts (int, optional) The number of encoder conv filter size. econv_chans (int, optional) The number of encoder conv filter channels. use_batch_norm (bool, optional) Whether to use batch normalization. use_residual (bool, optional) Whether to use residual connection. dropout_rate (float, optional) Dropout rate. \"\"\" super ( Encoder , self ) . __init__ () # store the hyperparameters self . idim = idim self . use_residual = use_residual # define network layer modules self . embed = torch . nn . Embedding ( idim , embed_dim , padding_idx = padding_idx ) if econv_layers > 0 : self . convs = torch . nn . ModuleList () for layer in six . moves . range ( econv_layers ): ichans = embed_dim if layer == 0 else econv_chans if use_batch_norm : self . convs += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , econv_chans , econv_filts , stride = 1 , padding = ( econv_filts - 1 ) // 2 , bias = False ), torch . nn . BatchNorm1d ( econv_chans ), torch . nn . ReLU (), torch . nn . Dropout ( dropout_rate ))] else : self . convs += [ torch . nn . Sequential ( torch . nn . Conv1d ( ichans , econv_chans , econv_filts , stride = 1 , padding = ( econv_filts - 1 ) // 2 , bias = False ), torch . nn . ReLU (), torch . nn . Dropout ( dropout_rate ))] else : self . convs = None if elayers > 0 : iunits = econv_chans if econv_layers != 0 else embed_dim self . blstm = torch . nn . LSTM ( iunits , eunits // 2 , elayers , batch_first = True , bidirectional = True ) else : self . blstm = None # initialize self . apply ( encoder_init ) forward ( self , xs , ilens = None ) Calculate forward propagation. Parameters: Name Type Description Default xs Tensor Batch of the padded sequence of character ids (B, Tmax). Padded value should be 0. required ilens LongTensor Batch of lengths of each input batch (B,). None Returns: Type Description Tensor Batch of the sequences of encoder states(B, Tmax, eunits). LongTensor: Batch of lengths of each sequence (B,) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , xs , ilens = None ): \"\"\"Calculate forward propagation. Args: xs (Tensor): Batch of the padded sequence of character ids (B, Tmax). Padded value should be 0. ilens (LongTensor): Batch of lengths of each input batch (B,). Returns: Tensor: Batch of the sequences of encoder states(B, Tmax, eunits). LongTensor: Batch of lengths of each sequence (B,) \"\"\" xs = self . embed ( xs ) . transpose ( 1 , 2 ) if self . convs is not None : for l in six . moves . range ( len ( self . convs )): if self . use_residual : xs += self . convs [ l ]( xs ) else : xs = self . convs [ l ]( xs ) if self . blstm is None : return xs . transpose ( 1 , 2 ) xs = pack_padded_sequence ( xs . transpose ( 1 , 2 ), ilens , batch_first = True ) self . blstm . flatten_parameters () xs , _ = self . blstm ( xs ) # (B, Tmax, C) xs , hlens = pad_packed_sequence ( xs , batch_first = True ) return xs , hlens inference ( self , x ) Inference. Parameters: Name Type Description Default x Tensor The sequeunce of character ids (T,). required Returns: Type Description Tensor The sequences of encoder states(T, eunits). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def inference ( self , x ): \"\"\"Inference. Args: x (Tensor): The sequeunce of character ids (T,). Returns: Tensor: The sequences of encoder states(T, eunits). \"\"\" assert len ( x . size ()) == 1 xs = x . unsqueeze ( 0 ) ilens = [ x . size ( 0 )] return self . forward ( xs , ilens )[ 0 ][ 0 ] encoder_init ( m ) Initialize encoder parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/tacotron2/encoder.py 17 18 19 20 def encoder_init ( m ): \"\"\"Initialize encoder parameters.\"\"\" if isinstance ( m , torch . nn . Conv1d ): torch . nn . init . xavier_uniform_ ( m . weight , torch . nn . init . calculate_gain ( 'relu' ))","title":"encoder"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer","text":"","title":"transformer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.add_sos_eos","text":"Unility functions for Transformer. add_sos_eos ( ys_pad , sos , eos , ignore_id ) Add <sos> and <eos> labels. Parameters: Name Type Description Default ys_pad torch.Tensor batch of padded target sequences (B, Lmax) required sos int index of <sos> required eos int index of <eos> required ignore_id int index of padding required Returns: Type Description torch.Tensor padded tensor (B, Lmax) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/add_sos_eos.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def add_sos_eos ( ys_pad , sos , eos , ignore_id ): \"\"\" Add `<sos>` and `<eos>` labels. Arguments: ys_pad (torch.Tensor): batch of padded target sequences (B, Lmax) sos (int): index of `<sos>` eos (int): index of `<eos>` ignore_id (int): index of padding Returns: torch.Tensor: padded tensor (B, Lmax) \"\"\" from tools.espnet_minimal import pad_list _sos = ys_pad . new ([ sos ]) _eos = ys_pad . new ([ eos ]) ys = [ y [ y != ignore_id ] for y in ys_pad ] # parse padded ys ys_in = [ torch . cat ([ _sos , y ], dim = 0 ) for y in ys ] ys_out = [ torch . cat ([ y , _eos ], dim = 0 ) for y in ys ] return pad_list ( ys_in , eos ), pad_list ( ys_out , ignore_id )","title":"add_sos_eos"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.attention","text":"Multi-Head Attention layer definition. MultiHeadedAttention Multi-Head Attention layer. :param int n_head: the number of head s :param int n_feat: the number of features :param float dropout_rate: dropout rate __init__ ( self , n_head , n_feat , dropout_rate ) special Construct an MultiHeadedAttention object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/attention.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , n_head , n_feat , dropout_rate ): \"\"\"Construct an MultiHeadedAttention object.\"\"\" super ( MultiHeadedAttention , self ) . __init__ () assert n_feat % n_head == 0 # We assume d_v always equals d_k self . d_k = n_feat // n_head self . h = n_head self . linear_q = nn . Linear ( n_feat , n_feat ) self . linear_k = nn . Linear ( n_feat , n_feat ) self . linear_v = nn . Linear ( n_feat , n_feat ) self . linear_out = nn . Linear ( n_feat , n_feat ) self . attn = None self . dropout = nn . Dropout ( p = dropout_rate ) forward ( self , query , key , value , mask ) Compute 'Scaled Dot Product Attention'. :param torch.Tensor query: (batch, time1, size) :param torch.Tensor key: (batch, time2, size) :param torch.Tensor value: (batch, time2, size) :param torch.Tensor mask: (batch, time1, time2) :param torch.nn.Dropout dropout: :return torch.Tensor: attentined and transformed value (batch, time1, d_model) weighted by the query dot key attention (batch, head, time1, time2) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/attention.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def forward ( self , query , key , value , mask ): \"\"\"Compute 'Scaled Dot Product Attention'. :param torch.Tensor query: (batch, time1, size) :param torch.Tensor key: (batch, time2, size) :param torch.Tensor value: (batch, time2, size) :param torch.Tensor mask: (batch, time1, time2) :param torch.nn.Dropout dropout: :return torch.Tensor: attentined and transformed `value` (batch, time1, d_model) weighted by the query dot key attention (batch, head, time1, time2) \"\"\" n_batch = query . size ( 0 ) q = self . linear_q ( query ) . view ( n_batch , - 1 , self . h , self . d_k ) k = self . linear_k ( key ) . view ( n_batch , - 1 , self . h , self . d_k ) v = self . linear_v ( value ) . view ( n_batch , - 1 , self . h , self . d_k ) q = q . transpose ( 1 , 2 ) # (batch, head, time1, d_k) k = k . transpose ( 1 , 2 ) # (batch, head, time2, d_k) v = v . transpose ( 1 , 2 ) # (batch, head, time2, d_k) scores = torch . matmul ( q , k . transpose ( - 2 , - 1 )) / math . sqrt ( self . d_k ) # (batch, head, time1, time2) if mask is not None : mask = mask . unsqueeze ( 1 ) . eq ( 0 ) # (batch, 1, time1, time2) min_value = float ( numpy . finfo ( torch . tensor ( 0 , dtype = scores . dtype ) . numpy () . dtype ) . min ) scores = scores . masked_fill ( mask , min_value ) self . attn = torch . softmax ( scores , dim =- 1 ) . masked_fill ( mask , 0.0 ) # (batch, head, time1, time2) else : self . attn = torch . softmax ( scores , dim =- 1 ) # (batch, head, time1, time2) p_attn = self . dropout ( self . attn ) x = torch . matmul ( p_attn , v ) # (batch, head, time1, d_k) x = x . transpose ( 1 , 2 ) . contiguous () . view ( n_batch , - 1 , self . h * self . d_k ) # (batch, time1, d_model) return self . linear_out ( x ) # (batch, time1, d_model)","title":"attention"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.decoder","text":"Decoder definition. Decoder Transfomer decoder module. :param int odim: output dim :param int attention_dim: dimention of attention :param int attention_heads: the number of heads of multi head attention :param int linear_units: the number of units of position-wise feed forward :param int num_blocks: the number of decoder blocks :param float dropout_rate: dropout rate :param float attention_dropout_rate: dropout rate for attention :param str or torch.nn.Module input_layer: input layer type :param bool use_output_layer: whether to use output layer :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) __init__ ( self , odim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , self_attention_dropout_rate = 0.0 , src_attention_dropout_rate = 0.0 , input_layer = 'embed' , use_output_layer = True , pos_enc_class =< class ' tools . espnet_minimal . nets . pytorch_backend . transformer . embedding . PositionalEncoding '>, normalize_before=True, concat_after=False) special Construct an Decoder object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , odim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , self_attention_dropout_rate = 0.0 , src_attention_dropout_rate = 0.0 , input_layer = \"embed\" , use_output_layer = True , pos_enc_class = PositionalEncoding , normalize_before = True , concat_after = False ): \"\"\"Construct an Decoder object.\"\"\" torch . nn . Module . __init__ ( self ) if input_layer == \"embed\" : self . embed = torch . nn . Sequential ( torch . nn . Embedding ( odim , attention_dim ), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif input_layer == \"linear\" : self . embed = torch . nn . Sequential ( torch . nn . Linear ( odim , attention_dim ), torch . nn . LayerNorm ( attention_dim ), torch . nn . Dropout ( dropout_rate ), torch . nn . ReLU (), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif isinstance ( input_layer , torch . nn . Module ): self . embed = torch . nn . Sequential ( input_layer , pos_enc_class ( attention_dim , positional_dropout_rate ) ) else : raise NotImplementedError ( \"only `embed` or torch.nn.Module is supported.\" ) self . normalize_before = normalize_before self . decoders = repeat ( num_blocks , lambda : DecoderLayer ( attention_dim , MultiHeadedAttention ( attention_heads , attention_dim , self_attention_dropout_rate ), MultiHeadedAttention ( attention_heads , attention_dim , src_attention_dropout_rate ), PositionwiseFeedForward ( attention_dim , linear_units , dropout_rate ), dropout_rate , normalize_before , concat_after ) ) if self . normalize_before : self . after_norm = LayerNorm ( attention_dim ) if use_output_layer : self . output_layer = torch . nn . Linear ( attention_dim , odim ) else : self . output_layer = None forward ( self , tgt , tgt_mask , memory , memory_mask ) Forward decoder. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) if input_layer == \"embed\" input tensor (batch, maxlen_out, #mels) in the other cases :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param torch.Tensor memory_mask: encoded memory mask, (batch, maxlen_in) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :return x: decoded token score before softmax (batch, maxlen_out, token) if use_output_layer is True, final block outputs (batch, maxlen_out, attention_dim) in the other cases :rtype: torch.Tensor :return tgt_mask: score mask before softmax (batch, maxlen_out) :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def forward ( self , tgt , tgt_mask , memory , memory_mask ): \"\"\"Forward decoder. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) if input_layer == \"embed\" input tensor (batch, maxlen_out, #mels) in the other cases :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param torch.Tensor memory_mask: encoded memory mask, (batch, maxlen_in) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :return x: decoded token score before softmax (batch, maxlen_out, token) if use_output_layer is True, final block outputs (batch, maxlen_out, attention_dim) in the other cases :rtype: torch.Tensor :return tgt_mask: score mask before softmax (batch, maxlen_out) :rtype: torch.Tensor \"\"\" x = self . embed ( tgt ) x , tgt_mask , memory , memory_mask = self . decoders ( x , tgt_mask , memory , memory_mask ) if self . normalize_before : x = self . after_norm ( x ) if self . output_layer is not None : x = self . output_layer ( x ) return x , tgt_mask forward_one_step ( self , tgt , tgt_mask , memory , cache = None ) Forward one step. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param List[torch.Tensor] cache: cached output list of (batch, max_time_out-1, size) :return y, cache: NN output value and cache per self.decoders . y.shape is (batch, maxlen_out, token) :rtype: Tuple[torch.Tensor, List[torch.Tensor]] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def forward_one_step ( self , tgt , tgt_mask , memory , cache = None ): \"\"\"Forward one step. :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out) :param torch.Tensor tgt_mask: input token mask, (batch, maxlen_out) dtype=torch.uint8 in PyTorch 1.2- dtype=torch.bool in PyTorch 1.2+ (include 1.2) :param torch.Tensor memory: encoded memory, float32 (batch, maxlen_in, feat) :param List[torch.Tensor] cache: cached output list of (batch, max_time_out-1, size) :return y, cache: NN output value and cache per `self.decoders`. `y.shape` is (batch, maxlen_out, token) :rtype: Tuple[torch.Tensor, List[torch.Tensor]] \"\"\" x = self . embed ( tgt ) if cache is None : cache = [ None ] * len ( self . decoders ) new_cache = [] for c , decoder in zip ( cache , self . decoders ): x , tgt_mask , memory , memory_mask = decoder ( x , tgt_mask , memory , None , cache = c ) new_cache . append ( x ) if self . normalize_before : y = self . after_norm ( x [:, - 1 ]) else : y = x [:, - 1 ] if self . output_layer is not None : y = torch . log_softmax ( self . output_layer ( y ), dim =- 1 ) return y , new_cache score ( self , ys , state , x ) Score. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def score ( self , ys , state , x ): \"\"\"Score.\"\"\" # TODO(karita): remove this section after all ScorerInterface implements batch decoding if ys . dim () == 1 : ys_mask = subsequent_mask ( len ( ys ), device = x . device ) . unsqueeze ( 0 ) logp , state = self . forward_one_step ( ys . unsqueeze ( 0 ), ys_mask , x . unsqueeze ( 0 ), cache = state ) return logp . squeeze ( 0 ), state # merge states n_batch = len ( ys ) n_layers = len ( self . decoders ) if state [ 0 ] is None : batch_state = None else : # transpose state of [batch, layer] into [layer, batch] batch_state = [ torch . stack ([ state [ b ][ l ] for b in range ( n_batch )]) for l in range ( n_layers )] # batch decoding ys_mask = subsequent_mask ( ys . size ( - 1 ), device = x . device ) . unsqueeze ( 0 ) logp , state = self . forward_one_step ( ys , ys_mask , x , cache = batch_state ) # transpose state of [layer, batch] into [batch, layer] state_list = [[ state [ l ][ b ] for l in range ( n_layers )] for b in range ( n_batch )] return logp , state_list","title":"decoder"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.decoder_layer","text":"Decoder self-attention layer definition. DecoderLayer Single decoder layer module. :param int size: input dim :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.attention.MultiHeadedAttention self_attn: self attention module :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.attention.MultiHeadedAttention src_attn: source attention module :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward feed_forward: feed forward layer module :param float dropout_rate: dropout rate :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) __init__ ( self , size , self_attn , src_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ) special Construct an DecoderLayer object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder_layer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , size , self_attn , src_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ): \"\"\"Construct an DecoderLayer object.\"\"\" super ( DecoderLayer , self ) . __init__ () self . size = size self . self_attn = self_attn self . src_attn = src_attn self . feed_forward = feed_forward self . norm1 = LayerNorm ( size ) self . norm2 = LayerNorm ( size ) self . norm3 = LayerNorm ( size ) self . dropout = nn . Dropout ( dropout_rate ) self . normalize_before = normalize_before self . concat_after = concat_after if self . concat_after : self . concat_linear1 = nn . Linear ( size + size , size ) self . concat_linear2 = nn . Linear ( size + size , size ) forward ( self , tgt , tgt_mask , memory , memory_mask , cache = None ) Compute decoded features. Parameters: Name Type Description Default tgt torch.Tensor decoded previous target features (batch, max_time_out, size) required tgt_mask torch.Tensor mask for x (batch, max_time_out) required memory torch.Tensor encoded source features (batch, max_time_in, size) required memory_mask torch.Tensor mask for memory (batch, max_time_in) required cache torch.Tensor cached output (batch, max_time_out-1, size) None Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/decoder_layer.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def forward ( self , tgt , tgt_mask , memory , memory_mask , cache = None ): \"\"\"Compute decoded features. Args: tgt (torch.Tensor): decoded previous target features (batch, max_time_out, size) tgt_mask (torch.Tensor): mask for x (batch, max_time_out) memory (torch.Tensor): encoded source features (batch, max_time_in, size) memory_mask (torch.Tensor): mask for memory (batch, max_time_in) cache (torch.Tensor): cached output (batch, max_time_out-1, size) \"\"\" residual = tgt if self . normalize_before : tgt = self . norm1 ( tgt ) if cache is None : tgt_q = tgt tgt_q_mask = tgt_mask else : # compute only the last frame query keeping dim: max_time_out -> 1 assert cache . shape == ( tgt . shape [ 0 ], tgt . shape [ 1 ] - 1 , self . size ), \\ f \" { cache . shape } == { ( tgt . shape [ 0 ], tgt . shape [ 1 ] - 1 , self . size ) } \" tgt_q = tgt [:, - 1 :, :] residual = residual [:, - 1 :, :] tgt_q_mask = None if tgt_mask is not None : tgt_q_mask = tgt_mask [:, - 1 :, :] if self . concat_after : tgt_concat = torch . cat (( tgt_q , self . self_attn ( tgt_q , tgt , tgt , tgt_q_mask )), dim =- 1 ) x = residual + self . concat_linear1 ( tgt_concat ) else : x = residual + self . dropout ( self . self_attn ( tgt_q , tgt , tgt , tgt_q_mask )) if not self . normalize_before : x = self . norm1 ( x ) residual = x if self . normalize_before : x = self . norm2 ( x ) if self . concat_after : x_concat = torch . cat (( x , self . src_attn ( x , memory , memory , memory_mask )), dim =- 1 ) x = residual + self . concat_linear2 ( x_concat ) else : x = residual + self . dropout ( self . src_attn ( x , memory , memory , memory_mask )) if not self . normalize_before : x = self . norm2 ( x ) residual = x if self . normalize_before : x = self . norm3 ( x ) x = residual + self . dropout ( self . feed_forward ( x )) if not self . normalize_before : x = self . norm3 ( x ) if cache is not None : x = torch . cat ([ cache , x ], dim = 1 ) return x , tgt_mask , memory , memory_mask","title":"decoder_layer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.embedding","text":"Positonal Encoding Module. PositionalEncoding Positional encoding. :param int d_model: embedding dim :param float dropout_rate: dropout rate :param int max_len: maximum input length __init__ ( self , d_model , dropout_rate , max_len = 5000 ) special Construct an PositionalEncoding object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 37 38 39 40 41 42 43 44 45 def __init__ ( self , d_model , dropout_rate , max_len = 5000 ): \"\"\"Construct an PositionalEncoding object.\"\"\" super ( PositionalEncoding , self ) . __init__ () self . d_model = d_model self . xscale = math . sqrt ( self . d_model ) self . dropout = torch . nn . Dropout ( p = dropout_rate ) self . pe = None self . extend_pe ( torch . tensor ( 0.0 ) . expand ( 1 , max_len )) self . _register_load_state_dict_pre_hook ( _pre_hook ) extend_pe ( self , x ) Reset the positional encodings. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def extend_pe ( self , x ): \"\"\"Reset the positional encodings.\"\"\" if self . pe is not None : if self . pe . size ( 1 ) >= x . size ( 1 ): if self . pe . dtype != x . dtype or self . pe . device != x . device : self . pe = self . pe . to ( dtype = x . dtype , device = x . device ) return pe = torch . zeros ( x . size ( 1 ), self . d_model ) position = torch . arange ( 0 , x . size ( 1 ), dtype = torch . float32 ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , self . d_model , 2 , dtype = torch . float32 ) * - ( math . log ( 10000.0 ) / self . d_model )) pe [:, 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 1 :: 2 ] = torch . cos ( position * div_term ) pe = pe . unsqueeze ( 0 ) self . pe = pe . to ( device = x . device , dtype = x . dtype ) forward ( self , x ) Add positional encoding. Parameters: Name Type Description Default x Tensor Input. Its shape is (batch, time, ...) required Returns: Type Description torch.Tensor Encoded tensor. Its shape is (batch, time, ...) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 63 64 65 66 67 68 69 70 71 72 73 74 75 def forward ( self , x : torch . Tensor ): \"\"\"Add positional encoding. Args: x (torch.Tensor): Input. Its shape is (batch, time, ...) Returns: torch.Tensor: Encoded tensor. Its shape is (batch, time, ...) \"\"\" self . extend_pe ( x ) x = x * self . xscale + self . pe [:, : x . size ( 1 )] return self . dropout ( x ) ScaledPositionalEncoding Scaled positional encoding module. See also: Sec. 3.2 https://arxiv.org/pdf/1809.08895.pdf __init__ ( self , d_model , dropout_rate , max_len = 5000 ) special Initialize class. :param int d_model: embedding dim :param float dropout_rate: dropout rate :param int max_len: maximum input length Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , d_model , dropout_rate , max_len = 5000 ): \"\"\"Initialize class. :param int d_model: embedding dim :param float dropout_rate: dropout rate :param int max_len: maximum input length \"\"\" super () . __init__ ( d_model = d_model , dropout_rate = dropout_rate , max_len = max_len ) self . alpha = torch . nn . Parameter ( torch . tensor ( 1.0 )) forward ( self , x ) Add positional encoding. Parameters: Name Type Description Default x torch.Tensor Input. Its shape is (batch, time, ...) required Returns: Type Description torch.Tensor Encoded tensor. Its shape is (batch, time, ...) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 100 101 102 103 104 105 106 107 108 109 110 111 112 def forward ( self , x ): \"\"\"Add positional encoding. Args: x (torch.Tensor): Input. Its shape is (batch, time, ...) Returns: torch.Tensor: Encoded tensor. Its shape is (batch, time, ...) \"\"\" self . extend_pe ( x ) x = x + self . alpha * self . pe [:, : x . size ( 1 )] return self . dropout ( x ) reset_parameters ( self ) Reset parameters. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/embedding.py 96 97 98 def reset_parameters ( self ): \"\"\"Reset parameters.\"\"\" self . alpha . data = torch . tensor ( 1.0 )","title":"embedding"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.encoder","text":"Encoder definition. Encoder Transformer encoder module. :param int idim: input dim :param int attention_dim: dimention of attention :param int attention_heads: the number of heads of multi head attention :param int linear_units: the number of units of position-wise feed forward :param int num_blocks: the number of decoder blocks :param float dropout_rate: dropout rate :param float attention_dropout_rate: dropout rate in attention :param float positional_dropout_rate: dropout rate after adding positional encoding :param str or torch.nn.Module input_layer: input layer type :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) :param str positionwise_layer_type: linear of conv1d :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer :param int padding_idx: padding_idx for input_layer=embed __init__ ( self , idim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , attention_dropout_rate = 0.0 , input_layer = 'conv2d' , pos_enc_class =< class ' tools . espnet_minimal . nets . pytorch_backend . transformer . embedding . PositionalEncoding '>, normalize_before=True, concat_after=False, positionwise_layer_type=' linear ', positionwise_conv_kernel_size=1, padding_idx=-1) special Construct an Encoder object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , idim , attention_dim = 256 , attention_heads = 4 , linear_units = 2048 , num_blocks = 6 , dropout_rate = 0.1 , positional_dropout_rate = 0.1 , attention_dropout_rate = 0.0 , input_layer = \"conv2d\" , pos_enc_class = PositionalEncoding , normalize_before = True , concat_after = False , positionwise_layer_type = \"linear\" , positionwise_conv_kernel_size = 1 , padding_idx =- 1 ): \"\"\"Construct an Encoder object.\"\"\" super ( Encoder , self ) . __init__ () if input_layer == \"linear\" : self . embed = torch . nn . Sequential ( torch . nn . Linear ( idim , attention_dim ), torch . nn . LayerNorm ( attention_dim ), torch . nn . Dropout ( dropout_rate ), torch . nn . ReLU (), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif input_layer == \"conv2d\" : self . embed = Conv2dSubsampling ( idim , attention_dim , dropout_rate ) elif input_layer == \"embed\" : self . embed = torch . nn . Sequential ( torch . nn . Embedding ( idim , attention_dim , padding_idx = padding_idx ), pos_enc_class ( attention_dim , positional_dropout_rate ) ) elif isinstance ( input_layer , torch . nn . Module ): self . embed = torch . nn . Sequential ( input_layer , pos_enc_class ( attention_dim , positional_dropout_rate ), ) elif input_layer is None : self . embed = torch . nn . Sequential ( pos_enc_class ( attention_dim , positional_dropout_rate ) ) else : raise ValueError ( \"unknown input_layer: \" + input_layer ) self . normalize_before = normalize_before if positionwise_layer_type == \"linear\" : positionwise_layer = PositionwiseFeedForward positionwise_layer_args = ( attention_dim , linear_units , dropout_rate ) elif positionwise_layer_type == \"conv1d\" : positionwise_layer = MultiLayeredConv1d positionwise_layer_args = ( attention_dim , linear_units , positionwise_conv_kernel_size , dropout_rate ) elif positionwise_layer_type == \"conv1d-linear\" : positionwise_layer = Conv1dLinear positionwise_layer_args = ( attention_dim , linear_units , positionwise_conv_kernel_size , dropout_rate ) else : raise NotImplementedError ( \"Support only linear or conv1d.\" ) self . encoders = repeat ( num_blocks , lambda : EncoderLayer ( attention_dim , MultiHeadedAttention ( attention_heads , attention_dim , attention_dropout_rate ), positionwise_layer ( * positionwise_layer_args ), dropout_rate , normalize_before , concat_after ) ) if self . normalize_before : self . after_norm = LayerNorm ( attention_dim ) forward ( self , xs , masks ) Embed positions in tensor. :param torch.Tensor xs: input tensor :param torch.Tensor masks: input mask :return: position embedded tensor and mask :rtype Tuple[torch.Tensor, torch.Tensor]: Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def forward ( self , xs , masks ): \"\"\"Embed positions in tensor. :param torch.Tensor xs: input tensor :param torch.Tensor masks: input mask :return: position embedded tensor and mask :rtype Tuple[torch.Tensor, torch.Tensor]: \"\"\" if isinstance ( self . embed , Conv2dSubsampling ): xs , masks = self . embed ( xs , masks ) else : xs = self . embed ( xs ) xs , masks = self . encoders ( xs , masks ) if self . normalize_before : xs = self . after_norm ( xs ) return xs , masks","title":"encoder"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.encoder_layer","text":"Encoder self-attention layer definition. EncoderLayer Encoder layer module. :param int size: input dim :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.attention.MultiHeadedAttention self_attn: self attention module :param services.hci.speech.espnet_minimal.nets.pytorch_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward feed_forward: feed forward module :param float dropout_rate: dropout rate :param bool normalize_before: whether to use layer_norm before the first block :param bool concat_after: whether to concat attention layer's input and output if True, additional linear will be applied. i.e. x -> x + linear(concat(x, att(x))) if False, no additional linear will be applied. i.e. x -> x + att(x) __init__ ( self , size , self_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ) special Construct an EncoderLayer object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder_layer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , size , self_attn , feed_forward , dropout_rate , normalize_before = True , concat_after = False ): \"\"\"Construct an EncoderLayer object.\"\"\" super ( EncoderLayer , self ) . __init__ () self . self_attn = self_attn self . feed_forward = feed_forward self . norm1 = LayerNorm ( size ) self . norm2 = LayerNorm ( size ) self . dropout = nn . Dropout ( dropout_rate ) self . size = size self . normalize_before = normalize_before self . concat_after = concat_after if self . concat_after : self . concat_linear = nn . Linear ( size + size , size ) forward ( self , x , mask ) Compute encoded features. :param torch.Tensor x: encoded source features (batch, max_time_in, size) :param torch.Tensor mask: mask for x (batch, max_time_in) :rtype: Tuple[torch.Tensor, torch.Tensor] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/encoder_layer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def forward ( self , x , mask ): \"\"\"Compute encoded features. :param torch.Tensor x: encoded source features (batch, max_time_in, size) :param torch.Tensor mask: mask for x (batch, max_time_in) :rtype: Tuple[torch.Tensor, torch.Tensor] \"\"\" residual = x if self . normalize_before : x = self . norm1 ( x ) if self . concat_after : x_concat = torch . cat (( x , self . self_attn ( x , x , x , mask )), dim =- 1 ) x = residual + self . concat_linear ( x_concat ) else : x = residual + self . dropout ( self . self_attn ( x , x , x , mask )) if not self . normalize_before : x = self . norm1 ( x ) residual = x if self . normalize_before : x = self . norm2 ( x ) x = residual + self . dropout ( self . feed_forward ( x )) if not self . normalize_before : x = self . norm2 ( x ) return x , mask","title":"encoder_layer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.initializer","text":"Parameter initialization. initialize ( model , init_type = 'pytorch' ) Initialize Transformer module. :param torch.nn.Module model: transformer instance :param str init_type: initialization type Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/initializer.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def initialize ( model , init_type = \"pytorch\" ): \"\"\"Initialize Transformer module. :param torch.nn.Module model: transformer instance :param str init_type: initialization type \"\"\" if init_type == \"pytorch\" : return # weight init for p in model . parameters (): if p . dim () > 1 : if init_type == \"xavier_uniform\" : torch . nn . init . xavier_uniform_ ( p . data ) elif init_type == \"xavier_normal\" : torch . nn . init . xavier_normal_ ( p . data ) elif init_type == \"kaiming_uniform\" : torch . nn . init . kaiming_uniform_ ( p . data , nonlinearity = \"relu\" ) elif init_type == \"kaiming_normal\" : torch . nn . init . kaiming_normal_ ( p . data , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown initialization: \" + init_type ) # bias init for p in model . parameters (): if p . dim () == 1 : p . data . zero_ () # reset some modules with default init for m in model . modules (): if isinstance ( m , ( torch . nn . Embedding , LayerNorm )): m . reset_parameters ()","title":"initializer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.label_smoothing_loss","text":"Label smoothing module. LabelSmoothingLoss Label-smoothing loss. :param int size: the number of class :param int padding_idx: ignored class id :param float smoothing: smoothing rate (0.0 means the conventional CE) :param bool normalize_length: normalize loss by sequence length if True :param torch.nn.Module criterion: loss function to be smoothed __init__ ( self , size , padding_idx , smoothing , normalize_length = False , criterion = KLDivLoss ()) special Construct an LabelSmoothingLoss object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/label_smoothing_loss.py 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , size , padding_idx , smoothing , normalize_length = False , criterion = nn . KLDivLoss ( reduce = False )): \"\"\"Construct an LabelSmoothingLoss object.\"\"\" super ( LabelSmoothingLoss , self ) . __init__ () self . criterion = criterion self . padding_idx = padding_idx self . confidence = 1.0 - smoothing self . smoothing = smoothing self . size = size self . true_dist = None self . normalize_length = normalize_length forward ( self , x , target ) Compute loss between x and target. :param torch.Tensor x: prediction (batch, seqlen, class) :param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen) :return: scalar float value :rtype torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/label_smoothing_loss.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def forward ( self , x , target ): \"\"\"Compute loss between x and target. :param torch.Tensor x: prediction (batch, seqlen, class) :param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen) :return: scalar float value :rtype torch.Tensor \"\"\" assert x . size ( 2 ) == self . size batch_size = x . size ( 0 ) x = x . view ( - 1 , self . size ) target = target . view ( - 1 ) with torch . no_grad (): true_dist = x . clone () true_dist . fill_ ( self . smoothing / ( self . size - 1 )) ignore = target == self . padding_idx # (B,) total = len ( target ) - ignore . sum () . item () target = target . masked_fill ( ignore , 0 ) # avoid -1 index true_dist . scatter_ ( 1 , target . unsqueeze ( 1 ), self . confidence ) kl = self . criterion ( torch . log_softmax ( x , dim = 1 ), true_dist ) denom = total if self . normalize_length else batch_size return kl . masked_fill ( ignore . unsqueeze ( 1 ), 0 ) . sum () / denom","title":"label_smoothing_loss"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.layer_norm","text":"Layer normalization module. LayerNorm Layer normalization module. :param int nout: output dim size :param int dim: dimension to be normalized __init__ ( self , nout , dim =- 1 ) special Construct an LayerNorm object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/layer_norm.py 19 20 21 22 def __init__ ( self , nout , dim =- 1 ): \"\"\"Construct an LayerNorm object.\"\"\" super ( LayerNorm , self ) . __init__ ( nout , eps = 1e-12 ) self . dim = dim forward ( self , x ) Apply layer normalization. :param torch.Tensor x: input tensor :return: layer normalized tensor :rtype torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/layer_norm.py 24 25 26 27 28 29 30 31 32 33 def forward ( self , x ): \"\"\"Apply layer normalization. :param torch.Tensor x: input tensor :return: layer normalized tensor :rtype torch.Tensor \"\"\" if self . dim == - 1 : return super ( LayerNorm , self ) . forward ( x ) return super ( LayerNorm , self ) . forward ( x . transpose ( 1 , - 1 )) . transpose ( 1 , - 1 )","title":"layer_norm"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.mask","text":"Mask module. subsequent_mask ( size , device = 'cpu' , dtype = torch . bool ) Create mask for subsequent steps (1, size, size). :param int size: size of mask :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device :param torch.dtype dtype: result dtype :rtype: torch.Tensor subsequent_mask(3) [[1, 0, 0], [1, 1, 0], [1, 1, 1]] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/mask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def subsequent_mask ( size , device = \"cpu\" , dtype = datatype ): \"\"\"Create mask for subsequent steps (1, size, size). :param int size: size of mask :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device :param torch.dtype dtype: result dtype :rtype: torch.Tensor >>> subsequent_mask(3) [[1, 0, 0], [1, 1, 0], [1, 1, 1]] \"\"\" ret = torch . ones ( size , size , device = device , dtype = dtype ) return torch . tril ( ret , out = ret ) target_mask ( ys_in_pad , ignore_id ) Create mask for decoder self-attention. :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :param int ignore_id: index of padding :param torch.dtype dtype: result dtype :rtype: torch.Tensor Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/mask.py 32 33 34 35 36 37 38 39 40 41 42 def target_mask ( ys_in_pad , ignore_id ): \"\"\"Create mask for decoder self-attention. :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax) :param int ignore_id: index of padding :param torch.dtype dtype: result dtype :rtype: torch.Tensor \"\"\" ys_mask = ys_in_pad != ignore_id m = subsequent_mask ( ys_mask . size ( - 1 ), device = ys_mask . device ) . unsqueeze ( 0 ) return ys_mask . unsqueeze ( - 2 ) & m","title":"mask"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.multi_layer_conv","text":"Layer modules for FFT block in FastSpeech (Feed-forward Transformer). Conv1dLinear Conv1D + Linear for Transformer block. A variant of MultiLayeredConv1d, which replaces second conv-layer to linear. __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ) special Initialize Conv1dLinear module. Parameters: Name Type Description Default in_chans int Number of input channels. required hidden_chans int Number of hidden channels. required kernel_size int Kernel size of conv1d. required dropout_rate float Dropout rate. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ): \"\"\"Initialize Conv1dLinear module. Args: in_chans (int): Number of input channels. hidden_chans (int): Number of hidden channels. kernel_size (int): Kernel size of conv1d. dropout_rate (float): Dropout rate. \"\"\" super ( Conv1dLinear , self ) . __init__ () self . w_1 = torch . nn . Conv1d ( in_chans , hidden_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ) self . w_2 = torch . nn . Linear ( hidden_chans , in_chans ) self . dropout = torch . nn . Dropout ( dropout_rate ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of input tensors (B, ..., in_chans). required Returns: Type Description Tensor Batch of output tensors (B, ..., hidden_chans). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 77 78 79 80 81 82 83 84 85 86 87 88 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of input tensors (B, ..., in_chans). Returns: Tensor: Batch of output tensors (B, ..., hidden_chans). \"\"\" x = torch . relu ( self . w_1 ( x . transpose ( - 1 , 1 ))) . transpose ( - 1 , 1 ) return self . w_2 ( self . dropout ( x )) MultiLayeredConv1d Multi-layered conv1d for Transformer block. This is a module of multi - leyered conv1d designed to replace positionwise feed - forward network in Transforner block , which is introduced in ` FastSpeech : Fast , Robust and Controllable Text to Speech ` _ . .. _ ` FastSpeech : Fast , Robust and Controllable Text to Speech ` : https : // arxiv . org / pdf / 1905 . 09263 . pdf __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ) special Initialize MultiLayeredConv1d module. Parameters: Name Type Description Default in_chans int Number of input channels. required hidden_chans int Number of hidden channels. required kernel_size int Kernel size of conv1d. required dropout_rate float Dropout rate. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , in_chans , hidden_chans , kernel_size , dropout_rate ): \"\"\"Initialize MultiLayeredConv1d module. Args: in_chans (int): Number of input channels. hidden_chans (int): Number of hidden channels. kernel_size (int): Kernel size of conv1d. dropout_rate (float): Dropout rate. \"\"\" super ( MultiLayeredConv1d , self ) . __init__ () self . w_1 = torch . nn . Conv1d ( in_chans , hidden_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ) self . w_2 = torch . nn . Conv1d ( hidden_chans , in_chans , kernel_size , stride = 1 , padding = ( kernel_size - 1 ) // 2 ) self . dropout = torch . nn . Dropout ( dropout_rate ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Batch of input tensors (B, ..., in_chans). required Returns: Type Description Tensor Batch of output tensors (B, ..., hidden_chans). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/multi_layer_conv.py 40 41 42 43 44 45 46 47 48 49 50 51 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Batch of input tensors (B, ..., in_chans). Returns: Tensor: Batch of output tensors (B, ..., hidden_chans). \"\"\" x = torch . relu ( self . w_1 ( x . transpose ( - 1 , 1 ))) . transpose ( - 1 , 1 ) return self . w_2 ( self . dropout ( x ) . transpose ( - 1 , 1 )) . transpose ( - 1 , 1 )","title":"multi_layer_conv"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.optimizer","text":"Optimizer module. NoamOpt Optim wrapper that implements rate. param_groups property readonly Return param_groups. __init__ ( self , model_size , factor , warmup , optimizer ) special Construct an NoamOpt object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 15 16 17 18 19 20 21 22 def __init__ ( self , model_size , factor , warmup , optimizer ): \"\"\"Construct an NoamOpt object.\"\"\" self . optimizer = optimizer self . _step = 0 self . warmup = warmup self . factor = factor self . model_size = model_size self . _rate = 0 load_state_dict ( self , state_dict ) Load state_dict. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 60 61 62 63 64 65 66 def load_state_dict ( self , state_dict ): \"\"\"Load state_dict.\"\"\" for key , value in state_dict . items (): if key == \"optimizer\" : self . optimizer . load_state_dict ( state_dict [ \"optimizer\" ]) else : setattr ( self , key , value ) rate ( self , step = None ) Implement lrate above. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 38 39 40 41 42 43 def rate ( self , step = None ): \"\"\"Implement `lrate` above.\"\"\" if step is None : step = self . _step return self . factor * self . model_size ** ( - 0.5 ) \\ * min ( step ** ( - 0.5 ), step * self . warmup ** ( - 1.5 )) state_dict ( self ) Return state_dict. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 49 50 51 52 53 54 55 56 57 58 def state_dict ( self ): \"\"\"Return state_dict.\"\"\" return { \"_step\" : self . _step , \"warmup\" : self . warmup , \"factor\" : self . factor , \"model_size\" : self . model_size , \"_rate\" : self . _rate , \"optimizer\" : self . optimizer . state_dict () } step ( self ) Update parameters and rate. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 29 30 31 32 33 34 35 36 def step ( self ): \"\"\"Update parameters and rate.\"\"\" self . _step += 1 rate = self . rate () for p in self . optimizer . param_groups : p [ 'lr' ] = rate self . _rate = rate self . optimizer . step () zero_grad ( self ) Reset gradient. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 45 46 47 def zero_grad ( self ): \"\"\"Reset gradient.\"\"\" self . optimizer . zero_grad () get_std_opt ( model , d_model , warmup , factor ) Get standard NoamOpt. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/optimizer.py 69 70 71 72 def get_std_opt ( model , d_model , warmup , factor ): \"\"\"Get standard NoamOpt.\"\"\" base = torch . optim . Adam ( model . parameters (), lr = 0 , betas = ( 0.9 , 0.98 ), eps = 1e-9 ) return NoamOpt ( d_model , factor , warmup , base )","title":"optimizer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.plot","text":"PlotAttentionReport __call__ ( self , trainer ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 70 71 72 73 def __call__ ( self , trainer ): attn_dict = self . get_attention_weights () suffix = \"ep.{.updater.epoch}.png\" . format ( trainer ) self . plotfn ( self . data , attn_dict , self . outdir , suffix , savefig ) get_attention_weights ( self ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 75 76 77 78 79 80 81 def get_attention_weights ( self ): batch = self . converter ([ self . transform ( self . data )], self . device ) if isinstance ( batch , tuple ): att_ws = self . att_vis_fn ( * batch ) elif isinstance ( batch , dict ): att_ws = self . att_vis_fn ( ** batch ) return att_ws log_attentions ( self , logger , step ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 83 84 85 86 87 88 89 90 def log_attentions ( self , logger , step ): def log_fig ( plot , filename ): from os.path import basename logger . add_figure ( basename ( filename ), plot , step ) plt . clf () attn_dict = self . get_attention_weights () self . plotfn ( self . data , attn_dict , self . outdir , \"\" , log_fig ) plotfn ( self , * args , ** kwargs ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 67 68 def plotfn ( self , * args , ** kwargs ): plot_multi_head_attention ( * args , ** kwargs ) plot_multi_head_attention ( data , attn_dict , outdir , suffix = 'png' , savefn =< function savefig at 0x7f918a81a700 > ) Plot multi head attentions :param dict data: utts info from json file :param dict[str, torch.Tensor] attn_dict: multi head attention dict. values should be torch.Tensor (head, input_length, output_length) :param str outdir: dir to save fig :param str suffix: filename suffix including image type (e.g., png) :param savefn: function to save Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def plot_multi_head_attention ( data , attn_dict , outdir , suffix = \"png\" , savefn = savefig ): \"\"\"Plot multi head attentions :param dict data: utts info from json file :param dict[str, torch.Tensor] attn_dict: multi head attention dict. values should be torch.Tensor (head, input_length, output_length) :param str outdir: dir to save fig :param str suffix: filename suffix including image type (e.g., png) :param savefn: function to save \"\"\" for name , att_ws in attn_dict . items (): for idx , att_w in enumerate ( att_ws ): filename = \" %s / %s . %s . %s \" % ( outdir , data [ idx ][ 0 ], name , suffix ) dec_len = int ( data [ idx ][ 1 ][ 'output' ][ 0 ][ 'shape' ][ 0 ]) enc_len = int ( data [ idx ][ 1 ][ 'input' ][ 0 ][ 'shape' ][ 0 ]) if \"encoder\" in name : att_w = att_w [:, : enc_len , : enc_len ] elif \"decoder\" in name : if \"self\" in name : att_w = att_w [:, : dec_len , : dec_len ] else : att_w = att_w [:, : dec_len , : enc_len ] else : logging . warning ( \"unknown name for shaping attention\" ) fig = _plot_and_save_attention ( att_w , filename ) savefn ( fig , filename ) savefig ( plot , filename ) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/plot.py 32 33 34 def savefig ( plot , filename ): plot . savefig ( filename ) plt . clf ()","title":"plot"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.positionwise_feed_forward","text":"Positionwise feed forward layer definition. PositionwiseFeedForward Positionwise feed forward layer. :param int idim: input dimenstion :param int hidden_units: number of hidden units :param float dropout_rate: dropout rate __init__ ( self , idim , hidden_units , dropout_rate ) special Construct an PositionwiseFeedForward object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/positionwise_feed_forward.py 21 22 23 24 25 26 def __init__ ( self , idim , hidden_units , dropout_rate ): \"\"\"Construct an PositionwiseFeedForward object.\"\"\" super ( PositionwiseFeedForward , self ) . __init__ () self . w_1 = torch . nn . Linear ( idim , hidden_units ) self . w_2 = torch . nn . Linear ( hidden_units , idim ) self . dropout = torch . nn . Dropout ( dropout_rate ) forward ( self , x ) Forward funciton. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/positionwise_feed_forward.py 28 29 30 def forward ( self , x ): \"\"\"Forward funciton.\"\"\" return self . w_2 ( self . dropout ( torch . relu ( self . w_1 ( x ))))","title":"positionwise_feed_forward"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.repeat","text":"Repeat the same layer definition. MultiSequential Multi-input multi-output torch.nn.Sequential. forward ( self , * args ) Repeat. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/repeat.py 15 16 17 18 19 def forward ( self , * args ): \"\"\"Repeat.\"\"\" for m in self : args = m ( * args ) return args repeat ( N , fn ) Repeat module N times. :param int N: repeat time :param function fn: function to generate module :return: repeated modules :rtype: MultiSequential Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/repeat.py 22 23 24 25 26 27 28 29 30 def repeat ( N , fn ): \"\"\"Repeat module N times. :param int N: repeat time :param function fn: function to generate module :return: repeated modules :rtype: MultiSequential \"\"\" return MultiSequential ( * [ fn () for _ in range ( N )])","title":"repeat"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.transformer.subsampling","text":"Subsampling layer definition. Conv2dSubsampling Convolutional 2D subsampling (to 1/4 length). :param int idim: input dim :param int odim: output dim :param flaot dropout_rate: dropout rate __init__ ( self , idim , odim , dropout_rate ) special Construct an Conv2dSubsampling object. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/subsampling.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , idim , odim , dropout_rate ): \"\"\"Construct an Conv2dSubsampling object.\"\"\" super ( Conv2dSubsampling , self ) . __init__ () self . conv = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , odim , 3 , 2 ), torch . nn . ReLU (), torch . nn . Conv2d ( odim , odim , 3 , 2 ), torch . nn . ReLU () ) self . out = torch . nn . Sequential ( torch . nn . Linear ( odim * ((( idim - 1 ) // 2 - 1 ) // 2 ), odim ), PositionalEncoding ( odim , dropout_rate ) ) forward ( self , x , x_mask ) Subsample x. :param torch.Tensor x: input tensor :param torch.Tensor x_mask: input mask :return: subsampled x and mask :rtype Tuple[torch.Tensor, torch.Tensor] Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/transformer/subsampling.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def forward ( self , x , x_mask ): \"\"\"Subsample x. :param torch.Tensor x: input tensor :param torch.Tensor x_mask: input mask :return: subsampled x and mask :rtype Tuple[torch.Tensor, torch.Tensor] \"\"\" x = x . unsqueeze ( 1 ) # (b, c, t, f) x = self . conv ( x ) b , c , t , f = x . size () x = self . out ( x . transpose ( 1 , 2 ) . contiguous () . view ( b , t , c * f )) if x_mask is None : return x , None return x , x_mask [:, :, : - 2 : 2 ][:, :, : - 2 : 2 ]","title":"subsampling"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet","text":"This code is based on https://github.com/kan-bayashi/PytorchWaveNetVocoder.","title":"wavenet"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet.CausalConv1d","text":"1D dilated causal convolution. __init__ ( self , in_channels , out_channels , kernel_size , dilation = 1 , bias = True ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 99 100 101 102 103 104 105 106 107 def __init__ ( self , in_channels , out_channels , kernel_size , dilation = 1 , bias = True ): super ( CausalConv1d , self ) . __init__ () self . in_channels = in_channels self . out_channels = out_channels self . kernel_size = kernel_size self . dilation = dilation self . padding = padding = ( kernel_size - 1 ) * dilation self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , padding = padding , dilation = dilation , bias = bias ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Input tensor with the shape (B, in_channels, T). required Returns: Type Description Tensor Tensor with the shape (B, out_channels, T) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Input tensor with the shape (B, in_channels, T). Returns: Tensor: Tensor with the shape (B, out_channels, T) \"\"\" x = self . conv ( x ) if self . padding != 0 : x = x [:, :, : - self . padding ] return x","title":"CausalConv1d"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet.OneHot","text":"Convert to one-hot vector. !!! args depth (int): Dimension of one-hot vector. __init__ ( self , depth ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 75 76 77 def __init__ ( self , depth ): super ( OneHot , self ) . __init__ () self . depth = depth forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x LongTensor long tensor variable with the shape (B, T) required Returns: Type Description Tensor float tensor variable with the shape (B, depth, T) Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (LongTensor): long tensor variable with the shape (B, T) Returns: Tensor: float tensor variable with the shape (B, depth, T) \"\"\" x = x % self . depth x = torch . unsqueeze ( x , 2 ) x_onehot = x . new_zeros ( x . size ( 0 ), x . size ( 1 ), self . depth ) . float () return x_onehot . scatter_ ( 2 , x , 1 )","title":"OneHot"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet.UpSampling","text":"Upsampling layer with deconvolution. !!! args upsampling_factor (int): Upsampling factor. __init__ ( self , upsampling_factor , bias = True ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 133 134 135 136 137 138 139 140 def __init__ ( self , upsampling_factor , bias = True ): super ( UpSampling , self ) . __init__ () self . upsampling_factor = upsampling_factor self . bias = bias self . conv = nn . ConvTranspose2d ( 1 , 1 , kernel_size = ( 1 , self . upsampling_factor ), stride = ( 1 , self . upsampling_factor ), bias = self . bias ) forward ( self , x ) Calculate forward propagation. Parameters: Name Type Description Default x Tensor Input tensor with the shape (B, C, T) required Returns: Type Description Tensor Tensor with the shape (B, C, T') where T' = T * upsampling_factor. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 142 143 144 145 146 147 148 149 150 151 152 153 154 def forward ( self , x ): \"\"\"Calculate forward propagation. Args: x (Tensor): Input tensor with the shape (B, C, T) Returns: Tensor: Tensor with the shape (B, C, T') where T' = T * upsampling_factor. \"\"\" x = x . unsqueeze ( 1 ) # B x 1 x C x T x = self . conv ( x ) # B x 1 x C x T' return x . squeeze ( 1 )","title":"UpSampling"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet.WaveNet","text":"Conditional wavenet. !!! args n_quantize (int): Number of quantization. n_aux (int): Number of aux feature dimension. n_resch (int): Number of filter channels for residual block. n_skipch (int): Number of filter channels for skip connection. dilation_depth (int): Number of dilation depth (e.g. if set 10, max dilation = 2^(10-1)). dilation_repeat (int): Number of dilation repeat. kernel_size (int): Filter size of dilated causal convolution. upsampling_factor (int): Upsampling factor. __init__ ( self , n_quantize = 256 , n_aux = 28 , n_resch = 512 , n_skipch = 256 , dilation_depth = 10 , dilation_repeat = 3 , kernel_size = 2 , upsampling_factor = 0 ) special Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def __init__ ( self , n_quantize = 256 , n_aux = 28 , n_resch = 512 , n_skipch = 256 , dilation_depth = 10 , dilation_repeat = 3 , kernel_size = 2 , upsampling_factor = 0 ): super ( WaveNet , self ) . __init__ () self . n_aux = n_aux self . n_quantize = n_quantize self . n_resch = n_resch self . n_skipch = n_skipch self . kernel_size = kernel_size self . dilation_depth = dilation_depth self . dilation_repeat = dilation_repeat self . upsampling_factor = upsampling_factor self . dilations = [ 2 ** i for i in range ( self . dilation_depth )] * self . dilation_repeat self . receptive_field = ( self . kernel_size - 1 ) * sum ( self . dilations ) + 1 # for preprocessing self . onehot = OneHot ( self . n_quantize ) self . causal = CausalConv1d ( self . n_quantize , self . n_resch , self . kernel_size ) if self . upsampling_factor > 0 : self . upsampling = UpSampling ( self . upsampling_factor ) # for residual blocks self . dil_sigmoid = nn . ModuleList () self . dil_tanh = nn . ModuleList () self . aux_1x1_sigmoid = nn . ModuleList () self . aux_1x1_tanh = nn . ModuleList () self . skip_1x1 = nn . ModuleList () self . res_1x1 = nn . ModuleList () for d in self . dilations : self . dil_sigmoid += [ CausalConv1d ( self . n_resch , self . n_resch , self . kernel_size , d )] self . dil_tanh += [ CausalConv1d ( self . n_resch , self . n_resch , self . kernel_size , d )] self . aux_1x1_sigmoid += [ nn . Conv1d ( self . n_aux , self . n_resch , 1 )] self . aux_1x1_tanh += [ nn . Conv1d ( self . n_aux , self . n_resch , 1 )] self . skip_1x1 += [ nn . Conv1d ( self . n_resch , self . n_skipch , 1 )] self . res_1x1 += [ nn . Conv1d ( self . n_resch , self . n_resch , 1 )] # for postprocessing self . conv_post_1 = nn . Conv1d ( self . n_skipch , self . n_skipch , 1 ) self . conv_post_2 = nn . Conv1d ( self . n_skipch , self . n_quantize , 1 ) forward ( self , x , h ) Calculate forward propagation. Parameters: Name Type Description Default x LongTensor Quantized input waveform tensor with the shape (B, T). required h Tensor Auxiliary feature tensor with the shape (B, n_aux, T). required Returns: Type Description Tensor Logits with the shape (B, T, n_quantize). Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def forward ( self , x , h ): \"\"\"Calculate forward propagation. Args: x (LongTensor): Quantized input waveform tensor with the shape (B, T). h (Tensor): Auxiliary feature tensor with the shape (B, n_aux, T). Returns: Tensor: Logits with the shape (B, T, n_quantize). \"\"\" # preprocess output = self . _preprocess ( x ) if self . upsampling_factor > 0 : h = self . upsampling ( h ) # residual block skip_connections = [] for l in range ( len ( self . dilations )): output , skip = self . _residual_forward ( output , h , self . dil_sigmoid [ l ], self . dil_tanh [ l ], self . aux_1x1_sigmoid [ l ], self . aux_1x1_tanh [ l ], self . skip_1x1 [ l ], self . res_1x1 [ l ]) skip_connections . append ( skip ) # skip-connection part output = sum ( skip_connections ) output = self . _postprocess ( output ) return output generate ( self , x , h , n_samples , interval = None , mode = 'sampling' ) Generate a waveform with fast genration algorithm. This generation based on Fast WaveNet Generation Algorithm _. Parameters: Name Type Description Default x LongTensor Initial waveform tensor with the shape (T,). required h Tensor Auxiliary feature tensor with the shape (n_samples + T, n_aux). required n_samples int Number of samples to be generated. required interval int Log interval. None mode str \"sampling\" or \"argmax\". 'sampling' Returns: Type Description ndarray Generated quantized waveform (n_samples). .. _ Fast WaveNet Generation Algorithm : https://arxiv.org/abs/1611.09482 Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def generate ( self , x , h , n_samples , interval = None , mode = \"sampling\" ): \"\"\"Generate a waveform with fast genration algorithm. This generation based on `Fast WaveNet Generation Algorithm`_. Args: x (LongTensor): Initial waveform tensor with the shape (T,). h (Tensor): Auxiliary feature tensor with the shape (n_samples + T, n_aux). n_samples (int): Number of samples to be generated. interval (int, optional): Log interval. mode (str, optional): \"sampling\" or \"argmax\". Return: ndarray: Generated quantized waveform (n_samples). .. _`Fast WaveNet Generation Algorithm`: https://arxiv.org/abs/1611.09482 \"\"\" # reshape inputs assert len ( x . shape ) == 1 assert len ( h . shape ) == 2 and h . shape [ 1 ] == self . n_aux x = x . unsqueeze ( 0 ) h = h . transpose ( 0 , 1 ) . unsqueeze ( 0 ) # perform upsampling if self . upsampling_factor > 0 : h = self . upsampling ( h ) # padding for shortage if n_samples > h . shape [ 2 ]: h = F . pad ( h , ( 0 , n_samples - h . shape [ 2 ]), \"replicate\" ) # padding if the length less than n_pad = self . receptive_field - x . size ( 1 ) if n_pad > 0 : x = F . pad ( x , ( n_pad , 0 ), \"constant\" , self . n_quantize // 2 ) h = F . pad ( h , ( n_pad , 0 ), \"replicate\" ) # prepare buffer output = self . _preprocess ( x ) h_ = h [:, :, : x . size ( 1 )] output_buffer = [] buffer_size = [] for l , d in enumerate ( self . dilations ): output , _ = self . _residual_forward ( output , h_ , self . dil_sigmoid [ l ], self . dil_tanh [ l ], self . aux_1x1_sigmoid [ l ], self . aux_1x1_tanh [ l ], self . skip_1x1 [ l ], self . res_1x1 [ l ]) if d == 2 ** ( self . dilation_depth - 1 ): buffer_size . append ( self . kernel_size - 1 ) else : buffer_size . append ( d * 2 * ( self . kernel_size - 1 )) output_buffer . append ( output [:, :, - buffer_size [ l ] - 1 : - 1 ]) # generate samples = x [ 0 ] start_time = time . time () for i in range ( n_samples ): output = samples [ - self . kernel_size * 2 + 1 :] . unsqueeze ( 0 ) output = self . _preprocess ( output ) h_ = h [:, :, samples . size ( 0 ) - 1 ] . contiguous () . view ( 1 , self . n_aux , 1 ) output_buffer_next = [] skip_connections = [] for l , d in enumerate ( self . dilations ): output , skip = self . _generate_residual_forward ( output , h_ , self . dil_sigmoid [ l ], self . dil_tanh [ l ], self . aux_1x1_sigmoid [ l ], self . aux_1x1_tanh [ l ], self . skip_1x1 [ l ], self . res_1x1 [ l ]) output = torch . cat ([ output_buffer [ l ], output ], dim = 2 ) output_buffer_next . append ( output [:, :, - buffer_size [ l ]:]) skip_connections . append ( skip ) # update buffer output_buffer = output_buffer_next # get predicted sample output = sum ( skip_connections ) output = self . _postprocess ( output )[ 0 ] if mode == \"sampling\" : posterior = F . softmax ( output [ - 1 ], dim = 0 ) dist = torch . distributions . Categorical ( posterior ) sample = dist . sample () . unsqueeze ( 0 ) elif mode == \"argmax\" : sample = output . argmax ( - 1 ) else : logging . error ( \"mode should be sampling or argmax\" ) sys . exit ( 1 ) samples = torch . cat ([ samples , sample ], dim = 0 ) # show progress if interval is not None and ( i + 1 ) % interval == 0 : elapsed_time_per_sample = ( time . time () - start_time ) / interval logging . info ( \" %d / %d estimated time = %.3f sec ( %.3f sec / sample)\" % ( i + 1 , n_samples , ( n_samples - i - 1 ) * elapsed_time_per_sample , elapsed_time_per_sample )) start_time = time . time () return samples [ - n_samples :] . cpu () . numpy ()","title":"WaveNet"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet.decode_mu_law","text":"Perform mu-law decoding. Parameters: Name Type Description Default x ndarray Quantized audio signal with the range from 0 to mu - 1. required mu int Quantized level. 256 Returns: Type Description ndarray Audio signal with the range from -1 to 1. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def decode_mu_law ( y , mu = 256 ): \"\"\"Perform mu-law decoding. Args: x (ndarray): Quantized audio signal with the range from 0 to mu - 1. mu (int): Quantized level. Returns: ndarray: Audio signal with the range from -1 to 1. \"\"\" mu = mu - 1 fx = ( y - 0.5 ) / mu * 2 - 1 x = np . sign ( fx ) / mu * (( 1 + mu ) ** np . abs ( fx ) - 1 ) return x","title":"decode_mu_law()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet.encode_mu_law","text":"Perform mu-law encoding. Parameters: Name Type Description Default x ndarray Audio signal with the range from -1 to 1. required mu int Quantized level. 256 Returns: Type Description ndarray Quantized audio signal with the range from 0 to mu - 1. Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def encode_mu_law ( x , mu = 256 ): \"\"\"Perform mu-law encoding. Args: x (ndarray): Audio signal with the range from -1 to 1. mu (int): Quantized level. Returns: ndarray: Quantized audio signal with the range from 0 to mu - 1. \"\"\" mu = mu - 1 fx = np . sign ( x ) * np . log ( 1 + mu * np . abs ( x )) / np . log ( 1 + mu ) return np . floor (( fx + 1 ) / 2 * mu + 0.5 ) . astype ( np . int64 )","title":"encode_mu_law()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.pytorch_backend.wavenet.initialize","text":"Initilize conv layers with xavier. Parameters: Name Type Description Default m torch.nn.Module Torch module. required Source code in adviser/tools/espnet_minimal/nets/pytorch_backend/wavenet.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def initialize ( m ): \"\"\"Initilize conv layers with xavier. Args: m (torch.nn.Module): Torch module. \"\"\" if isinstance ( m , nn . Conv1d ): nn . init . xavier_uniform_ ( m . weight ) nn . init . constant_ ( m . bias , 0.0 ) if isinstance ( m , nn . ConvTranspose2d ): nn . init . constant_ ( m . weight , 1.0 ) nn . init . constant_ ( m . bias , 0.0 )","title":"initialize()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface","text":"Scorer interface module.","title":"scorer_interface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.BatchScorerInterface","text":"Batch scorer interface.","title":"BatchScorerInterface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.BatchScorerInterface.score","text":"Score new token batch (required). Parameters: Name Type Description Default ys Tensor torch.int64 prefix tokens (n_batch, ylen). required states List[Any] Scorer states for prefix tokens. required xs Tensor The encoder feature that generates ys (n_batch, xlen, n_feat). required Returns: Type Description Tuple[torch.Tensor, List[Any]] tuple[torch.Tensor, List[Any]]: Tuple of batchfied scores for next token with shape of (n_batch, n_vocab) and next state list for ys. Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def score ( self , ys : torch . Tensor , states : List [ Any ], xs : torch . Tensor ) -> Tuple [ torch . Tensor , List [ Any ]]: \"\"\"Score new token batch (required). Args: ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen). states (List[Any]): Scorer states for prefix tokens. xs (torch.Tensor): The encoder feature that generates ys (n_batch, xlen, n_feat). Returns: tuple[torch.Tensor, List[Any]]: Tuple of batchfied scores for next token with shape of `(n_batch, n_vocab)` and next state list for ys. \"\"\" raise NotImplementedError","title":"score()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.PartialScorerInterface","text":"Partial scorer interface for beam search. The partial scorer performs scoring when non - partial scorer finished scoring , and recieves pre - pruned next tokens to score because it is too heavy to score all the tokens . !!! examples * Prefix search for connectionist - temporal - classification models * : class : ` services . hci . speech . espnet_minimal . nets . scorers . ctc . CTCPrefixScorer `","title":"PartialScorerInterface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.PartialScorerInterface.score_partial","text":"Score new token (required). Parameters: Name Type Description Default y Tensor 1D prefix token required next_tokens Tensor torch.int64 next token to score required state Any decoder state for prefix tokens required x Tensor The encoder feature that generates ys required Returns: Type Description Tuple[torch.Tensor, Any] tuple[torch.Tensor, Any]: Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def score_partial ( self , y : torch . Tensor , next_tokens : torch . Tensor , state : Any , x : torch . Tensor ) \\ -> Tuple [ torch . Tensor , Any ]: \"\"\"Score new token (required). Args: y (torch.Tensor): 1D prefix token next_tokens (torch.Tensor): torch.int64 next token to score state: decoder state for prefix tokens x (torch.Tensor): The encoder feature that generates ys Returns: tuple[torch.Tensor, Any]: Tuple of a score tensor for y that has a shape `(len(next_tokens),)` and next state for ys \"\"\" raise NotImplementedError","title":"score_partial()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.ScorerInterface","text":"Scorer interface for beam search. The scorer performs scoring of the all tokens in vocabulary . !!! examples * Search heuristics * : class : ` services . hci . speech . espnet_minimal . nets . scorers . length_bonus . LengthBonus ` * Decoder networks of the sequence - to - sequence models * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . nets . transformer . decoder . Decoder ` * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . nets . rnn . decoders . Decoder ` * Neural language models * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . lm . transformer . TransformerLM ` * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . lm . default . DefaultRNNLM ` * : class : ` services . hci . speech . espnet_minimal . nets . pytorch_backend . lm . seq_rnn . SequentialRNNLM `","title":"ScorerInterface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.ScorerInterface.final_score","text":"Score eos (optional). Parameters: Name Type Description Default state Any Scorer state for prefix tokens required Returns: Type Description float float: final score Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 68 69 70 71 72 73 74 75 76 77 78 def final_score ( self , state : Any ) -> float : \"\"\"Score eos (optional). Args: state: Scorer state for prefix tokens Returns: float: final score \"\"\" return 0.0","title":"final_score()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.ScorerInterface.init_state","text":"Get an initial state for decoding (optional). Parameters: Name Type Description Default x Tensor The encoded feature tensor required Returns: initial state Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 28 29 30 31 32 33 34 35 36 37 def init_state ( self , x : torch . Tensor ) -> Any : \"\"\"Get an initial state for decoding (optional). Args: x (torch.Tensor): The encoded feature tensor Returns: initial state \"\"\" return None","title":"init_state()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.ScorerInterface.score","text":"Score new token (required). Parameters: Name Type Description Default y Tensor 1D torch.int64 prefix tokens. required state Any Scorer state for prefix tokens required x Tensor The encoder feature that generates ys. required Returns: Type Description Tuple[torch.Tensor, Any] tuple[torch.Tensor, Any]: Tuple of scores for next token that has a shape of (n_vocab) and next state for ys Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def score ( self , y : torch . Tensor , state : Any , x : torch . Tensor ) -> Tuple [ torch . Tensor , Any ]: \"\"\"Score new token (required). Args: y (torch.Tensor): 1D torch.int64 prefix tokens. state: Scorer state for prefix tokens x (torch.Tensor): The encoder feature that generates ys. Returns: tuple[torch.Tensor, Any]: Tuple of scores for next token that has a shape of `(n_vocab)` and next state for ys \"\"\" raise NotImplementedError","title":"score()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorer_interface.ScorerInterface.select_state","text":"Select state with relative ids in the main beam search. Parameters: Name Type Description Default state Any Decoder state for prefix tokens required i int Index to select a state in the main beam search required Returns: Type Description Any state: pruned state Source code in adviser/tools/espnet_minimal/nets/scorer_interface.py 39 40 41 42 43 44 45 46 47 48 49 50 def select_state ( self , state : Any , i : int ) -> Any : \"\"\"Select state with relative ids in the main beam search. Args: state: Decoder state for prefix tokens i (int): Index to select a state in the main beam search Returns: state: pruned state \"\"\" return None if state is None else state [ i ]","title":"select_state()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorers","text":"","title":"scorers"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorers.ctc","text":"ScorerInterface implementation for CTC.","title":"ctc"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.scorers.ctc.CTCPrefixScorer","text":"Decoder interface wrapper for CTCPrefixScore. __init__ ( self , ctc , eos ) special Initialize class. Parameters: Name Type Description Default ctc Module The CTC implementaiton. For example, :class: services.hci.speech.espnet_minimal.nets.pytorch_backend.ctc.CTC required eos int The end-of-sequence id. required Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , ctc : torch . nn . Module , eos : int ): \"\"\"Initialize class. Args: ctc (torch.nn.Module): The CTC implementaiton. For example, :class:`services.hci.speech.espnet_minimal.nets.pytorch_backend.ctc.CTC` eos (int): The end-of-sequence id. \"\"\" self . ctc = ctc self . eos = eos self . impl = None init_state ( self , x ) Get an initial state for decoding. Parameters: Name Type Description Default x Tensor The encoded feature tensor required Returns: initial state Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def init_state ( self , x : torch . Tensor ): \"\"\"Get an initial state for decoding. Args: x (torch.Tensor): The encoded feature tensor Returns: initial state \"\"\" logp = self . ctc . log_softmax ( x . unsqueeze ( 0 )) . detach () . squeeze ( 0 ) . cpu () . numpy () # TODO(karita): use CTCPrefixScoreTH self . impl = CTCPrefixScore ( logp , 0 , self . eos , np ) return 0 , self . impl . initial_state () score_partial ( self , y , ids , state , x ) Score new token. Parameters: Name Type Description Default y torch.Tensor 1D prefix token required next_tokens torch.Tensor torch.int64 next token to score required state decoder state for prefix tokens required x torch.Tensor 2D encoder feature that generates ys required Returns: Type Description tuple[torch.Tensor, Any] Tuple of a score tensor for y that has a shape (len(next_tokens),) and next state for ys Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def score_partial ( self , y , ids , state , x ): \"\"\"Score new token. Args: y (torch.Tensor): 1D prefix token next_tokens (torch.Tensor): torch.int64 next token to score state: decoder state for prefix tokens x (torch.Tensor): 2D encoder feature that generates ys Returns: tuple[torch.Tensor, Any]: Tuple of a score tensor for y that has a shape `(len(next_tokens),)` and next state for ys \"\"\" prev_score , state = state presub_score , new_st = self . impl ( y . cpu (), ids . cpu (), state ) tscore = torch . as_tensor ( presub_score - prev_score , device = x . device , dtype = x . dtype ) return tscore , ( presub_score , new_st ) select_state ( self , state , i ) Select state with relative ids in the main beam search. Parameters: Name Type Description Default state Decoder state for prefix tokens required i int Index to select a state in the main beam search required Returns: Type Description state pruned state Source code in adviser/tools/espnet_minimal/nets/scorers/ctc.py 39 40 41 42 43 44 45 46 47 48 49 50 51 def select_state ( self , state , i ): \"\"\"Select state with relative ids in the main beam search. Args: state: Decoder state for prefix tokens i (int): Index to select a state in the main beam search Returns: state: pruned state \"\"\" sc , st = state return sc [ i ], st [ i ]","title":"CTCPrefixScorer"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface","text":"TTS Interface realted modules.","title":"tts_interface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface","text":"TTS Interface for ESPnet model implementation.","title":"TTSInterface"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface.base_plot_keys","text":"Return base key names to plot during training. The keys should match what chainer.reporter reports. if you add the key loss , the reporter will report main/loss and validation/main/loss values. also loss.png will be created as a figure visulizing main/loss and validation/main/loss values. Returns: Type Description list[str] Base keys to plot during training.","title":"base_plot_keys"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface.__init__","text":"Initilize TTS module. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 19 20 21 def __init__ ( self ): \"\"\"Initilize TTS module.\"\"\" self . reporter = None","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface.add_arguments","text":"Add model specific argments to parser. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 14 15 16 17 @staticmethod def add_arguments ( parser ): \"\"\"Add model specific argments to parser.\"\"\" return parser","title":"add_arguments()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface.calculate_all_attentions","text":"Calculate TTS attention weights. Parameters: Name Type Description Default Tensor Batch of attention weights (B, Lmax, Tmax). required Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 43 44 45 46 47 48 49 50 def calculate_all_attentions ( self , * args , ** kwargs ): \"\"\"Calculate TTS attention weights. Args: Tensor: Batch of attention weights (B, Lmax, Tmax). \"\"\" raise NotImplementedError ( \"calculate_all_attentions method is not implemented\" )","title":"calculate_all_attentions()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface.forward","text":"Calculate TTS forward propagation. Returns: Type Description Tensor Loss value. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 23 24 25 26 27 28 29 30 def forward ( self , * args , ** kwargs ): \"\"\"Calculate TTS forward propagation. Returns: Tensor: Loss value. \"\"\" raise NotImplementedError ( \"forward method is not implemented\" )","title":"forward()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface.inference","text":"Generate the sequence of features given the sequences of characters. Returns: Type Description Tensor The sequence of generated features (L, odim). Tensor: The sequence of stop probabilities (L,). Tensor: The sequence of attention weights (L, T). Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 32 33 34 35 36 37 38 39 40 41 def inference ( self , * args , ** kwargs ): \"\"\"Generate the sequence of features given the sequences of characters. Returns: Tensor: The sequence of generated features (L, odim). Tensor: The sequence of stop probabilities (L,). Tensor: The sequence of attention weights (L, T). \"\"\" raise NotImplementedError ( \"inference method is not implemented\" )","title":"inference()"},{"location":"api/tools/#adviser.tools.espnet_minimal.nets.tts_interface.TTSInterface.load_pretrained_model","text":"Load pretrained model parameters. Source code in adviser/tools/espnet_minimal/nets/tts_interface.py 52 53 54 def load_pretrained_model ( self , model_path ): \"\"\"Load pretrained model parameters.\"\"\" torch_load ( model_path , self )","title":"load_pretrained_model()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils","text":"","title":"utils"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.check_kwargs","text":"","title":"check_kwargs"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.check_kwargs.check_kwargs","text":"check kwargs are valid for func If kwargs are invalid, raise TypeError as same as python default :param function func: function to be validated :param dict kwargs: keyword arguments for func :param str name: name used in TypeError (default is func name) Source code in adviser/tools/espnet_minimal/utils/check_kwargs.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def check_kwargs ( func , kwargs , name = None ): \"\"\"check kwargs are valid for func If kwargs are invalid, raise TypeError as same as python default :param function func: function to be validated :param dict kwargs: keyword arguments for func :param str name: name used in TypeError (default is func name) \"\"\" try : params = inspect . signature ( func ) . parameters except ValueError : return if name is None : name = func . __name__ for k in kwargs . keys (): if k not in params : raise TypeError ( f \" { name } () got an unexpected keyword argument ' { k } '\" )","title":"check_kwargs()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers","text":"","title":"cli_readers"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.HDF5Reader","text":"","title":"HDF5Reader"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.HDF5Reader.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , rspecifier , return_shape = False ): if ':' not in rspecifier : raise ValueError ( 'Give \"rspecifier\" such as \"ark:some.ark: {} \"' . format ( self . rspecifier )) self . rspecifier = rspecifier self . ark_or_scp , self . filepath = self . rspecifier . split ( ':' , 1 ) if self . ark_or_scp not in [ 'ark' , 'scp' ]: raise ValueError ( f 'Must be scp or ark: { self . ark_or_scp } ' ) self . return_shape = return_shape","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.HDF5Reader.__iter__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __iter__ ( self ): if self . ark_or_scp == 'scp' : hdf5_dict = {} with open ( self . filepath , 'r' , encoding = 'utf-8' ) as f : for line in f : key , value = line . rstrip () . split ( None , 1 ) if ':' not in value : raise RuntimeError ( 'scp file for hdf5 should be like: ' '\"uttid filepath.h5:key\": {} ( {} )' . format ( line , self . filepath )) path , h5_key = value . split ( ':' , 1 ) hdf5_file = hdf5_dict . get ( path ) if hdf5_file is None : try : hdf5_file = h5py . File ( path , 'r' ) except Exception : logging . error ( 'Error when loading {} ' . format ( path )) raise hdf5_dict [ path ] = hdf5_file try : data = hdf5_file [ h5_key ] except Exception : logging . error ( 'Error when loading {} with key= {} ' . format ( path , h5_key )) raise if self . return_shape : yield key , data . shape else : yield key , data [()] # Closing all files for k in hdf5_dict : try : hdf5_dict [ k ] . close () except Exception : pass else : if self . filepath == '-' : # Required h5py>=2.9 filepath = io . BytesIO ( sys . stdin . buffer . read ()) else : filepath = self . filepath with h5py . File ( filepath , 'r' ) as f : for key in f : if self . return_shape : yield key , f [ key ] . shape else : yield key , f [ key ][()]","title":"__iter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.KaldiReader","text":"","title":"KaldiReader"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.KaldiReader.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 54 55 56 57 def __init__ ( self , rspecifier , return_shape = False , segments = None ): self . rspecifier = rspecifier self . return_shape = return_shape self . segments = segments","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.KaldiReader.__iter__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 59 60 61 62 63 64 65 def __iter__ ( self ): with kaldiio . ReadHelper ( self . rspecifier , segments = self . segments ) as reader : for key , array in reader : if self . return_shape : array = array . shape yield key , array","title":"__iter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.SoundHDF5Reader","text":"","title":"SoundHDF5Reader"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.SoundHDF5Reader.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 138 139 140 141 142 143 144 145 def __init__ ( self , rspecifier , return_shape = False ): if ':' not in rspecifier : raise ValueError ( 'Give \"rspecifier\" such as \"ark:some.ark: {} \"' . format ( rspecifier )) self . ark_or_scp , self . filepath = rspecifier . split ( ':' , 1 ) if self . ark_or_scp not in [ 'ark' , 'scp' ]: raise ValueError ( f 'Must be scp or ark: { self . ark_or_scp } ' ) self . return_shape = return_shape","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.SoundHDF5Reader.__iter__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def __iter__ ( self ): if self . ark_or_scp == 'scp' : hdf5_dict = {} with open ( self . filepath , 'r' , encoding = 'utf-8' ) as f : for line in f : key , value = line . rstrip () . split ( None , 1 ) if ':' not in value : raise RuntimeError ( 'scp file for hdf5 should be like: ' '\"uttid filepath.h5:key\": {} ( {} )' . format ( line , self . filepath )) path , h5_key = value . split ( ':' , 1 ) hdf5_file = hdf5_dict . get ( path ) if hdf5_file is None : try : hdf5_file = SoundHDF5File ( path , 'r' ) except Exception : logging . error ( 'Error when loading {} ' . format ( path )) raise hdf5_dict [ path ] = hdf5_file try : data = hdf5_file [ h5_key ] except Exception : logging . error ( 'Error when loading {} with key= {} ' . format ( path , h5_key )) raise # Change Tuple[ndarray, int] -> Tuple[int, ndarray] # (soundfile style -> scipy style) array , rate = data if self . return_shape : array = array . shape yield key , ( rate , array ) # Closing all files for k in hdf5_dict : try : hdf5_dict [ k ] . close () except Exception : pass else : if self . filepath == '-' : # Required h5py>=2.9 filepath = io . BytesIO ( sys . stdin . buffer . read ()) else : filepath = self . filepath for key , ( a , r ) in SoundHDF5File ( filepath , 'r' ) . items (): if self . return_shape : a = a . shape yield key , ( r , a )","title":"__iter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.SoundReader","text":"","title":"SoundReader"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.SoundReader.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 205 206 207 208 209 210 211 212 213 def __init__ ( self , rspecifier , return_shape = False ): if ':' not in rspecifier : raise ValueError ( 'Give \"rspecifier\" such as \"scp:some.scp: {} \"' . format ( rspecifier )) self . ark_or_scp , self . filepath = rspecifier . split ( ':' , 1 ) if self . ark_or_scp != 'scp' : raise ValueError ( 'Only supporting \"scp\" for sound file: {} ' . format ( self . ark_or_scp )) self . return_shape = return_shape","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.SoundReader.__iter__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 215 216 217 218 219 220 221 222 223 224 225 def __iter__ ( self ): with open ( self . filepath , 'r' , encoding = 'utf-8' ) as f : for line in f : key , sound_file_path = line . rstrip () . split ( None , 1 ) # Assume PCM16 array , rate = soundfile . read ( sound_file_path , dtype = 'int16' ) # Change Tuple[ndarray, int] -> Tuple[int, ndarray] # (soundfile style -> scipy style) if self . return_shape : array = array . shape yield key , ( rate , array )","title":"__iter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_readers.file_reader_helper","text":"Read uttid and array in kaldi style This function might be a bit confusing as \"ark\" is used for HDF5 to imitate \"kaldi-rspecifier\". Parameters: Name Type Description Default rspecifier str Give as \"ark:feats.ark\" or \"scp:feats.scp\" required filetype str \"mat\" is kaldi-martix, \"hdf5\": HDF5 'mat' return_shape bool Return the shape of the matrix, instead of the matrix. This can reduce IO cost for HDF5. False Returns: Type Description Generator[Tuple[str, np.ndarray], None, None] Examples Read from kaldi-matrix ark file: for u, array in file_reader_helper('ark:feats.ark', 'mat'): ... array Read from HDF5 file: for u, array in file_reader_helper('ark:feats.h5', 'hdf5'): ... array Source code in adviser/tools/espnet_minimal/utils/cli_readers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def file_reader_helper ( rspecifier : str , filetype : str = 'mat' , return_shape : bool = False , segments : str = None ): \"\"\"Read uttid and array in kaldi style This function might be a bit confusing as \"ark\" is used for HDF5 to imitate \"kaldi-rspecifier\". Args: rspecifier: Give as \"ark:feats.ark\" or \"scp:feats.scp\" filetype: \"mat\" is kaldi-martix, \"hdf5\": HDF5 return_shape: Return the shape of the matrix, instead of the matrix. This can reduce IO cost for HDF5. Returns: Generator[Tuple[str, np.ndarray], None, None]: Examples: Read from kaldi-matrix ark file: >>> for u, array in file_reader_helper('ark:feats.ark', 'mat'): ... array Read from HDF5 file: >>> for u, array in file_reader_helper('ark:feats.h5', 'hdf5'): ... array \"\"\" if filetype == 'mat' : return KaldiReader ( rspecifier , return_shape = return_shape , segments = segments ) elif filetype == 'hdf5' : return HDF5Reader ( rspecifier , return_shape = return_shape ) elif filetype == 'sound.hdf5' : return SoundHDF5Reader ( rspecifier , return_shape = return_shape ) elif filetype == 'sound' : return SoundReader ( rspecifier , return_shape = return_shape ) else : raise NotImplementedError ( f 'filetype= { filetype } ' )","title":"file_reader_helper()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_utils","text":"","title":"cli_utils"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_utils.assert_scipy_wav_style","text":"Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 33 34 35 36 37 38 def assert_scipy_wav_style ( value ): assert is_scipy_wav_style ( value ), \\ 'Must be Tuple[int, numpy.ndarray], but got {} ' . format ( type ( value ) if not isinstance ( value , Sequence ) else ' {} [ {} ]' . format ( type ( value ), ', ' . join ( str ( type ( v )) for v in value )))","title":"assert_scipy_wav_style()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_utils.get_commandline_args","text":"Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 13 14 15 16 17 18 19 20 21 22 23 def get_commandline_args (): extra_chars = [ ' ' , ';' , '&' , '(' , ')' , '|' , '^' , '<' , '>' , '?' , '*' , '[' , ']' , '$' , '`' , '\"' , ' \\\\ ' , '!' , '{' , '}' ] # Escape the extra characters for shell argv = [ arg . replace ( ' \\' ' , ' \\'\\\\\\'\\' ' ) if all ( char not in arg for char in extra_chars ) else ' \\' ' + arg . replace ( ' \\' ' , ' \\'\\\\\\'\\' ' ) + ' \\' ' for arg in sys . argv ] return sys . executable + ' ' + ' ' . join ( argv )","title":"get_commandline_args()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_utils.is_scipy_wav_style","text":"Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 26 27 28 29 30 def is_scipy_wav_style ( value ): # If Tuple[int, numpy.ndarray] or not return ( isinstance ( value , Sequence ) and len ( value ) == 2 and isinstance ( value [ 0 ], int ) and isinstance ( value [ 1 ], numpy . ndarray ))","title":"is_scipy_wav_style()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_utils.strtobool","text":"Source code in adviser/tools/espnet_minimal/utils/cli_utils.py 8 9 10 def strtobool ( x ): # distutils.util.strtobool returns integer, but it's confusing, return bool ( dist_strtobool ( x ))","title":"strtobool()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers","text":"","title":"cli_writers"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.BaseWriter","text":"","title":"BaseWriter"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.BaseWriter.__enter__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 77 78 def __enter__ ( self ): return self","title":"__enter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.BaseWriter.__exit__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 80 81 def __exit__ ( self , exc_type , exc_val , exc_tb ): self . close ()","title":"__exit__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.BaseWriter.__setitem__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 74 75 def __setitem__ ( self , key , value ): raise NotImplementedError","title":"__setitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.BaseWriter.close","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def close ( self ): try : self . writer . close () except Exception : pass if self . writer_scp is not None : try : self . writer_scp . close () except Exception : pass if self . writer_nframe is not None : try : self . writer_nframe . close () except Exception : pass","title":"close()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.HDF5Writer","text":"HDF5Writer !!! examples >>> with HDF5Writer('ark:out.h5', compress=True) as f: ... f['key'] = array","title":"HDF5Writer"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.HDF5Writer.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , wspecifier , write_num_frames = None , compress = False ): spec_dict = parse_wspecifier ( wspecifier ) self . filename = spec_dict [ 'ark' ] if compress : self . kwargs = { 'compression' : 'gzip' } else : self . kwargs = {} self . writer = h5py . File ( spec_dict [ 'ark' ], 'w' ) if 'scp' in spec_dict : self . writer_scp = open ( spec_dict [ 'scp' ], 'w' , encoding = 'utf-8' ) else : self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.HDF5Writer.__setitem__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 190 191 192 193 194 195 196 def __setitem__ ( self , key , value ): self . writer . create_dataset ( key , data = value , ** self . kwargs ) if self . writer_scp is not None : self . writer_scp . write ( f ' { key } { self . filename } : { key } \\n ' ) if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( value ) } \\n ' )","title":"__setitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.KaldiWriter","text":"","title":"KaldiWriter"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.KaldiWriter.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , wspecifier , write_num_frames = None , compress = False , compression_method = 2 ): if compress : self . writer = kaldiio . WriteHelper ( wspecifier , compression_method = compression_method ) else : self . writer = kaldiio . WriteHelper ( wspecifier ) self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.KaldiWriter.__setitem__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 137 138 139 140 def __setitem__ ( self , key , value ): self . writer [ key ] = value if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( value ) } \\n ' )","title":"__setitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.SoundHDF5Writer","text":"SoundHDF5Writer !!! examples >>> fs = 16000 >>> with SoundHDF5Writer('ark:out.h5') as f: ... f['key'] = fs, array","title":"SoundHDF5Writer"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.SoundHDF5Writer.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , wspecifier , write_num_frames = None , pcm_format = 'wav' ): self . pcm_format = pcm_format spec_dict = parse_wspecifier ( wspecifier ) self . filename = spec_dict [ 'ark' ] self . writer = SoundHDF5File ( spec_dict [ 'ark' ], 'w' , format = self . pcm_format ) if 'scp' in spec_dict : self . writer_scp = open ( spec_dict [ 'scp' ], 'w' , encoding = 'utf-8' ) else : self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.SoundHDF5Writer.__setitem__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 223 224 225 226 227 228 229 230 231 232 233 def __setitem__ ( self , key , value ): assert_scipy_wav_style ( value ) # Change Tuple[int, ndarray] -> Tuple[ndarray, int] # (scipy style -> soundfile style) value = ( value [ 1 ], value [ 0 ]) self . writer . create_dataset ( key , data = value ) if self . writer_scp is not None : self . writer_scp . write ( f ' { key } { self . filename } : { key } \\n ' ) if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( value [ 0 ]) } \\n ' )","title":"__setitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.SoundWriter","text":"SoundWriter !!! examples >>> fs = 16000 >>> with SoundWriter('ark,scp:outdir,out.scp') as f: ... f['key'] = fs, array","title":"SoundWriter"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.SoundWriter.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def __init__ ( self , wspecifier , write_num_frames = None , pcm_format = 'wav' ): self . pcm_format = pcm_format spec_dict = parse_wspecifier ( wspecifier ) # e.g. ark,scp:dirname,wav.scp # -> The wave files are found in dirname/*.wav self . dirname = spec_dict [ 'ark' ] Path ( self . dirname ) . mkdir ( parents = True , exist_ok = True ) self . writer = None if 'scp' in spec_dict : self . writer_scp = open ( spec_dict [ 'scp' ], 'w' , encoding = 'utf-8' ) else : self . writer_scp = None if write_num_frames is not None : self . writer_nframe = get_num_frames_writer ( write_num_frames ) else : self . writer_nframe = None","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.SoundWriter.__setitem__","text":"Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 263 264 265 266 267 268 269 270 271 272 def __setitem__ ( self , key , value ): assert_scipy_wav_style ( value ) rate , signal = value wavfile = Path ( self . dirname ) / ( key + '.' + self . pcm_format ) soundfile . write ( wavfile , signal . astype ( numpy . int16 ), rate ) if self . writer_scp is not None : self . writer_scp . write ( f ' { key } { wavfile } \\n ' ) if self . writer_nframe is not None : self . writer_nframe . write ( f ' { key } { len ( signal ) } \\n ' )","title":"__setitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.file_writer_helper","text":"Write matrices in kaldi style Parameters: Name Type Description Default wspecifier str e.g. ark,scp:out.ark,out.scp required filetype str \"mat\" is kaldi-martix, \"hdf5\": HDF5 'mat' write_num_frames str e.g. 'ark,t:num_frames.txt' None compress bool Compress or not False compression_method int Specify compression level 2 Write in kaldi-matrix-ark with \"kaldi-scp\" file: with file_writer_helper('ark,scp:out.ark,out.scp') as f: f['uttid'] = array This \"scp\" has the following format: uttidA out.ark:1234 uttidB out.ark:2222 where, 1234 and 2222 points the strating byte address of the matrix. (For detail, see official documentation of Kaldi) Write in HDF5 with \"scp\" file: with file_writer_helper('ark,scp:out.h5,out.scp', 'hdf5') as f: f['uttid'] = array This \"scp\" file is created as: uttidA out.h5:uttidA uttidB out.h5:uttidB HDF5 can be, unlike \"kaldi-ark\", accessed to any keys, so originally \"scp\" is not required for random-reading. Nevertheless we create \"scp\" for HDF5 because it is useful for some use-case. e.g. Concatenation, Splitting. Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def file_writer_helper ( wspecifier : str , filetype : str = 'mat' , write_num_frames : str = None , compress : bool = False , compression_method : int = 2 , pcm_format : str = 'wav' ): \"\"\"Write matrices in kaldi style Args: wspecifier: e.g. ark,scp:out.ark,out.scp filetype: \"mat\" is kaldi-martix, \"hdf5\": HDF5 write_num_frames: e.g. 'ark,t:num_frames.txt' compress: Compress or not compression_method: Specify compression level Write in kaldi-matrix-ark with \"kaldi-scp\" file: >>> with file_writer_helper('ark,scp:out.ark,out.scp') as f: >>> f['uttid'] = array This \"scp\" has the following format: uttidA out.ark:1234 uttidB out.ark:2222 where, 1234 and 2222 points the strating byte address of the matrix. (For detail, see official documentation of Kaldi) Write in HDF5 with \"scp\" file: >>> with file_writer_helper('ark,scp:out.h5,out.scp', 'hdf5') as f: >>> f['uttid'] = array This \"scp\" file is created as: uttidA out.h5:uttidA uttidB out.h5:uttidB HDF5 can be, unlike \"kaldi-ark\", accessed to any keys, so originally \"scp\" is not required for random-reading. Nevertheless we create \"scp\" for HDF5 because it is useful for some use-case. e.g. Concatenation, Splitting. \"\"\" if filetype == 'mat' : return KaldiWriter ( wspecifier , write_num_frames = write_num_frames , compress = compress , compression_method = compression_method ) elif filetype == 'hdf5' : return HDF5Writer ( wspecifier , write_num_frames = write_num_frames , compress = compress ) elif filetype == 'sound.hdf5' : return SoundHDF5Writer ( wspecifier , write_num_frames = write_num_frames , pcm_format = pcm_format ) elif filetype == 'sound' : return SoundWriter ( wspecifier , write_num_frames = write_num_frames , pcm_format = pcm_format ) else : raise NotImplementedError ( f 'filetype= { filetype } ' )","title":"file_writer_helper()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.get_num_frames_writer","text":"get_num_frames_writer Examples get_num_frames_writer('ark,t:num_frames.txt') Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_num_frames_writer ( write_num_frames : str ): \"\"\"get_num_frames_writer Examples: >>> get_num_frames_writer('ark,t:num_frames.txt') \"\"\" if write_num_frames is not None : if ':' not in write_num_frames : raise ValueError ( 'Must include \":\", write_num_frames= {} ' . format ( write_num_frames )) nframes_type , nframes_file = write_num_frames . split ( ':' , 1 ) if nframes_type != 'ark,t' : raise ValueError ( 'Only supporting text mode. ' 'e.g. --write-num-frames=ark,t:foo.txt :' ' {} ' . format ( nframes_type )) return open ( nframes_file , 'w' , encoding = 'utf-8' )","title":"get_num_frames_writer()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.cli_writers.parse_wspecifier","text":"Parse wspecifier to dict Examples parse_wspecifier('ark,scp:out.ark,out.scp') {'ark': 'out.ark', 'scp': 'out.scp'} Source code in adviser/tools/espnet_minimal/utils/cli_writers.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def parse_wspecifier ( wspecifier : str ) -> Dict [ str , str ]: \"\"\"Parse wspecifier to dict Examples: >>> parse_wspecifier('ark,scp:out.ark,out.scp') {'ark': 'out.ark', 'scp': 'out.scp'} \"\"\" ark_scp , filepath = wspecifier . split ( ':' , 1 ) if ark_scp not in [ 'ark' , 'scp,ark' , 'ark,scp' ]: raise ValueError ( ' {} is not allowed: {} ' . format ( ark_scp , wspecifier )) ark_scps = ark_scp . split ( ',' ) filepaths = filepath . split ( ',' ) if len ( ark_scps ) != len ( filepaths ): raise ValueError ( 'Mismatch: {} and {} ' . format ( ark_scp , filepath )) spec_dict = dict ( zip ( ark_scps , filepaths )) return spec_dict","title":"parse_wspecifier()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset","text":"This module contains pytorch dataset and dataloader implementation for chainer training.","title":"dataset"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader","text":"Pytorch dataloader in chainer style. !!! args all args for torch.utils.data.dataloader.Dataloader","title":"ChainerDataLoader"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader.epoch_detail","text":"Epoch_detail required by chainer.","title":"epoch_detail"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader.__init__","text":"Init function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 44 45 46 47 48 49 50 51 def __init__ ( self , ** kwargs ): \"\"\"Init function.\"\"\" self . loader = torch . utils . data . dataloader . DataLoader ( ** kwargs ) self . len = len ( kwargs [ 'dataset' ]) self . current_position = 0 self . epoch = 0 self . iter = None self . kwargs = kwargs","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader.__iter__","text":"Implement iter function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 68 69 70 71 def __iter__ ( self ): \"\"\"Implement iter function.\"\"\" for batch in self . loader : yield batch","title":"__iter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader.finalize","text":"Implement finalize function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 90 91 92 def finalize ( self ): \"\"\"Implement finalize function.\"\"\" del self . loader","title":"finalize()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader.next","text":"Implement next function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def next ( self ): \"\"\"Implement next function.\"\"\" if self . iter is None : self . iter = iter ( self . loader ) try : ret = next ( self . iter ) except StopIteration : self . iter = None return self . next () self . current_position += 1 if self . current_position == self . len : self . epoch = self . epoch + 1 self . current_position = 0 return ret","title":"next()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader.serialize","text":"Serialize and deserialize function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 78 79 80 81 82 83 def serialize ( self , serializer ): \"\"\"Serialize and deserialize function.\"\"\" epoch = serializer ( 'epoch' , self . epoch ) current_position = serializer ( 'current_position' , self . current_position ) self . epoch = epoch self . current_position = current_position","title":"serialize()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.ChainerDataLoader.start_shuffle","text":"Shuffle function for sortagrad. Source code in adviser/tools/espnet_minimal/utils/dataset.py 85 86 87 88 def start_shuffle ( self ): \"\"\"Shuffle function for sortagrad.\"\"\" self . kwargs [ 'shuffle' ] = True self . loader = torch . utils . data . dataloader . DataLoader ( ** self . kwargs )","title":"start_shuffle()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.TransformDataset","text":"Transform Dataset for pytorch backend. !!! args data: list object from make_batchset transfrom: transform function","title":"TransformDataset"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.TransformDataset.__getitem__","text":"[] operator. Source code in adviser/tools/espnet_minimal/utils/dataset.py 31 32 33 def __getitem__ ( self , idx ): \"\"\"[] operator.\"\"\" return self . transform ( self . data [ idx ])","title":"__getitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.TransformDataset.__init__","text":"Init function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 21 22 23 24 25 def __init__ ( self , data , transform ): \"\"\"Init function.\"\"\" super ( TransformDataset ) . __init__ () self . data = data self . transform = transform","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dataset.TransformDataset.__len__","text":"Len function. Source code in adviser/tools/espnet_minimal/utils/dataset.py 27 28 29 def __len__ ( self ): \"\"\"Len function.\"\"\" return len ( self . data )","title":"__len__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.deterministic_utils","text":"","title":"deterministic_utils"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.deterministic_utils.set_deterministic_chainer","text":"Ensures chainer produces deterministic results depending on the program arguments :param Namespace args: The program arguments Source code in adviser/tools/espnet_minimal/utils/deterministic_utils.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def set_deterministic_chainer ( args ): \"\"\"Ensures chainer produces deterministic results depending on the program arguments :param Namespace args: The program arguments \"\"\" # seed setting (chainer seed may not need it) os . environ [ 'CHAINER_SEED' ] = str ( args . seed ) logging . info ( 'chainer seed = ' + os . environ [ 'CHAINER_SEED' ]) # debug mode setting # 0 would be fastest, but 1 seems to be reasonable # considering reproducibility # remove type check if args . debugmode < 2 : chainer . config . type_check = False logging . info ( 'chainer type check is disabled' ) # use deterministic computation or not if args . debugmode < 1 : chainer . config . cudnn_deterministic = False logging . info ( 'chainer cudnn deterministic is disabled' ) else : chainer . config . cudnn_deterministic = True","title":"set_deterministic_chainer()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.deterministic_utils.set_deterministic_pytorch","text":"Ensures pytorch produces deterministic results depending on the program arguments :param Namespace args: The program arguments Source code in adviser/tools/espnet_minimal/utils/deterministic_utils.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def set_deterministic_pytorch ( args ): \"\"\"Ensures pytorch produces deterministic results depending on the program arguments :param Namespace args: The program arguments \"\"\" # seed setting torch . manual_seed ( args . seed ) # debug mode setting # 0 would be fastest, but 1 seems to be reasonable # considering reproducibility # remove type check torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False # https://github.com/pytorch/pytorch/issues/6351 if args . debugmode < 2 : chainer . config . type_check = False logging . info ( 'torch type check is disabled' ) # use deterministic computation or not if args . debugmode < 1 : torch . backends . cudnn . deterministic = False torch . backends . cudnn . benchmark = True logging . info ( 'torch cudnn deterministic is disabled' )","title":"set_deterministic_pytorch()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dynamic_import","text":"","title":"dynamic_import"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.dynamic_import.dynamic_import","text":"dynamic import module and class :param str import_path: syntax 'module_name:class_name' e.g., 'services.hci.speech.espnet_minimal.transform.add_deltas:AddDeltas' :param dict alias: shortcut for registered class :return: imported class Source code in adviser/tools/espnet_minimal/utils/dynamic_import.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def dynamic_import ( import_path , alias = dict ()): \"\"\"dynamic import module and class :param str import_path: syntax 'module_name:class_name' e.g., 'services.hci.speech.espnet_minimal.transform.add_deltas:AddDeltas' :param dict alias: shortcut for registered class :return: imported class \"\"\" if import_path not in alias and ':' not in import_path : raise ValueError ( 'import_path should be one of {} or ' 'include \":\", e.g. \"services.hci.speech.espnet_minimal.transform.add_deltas:AddDeltas\" : ' ' {} ' . format ( set ( alias ), import_path )) if ':' not in import_path : import_path = alias [ import_path ] module_name , objname = import_path . split ( ':' ) m = importlib . import_module ( module_name ) return getattr ( m , objname )","title":"dynamic_import()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.fill_missing_args","text":"","title":"fill_missing_args"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.fill_missing_args.fill_missing_args","text":"Fill missing arguments in args. Parameters: Name Type Description Default args Namespace or None Namesapce containing hyperparameters. required add_arguments function Function to add arguments. required Returns: Type Description Namespace Arguments whose missing ones are filled with default value. Examples from argparse import Namespace from services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2 args = Namespace() fill_missing_args(args, Tacotron2.add_arguments_fn) Namespace(aconv_chans=32, aconv_filts=15, adim=512, atype='location', ...) Source code in adviser/tools/espnet_minimal/utils/fill_missing_args.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def fill_missing_args ( args , add_arguments ): \"\"\"Fill missing arguments in args. Args: args (Namespace or None): Namesapce containing hyperparameters. add_arguments (function): Function to add arguments. Returns: Namespace: Arguments whose missing ones are filled with default value. Examples: >>> from argparse import Namespace >>> from services.hci.speech.espnet_minimal.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2 >>> args = Namespace() >>> fill_missing_args(args, Tacotron2.add_arguments_fn) Namespace(aconv_chans=32, aconv_filts=15, adim=512, atype='location', ...) \"\"\" # check argument type assert isinstance ( args , argparse . Namespace ) or args is None assert callable ( add_arguments ) # get default arguments default_args , _ = add_arguments ( argparse . ArgumentParser ()) . parse_known_args () # convert to dict args = {} if args is None else vars ( args ) default_args = vars ( default_args ) for key , value in default_args . items (): if key not in args : logging . info ( \"attribute \\\" %s \\\" does not exist. use default %s .\" % ( key , str ( value ))) args [ key ] = value # Note from Florian: # I believe this is where the wrong # arguments are introduced... no idea # however where the arguments we actually # load go missing. return argparse . Namespace ( ** args )","title":"fill_missing_args()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils","text":"","title":"io_utils"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.LoadInputsAndTargets","text":"Create a mini-batch from a list of dicts >>> batch = [ ('utt1', ... dict(input=[dict(feat='some.ark:123', ... filetype='mat', ... name='input1', ... shape=[100, 80 ] ) ] , ... output =[ dict(tokenid='1 2 3 4', ... name='target1', ... shape=[4, 31 ] ) ]] )) >>> l = LoadInputsAndTargets () >>> feat , target = l ( batch ) : param : str mode : Specify the task mode , \"asr\" or \"tts\" : param : str preprocess_conf : The path of a json file for pre - processing : param : bool load_input : If False , not to load the input data : param : bool load_output : If False , not to load the output data : param : bool sort_in_input_length : Sort the mini - batch in descending order of the input length : param : bool use_speaker_embedding : Used for tts mode only : param : bool use_second_target : Used for tts mode only : param : dict preprocess_args : Set some optional arguments for preprocessing : param : Optional [ dict ] preprocess_args : Used for tts mode only","title":"LoadInputsAndTargets"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.LoadInputsAndTargets.__call__","text":"Function to load inputs and targets from list of dicts :param List[Tuple[str, dict]] batch: list of dict which is subset of loaded data.json :return: list of input token id sequences [(L_1), (L_2), ..., (L_B)] :return: list of input feature sequences [(T_1, D), (T_2, D), ..., (T_B, D)] :rtype: list of float ndarray :return: list of target token id sequences [(L_1), (L_2), ..., (L_B)] :rtype: list of int ndarray Source code in adviser/tools/espnet_minimal/utils/io_utils.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def __call__ ( self , batch ): \"\"\"Function to load inputs and targets from list of dicts :param List[Tuple[str, dict]] batch: list of dict which is subset of loaded data.json :return: list of input token id sequences [(L_1), (L_2), ..., (L_B)] :return: list of input feature sequences [(T_1, D), (T_2, D), ..., (T_B, D)] :rtype: list of float ndarray :return: list of target token id sequences [(L_1), (L_2), ..., (L_B)] :rtype: list of int ndarray \"\"\" x_feats_dict = OrderedDict () # OrderedDict[str, List[np.ndarray]] y_feats_dict = OrderedDict () # OrderedDict[str, List[np.ndarray]] uttid_list = [] # List[str] for uttid , info in batch : uttid_list . append ( uttid ) if self . load_input : # Note(kamo): This for-loop is for multiple inputs for idx , inp in enumerate ( info [ 'input' ]): # {\"input\": # [{\"feat\": \"some/path.h5:F01_050C0101_PED_REAL\", # \"filetype\": \"hdf5\", # \"name\": \"input1\", ...}], ...} x = self . _get_from_loader ( filepath = inp [ 'feat' ], filetype = inp . get ( 'filetype' , 'mat' )) x_feats_dict . setdefault ( inp [ 'name' ], []) . append ( x ) # FIXME(kamo): Dirty way to load only speaker_embedding without the other inputs elif self . mode == 'tts' and self . use_speaker_embedding : for idx , inp in enumerate ( info [ 'input' ]): if idx != 1 and len ( info [ 'input' ]) > 1 : x = None else : x = self . _get_from_loader ( filepath = inp [ 'feat' ], filetype = inp . get ( 'filetype' , 'mat' )) x_feats_dict . setdefault ( inp [ 'name' ], []) . append ( x ) if self . load_output : if self . mode == 'mt' : x = np . fromiter ( map ( int , info [ 'output' ][ 1 ][ 'tokenid' ] . split ()), dtype = np . int64 ) x_feats_dict . setdefault ( info [ 'output' ][ 1 ][ 'name' ], []) . append ( x ) for idx , inp in enumerate ( info [ 'output' ]): if 'tokenid' in inp : # ======= Legacy format for output ======= # {\"output\": [{\"tokenid\": \"1 2 3 4\"}]) x = np . fromiter ( map ( int , inp [ 'tokenid' ] . split ()), dtype = np . int64 ) else : # ======= New format ======= # {\"input\": # [{\"feat\": \"some/path.h5:F01_050C0101_PED_REAL\", # \"filetype\": \"hdf5\", # \"name\": \"target1\", ...}], ...} x = self . _get_from_loader ( filepath = inp [ 'feat' ], filetype = inp . get ( 'filetype' , 'mat' )) y_feats_dict . setdefault ( inp [ 'name' ], []) . append ( x ) if self . mode == 'asr' : return_batch , uttid_list = self . _create_batch_asr ( x_feats_dict , y_feats_dict , uttid_list ) elif self . mode == 'tts' : _ , info = batch [ 0 ] eos = int ( info [ 'output' ][ 0 ][ 'shape' ][ 1 ]) - 1 return_batch , uttid_list = self . _create_batch_tts ( x_feats_dict , y_feats_dict , uttid_list , eos ) elif self . mode == 'mt' : return_batch , uttid_list = self . _create_batch_mt ( x_feats_dict , y_feats_dict , uttid_list ) else : raise NotImplementedError if self . preprocessing is not None : # Apply pre-processing all input features for x_name in return_batch . keys (): if x_name . startswith ( \"input\" ): return_batch [ x_name ] = self . preprocessing ( return_batch [ x_name ], uttid_list , ** self . preprocess_args ) # Doesn't return the names now. return tuple ( return_batch . values ())","title":"__call__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.LoadInputsAndTargets.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , mode = 'asr' , preprocess_conf = None , load_input = True , load_output = True , sort_in_input_length = True , use_speaker_embedding = False , use_second_target = False , preprocess_args = None , keep_all_data_on_mem = False , ): self . _loaders = {} if mode not in [ 'asr' , 'tts' , 'mt' ]: raise ValueError ( 'Only asr or tts are allowed: mode= {} ' . format ( mode )) if preprocess_conf is not None : self . preprocessing = Transformation ( preprocess_conf ) logging . warning ( '[Experimental feature] Some preprocessing will be done ' 'for the mini-batch creation using {} ' . format ( self . preprocessing )) else : # If conf doesn't exist, this function don't touch anything. self . preprocessing = None if use_second_target and use_speaker_embedding and mode == 'tts' : raise ValueError ( 'Choose one of \"use_second_target\" and ' '\"use_speaker_embedding \"' ) if ( use_second_target or use_speaker_embedding ) and mode != 'tts' : logging . warning ( '\"use_second_target\" and \"use_speaker_embedding\" is ' 'used only for tts mode' ) self . mode = mode self . load_output = load_output self . load_input = load_input self . sort_in_input_length = sort_in_input_length self . use_speaker_embedding = use_speaker_embedding self . use_second_target = use_second_target if preprocess_args is None : self . preprocess_args = {} else : assert isinstance ( preprocess_args , dict ), type ( preprocess_args ) self . preprocess_args = dict ( preprocess_args ) self . keep_all_data_on_mem = keep_all_data_on_mem","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File","text":"Collecting sound files to a HDF5 file >>> f = SoundHDF5File ( 'a.flac.h5' , mode = 'a' ) >>> array = np . random . randint ( 0 , 100 , 100 , dtype = np . int16 ) >>> f [ 'id' ] = ( array , 16000 ) >>> array , rate = f [ 'id' ] : param : str filepath : : param : str mode : : param : str format : The type used when saving wav . flac , nist , htk , etc . : param : str dtype :","title":"SoundHDF5File"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__contains__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 536 537 def __contains__ ( self , item ): return item in self . file","title":"__contains__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__enter__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 542 543 def __enter__ ( self ): return self","title":"__enter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__exit__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 545 546 def __exit__ ( self , exc_type , exc_val , exc_tb ): self . file . close ()","title":"__exit__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__getitem__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 516 517 518 519 520 def __getitem__ ( self , key ): data = self . file [ key ][()] f = io . BytesIO ( data . tobytes ()) array , rate = soundfile . read ( f , dtype = self . dtype ) return array , rate","title":"__getitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__init__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 def __init__ ( self , filepath , mode = 'r+' , format = None , dtype = 'int16' , ** kwargs ): self . filepath = filepath self . mode = mode self . dtype = dtype self . file = h5py . File ( filepath , mode , ** kwargs ) if format is None : # filepath = a.flac.h5 -> format = flac second_ext = os . path . splitext ( os . path . splitext ( filepath )[ 0 ])[ 1 ] format = second_ext [ 1 :] if format . upper () not in soundfile . available_formats (): # If not found, flac is selected format = 'flac' # This format affects only saving self . format = format","title":"__init__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__iter__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 533 534 def __iter__ ( self ): return iter ( self . file )","title":"__iter__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__len__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 539 540 def __len__ ( self , item ): return len ( self . file )","title":"__len__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__repr__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 502 503 504 def __repr__ ( self ): return '<SoundHDF5 file \" {} \" (mode {} , format {} , type {} )>' \\ . format ( self . filepath , self . mode , self . format , self . dtype )","title":"__repr__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.__setitem__","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 513 514 def __setitem__ ( self , name , data ): self . create_dataset ( name , data = data )","title":"__setitem__()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.close","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 548 549 def close ( self ): self . file . close ()","title":"close()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.create_dataset","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 506 507 508 509 510 511 def create_dataset ( self , name , shape = None , data = None , ** kwds ): f = io . BytesIO () array , rate = data soundfile . write ( f , array , rate , format = self . format ) self . file . create_dataset ( name , shape = shape , data = np . void ( f . getvalue ()), ** kwds )","title":"create_dataset()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.items","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 529 530 531 def items ( self ): for k in self . file : yield k , self [ k ]","title":"items()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.keys","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 522 523 def keys ( self ): return self . file . keys ()","title":"keys()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.io_utils.SoundHDF5File.values","text":"Source code in adviser/tools/espnet_minimal/utils/io_utils.py 525 526 527 def values ( self ): for k in self . file : yield self [ k ]","title":"values()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment","text":"This implementation is modified from https://github.com/zcaceres/spec_augment MIT License Copyright (c) 2019 Zach Caceres Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETjjHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"spec_augment"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.apply_interpolation","text":"Apply polyharmonic interpolation model to data. Notes Given coefficients w and v for the interpolation model, we evaluate interpolated function values at query_points. Parameters: Name Type Description Default query_points [b, m, d] x values to evaluate the interpolation at required train_points [b, n, d] x values that act as the interpolation centers ( the c variables in the wikipedia article) w: [b, n, k] weights on each interpolation center v: [b, d, k] weights on each input dimension required order order of the interpolation required Returns: Type Description Polyharmonic interpolation evaluated at points defined in query_points. Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def apply_interpolation ( query_points , train_points , w , v , order ): \"\"\"Apply polyharmonic interpolation model to data. Notes: Given coefficients w and v for the interpolation model, we evaluate interpolated function values at query_points. Args: query_points: `[b, m, d]` x values to evaluate the interpolation at train_points: `[b, n, d]` x values that act as the interpolation centers ( the c variables in the wikipedia article) w: `[b, n, k]` weights on each interpolation center v: `[b, d, k]` weights on each input dimension order: order of the interpolation Returns: Polyharmonic interpolation evaluated at points defined in query_points. \"\"\" query_points = query_points . unsqueeze ( 0 ) # First, compute the contribution from the rbf term. pairwise_dists = cross_squared_distance_matrix ( query_points . float (), train_points . float ()) phi_pairwise_dists = phi ( pairwise_dists , order ) rbf_term = torch . matmul ( phi_pairwise_dists , w ) # Then, compute the contribution from the linear term. # Pad query_points with ones, for the bias term in the linear model. ones = torch . ones_like ( query_points [ ... , : 1 ]) query_points_pad = torch . cat (( query_points , ones ), 2 ) . float () linear_term = torch . matmul ( query_points_pad , v ) return rbf_term + linear_term","title":"apply_interpolation()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.create_dense_flows","text":"Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 180 181 182 def create_dense_flows ( flattened_flows , batch_size , image_height , image_width ): # possibly .view return torch . reshape ( flattened_flows , [ batch_size , image_height , image_width , 2 ])","title":"create_dense_flows()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.cross_squared_distance_matrix","text":"Pairwise squared distance between two (batch) matrices' rows (2nd dim). Computes the pairwise distances between rows of x and rows of y x: [batch_size, n, d] float Tensor y: [batch_size, m, d] float Tensor squared_dists: [batch_size, n, m] float Tensor , where squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2 Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def cross_squared_distance_matrix ( x , y ): \"\"\"Pairwise squared distance between two (batch) matrices' rows (2nd dim). Computes the pairwise distances between rows of x and rows of y Args: x: [batch_size, n, d] float `Tensor` y: [batch_size, m, d] float `Tensor` Returns: squared_dists: [batch_size, n, m] float `Tensor`, where squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2 \"\"\" x_norm_squared = torch . sum ( torch . mul ( x , x )) y_norm_squared = torch . sum ( torch . mul ( y , y )) x_y_transpose = torch . matmul ( x . squeeze ( 0 ), y . squeeze ( 0 ) . transpose ( 0 , 1 )) # squared_dists[b,i,j] = ||x_bi - y_bj||^2 = x_bi'x_bi- 2x_bi'x_bj + x_bj'x_bj squared_dists = x_norm_squared - 2 * x_y_transpose + y_norm_squared return squared_dists . float ()","title":"cross_squared_distance_matrix()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.dense_image_warp","text":"Image warping using per-pixel flow vectors. Apply a non-linear warp to the image, where the warp is specified by a dense flow field of offset vectors that define the correspondences of pixel values in the output image back to locations in the source image. Specifically, the pixel value at output[b, j, i, c] is images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c]. The locations specified by this formula do not necessarily map to an int index. Therefore, the pixel value is obtained by bilinear interpolation of the 4 nearest pixels around (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside of the image, we use the nearest pixel values at the image boundary. image: 4-D float Tensor with shape [batch, height, width, channels] . flow: A 4-D float Tensor with shape [batch, height, width, 2] . name: A name for the operation (optional). Note that image and flow can be of type tf.half, tf.float32, or tf.float64, and do not necessarily have to be the same type. A 4-D float Tensor with shape [batch, height, width, channels] and same type as input image. ValueError: if height < 2 or width < 2 or the inputs have the wrong number of dimensions. Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def dense_image_warp ( image , flow ): \"\"\"Image warping using per-pixel flow vectors. Apply a non-linear warp to the image, where the warp is specified by a dense flow field of offset vectors that define the correspondences of pixel values in the output image back to locations in the source image. Specifically, the pixel value at output[b, j, i, c] is images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c]. The locations specified by this formula do not necessarily map to an int index. Therefore, the pixel value is obtained by bilinear interpolation of the 4 nearest pixels around (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside of the image, we use the nearest pixel values at the image boundary. Args: image: 4-D float `Tensor` with shape `[batch, height, width, channels]`. flow: A 4-D float `Tensor` with shape `[batch, height, width, 2]`. name: A name for the operation (optional). Note that image and flow can be of type tf.half, tf.float32, or tf.float64, and do not necessarily have to be the same type. Returns: A 4-D float `Tensor` with shape`[batch, height, width, channels]` and same type as input image. Raises: ValueError: if height < 2 or width < 2 or the inputs have the wrong number of dimensions. \"\"\" image = image . unsqueeze ( 3 ) # add a single channel dimension to image tensor batch_size , height , width , channels = image . shape device = image . device # The flow is defined on the image grid. Turn the flow into a list of query # points in the grid space. grid_x , grid_y = torch . meshgrid ( torch . arange ( width , device = device ), torch . arange ( height , device = device )) stacked_grid = torch . stack (( grid_y , grid_x ), dim = 2 ) . float () batched_grid = stacked_grid . unsqueeze ( - 1 ) . permute ( 3 , 1 , 0 , 2 ) query_points_on_grid = batched_grid - flow query_points_flattened = torch . reshape ( query_points_on_grid , [ batch_size , height * width , 2 ]) # Compute values at the query points, then reshape the result back to the # image grid. interpolated = interpolate_bilinear ( image , query_points_flattened ) interpolated = torch . reshape ( interpolated , [ batch_size , height , width , channels ]) return interpolated","title":"dense_image_warp()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.flatten_grid_locations","text":"Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 169 170 def flatten_grid_locations ( grid_locations , image_height , image_width ): return torch . reshape ( grid_locations , [ image_height * image_width , 2 ])","title":"flatten_grid_locations()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.freq_mask","text":"Frequency masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int F: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def freq_mask ( spec , F = 30 , num_masks = 1 , replace_with_zero = False ): \"\"\"Frequency masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int F: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean \"\"\" cloned = spec . unsqueeze ( 0 ) . clone () num_mel_channels = cloned . shape [ 2 ] for i in range ( 0 , num_masks ): f = random . randrange ( 0 , F ) f_zero = random . randrange ( 0 , num_mel_channels - f ) # avoids randrange error if values are equal and range is empty if ( f_zero == f_zero + f ): return cloned . squeeze ( 0 ) mask_end = random . randrange ( f_zero , f_zero + f ) if ( replace_with_zero ): cloned [ 0 ][:, f_zero : mask_end ] = 0 else : cloned [ 0 ][:, f_zero : mask_end ] = cloned . mean () return cloned . squeeze ( 0 )","title":"freq_mask()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.get_flat_grid_locations","text":"Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 173 174 175 176 177 def get_flat_grid_locations ( image_height , image_width , device ): y_range = torch . linspace ( 0 , image_height - 1 , image_height , device = device ) x_range = torch . linspace ( 0 , image_width - 1 , image_width , device = device ) y_grid , x_grid = torch . meshgrid ( y_range , x_range ) return torch . stack (( y_grid , x_grid ), - 1 ) . reshape ([ image_height * image_width , 2 ])","title":"get_flat_grid_locations()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.get_grid_locations","text":"Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 162 163 164 165 166 def get_grid_locations ( image_height , image_width , device ): y_range = torch . linspace ( 0 , image_height - 1 , image_height , device = device ) x_range = torch . linspace ( 0 , image_width - 1 , image_width , device = device ) y_grid , x_grid = torch . meshgrid ( y_range , x_range ) return torch . stack (( y_grid , x_grid ), - 1 )","title":"get_grid_locations()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.interpolate_bilinear","text":"Similar to Matlab's interp2 function. Notes Finds values for query points on a grid using bilinear interpolation. Parameters: Name Type Description Default grid a 4-D float Tensor of shape [batch, height, width, channels] . required query_points a 3-D float Tensor of N points with shape [batch, N, 2] . required name a name for the operation (optional). 'interpolate_bilinear' indexing whether the query points are specified as row and column (ij), or Cartesian coordinates (xy). 'ij' Returns: Type Description values a 3-D Tensor with shape [batch, N, channels] Exceptions: Type Description ValueError if the indexing mode is invalid, or if the shape of the inputs Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def interpolate_bilinear ( grid , query_points , name = 'interpolate_bilinear' , indexing = 'ij' ): \"\"\"Similar to Matlab's interp2 function. Notes: Finds values for query points on a grid using bilinear interpolation. Args: grid: a 4-D float `Tensor` of shape `[batch, height, width, channels]`. query_points: a 3-D float `Tensor` of N points with shape `[batch, N, 2]`. name: a name for the operation (optional). indexing: whether the query points are specified as row and column (ij), or Cartesian coordinates (xy). Returns: values: a 3-D `Tensor` with shape `[batch, N, channels]` Raises: ValueError: if the indexing mode is invalid, or if the shape of the inputs invalid. \"\"\" if indexing != 'ij' and indexing != 'xy' : raise ValueError ( 'Indexing mode must be \\' ij \\' or \\' xy \\' ' ) shape = grid . shape if len ( shape ) != 4 : msg = 'Grid must be 4 dimensional. Received size: ' raise ValueError ( msg + str ( grid . shape )) batch_size , height , width , channels = grid . shape shape = [ batch_size , height , width , channels ] query_type = query_points . dtype grid_type = grid . dtype grid_device = grid . device num_queries = query_points . shape [ 1 ] alphas = [] floors = [] ceils = [] index_order = [ 0 , 1 ] if indexing == 'ij' else [ 1 , 0 ] unstacked_query_points = query_points . unbind ( 2 ) for dim in index_order : queries = unstacked_query_points [ dim ] size_in_indexing_dimension = shape [ dim + 1 ] # max_floor is size_in_indexing_dimension - 2 so that max_floor + 1 # is still a valid index into the grid. max_floor = torch . tensor ( size_in_indexing_dimension - 2 , dtype = query_type , device = grid_device ) min_floor = torch . tensor ( 0.0 , dtype = query_type , device = grid_device ) maxx = torch . max ( min_floor , torch . floor ( queries )) floor = torch . min ( maxx , max_floor ) int_floor = floor . long () floors . append ( int_floor ) ceil = int_floor + 1 ceils . append ( ceil ) # alpha has the same type as the grid, as we will directly use alpha # when taking linear combinations of pixel values from the image. alpha = torch . tensor (( queries - floor ), dtype = grid_type , device = grid_device ) min_alpha = torch . tensor ( 0.0 , dtype = grid_type , device = grid_device ) max_alpha = torch . tensor ( 1.0 , dtype = grid_type , device = grid_device ) alpha = torch . min ( torch . max ( min_alpha , alpha ), max_alpha ) # Expand alpha to [b, n, 1] so we can use broadcasting # (since the alpha values don't depend on the channel). alpha = torch . unsqueeze ( alpha , 2 ) alphas . append ( alpha ) flattened_grid = torch . reshape ( grid , [ batch_size * height * width , channels ]) batch_offsets = torch . reshape ( torch . arange ( batch_size , device = grid_device ) * height * width , [ batch_size , 1 ]) # This wraps array_ops.gather. We reshape the image data such that the # batch, y, and x coordinates are pulled into the first dimension. # Then we gather. Finally, we reshape the output back. It's possible this # code would be made simpler by using array_ops.gather_nd. def gather ( y_coords , x_coords , name ): linear_coordinates = batch_offsets + y_coords * width + x_coords gathered_values = torch . gather ( flattened_grid . t (), 1 , linear_coordinates ) return torch . reshape ( gathered_values , [ batch_size , num_queries , channels ]) # grab the pixel values in the 4 corners around each query point top_left = gather ( floors [ 0 ], floors [ 1 ], 'top_left' ) top_right = gather ( floors [ 0 ], ceils [ 1 ], 'top_right' ) bottom_left = gather ( ceils [ 0 ], floors [ 1 ], 'bottom_left' ) bottom_right = gather ( ceils [ 0 ], ceils [ 1 ], 'bottom_right' ) interp_top = alphas [ 1 ] * ( top_right - top_left ) + top_left interp_bottom = alphas [ 1 ] * ( bottom_right - bottom_left ) + bottom_left interp = alphas [ 0 ] * ( interp_bottom - interp_top ) + interp_top return interp","title":"interpolate_bilinear()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.interpolate_spline","text":"Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 185 186 187 188 189 190 191 def interpolate_spline ( train_points , train_values , query_points , order , regularization_weight = 0.0 , ): # First, fit the spline to the observed data. w , v = solve_interpolation ( train_points , train_values , order , regularization_weight ) # Then, evaluate the spline at the query locations. query_values = apply_interpolation ( query_points , train_points , w , v , order ) return query_values","title":"interpolate_spline()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.phi","text":"Coordinate-wise nonlinearity used to define the order of the interpolation. See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition. r: input op order: interpolation order phi_k evaluated coordinate-wise on r, for k = r Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def phi ( r , order ): \"\"\"Coordinate-wise nonlinearity used to define the order of the interpolation. See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition. Args: r: input op order: interpolation order Returns: phi_k evaluated coordinate-wise on r, for k = r \"\"\" EPSILON = torch . tensor ( 1e-10 , device = r . device ) # using EPSILON prevents log(0), sqrt0), etc. # sqrt(0) is well-defined, but its gradient is not if order == 1 : r = torch . max ( r , EPSILON ) r = torch . sqrt ( r ) return r elif order == 2 : return 0.5 * r * torch . log ( torch . max ( r , EPSILON )) elif order == 4 : return 0.5 * torch . square ( r ) * torch . log ( torch . max ( r , EPSILON )) elif order % 2 == 0 : r = torch . max ( r , EPSILON ) return 0.5 * torch . pow ( r , 0.5 * order ) * torch . log ( r ) else : r = torch . max ( r , EPSILON ) return torch . pow ( r , 0.5 * order )","title":"phi()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.solve_interpolation","text":"Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def solve_interpolation ( train_points , train_values , order , regularization_weight ): device = train_points . device b , n , d = train_points . shape k = train_values . shape [ - 1 ] c = train_points f = train_values . float () matrix_a = phi ( cross_squared_distance_matrix ( c , c ), order ) . unsqueeze ( 0 ) # [b, n, n] # Append ones to the feature values for the bias term in the linear model. ones = torch . ones ( 1 , dtype = train_points . dtype , device = device ) . view ([ - 1 , 1 , 1 ]) matrix_b = torch . cat (( c , ones ), 2 ) . float () # [b, n, d + 1] # [b, n + d + 1, n] left_block = torch . cat (( matrix_a , torch . transpose ( matrix_b , 2 , 1 )), 1 ) num_b_cols = matrix_b . shape [ 2 ] # d + 1 # In Tensorflow, zeros are used here. Pytorch solve fails with zeros for some reason we don't understand. # So instead we use very tiny randn values (variance of one, zero mean) on one side of our multiplication. lhs_zeros = torch . randn (( b , num_b_cols , num_b_cols ), device = device ) / 1e10 right_block = torch . cat (( matrix_b , lhs_zeros ), 1 ) # [b, n + d + 1, d + 1] lhs = torch . cat (( left_block , right_block ), 2 ) # [b, n + d + 1, n + d + 1] rhs_zeros = torch . zeros (( b , d + 1 , k ), dtype = train_points . dtype , device = device ) . float () rhs = torch . cat (( f , rhs_zeros ), 1 ) # [b, n + d + 1, k] # Then, solve the linear system and unpack the results. X , LU = torch . gesv ( rhs , lhs ) w = X [:, : n , :] v = X [:, n :, :] return w , v","title":"solve_interpolation()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.sparse_image_warp","text":"Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def sparse_image_warp ( img_tensor , source_control_point_locations , dest_control_point_locations , interpolation_order = 2 , regularization_weight = 0.0 , num_boundaries_points = 0 ): device = img_tensor . device control_point_flows = dest_control_point_locations - source_control_point_locations batch_size , image_height , image_width = img_tensor . shape flattened_grid_locations = get_flat_grid_locations ( image_height , image_width , device ) flattened_flows = interpolate_spline ( dest_control_point_locations , control_point_flows , flattened_grid_locations , interpolation_order , regularization_weight ) dense_flows = create_dense_flows ( flattened_flows , batch_size , image_height , image_width ) warped_image = dense_image_warp ( img_tensor , dense_flows ) return warped_image , dense_flows","title":"sparse_image_warp()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.specaug","text":"SpecAugment SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition (https://arxiv.org/pdf/1904.08779.pdf) This implementation modified from https://github.com/zcaceres/spec_augment :param torch.Tensor spec: input tensor with the shape (T, dim) :param int W: time warp parameter :param int F: maximum width of each freq mask :param int T: maximum width of each time mask :param int num_freq_masks: number of frequency masks :param int num_time_masks: number of time masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def specaug ( spec , W = 5 , F = 30 , T = 40 , num_freq_masks = 2 , num_time_masks = 2 , replace_with_zero = False ): \"\"\"SpecAugment Reference: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition (https://arxiv.org/pdf/1904.08779.pdf) This implementation modified from https://github.com/zcaceres/spec_augment :param torch.Tensor spec: input tensor with the shape (T, dim) :param int W: time warp parameter :param int F: maximum width of each freq mask :param int T: maximum width of each time mask :param int num_freq_masks: number of frequency masks :param int num_time_masks: number of time masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean \"\"\" return time_mask ( freq_mask ( time_warp ( spec , W = W ), F = F , num_masks = num_freq_masks , replace_with_zero = replace_with_zero ), T = T , num_masks = num_time_masks , replace_with_zero = replace_with_zero )","title":"specaug()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.time_mask","text":"Time masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int T: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def time_mask ( spec , T = 40 , num_masks = 1 , replace_with_zero = False ): \"\"\"Time masking :param torch.Tensor spec: input tensor with shape (T, dim) :param int T: maximum width of each mask :param int num_masks: number of masks :param bool replace_with_zero: if True, masked parts will be filled with 0, if False, filled with mean \"\"\" cloned = spec . unsqueeze ( 0 ) . clone () len_spectro = cloned . shape [ 1 ] for i in range ( 0 , num_masks ): t = random . randrange ( 0 , T ) t_zero = random . randrange ( 0 , len_spectro - t ) # avoids randrange error if values are equal and range is empty if ( t_zero == t_zero + t ): return cloned . squeeze ( 0 ) mask_end = random . randrange ( t_zero , t_zero + t ) if ( replace_with_zero ): cloned [ 0 ][ t_zero : mask_end , :] = 0 else : cloned [ 0 ][ t_zero : mask_end , :] = cloned . mean () return cloned . squeeze ( 0 )","title":"time_mask()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.spec_augment.time_warp","text":"Time warping :param torch.Tensor spec: input tensor with shape (T, dim) :param int W: time warp parameter Source code in adviser/tools/espnet_minimal/utils/spec_augment.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def time_warp ( spec , W = 5 ): \"\"\"Time warping :param torch.Tensor spec: input tensor with shape (T, dim) :param int W: time warp parameter \"\"\" spec = spec . unsqueeze ( 0 ) spec_len = spec . shape [ 1 ] num_rows = spec . shape [ 2 ] device = spec . device y = num_rows // 2 horizontal_line_at_ctr = spec [ 0 , :, y ] assert len ( horizontal_line_at_ctr ) == spec_len point_to_warp = horizontal_line_at_ctr [ random . randrange ( W , spec_len - W )] assert isinstance ( point_to_warp , torch . Tensor ) # Uniform distribution from (0,W) with chance to be up to W negative dist_to_warp = random . randrange ( - W , W ) src_pts , dest_pts = ( torch . tensor ([[[ point_to_warp , y ]]], device = device ), torch . tensor ([[[ point_to_warp + dist_to_warp , y ]]], device = device )) warped_spectro , dense_flows = sparse_image_warp ( spec , src_pts , dest_pts ) return warped_spectro . squeeze ( 3 ) . squeeze ( 0 )","title":"time_warp()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training","text":"","title":"training"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.batchfy","text":"","title":"batchfy"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.batchfy.batchfy_by_bin","text":"Make variably sized batch set, which maximizes the number of bins up to batch_bins . :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_bins: Maximum frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every test batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def batchfy_by_bin ( sorted_data , batch_bins , num_batches = 0 , min_batch_size = 1 , shortest_first = False , ikey = \"input\" , okey = \"output\" ): \"\"\"Make variably sized batch set, which maximizes the number of bins up to `batch_bins`. :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_bins: Maximum frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every `test` batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches \"\"\" if batch_bins <= 0 : raise ValueError ( f \"invalid batch_bins= { batch_bins } \" ) length = len ( sorted_data ) idim = int ( sorted_data [ 0 ][ 1 ][ ikey ][ 0 ][ 'shape' ][ 1 ]) odim = int ( sorted_data [ 0 ][ 1 ][ okey ][ 0 ][ 'shape' ][ 1 ]) logging . info ( '# utts: ' + str ( len ( sorted_data ))) minibatches = [] start = 0 n = 0 while True : # Dynamic batch size depending on size of samples b = 0 next_size = 0 max_olen = 0 while next_size < batch_bins and ( start + b ) < length : ilen = int ( sorted_data [ start + b ][ 1 ][ ikey ][ 0 ][ 'shape' ][ 0 ]) * idim olen = int ( sorted_data [ start + b ][ 1 ][ okey ][ 0 ][ 'shape' ][ 0 ]) * odim if olen > max_olen : max_olen = olen next_size = ( max_olen + ilen ) * ( b + 1 ) if next_size <= batch_bins : b += 1 elif next_size == 0 : raise ValueError ( f \"Can't fit one sample in batch_bins ( { batch_bins } ): Please increase the value\" ) end = min ( length , start + max ( min_batch_size , b )) batch = sorted_data [ start : end ] if shortest_first : batch . reverse () minibatches . append ( batch ) # Check for min_batch_size and fixes the batches if needed i = - 1 while len ( minibatches [ i ]) < min_batch_size : missing = min_batch_size - len ( minibatches [ i ]) if - i == len ( minibatches ): minibatches [ i + 1 ] . extend ( minibatches [ i ]) minibatches = minibatches [ 1 :] break else : minibatches [ i ] . extend ( minibatches [ i - 1 ][: missing ]) minibatches [ i - 1 ] = minibatches [ i - 1 ][ missing :] i -= 1 if end == length : break start = end n += 1 if num_batches > 0 : minibatches = minibatches [: num_batches ] lengths = [ len ( x ) for x in minibatches ] logging . info ( str ( len ( minibatches )) + \" batches containing from \" + str ( min ( lengths )) + \" to \" + str ( max ( lengths )) + \" samples \" + \"(avg \" + str ( int ( np . mean ( lengths ))) + \" samples).\" ) return minibatches","title":"batchfy_by_bin()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.batchfy.batchfy_by_frame","text":"Make variably sized batch set, which maximizes the number of frames to max_batch_frame. :param Dict[str, Dict[str, Any]] sorteddata: dictionary loaded from data.json :param int max_frames_in: Maximum input frames of a batch :param int max_frames_out: Maximum output frames of a batch :param int max_frames_inout: Maximum input+output frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every test batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def batchfy_by_frame ( sorted_data , max_frames_in , max_frames_out , max_frames_inout , num_batches = 0 , min_batch_size = 1 , shortest_first = False , ikey = \"input\" , okey = \"output\" ): \"\"\"Make variably sized batch set, which maximizes the number of frames to max_batch_frame. :param Dict[str, Dict[str, Any]] sorteddata: dictionary loaded from data.json :param int max_frames_in: Maximum input frames of a batch :param int max_frames_out: Maximum output frames of a batch :param int max_frames_inout: Maximum input+output frames of a batch :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param int test: Return only every `test` batches :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS ikey=\"output\".) :param str okey: key to access output (for ASR okey=\"output\". for TTS okey=\"input\".) :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches \"\"\" if max_frames_in <= 0 and max_frames_out <= 0 and max_frames_inout <= 0 : raise ValueError ( f \"At least, one of `--batch-frames-in`, `--batch-frames-out` or `--batch-frames-inout` should be > 0\" ) length = len ( sorted_data ) minibatches = [] start = 0 end = 0 while end != length : # Dynamic batch size depending on size of samples b = 0 max_olen = 0 max_ilen = 0 while ( start + b ) < length : ilen = int ( sorted_data [ start + b ][ 1 ][ ikey ][ 0 ][ 'shape' ][ 0 ]) if ilen > max_frames_in and max_frames_in != 0 : raise ValueError ( f \"Can't fit one sample in --batch-frames-in ( { max_frames_in } ): Please increase the value\" ) olen = int ( sorted_data [ start + b ][ 1 ][ okey ][ 0 ][ 'shape' ][ 0 ]) if olen > max_frames_out and max_frames_out != 0 : raise ValueError ( f \"Can't fit one sample in --batch-frames-out ( { max_frames_out } ): Please increase the value\" ) if ilen + olen > max_frames_inout and max_frames_inout != 0 : raise ValueError ( f \"Can't fit one sample in --batch-frames-out ( { max_frames_inout } ): Please increase the value\" ) max_olen = max ( max_olen , olen ) max_ilen = max ( max_ilen , ilen ) in_ok = max_ilen * ( b + 1 ) <= max_frames_in or max_frames_in == 0 out_ok = max_olen * ( b + 1 ) <= max_frames_out or max_frames_out == 0 inout_ok = ( max_ilen + max_olen ) * ( b + 1 ) <= max_frames_inout or max_frames_inout == 0 if in_ok and out_ok and inout_ok : # add more seq in the minibatch b += 1 else : # no more seq in the minibatch break end = min ( length , start + b ) batch = sorted_data [ start : end ] if shortest_first : batch . reverse () minibatches . append ( batch ) # Check for min_batch_size and fixes the batches if needed i = - 1 while len ( minibatches [ i ]) < min_batch_size : missing = min_batch_size - len ( minibatches [ i ]) if - i == len ( minibatches ): minibatches [ i + 1 ] . extend ( minibatches [ i ]) minibatches = minibatches [ 1 :] break else : minibatches [ i ] . extend ( minibatches [ i - 1 ][: missing ]) minibatches [ i - 1 ] = minibatches [ i - 1 ][ missing :] i -= 1 start = end if num_batches > 0 : minibatches = minibatches [: num_batches ] lengths = [ len ( x ) for x in minibatches ] logging . info ( str ( len ( minibatches )) + \" batches containing from \" + str ( min ( lengths )) + \" to \" + str ( max ( lengths )) + \" samples\" + \"(avg \" + str ( int ( np . mean ( lengths ))) + \" samples).\" ) return minibatches","title":"batchfy_by_frame()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.batchfy.batchfy_by_seq","text":"Make batch set from json dictionary :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_size: batch size :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int min_batch_size: mininum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS, MT ikey=\"output\".) :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param str okey: key to access output (for ASR, MT okey=\"output\". for TTS okey=\"input\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) :return: List[List[Tuple[str, dict]]] list of batches Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def batchfy_by_seq ( sorted_data , batch_size , max_length_in , max_length_out , min_batch_size = 1 , shortest_first = False , ikey = \"input\" , iaxis = 0 , okey = \"output\" , oaxis = 0 ): \"\"\"Make batch set from json dictionary :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json :param int batch_size: batch size :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int min_batch_size: mininum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :param str ikey: key to access input (for ASR ikey=\"input\", for TTS, MT ikey=\"output\".) :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param str okey: key to access output (for ASR, MT okey=\"output\". for TTS okey=\"input\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) :return: List[List[Tuple[str, dict]]] list of batches \"\"\" if batch_size <= 0 : raise ValueError ( f \"Invalid batch_size= { batch_size } \" ) # check #utts is more than min_batch_size if len ( sorted_data ) < min_batch_size : raise ValueError ( f \"#utts( { len ( sorted_data ) } ) is less than min_batch_size( { min_batch_size } ).\" ) # make list of minibatches minibatches = [] start = 0 while True : _ , info = sorted_data [ start ] ilen = int ( info [ ikey ][ iaxis ][ 'shape' ][ 0 ]) olen = int ( info [ okey ][ oaxis ][ 'shape' ][ 0 ]) if oaxis >= 0 else max ( map ( lambda x : int ( x [ 'shape' ][ 0 ]), info [ okey ])) factor = max ( int ( ilen / max_length_in ), int ( olen / max_length_out )) # change batchsize depending on the input and output length # if ilen = 1000 and max_length_in = 800 # then b = batchsize / 2 # and max(min_batches, .) avoids batchsize = 0 bs = max ( min_batch_size , int ( batch_size / ( 1 + factor ))) end = min ( len ( sorted_data ), start + bs ) minibatch = sorted_data [ start : end ] if shortest_first : minibatch . reverse () # check each batch is more than minimum batchsize if len ( minibatch ) < min_batch_size : mod = min_batch_size - len ( minibatch ) % min_batch_size additional_minibatch = [ sorted_data [ i ] for i in np . random . randint ( 0 , start , mod )] if shortest_first : additional_minibatch . reverse () minibatch . extend ( additional_minibatch ) minibatches . append ( minibatch ) if end == len ( sorted_data ): break start = end # batch: List[List[Tuple[str, dict]]] return minibatches","title":"batchfy_by_seq()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.batchfy.batchfy_shuffle","text":"Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def batchfy_shuffle ( data , batch_size , min_batch_size , num_batches , shortest_first ): import random logging . info ( 'use shuffled batch.' ) sorted_data = random . sample ( data . items (), len ( data . items ())) logging . info ( '# utts: ' + str ( len ( sorted_data ))) # make list of minibatches minibatches = [] start = 0 while True : end = min ( len ( sorted_data ), start + batch_size ) # check each batch is more than minimum batchsize minibatch = sorted_data [ start : end ] if shortest_first : minibatch . reverse () if len ( minibatch ) < min_batch_size : mod = min_batch_size - len ( minibatch ) % min_batch_size additional_minibatch = [ sorted_data [ i ] for i in np . random . randint ( 0 , start , mod )] if shortest_first : additional_minibatch . reverse () minibatch . extend ( additional_minibatch ) minibatches . append ( minibatch ) if end == len ( sorted_data ): break start = end # for debugging if num_batches > 0 : minibatches = minibatches [: num_batches ] logging . info ( '# minibatches: ' + str ( len ( minibatches ))) return minibatches","title":"batchfy_shuffle()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.batchfy.make_batchset","text":"Make batch set from json dictionary if utts have \"category\" value, >>> data = {'utt1': {'category': 'A', 'input': ...}, ... 'utt2': {'category': 'B', 'input': ...}, ... 'utt3': {'category': 'B', 'input': ...}, ... 'utt4': {'category': 'A', 'input': ...}} >>> make_batchset(data, batchsize=2, ...) [[('utt1', ...), ('utt4', ...)], [('utt2', ...), ('utt3': ...)]] Note that if any utts doesn't have \"category\", perform as same as batchfy_by_{count} :param Dict[str, Dict[str, Any]] data: dictionary loaded from data.json :param int batch_size: maximum number of sequences in a minibatch. :param int batch_bins: maximum number of bins (frames x dim) in a minibatch. :param int batch_frames_in: maximum number of input frames in a minibatch. :param int batch_frames_out: maximum number of output frames in a minibatch. :param int batch_frames_out: maximum number of input+output frames in a minibatch. :param str count: strategy to count maximum size of batch. For choices, see services.hci.speech.espnet_minimal.asr.batchfy.BATCH_COUNT_CHOICES :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :return: List[List[Tuple[str, dict]]] list of batches :param str batch_sort_key: how to sort data before creating minibatches [\"input\", \"output\", \"shuffle\"] :param bool swap_io: if True, use \"input\" as output and \"output\" as input in data dict :param bool mt: if True, use 0-axis of \"output\" as output and 1-axis of \"output\" as input in data dict :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) Source code in adviser/tools/espnet_minimal/utils/training/batchfy.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def make_batchset ( data , batch_size = 0 , max_length_in = float ( \"inf\" ), max_length_out = float ( \"inf\" ), num_batches = 0 , min_batch_size = 1 , shortest_first = False , batch_sort_key = \"input\" , swap_io = False , mt = False , count = \"auto\" , batch_bins = 0 , batch_frames_in = 0 , batch_frames_out = 0 , batch_frames_inout = 0 , iaxis = 0 , oaxis = 0 ): \"\"\"Make batch set from json dictionary if utts have \"category\" value, >>> data = {'utt1': {'category': 'A', 'input': ...}, ... 'utt2': {'category': 'B', 'input': ...}, ... 'utt3': {'category': 'B', 'input': ...}, ... 'utt4': {'category': 'A', 'input': ...}} >>> make_batchset(data, batchsize=2, ...) [[('utt1', ...), ('utt4', ...)], [('utt2', ...), ('utt3': ...)]] Note that if any utts doesn't have \"category\", perform as same as batchfy_by_{count} :param Dict[str, Dict[str, Any]] data: dictionary loaded from data.json :param int batch_size: maximum number of sequences in a minibatch. :param int batch_bins: maximum number of bins (frames x dim) in a minibatch. :param int batch_frames_in: maximum number of input frames in a minibatch. :param int batch_frames_out: maximum number of output frames in a minibatch. :param int batch_frames_out: maximum number of input+output frames in a minibatch. :param str count: strategy to count maximum size of batch. For choices, see services.hci.speech.espnet_minimal.asr.batchfy.BATCH_COUNT_CHOICES :param int max_length_in: maximum length of input to decide adaptive batch size :param int max_length_out: maximum length of output to decide adaptive batch size :param int num_batches: # number of batches to use (for debug) :param int min_batch_size: minimum batch size (for multi-gpu) :param bool shortest_first: Sort from batch with shortest samples to longest if true, otherwise reverse :return: List[List[Tuple[str, dict]]] list of batches :param str batch_sort_key: how to sort data before creating minibatches [\"input\", \"output\", \"shuffle\"] :param bool swap_io: if True, use \"input\" as output and \"output\" as input in `data` dict :param bool mt: if True, use 0-axis of \"output\" as output and 1-axis of \"output\" as input in `data` dict :param int iaxis: dimension to access input (for ASR, TTS iaxis=0, for MT iaxis=\"1\".) :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.) \"\"\" # check args if count not in BATCH_COUNT_CHOICES : raise ValueError ( f \"arg 'count' ( { count } ) should be one of { BATCH_COUNT_CHOICES } \" ) if batch_sort_key not in BATCH_SORT_KEY_CHOICES : raise ValueError ( f \"arg 'batch_sort_key' ( { batch_sort_key } ) should be one of { BATCH_SORT_KEY_CHOICES } \" ) # TODO(karita): remove this by creating converter from ASR to TTS json format batch_sort_axis = 0 if swap_io : # for TTS ikey = \"output\" okey = \"input\" if batch_sort_key == \"input\" : batch_sort_key = \"output\" elif batch_sort_key == \"output\" : batch_sort_key = \"input\" elif mt : # for MT ikey = \"output\" okey = \"output\" batch_sort_key = \"output\" batch_sort_axis = 1 assert iaxis == 1 assert oaxis == 0 # NOTE: input is json['output'][1] and output is json['output'][0] else : ikey = \"input\" okey = \"output\" if count == \"auto\" : if batch_size != 0 : count = \"seq\" elif batch_bins != 0 : count = \"bin\" elif batch_frames_in != 0 or batch_frames_out != 0 or batch_frames_inout != 0 : count = \"frame\" else : raise ValueError ( f \"cannot detect `count` manually set one of { BATCH_COUNT_CHOICES } \" ) logging . info ( f \"count is auto detected as { count } \" ) if count != \"seq\" and batch_sort_key == \"shuffle\" : raise ValueError ( f \"batch_sort_key=shuffle is only available if batch_count=seq\" ) category2data = {} # Dict[str, dict] for k , v in data . items (): category2data . setdefault ( v . get ( 'category' ), {})[ k ] = v batches_list = [] # List[List[List[Tuple[str, dict]]]] for d in category2data . values (): if batch_sort_key == 'shuffle' : batches = batchfy_shuffle ( d , batch_size , min_batch_size , num_batches , shortest_first ) batches_list . append ( batches ) continue # sort it by input lengths (long to short) sorted_data = sorted ( d . items (), key = lambda data : int ( data [ 1 ][ batch_sort_key ][ batch_sort_axis ][ 'shape' ][ 0 ]), reverse = not shortest_first ) logging . info ( '# utts: ' + str ( len ( sorted_data ))) if count == \"seq\" : batches = batchfy_by_seq ( sorted_data , batch_size = batch_size , max_length_in = max_length_in , max_length_out = max_length_out , min_batch_size = min_batch_size , shortest_first = shortest_first , ikey = ikey , iaxis = iaxis , okey = okey , oaxis = oaxis ) if count == \"bin\" : batches = batchfy_by_bin ( sorted_data , batch_bins = batch_bins , min_batch_size = min_batch_size , shortest_first = shortest_first , ikey = ikey , okey = okey ) if count == \"frame\" : batches = batchfy_by_frame ( sorted_data , max_frames_in = batch_frames_in , max_frames_out = batch_frames_out , max_frames_inout = batch_frames_inout , min_batch_size = min_batch_size , shortest_first = shortest_first , ikey = ikey , okey = okey ) batches_list . append ( batches ) if len ( batches_list ) == 1 : batches = batches_list [ 0 ] else : # Concat list. This way is faster than \"sum(batch_list, [])\" batches = list ( itertools . chain ( * batches_list )) # for debugging if num_batches > 0 : batches = batches [: num_batches ] logging . info ( '# minibatches: ' + str ( len ( batches ))) # batch: List[List[Tuple[str, dict]]] return batches","title":"make_batchset()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.evaluator","text":"","title":"evaluator"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.evaluator.BaseEvaluator","text":"Base Evaluator in ESPnet __call__ ( self , trainer = None ) special Source code in adviser/tools/espnet_minimal/utils/training/evaluator.py 9 10 11 12 13 14 15 16 17 18 def __call__ ( self , trainer = None ): ret = super () . __call__ ( trainer ) try : if trainer is not None : # force tensorboard to report evaluation log tb_logger = trainer . get_extension ( TensorboardLogger . default_name ) tb_logger ( trainer ) except ValueError : pass return ret","title":"BaseEvaluator"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.iterators","text":"","title":"iterators"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.iterators.ShufflingEnabler","text":"An extension enabling shuffling on an Iterator __call__ ( self , trainer ) special Calls the enabler on the given iterator :param trainer: The iterator Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 20 21 22 23 24 25 26 27 28 def __call__ ( self , trainer ): \"\"\"Calls the enabler on the given iterator :param trainer: The iterator \"\"\" if not self . set : for iterator in self . iterators : iterator . start_shuffle () self . set = True __init__ ( self , iterators ) special Inits the ShufflingEnabler :param list[Iterator] iterators: The iterators to enable shuffling on Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 12 13 14 15 16 17 18 def __init__ ( self , iterators ): \"\"\"Inits the ShufflingEnabler :param list[Iterator] iterators: The iterators to enable shuffling on \"\"\" self . set = False self . iterators = iterators","title":"ShufflingEnabler"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.iterators.ToggleableShufflingMultiprocessIterator","text":"A MultiprocessIterator that can have its shuffling property activated during training __init__ ( self , dataset , batch_size , repeat = True , shuffle = True , n_processes = None , n_prefetch = 1 , shared_mem = None , maxtasksperchild = 20 ) special Init the iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat batches or not (enables multiple epochs) :param bool shuffle: Whether to shuffle the order of the batches :param int n_processes: How many processes to use :param int n_prefetch: The number of prefetch to use :param int shared_mem: How many memory to share between processes :param int maxtasksperchild: Maximum number of tasks per child Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , dataset , batch_size , repeat = True , shuffle = True , n_processes = None , n_prefetch = 1 , shared_mem = None , maxtasksperchild = 20 ): \"\"\"Init the iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat batches or not (enables multiple epochs) :param bool shuffle: Whether to shuffle the order of the batches :param int n_processes: How many processes to use :param int n_prefetch: The number of prefetch to use :param int shared_mem: How many memory to share between processes :param int maxtasksperchild: Maximum number of tasks per child \"\"\" super ( ToggleableShufflingMultiprocessIterator , self ) . __init__ ( dataset = dataset , batch_size = batch_size , repeat = repeat , shuffle = shuffle , n_processes = n_processes , n_prefetch = n_prefetch , shared_mem = shared_mem , maxtasksperchild = maxtasksperchild ) start_shuffle ( self ) Starts shuffling (or reshuffles) the batches Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 76 77 78 79 80 81 82 83 84 def start_shuffle ( self ): \"\"\"Starts shuffling (or reshuffles) the batches\"\"\" self . shuffle = True if int ( chainer . _version . __version__ [ 0 ]) <= 4 : self . _order = np . random . permutation ( len ( self . dataset )) else : self . order_sampler = ShuffleOrderSampler () self . _order = self . order_sampler ( np . arange ( len ( self . dataset )), 0 ) self . _set_prefetch_state ()","title":"ToggleableShufflingMultiprocessIterator"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.iterators.ToggleableShufflingSerialIterator","text":"A SerialIterator that can have its shuffling property activated during training __init__ ( self , dataset , batch_size , repeat = True , shuffle = True ) special Init the Iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat data (allow multiple epochs) :param bool shuffle: Whether to shuffle the batches Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 34 35 36 37 38 39 40 41 42 def __init__ ( self , dataset , batch_size , repeat = True , shuffle = True ): \"\"\"Init the Iterator :param torch.nn.Tensor dataset: The dataset to take batches from :param int batch_size: The batch size :param bool repeat: Whether to repeat data (allow multiple epochs) :param bool shuffle: Whether to shuffle the batches \"\"\" super ( ToggleableShufflingSerialIterator , self ) . __init__ ( dataset , batch_size , repeat , shuffle ) start_shuffle ( self ) Starts shuffling (or reshuffles) the batches Source code in adviser/tools/espnet_minimal/utils/training/iterators.py 44 45 46 47 48 49 50 51 def start_shuffle ( self ): \"\"\"Starts shuffling (or reshuffles) the batches\"\"\" self . _shuffle = True if int ( chainer . _version . __version__ [ 0 ]) <= 4 : self . _order = np . random . permutation ( len ( self . dataset )) else : self . order_sampler = ShuffleOrderSampler () self . _order = self . order_sampler ( np . arange ( len ( self . dataset )), 0 )","title":"ToggleableShufflingSerialIterator"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.tensorboard_logger","text":"","title":"tensorboard_logger"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.tensorboard_logger.TensorboardLogger","text":"A tensorboard logger extension __call__ ( self , trainer ) special Updates the events file with the new values :param trainer: The trainer Source code in adviser/tools/espnet_minimal/utils/training/tensorboard_logger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __call__ ( self , trainer ): \"\"\"Updates the events file with the new values :param trainer: The trainer \"\"\" observation = trainer . observation for k , v in observation . items (): if ( self . _entries is not None ) and ( k not in self . _entries ): continue if k is not None and v is not None : if 'cupy' in str ( type ( v )): v = v . get () if 'cupy' in str ( type ( k )): k = k . get () self . _logger . add_scalar ( k , v , trainer . updater . iteration ) if self . _att_reporter is not None and trainer . updater . get_iterator ( 'main' ) . epoch > self . _epoch : self . _epoch = trainer . updater . get_iterator ( 'main' ) . epoch self . _att_reporter . log_attentions ( self . _logger , trainer . updater . iteration ) __init__ ( self , logger , att_reporter = None , entries = None , epoch = 0 ) special Init the extension :param SummaryWriter logger: The logger to use :param PlotAttentionReporter att_reporter: The (optional) PlotAttentionReporter :param entries: The entries to watch :param int epoch: The starting epoch Source code in adviser/tools/espnet_minimal/utils/training/tensorboard_logger.py 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , logger , att_reporter = None , entries = None , epoch = 0 ): \"\"\"Init the extension :param SummaryWriter logger: The logger to use :param PlotAttentionReporter att_reporter: The (optional) PlotAttentionReporter :param entries: The entries to watch :param int epoch: The starting epoch \"\"\" self . _entries = entries self . _att_reporter = att_reporter self . _logger = logger self . _epoch = epoch","title":"TensorboardLogger"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.train_utils","text":"","title":"train_utils"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.train_utils.check_early_stop","text":"Checks if the training was stopped by an early stopping trigger and warns the user if it's the case :param trainer: The trainer used for training :param epochs: The maximum number of epochs Source code in adviser/tools/espnet_minimal/utils/training/train_utils.py 6 7 8 9 10 11 12 13 14 15 def check_early_stop ( trainer , epochs ): \"\"\"Checks if the training was stopped by an early stopping trigger and warns the user if it's the case :param trainer: The trainer used for training :param epochs: The maximum number of epochs \"\"\" end_epoch = trainer . updater . get_iterator ( 'main' ) . epoch if end_epoch < ( epochs - 1 ): logging . warning ( \"Hit early stop at epoch \" + str ( end_epoch ) + \" \\n You can change the patience or set it to 0 to run all epochs\" )","title":"check_early_stop()"},{"location":"api/tools/#adviser.tools.espnet_minimal.utils.training.train_utils.set_early_stop","text":"Sets the early stop trigger given the program arguments :param trainer: The trainer used for training :param args: The program arguments :param is_lm: If the trainer is for a LM (epoch instead of epochs) Source code in adviser/tools/espnet_minimal/utils/training/train_utils.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def set_early_stop ( trainer , args , is_lm = False ): \"\"\"Sets the early stop trigger given the program arguments :param trainer: The trainer used for training :param args: The program arguments :param is_lm: If the trainer is for a LM (epoch instead of epochs) \"\"\" patience = args . patience criterion = args . early_stop_criterion epochs = args . epoch if is_lm else args . epochs mode = 'max' if 'acc' in criterion else 'min' if patience > 0 : trainer . stop_trigger = chainer . training . triggers . EarlyStoppingTrigger ( monitor = criterion , mode = mode , patients = patience , max_trigger = ( epochs , 'epoch' ))","title":"set_early_stop()"},{"location":"api/tools/#adviser.tools.getopensmile","text":"","title":"getopensmile"},{"location":"api/tools/#adviser.tools.getopensmile.get_opensmile_executable_path","text":"Returns the path to the platform-specific openSMILE executable. If it can't be found, will download (and, depending on platform, try to compile) and then return the path. Source code in adviser/tools/getopensmile.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def get_opensmile_executable_path (): \"\"\" Returns the path to the platform-specific openSMILE executable. If it can't be found, will download (and, depending on platform, try to compile) and then return the path. \"\"\" openSmile_path = _get_opensmile_executable () if openSmile_path is None : print ( \"no openSMILE binary found\" ) _download_compile_opensmile () openSmile_path = _get_opensmile_executable () if openSmile_path is None : print ( \"failed to obtain and setup openSMILE. Exiting.\" ) exit () return openSmile_path","title":"get_opensmile_executable_path()"},{"location":"api/tools/#adviser.tools.getopensmile.get_root_dir","text":"Source code in adviser/tools/getopensmile.py 25 26 def get_root_dir (): return os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ )))","title":"get_root_dir()"},{"location":"api/utils/","text":"Utils \u00b6 \u00b6 beliefstate \u00b6 This module provides the BeliefState class. BeliefState \u00b6 A representation of the belief state, can be accessed like a dictionary. Includes information on: * current set of UserActTypes * informs to date (dictionary where key is slot and value is {value: probaility}) * requests for the current turn * number of db matches for given constraints * if the db matches can further be split __contains__ ( self , val ) special \u00b6 Source code in adviser/utils/beliefstate.py 62 63 def __contains__ ( self , val ): # assume return val in self . _history [ - 1 ] __getitem__ ( self , val ) special \u00b6 Source code in adviser/utils/beliefstate.py 43 44 45 46 47 48 49 50 def __getitem__ ( self , val ): # for indexing # if used with numbers: int (e.g. state[-2]) or slice (e.g. state[3:6]) if isinstance ( val , int ) or isinstance ( val , slice ): return self . _history [ val ] # interpret the number as turn # if used with strings (e.g. state['beliefs']) elif isinstance ( val , str ): # take the current turn's belief state return self . _history [ - 1 ][ val ] __init__ ( self , domain ) special \u00b6 Source code in adviser/utils/beliefstate.py 39 40 41 def __init__ ( self , domain : JSONLookupDomain ): self . domain = domain self . _history = [ self . _init_beliefstate ()] __iter__ ( self ) special \u00b6 Source code in adviser/utils/beliefstate.py 52 53 def __iter__ ( self ): return iter ( self . _history [ - 1 ]) __len__ ( self ) special \u00b6 Source code in adviser/utils/beliefstate.py 59 60 def __len__ ( self ): return len ( self . _history ) __repr__ ( self ) special \u00b6 Source code in adviser/utils/beliefstate.py 79 80 def __repr__ ( self ): return str ( self . _history [ - 1 ]) __setitem__ ( self , key , val ) special \u00b6 Source code in adviser/utils/beliefstate.py 55 56 57 def __setitem__ ( self , key , val ): # e.g. state['beliefs']['area']['west'] = 1.0 self . _history [ - 1 ][ key ] = val __str__ ( self ) special \u00b6 Source code in adviser/utils/beliefstate.py 82 83 def __str__ ( self ): return self . _recursive_repr ( self . _history [ - 1 ]) get_most_probable_inf_beliefs ( self , consider_NONE = True , threshold = 0.7 , max_results = 1 , turn_idx =- 1 ) \u00b6 Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Parameters: Name Type Description Default consider_NONE bool If True, slots where NONE values have the highest probability will not be added to the result. If False, slots where NONE values have the highest probability will look for the best value != NONE . True threshold float minimum probability to be accepted 0.7 max_results int return at most max_results best values per slot 1 turn_idx int index for accessing the belief state history (default = -1: use last turn) -1 Returns: Type Description Union(Dict[str, List[str]], Dict[str, str]) A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. Source code in adviser/utils/beliefstate.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def get_most_probable_inf_beliefs ( self , consider_NONE : bool = True , threshold : float = 0.7 , max_results : int = 1 , turn_idx : int = - 1 ): \"\"\" Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Args: consider_NONE: If True, slots where **NONE** values have the highest probability will not be added to the result. If False, slots where **NONE** values have the highest probability will look for the best value != **NONE**. threshold: minimum probability to be accepted max_results: return at most `max_results` best values per slot turn_idx: index for accessing the belief state history (default = -1: use last turn) Returns: Union(Dict[str, List[str]], Dict[str, str]): A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. \"\"\" candidates = {} informs = self . _history [ turn_idx ][ \"informs\" ] for slot in informs : # sort by belief sorted_slot_cands = sorted ( informs [ slot ] . items (), key = lambda kv : kv [ 1 ], reverse = True ) # restrict result count to specified maximum filtered_slot_cands = sorted_slot_cands [: max_results ] # threshold by probabilities filtered_slot_cands = [ slot_cand [ 0 ] for slot_cand in filtered_slot_cands if slot_cand [ 1 ] >= threshold ] if len ( filtered_slot_cands ) > 0 : # append results if any remain after filtering if max_results == 1 : # only float candidates [ slot ] = filtered_slot_cands [ 0 ] else : # list candidates [ slot ] = filtered_slot_cands return candidates get_most_probable_slot_beliefs ( self , slot , consider_NONE = True , threshold = 0.7 , max_results = 1 , turn_idx =- 1 ) \u00b6 Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Parameters: Name Type Description Default consider_NONE bool If True, slots where NONE values have the highest probability will not be added to the result. If False, slots where NONE values have the highest probability will look for the best value != NONE . True threshold float minimum probability to be accepted to the 0.7 max_results int return at most #max_results best values per slot 1 turn_idx int index for accessing the belief state history (default = -1: use last turn) -1 Returns: Type Description Union(Dict[str, List[str]], Dict[str, str]) A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. Source code in adviser/utils/beliefstate.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def get_most_probable_slot_beliefs ( self , slot : str , consider_NONE : bool = True , threshold : float = 0.7 , max_results : int = 1 , turn_idx : int = - 1 ): \"\"\" Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Args: consider_NONE: If True, slots where **NONE** values have the highest probability will not be added to the result. If False, slots where **NONE** values have the highest probability will look for the best value != **NONE**. threshold: minimum probability to be accepted to the max_results: return at most #max_results best values per slot turn_idx: index for accessing the belief state history (default = -1: use last turn) Returns: Union(Dict[str, List[str]], Dict[str, str]): A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. \"\"\" informs = self . _history [ turn_idx ][ \"informs\" ] candidates = [] if slot in informs : sorted_slot_cands = sorted ( informs [ slot ] . items (), key = lambda kv : kv [ 1 ], reverse = True ) # restrict result count to specified maximum filtered_slot_cands = sorted_slot_cands [: max_results ] # threshold by probabilities filtered_slot_cands = [ slot_cand [ 0 ] for slot_cand in filtered_slot_cands if slot_cand [ 1 ] >= threshold ] return candidates get_num_dbmatches ( self ) \u00b6 Updates the belief state's entry for the number of database matches given the constraints in the current turn. Source code in adviser/utils/beliefstate.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def get_num_dbmatches ( self ): \"\"\" Updates the belief state's entry for the number of database matches given the constraints in the current turn. \"\"\" # check how many db entities match the current constraints candidates = self . get_most_probable_inf_beliefs ( consider_NONE = True , threshold = 0.7 , max_results = 1 ) constraints = self . _remove_dontcare_slots ( candidates ) db_matches = self . domain . find_entities ( constraints , self . domain . get_informable_slots ()) num_matches = len ( db_matches ) # check if matching db entities could be discriminated by more # information from user discriminable = False if len ( db_matches ) > 1 : dontcare_slots = set ( candidates . keys ()) - set ( constraints . keys ()) informable_slots = set ( self . domain . get_informable_slots ()) - set ( self . domain . get_primary_key ()) for informable_slot in informable_slots : if informable_slot not in dontcare_slots : # this slot could be used to gather more information db_values_for_slot = set () for db_match in db_matches : db_values_for_slot . add ( db_match [ informable_slot ]) if len ( db_values_for_slot ) > 1 : # at least 2 different values for slot # ->can use this slot to differentiate between entities discriminable = True break return num_matches , discriminable get_requested_slots ( self , turn_idx =- 1 ) \u00b6 Returns the slots requested by the user Parameters: Name Type Description Default turn_idx int index for accessing the belief state history (default = -1: use last turn) -1 Source code in adviser/utils/beliefstate.py 192 193 194 195 196 197 198 199 200 201 202 def get_requested_slots ( self , turn_idx : int = - 1 ): \"\"\" Returns the slots requested by the user Args: turn_idx: index for accessing the belief state history (default = -1: use last turn) \"\"\" candidates = [] for req_slot in self . _history [ turn_idx ][ 'requests' ]: candidates . append ( req_slot ) return candidates start_new_turn ( self ) \u00b6 ONLY to be called by the belief state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules Source code in adviser/utils/beliefstate.py 85 86 87 88 89 90 91 92 def start_new_turn ( self ): \"\"\" ONLY to be called by the belief state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules \"\"\" # copy last turn's dict self . _history . append ( copy . deepcopy ( self . _history [ - 1 ])) common \u00b6 This modules provides a method to seed commonly used random generators. Language \u00b6 Set of recognized languaged __new__ ( cls , value ) special \u00b6 Source code in adviser/utils/common.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc init_random ( seed = None ) \u00b6 Initializes the random generators to allow seeding. Parameters: Name Type Description Default seed int The seed used for all random generators. None Source code in adviser/utils/common.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def init_random ( seed : int = None ): \"\"\" Initializes the random generators to allow seeding. Args: seed (int): The seed used for all random generators. \"\"\" global GLOBAL_SEED # pylint: disable=global-statement if GLOBAL_SEED is not None : return if seed is None : tmp_random = numpy . random . RandomState ( None ) GLOBAL_SEED = tmp_random . randint ( 2 ** 32 - 1 , dtype = 'uint32' ) else : GLOBAL_SEED = seed # initialize random generators numpy . random . seed ( GLOBAL_SEED ) random . seed ( GLOBAL_SEED ) try : # try to load torch and initialize random generator if available import torch torch . cuda . manual_seed_all ( GLOBAL_SEED ) # gpu torch . manual_seed ( GLOBAL_SEED ) # cpu except ImportError : pass try : # try to load tensorflow and initialize random generator if available import tensorflow tensorflow . random . set_random_seed ( GLOBAL_SEED ) except ImportError : pass # check whether all calls to torch.* use the same random generator (i.e. same instance) # works in a short test -- MS # print(torch.initial_seed()) # logger.info(\"Seed is {:d}\".format(GLOBAL_SEED)) return GLOBAL_SEED domain special \u00b6 domain \u00b6 Domain \u00b6 Abstract class for linking a domain with a data access method. Derive from this class if you need to implement a domain with a not yet supported data backend, otherwise choose a fitting existing child class. __init__ ( self , name ) special \u00b6 Source code in adviser/utils/domain/domain.py 27 28 def __init__ ( self , name : str ): self . name = name find_entities ( self , constraints ) \u00b6 Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict slot-value mapping of constraints required IMPORTANT: This function must be overridden! Source code in adviser/utils/domain/domain.py 38 39 40 41 42 43 44 45 46 def find_entities ( self , constraints : dict ): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): slot-value mapping of constraints IMPORTANT: This function must be overridden! \"\"\" raise NotImplementedError get_domain_name ( self ) \u00b6 Return the domain name of the current ontology. Returns: Type Description str object: Source code in adviser/utils/domain/domain.py 30 31 32 33 34 35 36 def get_domain_name ( self ) -> str : \"\"\" Return the domain name of the current ontology. Returns: object: \"\"\" return self . name jsonlookupdomain \u00b6 JSONLookupDomain \u00b6 Abstract class for linking a domain based on a JSON-ontology with a database access method (sqllite). __getstate__ ( self ) special \u00b6 Source code in adviser/utils/domain/jsonlookupdomain.py 69 70 71 72 73 74 def __getstate__ ( self ): # remove sql connection from state dict so that pickling works state = self . __dict__ . copy () if 'db' in state : del state [ 'db' ] return state __init__ ( self , name , json_ontology_file = None , sqllite_db_file = None , display_name = None ) special \u00b6 Loads the ontology from a json file and the data from a sqllite database. To create a new domain using this format, inherit from this class and overwrite the _get_domain_name_()-method to return your domain's name. Parameters: Name Type Description Default name str the domain's name used as an identifier required json_ontology_file str relative path to the ontology file (from the top-level adviser directory, e.g. resources/ontologies) None sqllite_db_file str relative path to the database file (from the top-level adviser directory, e.g. resources/databases) None display_name str the domain's name as it appears on the screen (e.g. containing whitespaces) None Source code in adviser/utils/domain/jsonlookupdomain.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , name : str , json_ontology_file : str = None , sqllite_db_file : str = None , \\ display_name : str = None ): \"\"\" Loads the ontology from a json file and the data from a sqllite database. To create a new domain using this format, inherit from this class and overwrite the _get_domain_name_()-method to return your domain's name. Arguments: name (str): the domain's name used as an identifier json_ontology_file (str): relative path to the ontology file (from the top-level adviser directory, e.g. resources/ontologies) sqllite_db_file (str): relative path to the database file (from the top-level adviser directory, e.g. resources/databases) display_name (str): the domain's name as it appears on the screen (e.g. containing whitespaces) \"\"\" super ( JSONLookupDomain , self ) . __init__ ( name ) root_dir = self . _get_root_dir () self . sqllite_db_file = sqllite_db_file # make sure to set default values in case of None json_ontology_file = json_ontology_file or os . path . join ( 'resources' , 'ontologies' , name + '.json' ) sqllite_db_file = sqllite_db_file or os . path . join ( 'resources' , 'databases' , name + '.db' ) self . ontology_json = json . load ( open ( root_dir + '/' + json_ontology_file )) # load database self . db = self . _load_db_to_memory ( root_dir + '/' + sqllite_db_file ) self . display_name = display_name if display_name is not None else name find_entities ( self , constraints , requested_slots =< tuple_iterator object at 0x7f9197e262e0 > ) \u00b6 Returns all entities from the data backend that meet the constraints, with values for the primary key and the system requestable slots (and optional slots, specifyable via requested_slots). Parameters: Name Type Description Default constraints dict Slot-value mapping of constraints. If empty, all entities in the database will be returned. required requested_slots Iterable list of slots that should be returned in addition to the system requestable slots and the primary key <tuple_iterator object at 0x7f9197e262e0> Source code in adviser/utils/domain/jsonlookupdomain.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints, with values for the primary key and the system requestable slots (and optional slots, specifyable via requested_slots). Args: constraints (dict): Slot-value mapping of constraints. If empty, all entities in the database will be returned. requested_slots (Iterable): list of slots that should be returned in addition to the system requestable slots and the primary key \"\"\" # values for name and all system requestable slots select_clause = \", \" . join ( set ([ self . get_primary_key ()]) | set ( self . get_system_requestable_slots ()) | set ( requested_slots )) query = \"SELECT {} FROM {} \" . format ( select_clause , self . get_domain_name ()) constraints = { slot : value . replace ( \"'\" , \"''\" ) for slot , value in constraints . items () if value is not None and str ( value ) . lower () != 'dontcare' } if constraints : query += ' WHERE ' + ' AND ' . join ( \" {} =' {} ' COLLATE NOCASE\" . format ( key , str ( val )) for key , val in constraints . items ()) return self . query_db ( query ) find_info_about_entity ( self , entity_id , requested_slots ) \u00b6 Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/utils/domain/jsonlookupdomain.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" if requested_slots : select_clause = \", \" . join ( sorted ( requested_slots )) # If the user hasn't specified any slots we don't know what they want so we give everything else : select_clause = \"*\" query = 'SELECT {} FROM {} WHERE {} =\" {} \";' . format ( select_clause , self . get_domain_name (), self . get_primary_key (), entity_id ) return self . query_db ( query ) get_display_name ( self ) \u00b6 Source code in adviser/utils/domain/jsonlookupdomain.py 176 177 def get_display_name ( self ): return self . display_name get_informable_slots ( self ) \u00b6 Returns a list of all informable slots. Source code in adviser/utils/domain/jsonlookupdomain.py 187 188 189 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return self . ontology_json [ 'informable' ] . keys () get_keyword ( self ) \u00b6 Source code in adviser/utils/domain/jsonlookupdomain.py 215 216 217 def get_keyword ( self ): if \"keyword\" in self . ontology_json : return self . ontology_json [ 'keyword' ] get_possible_values ( self , slot ) \u00b6 Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/utils/domain/jsonlookupdomain.py 191 192 193 194 195 196 197 198 199 200 201 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" return self . ontology_json [ 'informable' ][ slot ] get_primary_key ( self ) \u00b6 Returns the name of a column in the associated database which can be used to uniquely distinguish between database entities. Could be e.g. the name of a restaurant, an ID, ... Source code in adviser/utils/domain/jsonlookupdomain.py 203 204 205 206 207 def get_primary_key ( self ): \"\"\" Returns the name of a column in the associated database which can be used to uniquely distinguish between database entities. Could be e.g. the name of a restaurant, an ID, ... \"\"\" return self . ontology_json [ 'key' ] get_pronouns ( self , slot ) \u00b6 Source code in adviser/utils/domain/jsonlookupdomain.py 209 210 211 212 213 def get_pronouns ( self , slot ): if slot in self . ontology_json [ 'pronoun_map' ]: return self . ontology_json [ 'pronoun_map' ][ slot ] else : return [] get_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the user. Source code in adviser/utils/domain/jsonlookupdomain.py 179 180 181 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return self . ontology_json [ 'requestable' ] get_system_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the system. Source code in adviser/utils/domain/jsonlookupdomain.py 183 184 185 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return self . ontology_json [ 'system_requestable' ] query_db ( self , query_str ) \u00b6 Function for querying the sqlite3 db Parameters: Name Type Description Default query_str string sqlite3 query style string required Returns: Type Description (iterable) rows of the query response set Source code in adviser/utils/domain/jsonlookupdomain.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def query_db ( self , query_str ): \"\"\" Function for querying the sqlite3 db Args: query_str (string): sqlite3 query style string Return: (iterable): rows of the query response set \"\"\" if \"db\" not in self . __dict__ : root_dir = self . _get_root_dir () sqllite_db_file = self . sqllite_db_file or os . path . join ( 'resources' , 'databases' , self . name + '.db' ) self . db = self . _load_db_to_memory ( root_dir + '/' + sqllite_db_file ) cursor = self . db . cursor () cursor . execute ( query_str ) res = cursor . fetchall () return res lookupdomain \u00b6 LookupDomain \u00b6 Abstract class for linking a domain with a data access method. Derive from this class if you need to implement a domain with a not yet supported data backend, otherwise choose a fitting existing child class. __init__ ( self , identifier , display_name ) special \u00b6 Source code in adviser/utils/domain/lookupdomain.py 29 30 31 def __init__ ( self , identifier : str , display_name : str ): Domain . __init__ ( self , identifier ) self . display_name = display_name find_entities ( self , constraints , requested_slots =< tuple_iterator object at 0x7f9197e38580 > ) \u00b6 Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict slot-value mapping of constraints required IMPORTANT: This function must be overridden! Source code in adviser/utils/domain/lookupdomain.py 33 34 35 36 37 38 39 40 41 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): slot-value mapping of constraints IMPORTANT: This function must be overridden! \"\"\" raise NotImplementedError find_info_about_entity ( self , entity_id , requested_slots ) \u00b6 Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/utils/domain/lookupdomain.py 43 44 45 46 47 48 49 50 51 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" raise NotImplementedError get_display_name ( self ) \u00b6 Source code in adviser/utils/domain/lookupdomain.py 53 54 def get_display_name ( self ): return self . display_name get_informable_slots ( self ) \u00b6 Returns a list of all informable slots. Source code in adviser/utils/domain/lookupdomain.py 64 65 66 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" raise NotImplementedError get_keyword ( self ) \u00b6 Source code in adviser/utils/domain/lookupdomain.py 100 101 def get_keyword ( self ): raise NotImplementedError get_mandatory_slots ( self ) \u00b6 Returns a list of all mandatory slots. Source code in adviser/utils/domain/lookupdomain.py 68 69 70 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" raise NotImplementedError get_possible_values ( self , slot ) \u00b6 Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/utils/domain/lookupdomain.py 84 85 86 87 88 89 90 91 92 93 94 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" raise NotImplementedError get_primary_key ( self ) \u00b6 Returns the slot name that will be used as the 'name' of an entry Source code in adviser/utils/domain/lookupdomain.py 96 97 98 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" raise NotImplementedError get_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the user. Source code in adviser/utils/domain/lookupdomain.py 56 57 58 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" raise NotImplementedError get_system_requestable_slots ( self ) \u00b6 Returns a list of all slots requestable by the system. Source code in adviser/utils/domain/lookupdomain.py 60 61 62 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" raise NotImplementedError logger \u00b6 This module provides a logger for configurable output on different levels. DiasysLogger \u00b6 Logger class. This class enables logging to both a logfile and the console with different information levels . It also provides logging methods for the newly introduced information levels ( LogLevel . DIALOGS and LogLevel . RESULTS ). If file_level is set to LogLevel . NONE , no log file will be created . Otherwise , the output directory can be configured by setting log_folder . __init__ ( self , name = 'adviser' , console_log_lvl =< LogLevel . ERRORS : 40 > , file_log_lvl =< LogLevel . NONE : 100 > , logfile_folder = 'logs' , logfile_basename = 'log' ) special \u00b6 Source code in adviser/utils/logger.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , name : str = 'adviser' , console_log_lvl : LogLevel = LogLevel . ERRORS , file_log_lvl : LogLevel = LogLevel . NONE , logfile_folder : str = 'logs' , logfile_basename : str = 'log' ): # pylint: disable=too-many-arguments super ( DiasysLogger , self ) . __init__ ( name ) if file_log_lvl is not LogLevel . NONE : # configure output to log file os . makedirs ( os . path . realpath ( logfile_folder ), exist_ok = True ) log_file_name = logfile_basename + '_' + \\ str ( datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' )) + '.log' log_file_path = os . path . join ( os . path . realpath ( logfile_folder ), log_file_name ) file_handler = logging . FileHandler ( log_file_path , mode = 'w' ) file_handler . setLevel ( int ( file_log_lvl )) fh_formatter = MultilineFormatter ( ' %(asctime)s - %(message)s ' ) file_handler . setFormatter ( fh_formatter ) self . addHandler ( file_handler ) # configure output to console console_handler = logging . StreamHandler () console_handler . setLevel ( int ( console_log_lvl )) # ch_formatter = MultilineFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') ch_formatter = MultilineFormatter ( 'logger: %(message)s ' ) console_handler . setFormatter ( ch_formatter ) self . addHandler ( console_handler ) # log exceptions sys . excepthook = exception_logging_hook dialog_turn ( self , msg , dialog_act = None ) \u00b6 Logs a turn of a dialog Source code in adviser/utils/logger.py 113 114 115 116 117 118 119 def dialog_turn ( self , msg : str , dialog_act = None ): \"\"\" Logs a turn of a dialog \"\"\" log_msg = msg if dialog_act is not None : log_msg += \" \\n \" + str ( dialog_act ) self . log ( int ( LogLevel . DIALOGS ), log_msg ) result ( self , msg ) \u00b6 Logs the result of a dialog Source code in adviser/utils/logger.py 108 109 110 111 def result ( self , msg : str ): \"\"\" Logs the result of a dialog \"\"\" self . log ( int ( LogLevel . RESULTS ), msg ) LogLevel \u00b6 The available levels for the logger. __new__ ( cls , value ) special \u00b6 Source code in adviser/utils/logger.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc MultilineFormatter \u00b6 A formatter for the logger taking care of multiline messages. format ( self , record ) \u00b6 Format the specified record as text. The record's attribute dictionary is used as the operand to a string formatting operation which yields the returned string. Before formatting the dictionary, a couple of preparatory steps are carried out. The message attribute of the record is computed using LogRecord.getMessage(). If the formatting string uses the time (as determined by a call to usesTime(), formatTime() is called to format the event time. If there is exception information, it is formatted using formatException() and appended to the message. Source code in adviser/utils/logger.py 52 53 54 55 56 57 58 59 60 61 62 def format ( self , record : logging . LogRecord ): save_msg = record . msg output = \"\" for idx , line in enumerate ( save_msg . splitlines ()): if idx > 0 : output += \" \\n \" record . msg = line output += super () . format ( record ) record . msg = save_msg record . message = output return output exception_logging_hook ( exc_type , exc_value , exc_traceback ) \u00b6 Used as a hook to log exceptions. Source code in adviser/utils/logger.py 43 44 45 46 def exception_logging_hook ( exc_type , exc_value , exc_traceback ): \"\"\" Used as a hook to log exceptions. \"\"\" logging . getLogger ( 'adviser' ) . error ( \"Uncaught exception\" , exc_info = ( exc_type , exc_value , exc_traceback )) sysact \u00b6 This module provides the necessary classes for a system action. SysAct \u00b6 __eq__ ( self , other ) special \u00b6 Source code in adviser/utils/sysact.py 91 92 93 def __eq__ ( self , other ): return ( self . type == other . type and self . slot_values == other . slot_values ) __init__ ( self , act_type = None , slot_values = None ) special \u00b6 The class for a system action as used in the dialog. Parameters: Name Type Description Default act_type SysActionType The type of the system action. None slot_values Dict[str, List[str]] A mapping of slot -> value to which the system action refers depending on the action type - might be None . None Source code in adviser/utils/sysact.py 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , act_type : SysActionType = None , slot_values : Dict [ str , List [ str ]] = None ): \"\"\" The class for a system action as used in the dialog. Args: act_type (SysActionType): The type of the system action. slot_values (Dict[str, List[str]]): A mapping of ``slot -> value`` to which the system action refers depending on the action type - might be ``None``. \"\"\" self . type = act_type self . slot_values = slot_values if slot_values is not None else {} __repr__ ( self ) special \u00b6 Source code in adviser/utils/sysact.py 56 57 58 59 def __repr__ ( self ): return f \"\"\"SysAct(act_type= { self . type } { f \", { self . _slot_value_dict_to_str ( self . slot_values ) } \" if self . _slot_value_dict_to_str ( self . slot_values ) else \"\" } )\"\"\" __str__ ( self ) special \u00b6 Source code in adviser/utils/sysact.py 61 62 63 64 65 66 67 68 69 70 def __str__ ( self ): if self . type is not None : return self . type . value + \\ '(' + \\ self . _slot_value_dict_to_str ( self . slot_values ) + \\ ')' else : return 'SysAct(act_type=' + self . type + ', ' + \\ self . _slot_value_dict_to_str ( self . slot_values ) + \\ ')' add_value ( self , slot , value = None ) \u00b6 Add a value (or just a slot, if value=None) to the system act Source code in adviser/utils/sysact.py 72 73 74 75 76 77 def add_value ( self , slot , value = None ): \"\"\" Add a value (or just a slot, if value=None) to the system act \"\"\" if slot not in self . slot_values : self . slot_values [ slot ] = [] if value is not None : self . slot_values [ slot ] . append ( value ) get_values ( self , slot ) \u00b6 Return all values for slot Returns: Type Description Dict[str, List[str]] A list of values for slot or an empy list if there was no value specified for the given slot Source code in adviser/utils/sysact.py 79 80 81 82 83 84 85 86 87 88 89 def get_values ( self , slot ): \"\"\" Return all values for slot Returns: Dict[str, List[str]]: A list of values for slot or an empy list if there was no value specified for the given slot \"\"\" if slot not in self . slot_values : return [] else : return self . slot_values [ slot ] SysActionType \u00b6 The type for a system action as used in :class: SystemAct . __new__ ( cls , value ) special \u00b6 Source code in adviser/utils/sysact.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc topics \u00b6 Topic \u00b6 useract \u00b6 This module provides the necessary classes for a user action. UserAct \u00b6 __eq__ ( self , other ) special \u00b6 Source code in adviser/utils/useract.py 71 72 73 74 75 def __eq__ ( self , other ): # to check for equality for tests return ( self . type == other . type and self . slot == other . slot and self . value == other . value and self . score == other . score ) __hash__ ( self ) special \u00b6 Source code in adviser/utils/useract.py 77 78 def __hash__ ( self ): return hash ( self . type ) * hash ( self . slot ) * hash ( self . value ) * hash ( self . score ) __init__ ( self , text = '' , act_type = None , slot = None , value = None , score = 1.0 ) special \u00b6 The class for a user action as used in the dialog. Parameters: Name Type Description Default text str A textual representation of the user action. '' act_type UserActionType The type of the user action. None slot str The slot to which the user action refers - might be None depending on the user action. None value str The value to which the user action refers - might be None depending on the user action. None score float A value from 0 (not important) to 1 (important) indicating how important the information is for the belief state. 1.0 Source code in adviser/utils/useract.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , text : str = \"\" , act_type : UserActionType = None , slot : str = None , value : str = None , score : float = 1.0 ): \"\"\" The class for a user action as used in the dialog. Args: text (str): A textual representation of the user action. act_type (UserActionType): The type of the user action. slot (str): The slot to which the user action refers - might be ``None`` depending on the user action. value (str): The value to which the user action refers - might be ``None`` depending on the user action. score (float): A value from 0 (not important) to 1 (important) indicating how important the information is for the belief state. \"\"\" self . text = text self . type = act_type self . slot = slot self . value = value self . score = score __repr__ ( self ) special \u00b6 Source code in adviser/utils/useract.py 67 68 69 def __repr__ ( self ): return \"UserAct( \\\" {} \\\" , {} , {} , {} , {} )\" . format ( self . text , self . type , self . slot , self . value , self . score ) UserActionType \u00b6 The type for a user action as used in :class: UserAct . __new__ ( cls , value ) special \u00b6 Source code in adviser/utils/useract.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc userstate \u00b6 This module provides the UserState class. EmotionType \u00b6 The type for a user emotion as used in :class: UserState . __new__ ( cls , value ) special \u00b6 Source code in adviser/utils/userstate.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc EngagementType \u00b6 The type for a user engagement as used in :class: UserState . __new__ ( cls , value ) special \u00b6 Source code in adviser/utils/userstate.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc UserState \u00b6 The representation of a user state. Can be accessed like a dictionary __contains__ ( self , val ) special \u00b6 Source code in adviser/utils/userstate.py 67 68 def __contains__ ( self , val ): # assume return val in self . _history [ - 1 ] __getitem__ ( self , val ) special \u00b6 Source code in adviser/utils/userstate.py 49 50 51 52 53 54 55 56 def __getitem__ ( self , val ): # for indexing # if used with numbers: int (e.g. state[-2]) or slice (e.g. state[3:6]) if isinstance ( val , int ) or isinstance ( val , slice ): return self . _history [ val ] # interpret the number as turn # if used with strings (e.g. state['beliefs']) elif isinstance ( val , str ): # take the current turn's belief state return self . _history [ - 1 ][ val ] __init__ ( self ) special \u00b6 Source code in adviser/utils/userstate.py 46 47 def __init__ ( self ): self . _history = [ self . _init_userstate ()] __iter__ ( self ) special \u00b6 Source code in adviser/utils/userstate.py 58 59 def __iter__ ( self ): return iter ( self . _history [ - 1 ]) __len__ ( self ) special \u00b6 Source code in adviser/utils/userstate.py 64 65 def __len__ ( self ): return len ( self . _history ) __repr__ ( self ) special \u00b6 Source code in adviser/utils/userstate.py 70 71 def __repr__ ( self ): return str ( self . _history [ - 1 ]) __setitem__ ( self , key , val ) special \u00b6 Source code in adviser/utils/userstate.py 61 62 def __setitem__ ( self , key , val ): self . _history [ - 1 ][ key ] = val start_new_turn ( self ) \u00b6 ONLY to be called by the user state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules Source code in adviser/utils/userstate.py 73 74 75 76 77 78 79 80 def start_new_turn ( self ): \"\"\" ONLY to be called by the user state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules \"\"\" # copy last turn's dict self . _history . append ( copy . deepcopy ( self . _history [ - 1 ]))","title":"Utils"},{"location":"api/utils/#utils","text":"","title":"Utils"},{"location":"api/utils/#adviser.utils","text":"","title":"adviser.utils"},{"location":"api/utils/#adviser.utils.beliefstate","text":"This module provides the BeliefState class.","title":"beliefstate"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState","text":"A representation of the belief state, can be accessed like a dictionary. Includes information on: * current set of UserActTypes * informs to date (dictionary where key is slot and value is {value: probaility}) * requests for the current turn * number of db matches for given constraints * if the db matches can further be split","title":"BeliefState"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__contains__","text":"Source code in adviser/utils/beliefstate.py 62 63 def __contains__ ( self , val ): # assume return val in self . _history [ - 1 ]","title":"__contains__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__getitem__","text":"Source code in adviser/utils/beliefstate.py 43 44 45 46 47 48 49 50 def __getitem__ ( self , val ): # for indexing # if used with numbers: int (e.g. state[-2]) or slice (e.g. state[3:6]) if isinstance ( val , int ) or isinstance ( val , slice ): return self . _history [ val ] # interpret the number as turn # if used with strings (e.g. state['beliefs']) elif isinstance ( val , str ): # take the current turn's belief state return self . _history [ - 1 ][ val ]","title":"__getitem__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__init__","text":"Source code in adviser/utils/beliefstate.py 39 40 41 def __init__ ( self , domain : JSONLookupDomain ): self . domain = domain self . _history = [ self . _init_beliefstate ()]","title":"__init__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__iter__","text":"Source code in adviser/utils/beliefstate.py 52 53 def __iter__ ( self ): return iter ( self . _history [ - 1 ])","title":"__iter__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__len__","text":"Source code in adviser/utils/beliefstate.py 59 60 def __len__ ( self ): return len ( self . _history )","title":"__len__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__repr__","text":"Source code in adviser/utils/beliefstate.py 79 80 def __repr__ ( self ): return str ( self . _history [ - 1 ])","title":"__repr__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__setitem__","text":"Source code in adviser/utils/beliefstate.py 55 56 57 def __setitem__ ( self , key , val ): # e.g. state['beliefs']['area']['west'] = 1.0 self . _history [ - 1 ][ key ] = val","title":"__setitem__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.__str__","text":"Source code in adviser/utils/beliefstate.py 82 83 def __str__ ( self ): return self . _recursive_repr ( self . _history [ - 1 ])","title":"__str__()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.get_most_probable_inf_beliefs","text":"Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Parameters: Name Type Description Default consider_NONE bool If True, slots where NONE values have the highest probability will not be added to the result. If False, slots where NONE values have the highest probability will look for the best value != NONE . True threshold float minimum probability to be accepted 0.7 max_results int return at most max_results best values per slot 1 turn_idx int index for accessing the belief state history (default = -1: use last turn) -1 Returns: Type Description Union(Dict[str, List[str]], Dict[str, str]) A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. Source code in adviser/utils/beliefstate.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def get_most_probable_inf_beliefs ( self , consider_NONE : bool = True , threshold : float = 0.7 , max_results : int = 1 , turn_idx : int = - 1 ): \"\"\" Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Args: consider_NONE: If True, slots where **NONE** values have the highest probability will not be added to the result. If False, slots where **NONE** values have the highest probability will look for the best value != **NONE**. threshold: minimum probability to be accepted max_results: return at most `max_results` best values per slot turn_idx: index for accessing the belief state history (default = -1: use last turn) Returns: Union(Dict[str, List[str]], Dict[str, str]): A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. \"\"\" candidates = {} informs = self . _history [ turn_idx ][ \"informs\" ] for slot in informs : # sort by belief sorted_slot_cands = sorted ( informs [ slot ] . items (), key = lambda kv : kv [ 1 ], reverse = True ) # restrict result count to specified maximum filtered_slot_cands = sorted_slot_cands [: max_results ] # threshold by probabilities filtered_slot_cands = [ slot_cand [ 0 ] for slot_cand in filtered_slot_cands if slot_cand [ 1 ] >= threshold ] if len ( filtered_slot_cands ) > 0 : # append results if any remain after filtering if max_results == 1 : # only float candidates [ slot ] = filtered_slot_cands [ 0 ] else : # list candidates [ slot ] = filtered_slot_cands return candidates","title":"get_most_probable_inf_beliefs()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.get_most_probable_slot_beliefs","text":"Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Parameters: Name Type Description Default consider_NONE bool If True, slots where NONE values have the highest probability will not be added to the result. If False, slots where NONE values have the highest probability will look for the best value != NONE . True threshold float minimum probability to be accepted to the 0.7 max_results int return at most #max_results best values per slot 1 turn_idx int index for accessing the belief state history (default = -1: use last turn) -1 Returns: Type Description Union(Dict[str, List[str]], Dict[str, str]) A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. Source code in adviser/utils/beliefstate.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def get_most_probable_slot_beliefs ( self , slot : str , consider_NONE : bool = True , threshold : float = 0.7 , max_results : int = 1 , turn_idx : int = - 1 ): \"\"\" Extract the most probable value for each system requestable slot If the most probable value for a slot does not exceed the threshold, then the slot will not be added to the result at all. Args: consider_NONE: If True, slots where **NONE** values have the highest probability will not be added to the result. If False, slots where **NONE** values have the highest probability will look for the best value != **NONE**. threshold: minimum probability to be accepted to the max_results: return at most #max_results best values per slot turn_idx: index for accessing the belief state history (default = -1: use last turn) Returns: Union(Dict[str, List[str]], Dict[str, str]): A dict with mapping from slots to a list of values (if max_results > 1) or a value (if max_results == 1) containing the slots which have at least one value whose probability exceeds the specified threshold. \"\"\" informs = self . _history [ turn_idx ][ \"informs\" ] candidates = [] if slot in informs : sorted_slot_cands = sorted ( informs [ slot ] . items (), key = lambda kv : kv [ 1 ], reverse = True ) # restrict result count to specified maximum filtered_slot_cands = sorted_slot_cands [: max_results ] # threshold by probabilities filtered_slot_cands = [ slot_cand [ 0 ] for slot_cand in filtered_slot_cands if slot_cand [ 1 ] >= threshold ] return candidates","title":"get_most_probable_slot_beliefs()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.get_num_dbmatches","text":"Updates the belief state's entry for the number of database matches given the constraints in the current turn. Source code in adviser/utils/beliefstate.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def get_num_dbmatches ( self ): \"\"\" Updates the belief state's entry for the number of database matches given the constraints in the current turn. \"\"\" # check how many db entities match the current constraints candidates = self . get_most_probable_inf_beliefs ( consider_NONE = True , threshold = 0.7 , max_results = 1 ) constraints = self . _remove_dontcare_slots ( candidates ) db_matches = self . domain . find_entities ( constraints , self . domain . get_informable_slots ()) num_matches = len ( db_matches ) # check if matching db entities could be discriminated by more # information from user discriminable = False if len ( db_matches ) > 1 : dontcare_slots = set ( candidates . keys ()) - set ( constraints . keys ()) informable_slots = set ( self . domain . get_informable_slots ()) - set ( self . domain . get_primary_key ()) for informable_slot in informable_slots : if informable_slot not in dontcare_slots : # this slot could be used to gather more information db_values_for_slot = set () for db_match in db_matches : db_values_for_slot . add ( db_match [ informable_slot ]) if len ( db_values_for_slot ) > 1 : # at least 2 different values for slot # ->can use this slot to differentiate between entities discriminable = True break return num_matches , discriminable","title":"get_num_dbmatches()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.get_requested_slots","text":"Returns the slots requested by the user Parameters: Name Type Description Default turn_idx int index for accessing the belief state history (default = -1: use last turn) -1 Source code in adviser/utils/beliefstate.py 192 193 194 195 196 197 198 199 200 201 202 def get_requested_slots ( self , turn_idx : int = - 1 ): \"\"\" Returns the slots requested by the user Args: turn_idx: index for accessing the belief state history (default = -1: use last turn) \"\"\" candidates = [] for req_slot in self . _history [ turn_idx ][ 'requests' ]: candidates . append ( req_slot ) return candidates","title":"get_requested_slots()"},{"location":"api/utils/#adviser.utils.beliefstate.BeliefState.start_new_turn","text":"ONLY to be called by the belief state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules Source code in adviser/utils/beliefstate.py 85 86 87 88 89 90 91 92 def start_new_turn ( self ): \"\"\" ONLY to be called by the belief state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules \"\"\" # copy last turn's dict self . _history . append ( copy . deepcopy ( self . _history [ - 1 ]))","title":"start_new_turn()"},{"location":"api/utils/#adviser.utils.common","text":"This modules provides a method to seed commonly used random generators.","title":"common"},{"location":"api/utils/#adviser.utils.common.Language","text":"Set of recognized languaged","title":"Language"},{"location":"api/utils/#adviser.utils.common.Language.__new__","text":"Source code in adviser/utils/common.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/utils/#adviser.utils.common.init_random","text":"Initializes the random generators to allow seeding. Parameters: Name Type Description Default seed int The seed used for all random generators. None Source code in adviser/utils/common.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def init_random ( seed : int = None ): \"\"\" Initializes the random generators to allow seeding. Args: seed (int): The seed used for all random generators. \"\"\" global GLOBAL_SEED # pylint: disable=global-statement if GLOBAL_SEED is not None : return if seed is None : tmp_random = numpy . random . RandomState ( None ) GLOBAL_SEED = tmp_random . randint ( 2 ** 32 - 1 , dtype = 'uint32' ) else : GLOBAL_SEED = seed # initialize random generators numpy . random . seed ( GLOBAL_SEED ) random . seed ( GLOBAL_SEED ) try : # try to load torch and initialize random generator if available import torch torch . cuda . manual_seed_all ( GLOBAL_SEED ) # gpu torch . manual_seed ( GLOBAL_SEED ) # cpu except ImportError : pass try : # try to load tensorflow and initialize random generator if available import tensorflow tensorflow . random . set_random_seed ( GLOBAL_SEED ) except ImportError : pass # check whether all calls to torch.* use the same random generator (i.e. same instance) # works in a short test -- MS # print(torch.initial_seed()) # logger.info(\"Seed is {:d}\".format(GLOBAL_SEED)) return GLOBAL_SEED","title":"init_random()"},{"location":"api/utils/#adviser.utils.domain","text":"","title":"domain"},{"location":"api/utils/#adviser.utils.domain.domain","text":"","title":"domain"},{"location":"api/utils/#adviser.utils.domain.domain.Domain","text":"Abstract class for linking a domain with a data access method. Derive from this class if you need to implement a domain with a not yet supported data backend, otherwise choose a fitting existing child class.","title":"Domain"},{"location":"api/utils/#adviser.utils.domain.domain.Domain.__init__","text":"Source code in adviser/utils/domain/domain.py 27 28 def __init__ ( self , name : str ): self . name = name","title":"__init__()"},{"location":"api/utils/#adviser.utils.domain.domain.Domain.find_entities","text":"Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict slot-value mapping of constraints required IMPORTANT: This function must be overridden! Source code in adviser/utils/domain/domain.py 38 39 40 41 42 43 44 45 46 def find_entities ( self , constraints : dict ): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): slot-value mapping of constraints IMPORTANT: This function must be overridden! \"\"\" raise NotImplementedError","title":"find_entities()"},{"location":"api/utils/#adviser.utils.domain.domain.Domain.get_domain_name","text":"Return the domain name of the current ontology. Returns: Type Description str object: Source code in adviser/utils/domain/domain.py 30 31 32 33 34 35 36 def get_domain_name ( self ) -> str : \"\"\" Return the domain name of the current ontology. Returns: object: \"\"\" return self . name","title":"get_domain_name()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain","text":"","title":"jsonlookupdomain"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain","text":"Abstract class for linking a domain based on a JSON-ontology with a database access method (sqllite).","title":"JSONLookupDomain"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.__getstate__","text":"Source code in adviser/utils/domain/jsonlookupdomain.py 69 70 71 72 73 74 def __getstate__ ( self ): # remove sql connection from state dict so that pickling works state = self . __dict__ . copy () if 'db' in state : del state [ 'db' ] return state","title":"__getstate__()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.__init__","text":"Loads the ontology from a json file and the data from a sqllite database. To create a new domain using this format, inherit from this class and overwrite the _get_domain_name_()-method to return your domain's name. Parameters: Name Type Description Default name str the domain's name used as an identifier required json_ontology_file str relative path to the ontology file (from the top-level adviser directory, e.g. resources/ontologies) None sqllite_db_file str relative path to the database file (from the top-level adviser directory, e.g. resources/databases) None display_name str the domain's name as it appears on the screen (e.g. containing whitespaces) None Source code in adviser/utils/domain/jsonlookupdomain.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , name : str , json_ontology_file : str = None , sqllite_db_file : str = None , \\ display_name : str = None ): \"\"\" Loads the ontology from a json file and the data from a sqllite database. To create a new domain using this format, inherit from this class and overwrite the _get_domain_name_()-method to return your domain's name. Arguments: name (str): the domain's name used as an identifier json_ontology_file (str): relative path to the ontology file (from the top-level adviser directory, e.g. resources/ontologies) sqllite_db_file (str): relative path to the database file (from the top-level adviser directory, e.g. resources/databases) display_name (str): the domain's name as it appears on the screen (e.g. containing whitespaces) \"\"\" super ( JSONLookupDomain , self ) . __init__ ( name ) root_dir = self . _get_root_dir () self . sqllite_db_file = sqllite_db_file # make sure to set default values in case of None json_ontology_file = json_ontology_file or os . path . join ( 'resources' , 'ontologies' , name + '.json' ) sqllite_db_file = sqllite_db_file or os . path . join ( 'resources' , 'databases' , name + '.db' ) self . ontology_json = json . load ( open ( root_dir + '/' + json_ontology_file )) # load database self . db = self . _load_db_to_memory ( root_dir + '/' + sqllite_db_file ) self . display_name = display_name if display_name is not None else name","title":"__init__()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.find_entities","text":"Returns all entities from the data backend that meet the constraints, with values for the primary key and the system requestable slots (and optional slots, specifyable via requested_slots). Parameters: Name Type Description Default constraints dict Slot-value mapping of constraints. If empty, all entities in the database will be returned. required requested_slots Iterable list of slots that should be returned in addition to the system requestable slots and the primary key <tuple_iterator object at 0x7f9197e262e0> Source code in adviser/utils/domain/jsonlookupdomain.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints, with values for the primary key and the system requestable slots (and optional slots, specifyable via requested_slots). Args: constraints (dict): Slot-value mapping of constraints. If empty, all entities in the database will be returned. requested_slots (Iterable): list of slots that should be returned in addition to the system requestable slots and the primary key \"\"\" # values for name and all system requestable slots select_clause = \", \" . join ( set ([ self . get_primary_key ()]) | set ( self . get_system_requestable_slots ()) | set ( requested_slots )) query = \"SELECT {} FROM {} \" . format ( select_clause , self . get_domain_name ()) constraints = { slot : value . replace ( \"'\" , \"''\" ) for slot , value in constraints . items () if value is not None and str ( value ) . lower () != 'dontcare' } if constraints : query += ' WHERE ' + ' AND ' . join ( \" {} =' {} ' COLLATE NOCASE\" . format ( key , str ( val )) for key , val in constraints . items ()) return self . query_db ( query )","title":"find_entities()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.find_info_about_entity","text":"Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/utils/domain/jsonlookupdomain.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" if requested_slots : select_clause = \", \" . join ( sorted ( requested_slots )) # If the user hasn't specified any slots we don't know what they want so we give everything else : select_clause = \"*\" query = 'SELECT {} FROM {} WHERE {} =\" {} \";' . format ( select_clause , self . get_domain_name (), self . get_primary_key (), entity_id ) return self . query_db ( query )","title":"find_info_about_entity()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_display_name","text":"Source code in adviser/utils/domain/jsonlookupdomain.py 176 177 def get_display_name ( self ): return self . display_name","title":"get_display_name()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_informable_slots","text":"Returns a list of all informable slots. Source code in adviser/utils/domain/jsonlookupdomain.py 187 188 189 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" return self . ontology_json [ 'informable' ] . keys ()","title":"get_informable_slots()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_keyword","text":"Source code in adviser/utils/domain/jsonlookupdomain.py 215 216 217 def get_keyword ( self ): if \"keyword\" in self . ontology_json : return self . ontology_json [ 'keyword' ]","title":"get_keyword()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_possible_values","text":"Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/utils/domain/jsonlookupdomain.py 191 192 193 194 195 196 197 198 199 200 201 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" return self . ontology_json [ 'informable' ][ slot ]","title":"get_possible_values()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_primary_key","text":"Returns the name of a column in the associated database which can be used to uniquely distinguish between database entities. Could be e.g. the name of a restaurant, an ID, ... Source code in adviser/utils/domain/jsonlookupdomain.py 203 204 205 206 207 def get_primary_key ( self ): \"\"\" Returns the name of a column in the associated database which can be used to uniquely distinguish between database entities. Could be e.g. the name of a restaurant, an ID, ... \"\"\" return self . ontology_json [ 'key' ]","title":"get_primary_key()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_pronouns","text":"Source code in adviser/utils/domain/jsonlookupdomain.py 209 210 211 212 213 def get_pronouns ( self , slot ): if slot in self . ontology_json [ 'pronoun_map' ]: return self . ontology_json [ 'pronoun_map' ][ slot ] else : return []","title":"get_pronouns()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_requestable_slots","text":"Returns a list of all slots requestable by the user. Source code in adviser/utils/domain/jsonlookupdomain.py 179 180 181 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" return self . ontology_json [ 'requestable' ]","title":"get_requestable_slots()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.get_system_requestable_slots","text":"Returns a list of all slots requestable by the system. Source code in adviser/utils/domain/jsonlookupdomain.py 183 184 185 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" return self . ontology_json [ 'system_requestable' ]","title":"get_system_requestable_slots()"},{"location":"api/utils/#adviser.utils.domain.jsonlookupdomain.JSONLookupDomain.query_db","text":"Function for querying the sqlite3 db Parameters: Name Type Description Default query_str string sqlite3 query style string required Returns: Type Description (iterable) rows of the query response set Source code in adviser/utils/domain/jsonlookupdomain.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def query_db ( self , query_str ): \"\"\" Function for querying the sqlite3 db Args: query_str (string): sqlite3 query style string Return: (iterable): rows of the query response set \"\"\" if \"db\" not in self . __dict__ : root_dir = self . _get_root_dir () sqllite_db_file = self . sqllite_db_file or os . path . join ( 'resources' , 'databases' , self . name + '.db' ) self . db = self . _load_db_to_memory ( root_dir + '/' + sqllite_db_file ) cursor = self . db . cursor () cursor . execute ( query_str ) res = cursor . fetchall () return res","title":"query_db()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain","text":"","title":"lookupdomain"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain","text":"Abstract class for linking a domain with a data access method. Derive from this class if you need to implement a domain with a not yet supported data backend, otherwise choose a fitting existing child class.","title":"LookupDomain"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.__init__","text":"Source code in adviser/utils/domain/lookupdomain.py 29 30 31 def __init__ ( self , identifier : str , display_name : str ): Domain . __init__ ( self , identifier ) self . display_name = display_name","title":"__init__()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.find_entities","text":"Returns all entities from the data backend that meet the constraints. Parameters: Name Type Description Default constraints dict slot-value mapping of constraints required IMPORTANT: This function must be overridden! Source code in adviser/utils/domain/lookupdomain.py 33 34 35 36 37 38 39 40 41 def find_entities ( self , constraints : dict , requested_slots : Iterable = iter (())): \"\"\" Returns all entities from the data backend that meet the constraints. Args: constraints (dict): slot-value mapping of constraints IMPORTANT: This function must be overridden! \"\"\" raise NotImplementedError","title":"find_entities()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.find_info_about_entity","text":"Returns the values (stored in the data backend) of the specified slots for the specified entity. Parameters: Name Type Description Default entity_id str primary key value of the entity required requested_slots Iterable slot-value mapping of constraints required Source code in adviser/utils/domain/lookupdomain.py 43 44 45 46 47 48 49 50 51 def find_info_about_entity ( self , entity_id , requested_slots : Iterable ): \"\"\" Returns the values (stored in the data backend) of the specified slots for the specified entity. Args: entity_id (str): primary key value of the entity requested_slots (dict): slot-value mapping of constraints \"\"\" raise NotImplementedError","title":"find_info_about_entity()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_display_name","text":"Source code in adviser/utils/domain/lookupdomain.py 53 54 def get_display_name ( self ): return self . display_name","title":"get_display_name()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_informable_slots","text":"Returns a list of all informable slots. Source code in adviser/utils/domain/lookupdomain.py 64 65 66 def get_informable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all informable slots. \"\"\" raise NotImplementedError","title":"get_informable_slots()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_keyword","text":"Source code in adviser/utils/domain/lookupdomain.py 100 101 def get_keyword ( self ): raise NotImplementedError","title":"get_keyword()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_mandatory_slots","text":"Returns a list of all mandatory slots. Source code in adviser/utils/domain/lookupdomain.py 68 69 70 def get_mandatory_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all mandatory slots. \"\"\" raise NotImplementedError","title":"get_mandatory_slots()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_possible_values","text":"Returns all possible values for an informable slot Parameters: Name Type Description Default slot str name of the slot required Returns: Type Description List[str] a list of strings, each string representing one possible value for the specified slot. Source code in adviser/utils/domain/lookupdomain.py 84 85 86 87 88 89 90 91 92 93 94 def get_possible_values ( self , slot : str ) -> List [ str ]: \"\"\" Returns all possible values for an informable slot Args: slot (str): name of the slot Returns: a list of strings, each string representing one possible value for the specified slot. \"\"\" raise NotImplementedError","title":"get_possible_values()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_primary_key","text":"Returns the slot name that will be used as the 'name' of an entry Source code in adviser/utils/domain/lookupdomain.py 96 97 98 def get_primary_key ( self ) -> str : \"\"\" Returns the slot name that will be used as the 'name' of an entry \"\"\" raise NotImplementedError","title":"get_primary_key()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_requestable_slots","text":"Returns a list of all slots requestable by the user. Source code in adviser/utils/domain/lookupdomain.py 56 57 58 def get_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the user. \"\"\" raise NotImplementedError","title":"get_requestable_slots()"},{"location":"api/utils/#adviser.utils.domain.lookupdomain.LookupDomain.get_system_requestable_slots","text":"Returns a list of all slots requestable by the system. Source code in adviser/utils/domain/lookupdomain.py 60 61 62 def get_system_requestable_slots ( self ) -> List [ str ]: \"\"\" Returns a list of all slots requestable by the system. \"\"\" raise NotImplementedError","title":"get_system_requestable_slots()"},{"location":"api/utils/#adviser.utils.logger","text":"This module provides a logger for configurable output on different levels.","title":"logger"},{"location":"api/utils/#adviser.utils.logger.DiasysLogger","text":"Logger class. This class enables logging to both a logfile and the console with different information levels . It also provides logging methods for the newly introduced information levels ( LogLevel . DIALOGS and LogLevel . RESULTS ). If file_level is set to LogLevel . NONE , no log file will be created . Otherwise , the output directory can be configured by setting log_folder .","title":"DiasysLogger"},{"location":"api/utils/#adviser.utils.logger.DiasysLogger.__init__","text":"Source code in adviser/utils/logger.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , name : str = 'adviser' , console_log_lvl : LogLevel = LogLevel . ERRORS , file_log_lvl : LogLevel = LogLevel . NONE , logfile_folder : str = 'logs' , logfile_basename : str = 'log' ): # pylint: disable=too-many-arguments super ( DiasysLogger , self ) . __init__ ( name ) if file_log_lvl is not LogLevel . NONE : # configure output to log file os . makedirs ( os . path . realpath ( logfile_folder ), exist_ok = True ) log_file_name = logfile_basename + '_' + \\ str ( datetime . datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' )) + '.log' log_file_path = os . path . join ( os . path . realpath ( logfile_folder ), log_file_name ) file_handler = logging . FileHandler ( log_file_path , mode = 'w' ) file_handler . setLevel ( int ( file_log_lvl )) fh_formatter = MultilineFormatter ( ' %(asctime)s - %(message)s ' ) file_handler . setFormatter ( fh_formatter ) self . addHandler ( file_handler ) # configure output to console console_handler = logging . StreamHandler () console_handler . setLevel ( int ( console_log_lvl )) # ch_formatter = MultilineFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') ch_formatter = MultilineFormatter ( 'logger: %(message)s ' ) console_handler . setFormatter ( ch_formatter ) self . addHandler ( console_handler ) # log exceptions sys . excepthook = exception_logging_hook","title":"__init__()"},{"location":"api/utils/#adviser.utils.logger.DiasysLogger.dialog_turn","text":"Logs a turn of a dialog Source code in adviser/utils/logger.py 113 114 115 116 117 118 119 def dialog_turn ( self , msg : str , dialog_act = None ): \"\"\" Logs a turn of a dialog \"\"\" log_msg = msg if dialog_act is not None : log_msg += \" \\n \" + str ( dialog_act ) self . log ( int ( LogLevel . DIALOGS ), log_msg )","title":"dialog_turn()"},{"location":"api/utils/#adviser.utils.logger.DiasysLogger.result","text":"Logs the result of a dialog Source code in adviser/utils/logger.py 108 109 110 111 def result ( self , msg : str ): \"\"\" Logs the result of a dialog \"\"\" self . log ( int ( LogLevel . RESULTS ), msg )","title":"result()"},{"location":"api/utils/#adviser.utils.logger.LogLevel","text":"The available levels for the logger.","title":"LogLevel"},{"location":"api/utils/#adviser.utils.logger.LogLevel.__new__","text":"Source code in adviser/utils/logger.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/utils/#adviser.utils.logger.MultilineFormatter","text":"A formatter for the logger taking care of multiline messages.","title":"MultilineFormatter"},{"location":"api/utils/#adviser.utils.logger.MultilineFormatter.format","text":"Format the specified record as text. The record's attribute dictionary is used as the operand to a string formatting operation which yields the returned string. Before formatting the dictionary, a couple of preparatory steps are carried out. The message attribute of the record is computed using LogRecord.getMessage(). If the formatting string uses the time (as determined by a call to usesTime(), formatTime() is called to format the event time. If there is exception information, it is formatted using formatException() and appended to the message. Source code in adviser/utils/logger.py 52 53 54 55 56 57 58 59 60 61 62 def format ( self , record : logging . LogRecord ): save_msg = record . msg output = \"\" for idx , line in enumerate ( save_msg . splitlines ()): if idx > 0 : output += \" \\n \" record . msg = line output += super () . format ( record ) record . msg = save_msg record . message = output return output","title":"format()"},{"location":"api/utils/#adviser.utils.logger.exception_logging_hook","text":"Used as a hook to log exceptions. Source code in adviser/utils/logger.py 43 44 45 46 def exception_logging_hook ( exc_type , exc_value , exc_traceback ): \"\"\" Used as a hook to log exceptions. \"\"\" logging . getLogger ( 'adviser' ) . error ( \"Uncaught exception\" , exc_info = ( exc_type , exc_value , exc_traceback ))","title":"exception_logging_hook()"},{"location":"api/utils/#adviser.utils.sysact","text":"This module provides the necessary classes for a system action.","title":"sysact"},{"location":"api/utils/#adviser.utils.sysact.SysAct","text":"","title":"SysAct"},{"location":"api/utils/#adviser.utils.sysact.SysAct.__eq__","text":"Source code in adviser/utils/sysact.py 91 92 93 def __eq__ ( self , other ): return ( self . type == other . type and self . slot_values == other . slot_values )","title":"__eq__()"},{"location":"api/utils/#adviser.utils.sysact.SysAct.__init__","text":"The class for a system action as used in the dialog. Parameters: Name Type Description Default act_type SysActionType The type of the system action. None slot_values Dict[str, List[str]] A mapping of slot -> value to which the system action refers depending on the action type - might be None . None Source code in adviser/utils/sysact.py 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , act_type : SysActionType = None , slot_values : Dict [ str , List [ str ]] = None ): \"\"\" The class for a system action as used in the dialog. Args: act_type (SysActionType): The type of the system action. slot_values (Dict[str, List[str]]): A mapping of ``slot -> value`` to which the system action refers depending on the action type - might be ``None``. \"\"\" self . type = act_type self . slot_values = slot_values if slot_values is not None else {}","title":"__init__()"},{"location":"api/utils/#adviser.utils.sysact.SysAct.__repr__","text":"Source code in adviser/utils/sysact.py 56 57 58 59 def __repr__ ( self ): return f \"\"\"SysAct(act_type= { self . type } { f \", { self . _slot_value_dict_to_str ( self . slot_values ) } \" if self . _slot_value_dict_to_str ( self . slot_values ) else \"\" } )\"\"\"","title":"__repr__()"},{"location":"api/utils/#adviser.utils.sysact.SysAct.__str__","text":"Source code in adviser/utils/sysact.py 61 62 63 64 65 66 67 68 69 70 def __str__ ( self ): if self . type is not None : return self . type . value + \\ '(' + \\ self . _slot_value_dict_to_str ( self . slot_values ) + \\ ')' else : return 'SysAct(act_type=' + self . type + ', ' + \\ self . _slot_value_dict_to_str ( self . slot_values ) + \\ ')'","title":"__str__()"},{"location":"api/utils/#adviser.utils.sysact.SysAct.add_value","text":"Add a value (or just a slot, if value=None) to the system act Source code in adviser/utils/sysact.py 72 73 74 75 76 77 def add_value ( self , slot , value = None ): \"\"\" Add a value (or just a slot, if value=None) to the system act \"\"\" if slot not in self . slot_values : self . slot_values [ slot ] = [] if value is not None : self . slot_values [ slot ] . append ( value )","title":"add_value()"},{"location":"api/utils/#adviser.utils.sysact.SysAct.get_values","text":"Return all values for slot Returns: Type Description Dict[str, List[str]] A list of values for slot or an empy list if there was no value specified for the given slot Source code in adviser/utils/sysact.py 79 80 81 82 83 84 85 86 87 88 89 def get_values ( self , slot ): \"\"\" Return all values for slot Returns: Dict[str, List[str]]: A list of values for slot or an empy list if there was no value specified for the given slot \"\"\" if slot not in self . slot_values : return [] else : return self . slot_values [ slot ]","title":"get_values()"},{"location":"api/utils/#adviser.utils.sysact.SysActionType","text":"The type for a system action as used in :class: SystemAct .","title":"SysActionType"},{"location":"api/utils/#adviser.utils.sysact.SysActionType.__new__","text":"Source code in adviser/utils/sysact.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/utils/#adviser.utils.topics","text":"","title":"topics"},{"location":"api/utils/#adviser.utils.topics.Topic","text":"","title":"Topic"},{"location":"api/utils/#adviser.utils.useract","text":"This module provides the necessary classes for a user action.","title":"useract"},{"location":"api/utils/#adviser.utils.useract.UserAct","text":"","title":"UserAct"},{"location":"api/utils/#adviser.utils.useract.UserAct.__eq__","text":"Source code in adviser/utils/useract.py 71 72 73 74 75 def __eq__ ( self , other ): # to check for equality for tests return ( self . type == other . type and self . slot == other . slot and self . value == other . value and self . score == other . score )","title":"__eq__()"},{"location":"api/utils/#adviser.utils.useract.UserAct.__hash__","text":"Source code in adviser/utils/useract.py 77 78 def __hash__ ( self ): return hash ( self . type ) * hash ( self . slot ) * hash ( self . value ) * hash ( self . score )","title":"__hash__()"},{"location":"api/utils/#adviser.utils.useract.UserAct.__init__","text":"The class for a user action as used in the dialog. Parameters: Name Type Description Default text str A textual representation of the user action. '' act_type UserActionType The type of the user action. None slot str The slot to which the user action refers - might be None depending on the user action. None value str The value to which the user action refers - might be None depending on the user action. None score float A value from 0 (not important) to 1 (important) indicating how important the information is for the belief state. 1.0 Source code in adviser/utils/useract.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , text : str = \"\" , act_type : UserActionType = None , slot : str = None , value : str = None , score : float = 1.0 ): \"\"\" The class for a user action as used in the dialog. Args: text (str): A textual representation of the user action. act_type (UserActionType): The type of the user action. slot (str): The slot to which the user action refers - might be ``None`` depending on the user action. value (str): The value to which the user action refers - might be ``None`` depending on the user action. score (float): A value from 0 (not important) to 1 (important) indicating how important the information is for the belief state. \"\"\" self . text = text self . type = act_type self . slot = slot self . value = value self . score = score","title":"__init__()"},{"location":"api/utils/#adviser.utils.useract.UserAct.__repr__","text":"Source code in adviser/utils/useract.py 67 68 69 def __repr__ ( self ): return \"UserAct( \\\" {} \\\" , {} , {} , {} , {} )\" . format ( self . text , self . type , self . slot , self . value , self . score )","title":"__repr__()"},{"location":"api/utils/#adviser.utils.useract.UserActionType","text":"The type for a user action as used in :class: UserAct .","title":"UserActionType"},{"location":"api/utils/#adviser.utils.useract.UserActionType.__new__","text":"Source code in adviser/utils/useract.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/utils/#adviser.utils.userstate","text":"This module provides the UserState class.","title":"userstate"},{"location":"api/utils/#adviser.utils.userstate.EmotionType","text":"The type for a user emotion as used in :class: UserState .","title":"EmotionType"},{"location":"api/utils/#adviser.utils.userstate.EmotionType.__new__","text":"Source code in adviser/utils/userstate.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/utils/#adviser.utils.userstate.EngagementType","text":"The type for a user engagement as used in :class: UserState .","title":"EngagementType"},{"location":"api/utils/#adviser.utils.userstate.EngagementType.__new__","text":"Source code in adviser/utils/userstate.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def __new__ ( cls , value ): # all enum instances are actually created during class construction # without calling this method; this method is called by the metaclass' # __call__ (i.e. Color(3) ), and by pickle if type ( value ) is cls : # For lookups like Color(Color.RED) return value # by-value search for a matching enum member # see if it's in the reverse mapping (for hashable values) try : return cls . _value2member_map_ [ value ] except KeyError : # Not found, no need to do long O(n) search pass except TypeError : # not there, now do long search -- O(n) behavior for member in cls . _member_map_ . values (): if member . _value_ == value : return member # still not found -- try _missing_ hook try : exc = None result = cls . _missing_ ( value ) except Exception as e : exc = e result = None if isinstance ( result , cls ): return result else : ve_exc = ValueError ( \" %r is not a valid %s \" % ( value , cls . __name__ )) if result is None and exc is None : raise ve_exc elif exc is None : exc = TypeError ( 'error in %s ._missing_: returned %r instead of None or a valid member' % ( cls . __name__ , result ) ) exc . __context__ = ve_exc raise exc","title":"__new__()"},{"location":"api/utils/#adviser.utils.userstate.UserState","text":"The representation of a user state. Can be accessed like a dictionary","title":"UserState"},{"location":"api/utils/#adviser.utils.userstate.UserState.__contains__","text":"Source code in adviser/utils/userstate.py 67 68 def __contains__ ( self , val ): # assume return val in self . _history [ - 1 ]","title":"__contains__()"},{"location":"api/utils/#adviser.utils.userstate.UserState.__getitem__","text":"Source code in adviser/utils/userstate.py 49 50 51 52 53 54 55 56 def __getitem__ ( self , val ): # for indexing # if used with numbers: int (e.g. state[-2]) or slice (e.g. state[3:6]) if isinstance ( val , int ) or isinstance ( val , slice ): return self . _history [ val ] # interpret the number as turn # if used with strings (e.g. state['beliefs']) elif isinstance ( val , str ): # take the current turn's belief state return self . _history [ - 1 ][ val ]","title":"__getitem__()"},{"location":"api/utils/#adviser.utils.userstate.UserState.__init__","text":"Source code in adviser/utils/userstate.py 46 47 def __init__ ( self ): self . _history = [ self . _init_userstate ()]","title":"__init__()"},{"location":"api/utils/#adviser.utils.userstate.UserState.__iter__","text":"Source code in adviser/utils/userstate.py 58 59 def __iter__ ( self ): return iter ( self . _history [ - 1 ])","title":"__iter__()"},{"location":"api/utils/#adviser.utils.userstate.UserState.__len__","text":"Source code in adviser/utils/userstate.py 64 65 def __len__ ( self ): return len ( self . _history )","title":"__len__()"},{"location":"api/utils/#adviser.utils.userstate.UserState.__repr__","text":"Source code in adviser/utils/userstate.py 70 71 def __repr__ ( self ): return str ( self . _history [ - 1 ])","title":"__repr__()"},{"location":"api/utils/#adviser.utils.userstate.UserState.__setitem__","text":"Source code in adviser/utils/userstate.py 61 62 def __setitem__ ( self , key , val ): self . _history [ - 1 ][ key ] = val","title":"__setitem__()"},{"location":"api/utils/#adviser.utils.userstate.UserState.start_new_turn","text":"ONLY to be called by the user state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules Source code in adviser/utils/userstate.py 73 74 75 76 77 78 79 80 def start_new_turn ( self ): \"\"\" ONLY to be called by the user state tracker at the begin of each turn, to ensure the correct history can be accessed correctly by other modules \"\"\" # copy last turn's dict self . _history . append ( copy . deepcopy ( self . _history [ - 1 ]))","title":"start_new_turn()"},{"location":"tutorials/advanced/","text":"You can find the Jupyter Notebook in our GitHub repository . Advanced Topics With ADVISER 2.0 \u00b6 Now that you have covered how to work with the existing core services of the ADVISER 2.0 Toolkit, let's discuss some of the more advanced features, such as running a distributed system, emotion detection, how create a new domain, and how to add new services Creating a New Domain \u00b6 Database \u00b6 In order to create a new domain, you need an SQLite database with the following properties: * it contains a single table * each row represents an entity in the database * each column in the table represents a slot (entity property such as \"name\", \"color\", \"last_known_location\", etc) * binary slots should only have values true or false As a note, it is also possible to create a new domain with an API backend instead of a fixed database, but in this case, it may not be possible to use the ontology generation tool shown in the next session. Ontology \u00b6 Once you have a database, the next step is to create an ontology for the new domain. This can be done using the ontology creation tool provided with ADVISER 2.0. The tool is located in the folder resources/ontologies/create_ontology.py To use the tool, open a terminal and navigate to the folder with the ontology creation tool. The tool can then be called by typing the following command: python3 create_ontology.py path/to/your/database/YourDomainName.db As a fist step, you must choose the table you want to use (we will use the ImsCourses database located in resources/databases for demonstration purposes). The interface for this tool can be seen in the image below: Afterwards, you are asked to name your new domain and then apply the appropriate labels to the different slots. Possible slots will be shown and can be navigated through with the arrow keys. The space bar can be used to select any slots a proposed label applies to. An active selection is indicated by a blue circle. Possible slot types are * Informable: information the user can inform the system about * System requestable: information the system can actively ask the user for * Requestable: information the user can request from the system When the system asks you to identify the primary key, this means you should reference the column in your database which uniquely discriminates all database entries (preferably in a human-readable way, not just an index) - in case of the IMSCourses domain, this is the name of the course. Selection screens for slots / values look like this: And the end output should be a file in JSON format like the one shown in the excerpt below: As a final step the system will ask if you want to copy the database, if your database is not already located inside the folder resources/databases , you should select \"yes\" to this operation, so a copy of your database is where it will be expected when creating a new Domain object. After the tool terminates successfully, check that the following two files were created inside the folders resources/databases and resources/ontologies : * [YourDomainName].db * [YourDomainName].json Domain Object \u00b6 Once you have your ontology and database, you can create a Domain object for your new domain, as seen in the previous tutorial: from utils.domain.jsonlookupdomain import JSONLookupDomain your_domain_instance = JSONLookupDomain ( name = 'YourDomainName' , json_ontology_file = 'resources/databases/YourDomainName.json' , sqllite_db_file = 'resources/databases/YourDomainName.db' ) You can than use the object to instantiate the modules that constitute your dialog graph. NLU and NLG \u00b6 Another important thing to remember is to create NLU regexes and NLG templates for your new domain, see the previous tutorial if you have questions on this process. Policy \u00b6 In some cases, a new domain may require a new policy and new user_acts or sys_acts if the new domain requires functionality not provided by the original policy, it may be neccessary to expand the list of user or system acts. For reference, these lists are shown in the previous sections under the NLU and Policy sections respectively. If new acts are added, the policy must be expanded to be able to accept the new user acts as input and to generate the new system actions as output. This can be done by inheriting from the current policy. Creating a New Service \u00b6 As we saw in Tutorial 2, all of the modules in the ADVISER 2.0 toolkit are children of the Service class. This means in order to create a new module, you need to create a new service. In the previous tutorial we showed an example of how to do this for a simple case. In this section, we will go into more depth on dialog system specifics to consider when creating a service. Inheriting from the service class \u00b6 Step one is always to inherit from the service class. Doing this means that your new service requires a domain argument on instantiation, but also that it can take the following optional arguments (which you saw in Tutorial 3): * sub_topic_domains * pub_topic_domains which allow users to overwrite subsribe/publish topics on instantiation. This can be useful in some cases when combining domain specific services with non domain specific services. Determine what methods need to be decorated \u00b6 The next important step is to consider which methods in your new service should be decorated and what topics it should subscibe to/publish. As a general rule, only methods which will directly interact with other services need to be decorated. If all communication happens inside of a class, normal class methods are sufficient. Another important note, when decorating a method make sure that the list of subscribe topics matches the list of method arguments and that you have checked the topics you subsrcibe to will be published by another method and the topics which you publish will be subscribed to by another method. Managing Dialog-Dependent State \u00b6 Another important concern is dialog dependent state. That is, information which gets tracked within a service over the course of a dialog, but should be reset between dialogs. If you want to initialize/reset per-dialog state or want to perform other actions before the first / after the last dialog turn, you can overwrite the dialog_start and dialog_end methods provided by the Service class. These will automatically be called before the start and end of a dialog, so you do not need to worry about decorating them extra. In fact, since topics don't have any guarantees on order of delivery using these methods is preferable to decorating because the methods are guranteed to be called before the first dialog turn and after the last one respectively. Since our dialog system might be migrated to multi-session support at some point, we consider it best practice to initialize/reset all dialog-dependent state not in the constructor but rather inside these two methods. Adding Task-specific Feature Extraction \u00b6 Certain tasks, such as emotion recognition or backchanneling, require specific acoustic and/or visual features as input (see the following sections). To retain maximum modularity, we recommend that feature extraction is separated from the actual task. Therefore, in this section we look at an example of a speech feature extraction module which subscribes to an audio recording and publishes a feature vector. The feature extractor is a service, i.e. it inherits from the service class: from services.service import PublishSubscribe from services.service import Service from torchaudio.compliance import kaldi class SpeechFeatureExtractor ( Service ): \"\"\"Simple feature extractor which uses torchaudio to compute MFCCs.\"\"\" def __init__ ( self ): Service . __init__ ( self ) Now, let's create a simple decorated method for feature extraction. Note, in the current ADVISER implementation, 'speech_in' is a tuple that consists of a numpy array representing the audio recording and the sampling rate: (data, sample_rate). This way, everything can be handled in memory without needing to write and delete files. @PublishSubscribe ( sub_topics = [ 'speech_in' ], pub_topics = [ 'mfcc_features' ]) def speech_to_features ( self , speech_in ): features = kaldi . mfcc ( speech_in [ 0 ], sample_frequency = speech_in [ 1 ]) return { 'mfcc_features' : features } For the sake of illustration, this example uses torchaudio to extract MFCC features . In the current ADVISER implementation, we use the openSMILE toolkit to extract MFCCs and addtionally, GeMAPS features, which are used for emotion recognition. Adding Emotion Recognition \u00b6 The current implementation of ADVISER 2.0 provides a basic module for emotion recognition from speech features. The prerequisites for this module are: * A pre-trained model for emotion prediction * The corresponding acoustic features as input ( see section above ) Implementation and training of a machine learning model for emotion prediction is not part of this tutorial. However, in the current ADVISER 2.0 system, we provide basic multi-layer perceptron models which are trained on the MSP-IMPROV database [1]. In the following code, we see an example emotion recognition class. As with any other module, it inherits from the Service class and uses the PublishSubscribe decorator to communicate with other services. In this example, there is only one model for arousal level prediction involved. Since emotions can be represented in different ways (e.g. arousal/valence levels or categories like 'angry', 'happy', 'sad'), the published topic 'emotion' contains a dictionary which can hold the different predicted representations. class EmotionRecognition ( Service ): def __init__ ( self ): Service . __init__ ( self ) self . emotion_dir = os . path . dirname ( os . path . abspath ( __file__ )) model_path = << file path to emotion recognition models >> self . arousal_model = joblib . load ( os . path . join ( model_path , 'mlp_audio_arousal.joblib' )) @PublishSubscribe ( sub_topics = [ \"gemaps_features\" ], pub_topics = [ \"emotion\" ]) def predict_from_audio ( self , gemaps_features ): arousal_prediction = self . arousal_model . predict_proba ( gemaps_features ) return { 'emotion' : { 'arousal' : arousal_prediction }} Note, this emotion recognition module is a very basic implementation for illustration purposes. It can easily be improved by inserting more sophisticated machine learning models or by adding video features to perform multimodal emotion recognition. [1] Busso, Carlos, et al. \"MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception.\" IEEE Transactions on Affective Computing 8.1 (2016): 67-80. Using Emotion in a Dialog System \u00b6 The ADVISER 2.0 currently provides a UserStateTracker service which keeps track of the detected user emotion and user engagement level. This module works in conjunction with a naive EmotionPolicy service to map user emotion to a system emotional response. Currently this is done with a direct mapping from the recognized user emotion to the same emotion for the system response. This \"system emotion\" can then be used bye the HandcraftedEmotionNLG service to select an affective NLG template in order to react to user emotion. This system can be seen below. This direct mapping is obviously highly simplistic and may be expanded in future versions of ADVISER. Adding Backchanneling \u00b6 Backchannel prediction \u00b6 ADVISER 2.0 comes with an acoustic backchannel module that makes use of a pre-trained backchanneler model and MFCC features as input ( see section above ). The backchanneller implementation consists of a convolutional neural network model based on [1] and trained on the Switchboard benchmark dataset [2]. As input, it receives 13 Mel-frequency-cepstral coefficients from the user\u2019s speech signal. The model assigns one of three categories from the proactive backchanneling theory [3] to each user utterance {no-backchannel, backchannel-continuer and backchannel-assessment}. The predicted category is used to add the backchannel realization, such as Okay or Um-hum, at the begining the next system response. In the our project, you can find two python files: acoustic_backchanneller.py (Definition of the backchannelling module) PytorchAcousticBackchanneler.py (PyTorch implementation that loads the pretrained model for prediction) We present a code extract from the class acoustic backchanneller (service). As any other module, it inherits from the Service class and uses the PublishSubscribe decorator to communicate with other services. class AcousticBackchanneller ( Service ): def __init__ ( self ): Service . __init__ ( self ) self . speech_in_dir = os . path . dirname ( os . path . abspath ( __file__ )) + '/' self . trained_model_path = os . path . join ( 'resources' , 'models' , 'backchannel' ) + '/pytorch_acoustic_backchanneller.pt' self . load_model () def load_model ( self ): self . model = PytorchAcousticBackchanneler () self . model . load_state_dict ( torch . load ( self . trained_model_path )) self . model . eval () @PublishSubscribe ( sub_topics = [ 'mfcc_features' ], pub_topics = [ \"predicted_BC\" ]) def backchannel_prediction ( self , mfcc_features : np . array ): \"\"\"Takes temporary user utterance wav file and extracts features from it.\"\"\" scaler = preprocessing . StandardScaler () mfcc_features = scaler . fit_transform ( mfcc_features ) input_splits = self . split_input_data ( mfcc_features ) prediction = self . model ( input_splits ) . detach () . numpy () . argmax ( axis = 1 ) # Returning the majority, unless a BC appears, # class_int_mapping = {0: b'no_bc', 1: b'assessment', 2: b'continuer'} if len ( set ( prediction )) == 1 : return { 'predicted_BC' : prediction [ 0 ]} elif 1 in prediction and 2 in prediction : ones = len ( prediction [ prediction == 1 ]) twos = len ( prediction [ prediction == 2 ]) return { 'predicted_BC' : 1 if ones > twos else 2 } else : return { 'predicted_BC' : 1 if 1 in prediction else 2 } This backchanneller only makes use of acoustic features, however, a more complex module can be implemented, so that it can also profit from ASR trancriptions as shown in [2]. Integrating backchannel to the system's response \u00b6 After the backchannel prediction is done, the corresponding backchannel realization should be added to the system response. For simplicity, we decided to add it at the beginning of the system response already generated by the NLG module. This code can be found in the class BackchannelHandcraftedNLG(HandcraftedNLG) . Here we have a sample of the most relevant code. class BackchannelHandcraftedNLG ( HandcraftedNLG ): def __init__ ( self , domain : Domain , sub_topic_domains : Dict [ str , str ] = {}, template_file : str = None , logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): HandcraftedNLG . __init__ ( self , domain , template_file = None , logger = DiasysLogger (), template_file_german = None , language = None , sub_topic_domains = sub_topic_domains ) # class_int_mapping = {0: b'no_bc', 1: b'assessment', 2: b'continuer'} self . backchannels = { 0 : '' , 1 : 'Okay. ' , 2 : 'Um-hum. ' } @PublishSubscribe ( sub_topics = [ \"sys_act\" , 'predicted_BC' ], pub_topics = [ \"sys_utterance\" ]) def generate_system_utterance ( self , sys_act : SysAct = None , predicted_BC : int = None ) -> dict ( sys_utterance = str ): rule_found = True message = \"\" try : message = self . templates . create_message ( sys_act ) if 'Sorry' not in message : message = self . backchannels [ predicted_BC ] + message except BaseException as error : rule_found = False self . logger . error ( error ) raise ( error ) The backchanneller does not show variety in its realizations, however, this can be easily implemented if needed. [1] Daniel Ortega, Chia-Yu Li, NgocThang Vu. \"Oh,Jeez! or uh-huh?\" A listener- aware backchannel predictor on ASR transcriptions. ICASSP,2020. [2] D. Jurafsky and E. Shriberg. \u201cSwitchboard swbd-damsl shallow-discourse-function annotation coders manual.\u201d Institute of Cognitive Science Technical Report, 1997. [3] Charles Goodwin. 1986. Between and within: Alterna- tive sequential treatments of continuers and assess- ments.\" Journal of Human Studies. From Single- to Multidomain \u00b6 For more complex scenarios, it may make sense to split your dialog system into multiple domains. For example if your goal is to create a university student assistant bot. You may decide that as a start you want your system to help students find information about lecturers and help students to find out what the dining hall (Mensa) is serving. While in theory these two topics could be put together into the same domain, mensa information updates every day so accessing this through a web API is preferable to only having a fixed database. For the lecturers, however there is no web API, and this inofmration remains largely static, so a fixed database is preferable. At this point, since the data sources, and the actual topics of conversation for each topic are so different, giving each its own domain makes sense. But how do we do that? Domain Dependent Modules \u00b6 Services like the Natural Language Understanding, Belief State Tracker and Policy are domain dependent: they require domain-specific ontology knowledge (e.g. towns for weather, food names for the mensa) or database access to function. Rather than re-implementing these modules for your specific purposes, however, you can instantiate these services with the corresponding domains (one instance per domain). First we will handle importing all the modules we need and create our domain objects: # IMPORT DOMAINS AND SERVICES import sys import os from typing import List import time sys . path . insert ( 0 , os . path . abspath ( '../..' )) from services.service import Service , PublishSubscribe , RemoteService from services.nlu import HandcraftedNLU from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.policy.policy_api import HandcraftedPolicy as PolicyAPI from services.nlg import HandcraftedNLG from services.service import DialogSystem # from examples.webapi.mensa import MensaNLU from utils.domain.domain import Domain from utils.domain.jsonlookupdomain import JSONLookupDomain from utils.logger import DiasysLogger , LogLevel from examples.webapi.mensa.domain import MensaDomain from examples.webapi.mensa.nlu import MensaNLU # CREATE DOMAIN OBJECTS canteen = MensaDomain () lecturers = JSONLookupDomain ( 'ImsLecturers' ) Next let's start creating our Domain Dependent modules: * NLU is domain dependent because it needs access to different regex files depending on the domain # NLU needs domain to load correct regexe files lecturer_nlu = HandcraftedNLU ( domain = lecturers ) dining_hall_nlu = MensaNLU ( domain = canteen ) BST is domain dependent because it needs access to an ontology so it knows what the informable requestable slots it needs to track # BST needs domain to track correct informable/requestable slots lecturer_bst = HandcraftedBST ( domain = lecturers ) dining_hall_bst = HandcraftedBST ( domain = canteen ) Policy is domain dependent because it needs to know which database to query to determine the next system action # Policy needs domain to access the correct database lecturer_policy = HandcraftedPolicy ( domain = lecturers ) dining_hall_policy = PolicyAPI ( domain = canteen ) NLG is domain dependent because it needs to access the correct template files to generate natural language output # NLG needs to access domain dependent template files lecturer_nlg = HandcraftedNLG ( domain = lecturers ) dining_hall_nlg = HandcraftedNLG ( domain = canteen ) Domain Independent Modules \u00b6 In comparison to the previously shown services, there are also modules in a dialog system that are domain independent, such as ASR and TTS or console input and console output. Regardless of the domain, these modules do the same thing: take in a user input and pass it out as a string. Since these services do not change, domain-independent services only need to be instantiated once. An example using console input and console output modules is shown below. Where previously we passed in a domain, here since they are domain independent, we pass in an empty string. from services.hci.console import ConsoleInput , ConsoleOutput user_in = ConsoleInput ( domain = \"\" ) user_out = ConsoleOutput ( domain = \"\" ) Domain Tracker Module \u00b6 Once you have created all of domain dependent and independent modules, you will need some way to decide which domain should be active at a given time. That is where the DomainTracker module comes in. The goal of this module is to take in a domain independent user utterance and map it to the correct domain as shown in the image below: The domain tracker is something between domain dependent and domain independent. On instantiation, it takes in a list of possible domains which it can forward a user utterance to, although there is no need to create a new DomainTracker for each domain in the dialog system. In the current implementation, this module relies on keywords matching, inspired by commercial systems, to determine which domain should be active at any given time. This means that at any time, at most one domain will be active. However a more advanced domain tracker could be implemented to replace this in the future and might even be able to support dialog in multiple domains at the same time. To determine which domain should be active, the DomainTracker follows a series of simple rules shown in the image below: If there is only one domain, that domain is always active. If there are more domains, the tracker checks if it is the first turn. If so the system issues a greeting to the user and tells them what domains the dialog system can talk about. If not, the domain tracker checks for domain keywords. If any appear in the user utterance, the first to appear is the domain selected, as the dialog system is not yet capable of handling multiple domains active during the same turn. If there is no keyword, the tracker checks to see if a domain was active the previous turn, if yes, it is assumed that that domain remains active. If there was no active domain in the previous turn, the tracker checks to see if the user said 'bye', if so the tracker will also say 'bye'. If not, the tracker then reminds the user of the domains it is capable of tracking. This can also be seen in the code below: \"\"\" The domain tracker has to know all domains our application should be able to handle, but not not append domain names to topics by default, so it stores a list of domains, but doesn't forward any of them to the service base class constructor: \"\"\" class DomainTracker ( Service ): def __init__ ( self , domains : List [ Domain ], greet_on_first_turn : bool = False ): Service . __init__ ( self , domain = \"\" ) self . domains = domains self . current_domain = None self . greet_on_first_turn = greet_on_first_turn \"\"\" Since service output relies on per-dialog state (turn count, currently active domain), it needs to initialize this state before each new dialog:\"\"\" def dialog_start ( self ): self . turn = 0 \"\"\" Furthermore, it needs to subscribe to user utterances and forward them (`gen_user_utterance` is short for `generic user utterance`) as a domain-dependent user utterance, or, if no domain is active, publish a system utterance listing all available domains: \"\"\" @PublishSubscribe ( sub_topics = [ \"gen_user_utterance\" ], pub_topics = [ \"user_utterance\" , \"sys_utterance\" ]) def select_domain ( self , gen_user_utterance : str = None ) -> dict ( user_utterance = str ): self . turn += 1 if self . turn == 1 and self . greet_on_first_turn : return { 'sys_utterance' : \"Hello, please let me know how I can help you, I can discuss \" + f \"the following domains: { self . domains_to_str () } .\" } # if there is only a single domain, simply route the message forward if len ( self . domains ) == 1 : self . current_domain = self . domains [ 0 ] # make sure the utterance is lowercase if there is one user_utterance = gen_user_utterance if user_utterance : user_utterance = gen_user_utterance . lower () # perform keyword matching to see if any domains are explicitely made active active_domains = [ d for d in self . domains if d . get_keyword () in user_utterance ] # Even if no domain has been specified, we should be able to exit if \"bye\" in user_utterance and not self . current_domain : return { \"sys_utterance\" : \"Thank you, goodbye.\" } # if there are active domains, use the first one elif active_domains : out_key = f \"user_utterance/ { active_domains [ 0 ] . get_domain_name () } \" self . current_domain = active_domains [ 0 ] return { out_key : user_utterance } # if no domain is explicitely mentioned, assume the last one is still active elif self . current_domain : out_key = f \"user_utterance/ { self . current_domain . get_domain_name () } \" return { out_key : user_utterance } # Otherwise ask the user what domain they want else : return { \"sys_utterance\" : \"Hello, please let me know how I can help you, I can discuss \" + f \"the following domains: { self . domains_to_str () } .\" } \"\"\" Convert list of domains to a string for console output \"\"\" def domains_to_str ( self ): if len ( self . domains ) == 1 : return self . domains [ 0 ] . get_display_name () elif len ( self . domains ) == 2 : return \" and \" . join ([ d . get_display_name () for d in self . domains ]) else : return \", \" . join ([ d . get_display_name () for d in self . domains ][: - 1 ]) + f \", and { self . domains [ - 1 ] . get_display_name () } \" As a note, once the DomainTracker has selected a domain, this is appended to the output dictionary so that only modules of that same domain will receive the published message. Creating a Domain Tracker \u00b6 The code for creating a DomainTracker is nearly the same as any other module. However instead of taking in a domain argument as a string or Domain object, the DomainTracker takes in a domains argument which must be a list of domain objects domain_tracker = DomainTracker ( domains = [ lecturers , canteen ]) Putting it All Together \u00b6 The last thing left to do is now to combine all of this into one single dialog system and run it! * Sending an emty message to gen_user_utterance will trigger the domain tracker * The domian tracker will then see that we're on the first turn with no active domain, thus generating a system message which will inform the user of all available domains (lecturers and canteen) ds = DialogSystem ( services = [ user_in , user_out , # interfaces domain_tracker , lecturer_nlu , dining_hall_nlu , # NLUs lecturer_bst , dining_hall_bst , # BSTs lecturer_policy , dining_hall_policy , # Policies lecturer_nlg , dining_hall_nlg , # NLGs ]) ds . run_dialog ({ 'gen_user_utterance' : \"\" }) Alternatively, you can have a look at the run_chat.py file * calling python run_chat lecturers mensa from your terminal in the ADVISER main folder will do exactly the same * you can specify more options, e.g. for using ASR, TTS, etc. Running A Distributed Dialog System \u00b6 If you're relying on computationally intense services, e.g. ASR or TTS modules, ADVISER 2.0 provdies a way to split up dialog processing so these services can be run remotely, e.g. on more powerful machines, while the rest of the dialog system remains local. The easiest way to do this is to use ssh port-forwarding , but you can also specify arbitrary IP-adresses (you might have to adapt your router settings for this approach). Therefore, we will use the port-forwarding approach in the following description. The good news first: running some modules remotely, does not require any large changes! 1. The first change is that you will need to pass a unique identifier to your service constructor; so rather than just taking in a domain your service will also need to take in an identifier as shown below: class ConcatenateServiceWithDomain ( Service ): \"\"\" new here: identifier argument\"\"\" def __init__ ( self , domain : str = \"mydomain\" , identifier : str = None ): \"\"\" NEW: domain name! \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) \"\"\" nothing changes here \"\"\" @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } Next create an instance of your service with an identifier of your choosing in a script on your remote machine ; The only important thing when choosing the identifier is that you don't use the same identifier for two different service instances. But it is recommended that the name is at least somewhat descriptive. In the same script, call this services's run_standalone() method. This will kickoff a registration routine with the dialog system instance we're going to run on your local machine concatenate_service = ConcatenateServiceWithDomain ( identifier = \"myconcatenateservice1\" ) concatenate_service . run_standalone () On your local machine which is going to run the dialog system, create a placeholder, RemoteService (This is really just a dummy class storing the identifier). This allows the remote service to register with the dialog system; important is that you use the same identifier here as you used to create the remote service Then, go ahead and instantiate your dialog system, providing the remote service intance instead of the real service to the service list # create dummy service remote_concatenate_service = RemoteService ( identifier = \"myconcatenateservice1\" ) # pass dummy service in as part of dialog system ds_with_remote = DialogSystem ( services = [ remote_concatenate_service , # remote service placeholder print_service ]) # local print service From here on, you can call your dialog system with the usual run_dialog(...) routine But to be able to connect to the remote machine, you need to forward the required ports. For this, on your local machine, call port forwarding for * subscriber port (default: 65533) * publisher port (default: 65534) * remote register port (default: 65535) ssh -fNR 65533 :localhost:65533 adress.to.your.remote@machine ssh -fNR 65534 :localhost:65534 adress.to.your.remote@machine ssh -fNR 65535 :localhost:65535 adress.to.your.remote@machine If those ports are already in use on your machine, you may change them explicitly in the Service AND DialogSystem constructors, just be consistent. See the docstrings on these classes for more information. Once you start the concatenate_service.run_standalone() on your remote machine and the ds_with_remote.run_dialog(...) on your local machine, you will see notifications on the terminal giving you information about the connection state between your local and remote services. If you want to have a look at another example (with both services running on the same machine, so you don't need to call ssh for this), look at the run_chat.py file with --gui option enabled: this starts a webserver as a remote service connecting to the dialog system.","title":"Advanced"},{"location":"tutorials/advanced/#advanced-topics-with-adviser-20","text":"Now that you have covered how to work with the existing core services of the ADVISER 2.0 Toolkit, let's discuss some of the more advanced features, such as running a distributed system, emotion detection, how create a new domain, and how to add new services","title":"Advanced Topics With ADVISER 2.0"},{"location":"tutorials/advanced/#creating-a-new-domain","text":"","title":"Creating a New Domain"},{"location":"tutorials/advanced/#database","text":"In order to create a new domain, you need an SQLite database with the following properties: * it contains a single table * each row represents an entity in the database * each column in the table represents a slot (entity property such as \"name\", \"color\", \"last_known_location\", etc) * binary slots should only have values true or false As a note, it is also possible to create a new domain with an API backend instead of a fixed database, but in this case, it may not be possible to use the ontology generation tool shown in the next session.","title":"Database"},{"location":"tutorials/advanced/#ontology","text":"Once you have a database, the next step is to create an ontology for the new domain. This can be done using the ontology creation tool provided with ADVISER 2.0. The tool is located in the folder resources/ontologies/create_ontology.py To use the tool, open a terminal and navigate to the folder with the ontology creation tool. The tool can then be called by typing the following command: python3 create_ontology.py path/to/your/database/YourDomainName.db As a fist step, you must choose the table you want to use (we will use the ImsCourses database located in resources/databases for demonstration purposes). The interface for this tool can be seen in the image below: Afterwards, you are asked to name your new domain and then apply the appropriate labels to the different slots. Possible slots will be shown and can be navigated through with the arrow keys. The space bar can be used to select any slots a proposed label applies to. An active selection is indicated by a blue circle. Possible slot types are * Informable: information the user can inform the system about * System requestable: information the system can actively ask the user for * Requestable: information the user can request from the system When the system asks you to identify the primary key, this means you should reference the column in your database which uniquely discriminates all database entries (preferably in a human-readable way, not just an index) - in case of the IMSCourses domain, this is the name of the course. Selection screens for slots / values look like this: And the end output should be a file in JSON format like the one shown in the excerpt below: As a final step the system will ask if you want to copy the database, if your database is not already located inside the folder resources/databases , you should select \"yes\" to this operation, so a copy of your database is where it will be expected when creating a new Domain object. After the tool terminates successfully, check that the following two files were created inside the folders resources/databases and resources/ontologies : * [YourDomainName].db * [YourDomainName].json","title":"Ontology"},{"location":"tutorials/advanced/#domain-object","text":"Once you have your ontology and database, you can create a Domain object for your new domain, as seen in the previous tutorial: from utils.domain.jsonlookupdomain import JSONLookupDomain your_domain_instance = JSONLookupDomain ( name = 'YourDomainName' , json_ontology_file = 'resources/databases/YourDomainName.json' , sqllite_db_file = 'resources/databases/YourDomainName.db' ) You can than use the object to instantiate the modules that constitute your dialog graph.","title":"Domain Object"},{"location":"tutorials/advanced/#nlu-and-nlg","text":"Another important thing to remember is to create NLU regexes and NLG templates for your new domain, see the previous tutorial if you have questions on this process.","title":"NLU and NLG"},{"location":"tutorials/advanced/#policy","text":"In some cases, a new domain may require a new policy and new user_acts or sys_acts if the new domain requires functionality not provided by the original policy, it may be neccessary to expand the list of user or system acts. For reference, these lists are shown in the previous sections under the NLU and Policy sections respectively. If new acts are added, the policy must be expanded to be able to accept the new user acts as input and to generate the new system actions as output. This can be done by inheriting from the current policy.","title":"Policy"},{"location":"tutorials/advanced/#creating-a-new-service","text":"As we saw in Tutorial 2, all of the modules in the ADVISER 2.0 toolkit are children of the Service class. This means in order to create a new module, you need to create a new service. In the previous tutorial we showed an example of how to do this for a simple case. In this section, we will go into more depth on dialog system specifics to consider when creating a service.","title":"Creating a New Service"},{"location":"tutorials/advanced/#inheriting-from-the-service-class","text":"Step one is always to inherit from the service class. Doing this means that your new service requires a domain argument on instantiation, but also that it can take the following optional arguments (which you saw in Tutorial 3): * sub_topic_domains * pub_topic_domains which allow users to overwrite subsribe/publish topics on instantiation. This can be useful in some cases when combining domain specific services with non domain specific services.","title":"Inheriting from the service class"},{"location":"tutorials/advanced/#determine-what-methods-need-to-be-decorated","text":"The next important step is to consider which methods in your new service should be decorated and what topics it should subscibe to/publish. As a general rule, only methods which will directly interact with other services need to be decorated. If all communication happens inside of a class, normal class methods are sufficient. Another important note, when decorating a method make sure that the list of subscribe topics matches the list of method arguments and that you have checked the topics you subsrcibe to will be published by another method and the topics which you publish will be subscribed to by another method.","title":"Determine what methods need to be decorated"},{"location":"tutorials/advanced/#managing-dialog-dependent-state","text":"Another important concern is dialog dependent state. That is, information which gets tracked within a service over the course of a dialog, but should be reset between dialogs. If you want to initialize/reset per-dialog state or want to perform other actions before the first / after the last dialog turn, you can overwrite the dialog_start and dialog_end methods provided by the Service class. These will automatically be called before the start and end of a dialog, so you do not need to worry about decorating them extra. In fact, since topics don't have any guarantees on order of delivery using these methods is preferable to decorating because the methods are guranteed to be called before the first dialog turn and after the last one respectively. Since our dialog system might be migrated to multi-session support at some point, we consider it best practice to initialize/reset all dialog-dependent state not in the constructor but rather inside these two methods.","title":"Managing Dialog-Dependent State"},{"location":"tutorials/advanced/#adding-task-specific-feature-extraction","text":"Certain tasks, such as emotion recognition or backchanneling, require specific acoustic and/or visual features as input (see the following sections). To retain maximum modularity, we recommend that feature extraction is separated from the actual task. Therefore, in this section we look at an example of a speech feature extraction module which subscribes to an audio recording and publishes a feature vector. The feature extractor is a service, i.e. it inherits from the service class: from services.service import PublishSubscribe from services.service import Service from torchaudio.compliance import kaldi class SpeechFeatureExtractor ( Service ): \"\"\"Simple feature extractor which uses torchaudio to compute MFCCs.\"\"\" def __init__ ( self ): Service . __init__ ( self ) Now, let's create a simple decorated method for feature extraction. Note, in the current ADVISER implementation, 'speech_in' is a tuple that consists of a numpy array representing the audio recording and the sampling rate: (data, sample_rate). This way, everything can be handled in memory without needing to write and delete files. @PublishSubscribe ( sub_topics = [ 'speech_in' ], pub_topics = [ 'mfcc_features' ]) def speech_to_features ( self , speech_in ): features = kaldi . mfcc ( speech_in [ 0 ], sample_frequency = speech_in [ 1 ]) return { 'mfcc_features' : features } For the sake of illustration, this example uses torchaudio to extract MFCC features . In the current ADVISER implementation, we use the openSMILE toolkit to extract MFCCs and addtionally, GeMAPS features, which are used for emotion recognition.","title":"Adding Task-specific Feature Extraction"},{"location":"tutorials/advanced/#adding-emotion-recognition","text":"The current implementation of ADVISER 2.0 provides a basic module for emotion recognition from speech features. The prerequisites for this module are: * A pre-trained model for emotion prediction * The corresponding acoustic features as input ( see section above ) Implementation and training of a machine learning model for emotion prediction is not part of this tutorial. However, in the current ADVISER 2.0 system, we provide basic multi-layer perceptron models which are trained on the MSP-IMPROV database [1]. In the following code, we see an example emotion recognition class. As with any other module, it inherits from the Service class and uses the PublishSubscribe decorator to communicate with other services. In this example, there is only one model for arousal level prediction involved. Since emotions can be represented in different ways (e.g. arousal/valence levels or categories like 'angry', 'happy', 'sad'), the published topic 'emotion' contains a dictionary which can hold the different predicted representations. class EmotionRecognition ( Service ): def __init__ ( self ): Service . __init__ ( self ) self . emotion_dir = os . path . dirname ( os . path . abspath ( __file__ )) model_path = << file path to emotion recognition models >> self . arousal_model = joblib . load ( os . path . join ( model_path , 'mlp_audio_arousal.joblib' )) @PublishSubscribe ( sub_topics = [ \"gemaps_features\" ], pub_topics = [ \"emotion\" ]) def predict_from_audio ( self , gemaps_features ): arousal_prediction = self . arousal_model . predict_proba ( gemaps_features ) return { 'emotion' : { 'arousal' : arousal_prediction }} Note, this emotion recognition module is a very basic implementation for illustration purposes. It can easily be improved by inserting more sophisticated machine learning models or by adding video features to perform multimodal emotion recognition. [1] Busso, Carlos, et al. \"MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception.\" IEEE Transactions on Affective Computing 8.1 (2016): 67-80.","title":"Adding Emotion Recognition"},{"location":"tutorials/advanced/#using-emotion-in-a-dialog-system","text":"The ADVISER 2.0 currently provides a UserStateTracker service which keeps track of the detected user emotion and user engagement level. This module works in conjunction with a naive EmotionPolicy service to map user emotion to a system emotional response. Currently this is done with a direct mapping from the recognized user emotion to the same emotion for the system response. This \"system emotion\" can then be used bye the HandcraftedEmotionNLG service to select an affective NLG template in order to react to user emotion. This system can be seen below. This direct mapping is obviously highly simplistic and may be expanded in future versions of ADVISER.","title":"Using Emotion in a Dialog System"},{"location":"tutorials/advanced/#adding-backchanneling","text":"","title":"Adding Backchanneling"},{"location":"tutorials/advanced/#backchannel-prediction","text":"ADVISER 2.0 comes with an acoustic backchannel module that makes use of a pre-trained backchanneler model and MFCC features as input ( see section above ). The backchanneller implementation consists of a convolutional neural network model based on [1] and trained on the Switchboard benchmark dataset [2]. As input, it receives 13 Mel-frequency-cepstral coefficients from the user\u2019s speech signal. The model assigns one of three categories from the proactive backchanneling theory [3] to each user utterance {no-backchannel, backchannel-continuer and backchannel-assessment}. The predicted category is used to add the backchannel realization, such as Okay or Um-hum, at the begining the next system response. In the our project, you can find two python files: acoustic_backchanneller.py (Definition of the backchannelling module) PytorchAcousticBackchanneler.py (PyTorch implementation that loads the pretrained model for prediction) We present a code extract from the class acoustic backchanneller (service). As any other module, it inherits from the Service class and uses the PublishSubscribe decorator to communicate with other services. class AcousticBackchanneller ( Service ): def __init__ ( self ): Service . __init__ ( self ) self . speech_in_dir = os . path . dirname ( os . path . abspath ( __file__ )) + '/' self . trained_model_path = os . path . join ( 'resources' , 'models' , 'backchannel' ) + '/pytorch_acoustic_backchanneller.pt' self . load_model () def load_model ( self ): self . model = PytorchAcousticBackchanneler () self . model . load_state_dict ( torch . load ( self . trained_model_path )) self . model . eval () @PublishSubscribe ( sub_topics = [ 'mfcc_features' ], pub_topics = [ \"predicted_BC\" ]) def backchannel_prediction ( self , mfcc_features : np . array ): \"\"\"Takes temporary user utterance wav file and extracts features from it.\"\"\" scaler = preprocessing . StandardScaler () mfcc_features = scaler . fit_transform ( mfcc_features ) input_splits = self . split_input_data ( mfcc_features ) prediction = self . model ( input_splits ) . detach () . numpy () . argmax ( axis = 1 ) # Returning the majority, unless a BC appears, # class_int_mapping = {0: b'no_bc', 1: b'assessment', 2: b'continuer'} if len ( set ( prediction )) == 1 : return { 'predicted_BC' : prediction [ 0 ]} elif 1 in prediction and 2 in prediction : ones = len ( prediction [ prediction == 1 ]) twos = len ( prediction [ prediction == 2 ]) return { 'predicted_BC' : 1 if ones > twos else 2 } else : return { 'predicted_BC' : 1 if 1 in prediction else 2 } This backchanneller only makes use of acoustic features, however, a more complex module can be implemented, so that it can also profit from ASR trancriptions as shown in [2].","title":"Backchannel prediction"},{"location":"tutorials/advanced/#integrating-backchannel-to-the-systems-response","text":"After the backchannel prediction is done, the corresponding backchannel realization should be added to the system response. For simplicity, we decided to add it at the beginning of the system response already generated by the NLG module. This code can be found in the class BackchannelHandcraftedNLG(HandcraftedNLG) . Here we have a sample of the most relevant code. class BackchannelHandcraftedNLG ( HandcraftedNLG ): def __init__ ( self , domain : Domain , sub_topic_domains : Dict [ str , str ] = {}, template_file : str = None , logger : DiasysLogger = DiasysLogger (), template_file_german : str = None , language : Language = None ): HandcraftedNLG . __init__ ( self , domain , template_file = None , logger = DiasysLogger (), template_file_german = None , language = None , sub_topic_domains = sub_topic_domains ) # class_int_mapping = {0: b'no_bc', 1: b'assessment', 2: b'continuer'} self . backchannels = { 0 : '' , 1 : 'Okay. ' , 2 : 'Um-hum. ' } @PublishSubscribe ( sub_topics = [ \"sys_act\" , 'predicted_BC' ], pub_topics = [ \"sys_utterance\" ]) def generate_system_utterance ( self , sys_act : SysAct = None , predicted_BC : int = None ) -> dict ( sys_utterance = str ): rule_found = True message = \"\" try : message = self . templates . create_message ( sys_act ) if 'Sorry' not in message : message = self . backchannels [ predicted_BC ] + message except BaseException as error : rule_found = False self . logger . error ( error ) raise ( error ) The backchanneller does not show variety in its realizations, however, this can be easily implemented if needed. [1] Daniel Ortega, Chia-Yu Li, NgocThang Vu. \"Oh,Jeez! or uh-huh?\" A listener- aware backchannel predictor on ASR transcriptions. ICASSP,2020. [2] D. Jurafsky and E. Shriberg. \u201cSwitchboard swbd-damsl shallow-discourse-function annotation coders manual.\u201d Institute of Cognitive Science Technical Report, 1997. [3] Charles Goodwin. 1986. Between and within: Alterna- tive sequential treatments of continuers and assess- ments.\" Journal of Human Studies.","title":"Integrating backchannel to the system's response"},{"location":"tutorials/advanced/#from-single-to-multidomain","text":"For more complex scenarios, it may make sense to split your dialog system into multiple domains. For example if your goal is to create a university student assistant bot. You may decide that as a start you want your system to help students find information about lecturers and help students to find out what the dining hall (Mensa) is serving. While in theory these two topics could be put together into the same domain, mensa information updates every day so accessing this through a web API is preferable to only having a fixed database. For the lecturers, however there is no web API, and this inofmration remains largely static, so a fixed database is preferable. At this point, since the data sources, and the actual topics of conversation for each topic are so different, giving each its own domain makes sense. But how do we do that?","title":"From Single- to Multidomain"},{"location":"tutorials/advanced/#domain-dependent-modules","text":"Services like the Natural Language Understanding, Belief State Tracker and Policy are domain dependent: they require domain-specific ontology knowledge (e.g. towns for weather, food names for the mensa) or database access to function. Rather than re-implementing these modules for your specific purposes, however, you can instantiate these services with the corresponding domains (one instance per domain). First we will handle importing all the modules we need and create our domain objects: # IMPORT DOMAINS AND SERVICES import sys import os from typing import List import time sys . path . insert ( 0 , os . path . abspath ( '../..' )) from services.service import Service , PublishSubscribe , RemoteService from services.nlu import HandcraftedNLU from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.policy.policy_api import HandcraftedPolicy as PolicyAPI from services.nlg import HandcraftedNLG from services.service import DialogSystem # from examples.webapi.mensa import MensaNLU from utils.domain.domain import Domain from utils.domain.jsonlookupdomain import JSONLookupDomain from utils.logger import DiasysLogger , LogLevel from examples.webapi.mensa.domain import MensaDomain from examples.webapi.mensa.nlu import MensaNLU # CREATE DOMAIN OBJECTS canteen = MensaDomain () lecturers = JSONLookupDomain ( 'ImsLecturers' ) Next let's start creating our Domain Dependent modules: * NLU is domain dependent because it needs access to different regex files depending on the domain # NLU needs domain to load correct regexe files lecturer_nlu = HandcraftedNLU ( domain = lecturers ) dining_hall_nlu = MensaNLU ( domain = canteen ) BST is domain dependent because it needs access to an ontology so it knows what the informable requestable slots it needs to track # BST needs domain to track correct informable/requestable slots lecturer_bst = HandcraftedBST ( domain = lecturers ) dining_hall_bst = HandcraftedBST ( domain = canteen ) Policy is domain dependent because it needs to know which database to query to determine the next system action # Policy needs domain to access the correct database lecturer_policy = HandcraftedPolicy ( domain = lecturers ) dining_hall_policy = PolicyAPI ( domain = canteen ) NLG is domain dependent because it needs to access the correct template files to generate natural language output # NLG needs to access domain dependent template files lecturer_nlg = HandcraftedNLG ( domain = lecturers ) dining_hall_nlg = HandcraftedNLG ( domain = canteen )","title":"Domain Dependent Modules"},{"location":"tutorials/advanced/#domain-independent-modules","text":"In comparison to the previously shown services, there are also modules in a dialog system that are domain independent, such as ASR and TTS or console input and console output. Regardless of the domain, these modules do the same thing: take in a user input and pass it out as a string. Since these services do not change, domain-independent services only need to be instantiated once. An example using console input and console output modules is shown below. Where previously we passed in a domain, here since they are domain independent, we pass in an empty string. from services.hci.console import ConsoleInput , ConsoleOutput user_in = ConsoleInput ( domain = \"\" ) user_out = ConsoleOutput ( domain = \"\" )","title":"Domain Independent Modules"},{"location":"tutorials/advanced/#domain-tracker-module","text":"Once you have created all of domain dependent and independent modules, you will need some way to decide which domain should be active at a given time. That is where the DomainTracker module comes in. The goal of this module is to take in a domain independent user utterance and map it to the correct domain as shown in the image below: The domain tracker is something between domain dependent and domain independent. On instantiation, it takes in a list of possible domains which it can forward a user utterance to, although there is no need to create a new DomainTracker for each domain in the dialog system. In the current implementation, this module relies on keywords matching, inspired by commercial systems, to determine which domain should be active at any given time. This means that at any time, at most one domain will be active. However a more advanced domain tracker could be implemented to replace this in the future and might even be able to support dialog in multiple domains at the same time. To determine which domain should be active, the DomainTracker follows a series of simple rules shown in the image below: If there is only one domain, that domain is always active. If there are more domains, the tracker checks if it is the first turn. If so the system issues a greeting to the user and tells them what domains the dialog system can talk about. If not, the domain tracker checks for domain keywords. If any appear in the user utterance, the first to appear is the domain selected, as the dialog system is not yet capable of handling multiple domains active during the same turn. If there is no keyword, the tracker checks to see if a domain was active the previous turn, if yes, it is assumed that that domain remains active. If there was no active domain in the previous turn, the tracker checks to see if the user said 'bye', if so the tracker will also say 'bye'. If not, the tracker then reminds the user of the domains it is capable of tracking. This can also be seen in the code below: \"\"\" The domain tracker has to know all domains our application should be able to handle, but not not append domain names to topics by default, so it stores a list of domains, but doesn't forward any of them to the service base class constructor: \"\"\" class DomainTracker ( Service ): def __init__ ( self , domains : List [ Domain ], greet_on_first_turn : bool = False ): Service . __init__ ( self , domain = \"\" ) self . domains = domains self . current_domain = None self . greet_on_first_turn = greet_on_first_turn \"\"\" Since service output relies on per-dialog state (turn count, currently active domain), it needs to initialize this state before each new dialog:\"\"\" def dialog_start ( self ): self . turn = 0 \"\"\" Furthermore, it needs to subscribe to user utterances and forward them (`gen_user_utterance` is short for `generic user utterance`) as a domain-dependent user utterance, or, if no domain is active, publish a system utterance listing all available domains: \"\"\" @PublishSubscribe ( sub_topics = [ \"gen_user_utterance\" ], pub_topics = [ \"user_utterance\" , \"sys_utterance\" ]) def select_domain ( self , gen_user_utterance : str = None ) -> dict ( user_utterance = str ): self . turn += 1 if self . turn == 1 and self . greet_on_first_turn : return { 'sys_utterance' : \"Hello, please let me know how I can help you, I can discuss \" + f \"the following domains: { self . domains_to_str () } .\" } # if there is only a single domain, simply route the message forward if len ( self . domains ) == 1 : self . current_domain = self . domains [ 0 ] # make sure the utterance is lowercase if there is one user_utterance = gen_user_utterance if user_utterance : user_utterance = gen_user_utterance . lower () # perform keyword matching to see if any domains are explicitely made active active_domains = [ d for d in self . domains if d . get_keyword () in user_utterance ] # Even if no domain has been specified, we should be able to exit if \"bye\" in user_utterance and not self . current_domain : return { \"sys_utterance\" : \"Thank you, goodbye.\" } # if there are active domains, use the first one elif active_domains : out_key = f \"user_utterance/ { active_domains [ 0 ] . get_domain_name () } \" self . current_domain = active_domains [ 0 ] return { out_key : user_utterance } # if no domain is explicitely mentioned, assume the last one is still active elif self . current_domain : out_key = f \"user_utterance/ { self . current_domain . get_domain_name () } \" return { out_key : user_utterance } # Otherwise ask the user what domain they want else : return { \"sys_utterance\" : \"Hello, please let me know how I can help you, I can discuss \" + f \"the following domains: { self . domains_to_str () } .\" } \"\"\" Convert list of domains to a string for console output \"\"\" def domains_to_str ( self ): if len ( self . domains ) == 1 : return self . domains [ 0 ] . get_display_name () elif len ( self . domains ) == 2 : return \" and \" . join ([ d . get_display_name () for d in self . domains ]) else : return \", \" . join ([ d . get_display_name () for d in self . domains ][: - 1 ]) + f \", and { self . domains [ - 1 ] . get_display_name () } \" As a note, once the DomainTracker has selected a domain, this is appended to the output dictionary so that only modules of that same domain will receive the published message.","title":"Domain Tracker Module"},{"location":"tutorials/advanced/#creating-a-domain-tracker","text":"The code for creating a DomainTracker is nearly the same as any other module. However instead of taking in a domain argument as a string or Domain object, the DomainTracker takes in a domains argument which must be a list of domain objects domain_tracker = DomainTracker ( domains = [ lecturers , canteen ])","title":"Creating a Domain Tracker"},{"location":"tutorials/advanced/#putting-it-all-together","text":"The last thing left to do is now to combine all of this into one single dialog system and run it! * Sending an emty message to gen_user_utterance will trigger the domain tracker * The domian tracker will then see that we're on the first turn with no active domain, thus generating a system message which will inform the user of all available domains (lecturers and canteen) ds = DialogSystem ( services = [ user_in , user_out , # interfaces domain_tracker , lecturer_nlu , dining_hall_nlu , # NLUs lecturer_bst , dining_hall_bst , # BSTs lecturer_policy , dining_hall_policy , # Policies lecturer_nlg , dining_hall_nlg , # NLGs ]) ds . run_dialog ({ 'gen_user_utterance' : \"\" }) Alternatively, you can have a look at the run_chat.py file * calling python run_chat lecturers mensa from your terminal in the ADVISER main folder will do exactly the same * you can specify more options, e.g. for using ASR, TTS, etc.","title":"Putting it All Together"},{"location":"tutorials/advanced/#running-a-distributed-dialog-system","text":"If you're relying on computationally intense services, e.g. ASR or TTS modules, ADVISER 2.0 provdies a way to split up dialog processing so these services can be run remotely, e.g. on more powerful machines, while the rest of the dialog system remains local. The easiest way to do this is to use ssh port-forwarding , but you can also specify arbitrary IP-adresses (you might have to adapt your router settings for this approach). Therefore, we will use the port-forwarding approach in the following description. The good news first: running some modules remotely, does not require any large changes! 1. The first change is that you will need to pass a unique identifier to your service constructor; so rather than just taking in a domain your service will also need to take in an identifier as shown below: class ConcatenateServiceWithDomain ( Service ): \"\"\" new here: identifier argument\"\"\" def __init__ ( self , domain : str = \"mydomain\" , identifier : str = None ): \"\"\" NEW: domain name! \"\"\" Service . __init__ ( self , domain = domain , identifier = identifier ) \"\"\" nothing changes here \"\"\" @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } Next create an instance of your service with an identifier of your choosing in a script on your remote machine ; The only important thing when choosing the identifier is that you don't use the same identifier for two different service instances. But it is recommended that the name is at least somewhat descriptive. In the same script, call this services's run_standalone() method. This will kickoff a registration routine with the dialog system instance we're going to run on your local machine concatenate_service = ConcatenateServiceWithDomain ( identifier = \"myconcatenateservice1\" ) concatenate_service . run_standalone () On your local machine which is going to run the dialog system, create a placeholder, RemoteService (This is really just a dummy class storing the identifier). This allows the remote service to register with the dialog system; important is that you use the same identifier here as you used to create the remote service Then, go ahead and instantiate your dialog system, providing the remote service intance instead of the real service to the service list # create dummy service remote_concatenate_service = RemoteService ( identifier = \"myconcatenateservice1\" ) # pass dummy service in as part of dialog system ds_with_remote = DialogSystem ( services = [ remote_concatenate_service , # remote service placeholder print_service ]) # local print service From here on, you can call your dialog system with the usual run_dialog(...) routine But to be able to connect to the remote machine, you need to forward the required ports. For this, on your local machine, call port forwarding for * subscriber port (default: 65533) * publisher port (default: 65534) * remote register port (default: 65535) ssh -fNR 65533 :localhost:65533 adress.to.your.remote@machine ssh -fNR 65534 :localhost:65534 adress.to.your.remote@machine ssh -fNR 65535 :localhost:65535 adress.to.your.remote@machine If those ports are already in use on your machine, you may change them explicitly in the Service AND DialogSystem constructors, just be consistent. See the docstrings on these classes for more information. Once you start the concatenate_service.run_standalone() on your remote machine and the ds_with_remote.run_dialog(...) on your local machine, you will see notifications on the terminal giving you information about the connection state between your local and remote services. If you want to have a look at another example (with both services running on the same machine, so you don't need to call ssh for this), look at the run_chat.py file with --gui option enabled: this starts a webserver as a remote service connecting to the dialog system.","title":"Running A Distributed Dialog System"},{"location":"tutorials/dialogsystem/","text":"You can find the Jupyter Notebook in our GitHub repository . Task Oriented Dialog Systems with ADVISER 2.0 \u00b6 Already we have seen a little bit how services operate and the basic modules needed to make a dialog system. In this tutorial, we will introduce service based implementations of these modules which come as part of the ADVISER 2.0 toolkit. # FIRST SET UP ENVIRONMENT import sys import os from typing import List import time sys . path . append ( os . path . abspath ( '../..' )) from utils.topics import Topic from services.service import Service , PublishSubscribe , RemoteService from utils.domain.domain import Domain from utils.domain.jsonlookupdomain import JSONLookupDomain from utils.logger import DiasysLogger , LogLevel from services.hci import ConsoleInput , ConsoleOutput from services.nlu import HandcraftedNLU from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.nlg import HandcraftedNLG from services.domain_tracker import DomainTracker from services.service import DialogSystem from tensorboardX import SummaryWriter from services.policy.rl.experience_buffer import NaivePrioritizedBuffer from services.simulator import HandcraftedUserSimulator from services.policy import DQNPolicy from services.stats.evaluation import PolicyEvaluator Domains, Ontologies, and Databases/APIs \u00b6 Since this is a tutorial about task-oriented dialog systems, we need to introduce the concept of a domain here. A domain is a limited topic of conversation, e.g. weather, lecturers, or restaurants. In the context of the ADVISER 2.0 toolkit, a domain is defined by an ontology - which describes the types of entities which can be discussed in a domain, their properties, and which properties the user and system can ask after - and a database which can be queried by the system to anser user requests. Databases/APIs \u00b6 When we discuss task oriented dialog systems in this tutorial, we primarly mean systems where the user is trying to find an appropriate entity and/or information about a given entity's properties. For example, a user is trying to find the best superhero to save their city and once they find one, they want to know their last known location. To enable this, however, the system needs access to a database (or web API) which stores this kind of information. In the rest of this tutorial, we will work under the assumption that our dialog systems have access to an SQLite database which contains all relevant information for a domain. Ontology \u00b6 Ontologies are responsible for defining what things (entities and their properties) can be discussed in a given domain (topic). Below is an example ontology for a superhero domain. The entities in this domain are superheroes, and their properties include things such as name , primary_uniform_color , main_superpower , last_known_location . These properties are then split into three (overlapping) categories: * informables: Properties the user can tell the system to help accomplish their goal * eg. constraints to help them find the right superhero for their job * requestables: Properties the user can ask the system about to find out more information about an entity * eg. \"what is their secret identity?\" something that only makes sense after an entity has been suggested * system requestables: Properties the system can ask the user for to help it figure out what entity best fills the user's goal The general properties (loyalty, name, etc.) are called slots, and the specific instances of a slot (\"Avengers\", \"Aqua Man\", etc.) are called values. As one of the ontology's main purposes is to support in natural language understanding and a user will only provide values for informable slots, these are the only values which must be listed in the ontology. { \"informable\" : { \"loyality\" : [ \"Avengers\" , \"Justice League\" , \"Guardians of the Galaxy\" , ... ], \"main_superpower\" : [ \"Claws\" , \"Gadgets\" , \"Magic\" , ... ], \"name\" : [ \"Aqua Man\" , \"Batman\" , \"Black Widow\" , ... ], \"primary_uniform_color\" : [ \"Black\" , \"Blue\" , \"Yellow\" , ... ] }, \"key\" : \"name\" , \"requestable\" : [ \"name\" , \"primary_uniform_color\" , \"main_superpower\" , \"last_known_location\" , \"loyality\" , \"description\" , \"real_name\" ], \"system_requestable\" : [ \"primary_uniform_color\" , \"main_superpower\" , \"loyality\" ] } {'informable': {'loyality': ['Avengers', 'Justice League', 'Guardians of the Galaxy', Ellipsis], 'main_superpower': ['Claws', 'Gadgets', 'Magic', Ellipsis], 'name': ['Aqua Man', 'Batman', 'Black Widow', Ellipsis], 'primary_uniform_color': ['Black', 'Blue', 'Yellow', Ellipsis]}, 'key': 'name', 'requestable': ['name', 'primary_uniform_color', 'main_superpower', 'last_known_location', 'loyality', 'description', 'real_name'], 'system_requestable': ['primary_uniform_color', 'main_superpower', 'loyality']} This information is useful to e.g. NLU and Policy: A regular expression based NLU can match known values from the ontology against user input e.g. is italian in the user input or a synonym defined in the NLU? if so, this could be an inform act for the slot food A policy can construct actions based on the slots e.g. slot food is requestable and informable: inform_food request_food The Domain Class \u00b6 In the ADVISER 2.0 Toolkit, domains are subclasses of utils.domain.domain.Domain . The Domain class provides an abstract interface for connecting to the ontology and database associated with a domain e.g. the weather domain would provide means to contact a WebAPI for querying with user-specified constraints like location, day,... or could provide access to the ontology so the system would know what system requestable slots existed in the weather domain. In thins domain, rather than working with the abstract Domain class, we will primarily be using the class JSONLookupDomain , a subclass which provides querying functionality for an SQLite-Database (found in resources/databases ) and an access to an ontology file (found in resources/ontologies ) in JSON-format. Example: Let's create an example domain. For this we'll work with a domain called superhero which allows users to find the correct superhero for whatever problem is at hand. As mentioned domains require an ontology and a database to define them, therefore as we instantiate a new Domain, we will pass both of these is as files. If the ontology and database file names follow the following format: * ontologies/{domain_name}.json * databasees/{domain_name}.db then only the domain_name is necessary to pass to the JSONLookupDomain object to instantiate it as shown below: # Here we can instantiate the domain with just the name string since the paths to our ontology # and database files follow the correct pattern super_domain = JSONLookupDomain ( name = \"superhero\" ) Topics and Domains \u00b6 If you're providing domain-objects (or, equivalently, domain name strings) to a Service constructor, all subscribed and published topics for this service will implicitly append the domain name to allow for multi domain dialog systems. The exact reasons for this will be explained in more depth in the mutlidomain section of the next tutorial. Let's alter our ConcatenateService to accept a domain name: class ConcatenateServiceWithDomain ( Service ): def __init__ ( self , domain : str = \"mydomain\" ): \"\"\" NEW: domain name! \"\"\" Service . __init__ ( self , domain = domain ) @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): \"\"\" NOTE: This function did not change at all \"\"\" print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } If we would run the dialog system example above again, replacing the old ConcatenateService with the new ConcatenateServiceWithDomain , we would observe no more prints from it and also no more messages to topics C and D . This is because the toics for ConcatenateServiceWithDomain were internally remapped to: * A -> A/mydomain * B -> B/mydomain * C -> C/mydomain * D -> D/mydomain Subscribers perform prefix-based topic matching , that means in this case: * A/mydomain is NOT a prefix of A - no messages are received when publishing to A (or B or any other topic here) * same argument holds for B However: if concatenate would be called, we would see output by PrintService ! This is because * concatenate publishes to C/mydomain and PrintService subscribes to C * C is a prefix of C/mydomain ! * same argument holds for D If you provide all your services with the same domain, you're not going to experience any problems and do nothave to worry about these internal details in any way. However, in case you want to * go multidomain (see next tutorial) * combine domain-agnostic with domain-aware services (eg. a console input module and an NLU module) * change routing behaviour of our provided services without subclassing them and overwriting the respective functions the Service constructor offers a way to overwrite topics on instantiation: class Service : __init__ ( domain : Union [ str , Domain ] = \"\" , sub_topic_domains : Dict [ str , str ] = {}, pub_topic_domains : Dict [ str , str ] = {}, ... ) sub_topic_domains is a mapping for overwriting subscriber topics key : the original topic as given in the PublishSubscribe -decorator value : the domain name to append if len(value) > 0 : the value will be appended to the subscribed topic and a slash: key/value if len(value) == 0 : empty domain, subscribed topic will be simply: key pub_topic_domains works analogously So, if we change our constructor as follows: class ConcatenateServiceWithDomain ( Service ): def __init__ ( self , domain : str = \"mydomain\" , sub_topic_domains = { 'A' : '' , 'B' : '' }): \"\"\" NEW: domain name! \"\"\" Service . __init__ ( self , domain = domain ) @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): \"\"\" NOTE: This function did not change at all \"\"\" print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } everything will work again, meaning we will observe prints and published messages to C and D again, because we explicitly overwrite the appended domain with the empty string * A/mydomain -> A * B/mydomain -> B Another way to alter topics is to append domain strings manually in the return dictionary of any Publish/Subscribe function (Note: This only works, if you didn't provide a domain (or domain name) to the service base class constructor .): You may append any domain string to the dictionary keys, seperated by a slash: / . We could e.g. alter our ConcatenateService class: class ConcatenateService ( Service ): @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D/domain1' : result } else : return { 'C/domain2' : result } Now, our ConcatenateService gets all messages for topics starting with A and B (and independent of their domain) again, like in our very first ConcatenateService example. But, on the publishing side, we're now publishing to two different domains, domain1 and domain2 , by appending these domain names explicitly after the topic followed by the topic-domain-seperator: C/domain2 and D/domain1 . Equipped with these insights into the inner workings of the message routing mechanism, we can evolve our tutorial system to a multi-domain dialog system in the following chapter. Automatic Speech Recognition (ASR) \u00b6 To support spoken dialog systems, the ADVISER 2.0 toolkit provides an ASR module which converts spoken user utterances to text. In the current implementation, we provide the end-to-end ASR model for English language based on Transformer neural network architecture, however the exact ASR system can be swapped out to support other languages or architectures. We use the end-to-end speech processing toolkit ESPnet and the IMS-speech English multi-dataset recipe . The system was trained on LibriSpeech, Switchboard, TED-LIUM\\~3, AMI, WSJ, Common\\~Voice\\~3, SWC, VoxForge, and M-AILABS datasets. The output of the ASR model is a sequence of subword units, which include single characters as well as combinations of several characters, making the model lexicon independent and thus able to handle unseen words. Using the Speech Recognition Module \u00b6 To use the ASR module, provided in the ADVISER 2.0 toolkit, requires three classes: * SpeechRecorder : responsible for recording the user input * SpeechInputFeatureExtracter : responsible for feature extraction from recorded input * SpeechInputDecoder : responsible for converting input features to subword units The functionality is broken up in this way to allow other modules, such as the emotion recognition and backchanneling modules which will be seen in Tutorial Four, to also make use of the data extracted at each step. Example: To create instances of each of these three classes, see the code below NOTE: This only works if you have installed the requirements for the multimodal system (they are large, so this is not recommended if you will only be working with text) from services.hci.speech import SpeechInputDecoder from services.hci.speech import SpeechInputFeatureExtractor from services.hci.speech import SpeechRecorder conversation_log_dir = \"tutorial_logs\" use_cuda = False recorder = SpeechRecorder ( conversation_log_dir = conversation_log_dir ) speech_in_feature_extractor = SpeechInputFeatureExtractor () speech_in_decoder = SpeechInputDecoder ( conversation_log_dir = conversation_log_dir , use_cuda = use_cuda ) Rule-based Natural Language Understanding (NLU) \u00b6 Semantic representation of user actions \u00b6 The NLU module is responsible for mapping natural language user input to a machine readable semantic frame representation. In this case the semantic frame is composed of up to three parts: * intent : which determines what type of action the user wants to perform (eg. \"hello\", \"inform\", \"request\", etc.) * slot : type of entity property the user is talking about (eg. \"name\", \"super_power\", etc.) * value : exact value of the entity property (eg. \"Superman\", \"super strength\", etc.) For example the following utterances could be mapped to to the following intents: * \"Hello\" $\\rightarrow$ hello () * \"I want a superhero who wears blue\" $\\rightarrow$ inform ( primary_uniform_color = \"blue\" ) * \"What is the hero's secret identity? and their team affiliation?\" $\\rightarrow$ request ( secret_identity ), request ( loyalty ) Slots and values are defined in the ontology, but the intents which the system currently expects are found in the file utils/useract.py and listed here below: * Inform : user provides system with new constraints (eg. \"speed\" as a super power) * NegativeInform : user tells system new negative constraint (eg. \"not speed\" as a super power) * Request : user asks system for value of a slot (eg. \"what is the super power?\") * Hello : user greets system * Bye : user ends session * Thanks : user thanks system * Confirm : user confirms system utterance (eg. \"yes, I do want super speed\") * Deny : user denies system utterance (eg. \"No, I do not want super speed\") * RequestAlternatives : user asks for other entities that meet their constraints besides the one the system recommended (eg. \"is there anyone else besides the Flash?\") * Bad : user utterance couldn't be parsed to any other act * SelectDomain : only needed for multidomain systems, user says keyword associated with starting the domain These intents can be broken down into two sections: General and Domain-Specific. General acts are those like \"Hello\" and \"Bye\" which do not have slots or values associated with them and their regexes remain the same regardless of the domain. Specifying rules \u00b6 In order to understand the user input, one can specify a list of possible natural language realisations for each user act. For example, the user act hello() could be expressed by saying Hello , Hi or Hey . A common way of specifying these rules computationally is to create regular expressions. For example, the dialogue agent could recognise the user act hello() if the user input string matches the regular expression (hello|hi|hey) . However, it would be time-consuming and messy to write a list of natural language realisations for all possible combinations of intent, slot and value. Therefore, we created a new syntax which allows you to reduce the number of rules by creating functions so that you only need to write a rule once and can simply apply it to all needed slots/values. In the following subsections, we present the most important parts of this syntax. 1 General syntax \u00b6 The syntax uses keywords and indentation to create a nested command structure. This means that each line represents one command and each command starts with a keyword identifying the command type. Commands have a \"block level\" which is represented by their number of leading spaces (Level 0: 0 spaces, Level 1: 4 spaces, Level 2: 8 spaces, ...). Rule and function declaration commands are Level 0 commands. Any other commands can be inserted at any level >0 and are automatically added to their \"parent\" command. A parent command is the last command on the current level -1. In the following example, Command2 is added to Command1, Command4 is added to Command3 and Command5 is added to Command4. Command1 arguments Command2 arguments Command3 arguments Command4 arguments Command5 arguments 2 Commands \u00b6 Currently, the ADVISER NLU syntax provides five possible commands. Each command individually defines what its arguments have to look like and what kind of child commands it allows. This section will show how to use the commands. 2.1 Rule \u00b6 The rule command specifies a mapping from a regular expression to a specific intent (user act). The command itself consists of the keyword \"rule\" and a description of the user act as intent() for user acts which do not contain slots (e.g. hello() , bye() ) or intent(slot) for user acts with a slot (e.g. inform(primary_uniform_color) . The regex command can later be used as a child command to add information about the user utterances accepted and the slot's value. Regarding the child commands, a rule can have an infinite amount of child commands. These include the regular expressions for each intent and below those, any exceptions and additions that need to be specified. Exceptions are checked in the order of their appearance. Note: General user acts (those which do not take slots or values as input) are not autogenerated. If you want to edit these, you may do so directly in the file resources/nlu_regexes/GeneralRules.json , however be careful as this file is currently shared across all domains. 2.2 Regular Expression \u00b6 The regex command specifies a regular expression should map to the user act specified in the current rule block. RegEx commands consist of the keyword \"regex\" and a string wrapped by double quotes. They are usually added as child commands to rules, functions, exceptions and additions. The following example shows a valid rule specification, which matches to either \"hero\" or \"superhero\" and both the American and British spellings of the word \"color\". rule request(primary_uniform_color) regex \"what is the( super)?hero's uniform colo(u)?r\\?\" It is also possible to add multiple regular expressions to rule in order to account for more varied user utterances without making the regular expressions too complicated. rule request(primary_uniform_color) regex \"what is the( super)? hero's uniform colo(u)?r\\?\" regex \"what is the colo(u)?r of the( super)? hero's uniform\\?\" regex \"which colo(u)?r does the( super)? hero wear\\?\" For user acts such as inform , we also need to have access to the slot's value. The value can be accessed in the regular expression using curly braces. rule inform(primary_uniform_color) regex \"the colo(u)?r should be {primary_uniform_color}\" This way, we can specify one rule for each value of primary_uniform_color. For example, if the user types the colour should be green , the system detects the user act inform(primary_uniform_color=green) . Note: RegEx commands cannot take any child commands. 2.3 Exceptions \u00b6 Exceptions can be called inside a rule command/function to override the default messages. Exceptions consist of the keyword \"if\" and take a constraint as an argument. A constraint can currently only match the pattern variable = \"string\" . Exceptions particularly make sense in cases where you might prefer to use a synonym rather than the exact value in the database. For example, if a database stores colors by hexadecimal code, the user cannot be expected to know the code for each color. Therefore, we could add an exception using the common name for the color, so that instead of expecting colors in hexadecimal, the system will only recognize the common name for the color. rule inform(primary_uniform_color) regex \"the colo(u)?r should be {primary_uniform_color}\" if primary_uniform_color = \"#ff11bb\" regex \"the colo(u)?r should be pink\" Note : Exception commands can take the same child commands as rule commands. 2.4 Additions \u00b6 Additions work very similar to exceptions. The only differences are that \"add_if\" is used as the keyword and that the messages of the addition command do not override the regexes of the parent command, but instead add possible regexes to the existing ones, this is in particularly well suited to adding synonyms. In the following example, we allow the user to also say cyan or blue as a way to map to the primary_uniform_color blue . rule inform(primary_uniform_color) regex \"(the colo(u)?r|) should be {primary_uniform_color}\" if primary_uniform_color = \"#ff11bb\" regex \"the colo(u)?r should be pink\" add_if primary_uniform_color = \"blue\" regex \"the colo(u)?r should be cyan\" Note: Addition commands can take the same child commands as rule commands. 2.5 Functions \u00b6 Functions are similar to rules. They can take an arbitrary number of parameters and return a regular expression (or normal string). The function command consists of the keyword \"function\" and the function description using the pattern function_name(argument_one, argument_two) . Functions are particularly useful to improve the structure of your files. In the previous example, we have added cyan to be a synonym of blue . If we want to add synonyms for all colours, we would have to list all possible regexes for all possible synonyms which would result in a huge number of lines and would be quite a mess. Instead, we can specify a function which lists all synonymous colour strings for each possible colour value centrally at one point in the file and whenever we want to use synonyms, we can simply call this function: function color_synonym(generic_color) # the color itself is almost always a synonym regex \"{generic_color}\" # 'add_if' adds regular expressions to the existing ones (see above) add_if generic_color = \"red\" regex \"maroon\" add_if generic_color = \"green\" regex \"lime( green)?\" regex \"olive( green)?\" regex \"neon green\" add_if generic_color = \"blue\" regex \"cyan\" regex \"turquoise\" # 'if' overwrites (!) existing regular expressions if generic_color = \"#ff11bb\" \"pink\" \"magenta\" The inform rule would then be simplified to: rule inform(color) \"(the colo(u)?r|it) should be {color_synonym(color)}\" Note: Function commands can take the same child commands as rule commands. 3 Shortcuts and syntax simplification \u00b6 The examples in the previous section show the ADVISER NLU syntax, as it is read by the interpreter. For simplifications and for a better overview, we allow some simplifications and shortcuts. In a preprocessing step, the simplified template is automatically transformed into a valid syntax. 3.1 Comments and empty lines \u00b6 While the original syntax requires one command per line, in the preprocessing step, empty lines and lines that start with a hashtag (#) are removed from the template file. Please note that non-leading hashtags are not recognised as comments currently. 3.2 Tabs \u00b6 The original syntax requires four spaces per block level. Alternatively, you can also use a tab instead, since all tabs are replaced by four spaces in the preprocessing step. 3.3 Automatic insertion of the regex keyword \u00b6 Regexes do not necessarily need to be marked via the regex keyword. For all lines that start with double quotes, a regex keyword is automatically inserted at the beginning of the line in the preprocessing step. Example: rule request(primary_uniform_color) \"what is the( super)? hero's uniform colo(u)?r\\?\" 3.4 Colons \u00b6 A neater version of the example in 3.3 would be the following example: rule request(primary_uniform_color): \"what is the( super)? hero's uniform colo(u)?r\\?\" To allow this syntax, the preprocessor interprets everything coming after a colon (that\u2019s not wrapped by double quotes) as a new child command and moves it to the next line with the respective block level + 1. This way, the above example and the example from 3.3 are exactly the same after preprocessing. The NLU module \u00b6 ADVISER's NLU module makes use of NLU files which specify the regular expressions with the previously presented syntax. The pipeline for creating the NLU is the following: Make sure you have created a domain . Write your NLU file using the custom syntax (see above), name it {domain_name}.nlu and place it in the folder resources/nlu_regexes . Execute the script gen_regexes.py in the folder tools/regextemplates like this: python3 tools/regextemplates/gen_regexes.py {domain_name} {domain_name} Example: python3 tools/regextemplates/gen_regexes.py superhero superhero Check that the tool has created the files {domain_name}InformRules.json for inform acts and {domain_name}RequestRules.json for request acts inside the resources/nlu_regexes folder. Once you have all these files, you can use the HandcraftedNLU module in services/nlu by simply passing your domain object in the constructor. from utils.domain.jsonlookupdomain import JSONLookupDomain from services.nlu.nlu import HandcraftedNLU domain = JSONLookupDomain ( 'superhero' ) nlu = HandcraftedNLU ( domain = domain ) You can type some exemplary messages and see which user acts are recognised by the NLU, when you are done, type \"bye\" to exit the loop: user_input = input ( '>>> ' ) while user_input . strip () . lower () not in ( '' , 'exit' , 'bye' , 'goodbye' ): user_acts = nlu . extract_user_acts ( user_input )[ 'user_acts' ] print ( ' \\n ' . join ([ repr ( user_act ) for user_act in user_acts ])) user_input = input ( '>>> ' ) Rule-based Belief State Tracker (BST) \u00b6 The BST maintains a representation of the current dialog state. It is responsible for tracking the constraints a user provides over the course of a dialog and their probabilities, recording any requests from the current turn, tracking the types of user acts in the current turn, and registering the number of database matches for the given constraints. In the ADVISER 2.0 Toolkit, this state is built up over the course of the dialog as the user expresses more constraints. Probabilities for each constraint are determined by the NLU and are not currently further processed. An example of the beliefstate for the second turn in a dialog about IMS Courses can be seen below. {'user_acts': {<UserActionType.Inform: 'inform'>} 'informs': {'bachelor': {'true': 1.0 } 'turn': {'sose': 1.0 } 'ects': {'6': 1.0 } } 'requests': {} 'num_matches': 4 'discriminable': True } As mentioned previously the beliefstate is built up over time. Therfore there are currently only three entries in the informs dictionary as up to this point the user has only provided three constraints. Each constraint has a probability of 1.0 because neither the NLU nor BST have complex rules for handling ambiguous user utterances, but this is an area we may look to improve in the future. Example: Instantiating a rules based BST is as simple as running the code below bst = HandcraftedBST ( domain = super_domain ) Policy \u00b6 Rule-based Policy \u00b6 The policy is responsible for determining the system's next action. In ADVISER 2.0, we provide two implementations of a rule-based policy (one designed to work with a database and one designed to work with an API backend). Both follow a similar decision flow. Since we focus here on task oriented dialog systems, the policy tries to help a user fulfill their goal as quickly as possible. A simplified version of a the decision making process in a rules based policy can be seen below: On each turn, the policy is capable of choosing one next action, these actions include: Welcome : greet the user InformByName : tell the user some new information InformByAlternatives : provide the user an alternative entity Request : ask the user for more information Select : ask the user to select between a given set of choices RequestMore : ask the user if they need more help Bad : inform the user their last utterance couldn't be parsed Bye : end the dialog As seen in the above diagram, the policy first works to handle general (non-domain specific) user actions. It then queries the database and only asks the user for more information if there are too many entries and asking for more information will help narrow down results. In the actual policy the decisions are a bit more nuanced, but the graphic gives the general idea of the types of rules required for a rules-based policy. Example: Instantiating a rules-based policy can be done with the code below: policy = HandcraftedPolicy ( domain = super_domain ) Reinforcement Learning Policy \u00b6 Why reinforcement Learning \u00b6 Developing handcrafted policies can be quite time consuming and have difficulties adapting to unseen scenarios. Therefore a machine learning approach can be preferable. In this type of scenario, training a machine learning agent would circumvent needing to hard code a rule for every edge case scenario. Unfortunately this type of approach normally requires a lot of data to train the policy. Especially for new domains, this kind of data just does not exist, which is where reinforcement learning comes in. The basic idea behind reinforcement learning is that an agent has a certain set of actions which it can take and is placed in a certain environment (which it may either know in advance or must learn as it explores). The agent then tries out the different actions it can take, each action altering the state (or the agent's perception of the environment) and generating a reward. In this way, the agent learns how to navigate through it's environment and find the path which yields the highest reward. This process is shown in the graph below. As mentioned before, as an input the agent receives the current state and reward and it uses these to choose what it thinks the next best action will be. This action results in a new state and a new reward. RL in the Context of Dialog Policy \u00b6 In the context of dialog systems, the RL agent is the dialog policy and the actions it can take are defined by the SysActs . The environment is a user (simulated or real) and the state is represented by the beliefstate. The reward for each action backpropagates and is inversely proportional to the number of turns it took to complete the dialog + a fixed reward for dialogs where a user was able to fulfill their goal. In ADVISER 2.0, the RL policy is implemented using deep reinforcement learning. Similar to the Deep Q-learning algorithm , an action-value function is approximated by a neural network which outputs a value for each possible system action, given the vectorised representation of a turn\u2019s beliefstate as input. The exact action set is defined in the class RLPolicy where a set of base actions templates is defined with slot names from the ontology e.g. if you have a slot food in your ontology, it will create actions like inform#food , request#food and select#food which are used to inform a user about a certain type of food, to request the food type from the user or to let the user choose between multiple food types. These action templates are the initial output of the reinforcement learning policy, which then get expanded using beliefstate and database information. For more information on the architecture see the ADVISER paper . Example: As there are so many parameters for the RL policy, instantiating it requires slightly more code than instantiating other modules. To instantiate an RL policy with the parameters used in the ADVISER paper, see the code below: # Allows you to track training progress using tensorboard summary_writer = SummaryWriter ( os . path . join ( 'logs' , \"tutorial\" )) # logs summary statistics for each train/test epoch logger = DiasysLogger ( console_log_lvl = LogLevel . RESULTS , file_log_lvl = LogLevel . DIALOGS ) # Create RL policy instance with parameters used in ADVISER paper policy = DQNPolicy ( domain = domain , lr = 0.0001 , eps_start = 0.3 , gradient_clipping = 5.0 , buffer_cls = NaivePrioritizedBuffer , replay_buffer_size = 8192 , shared_layer_sizes = [ 256 ], train_dialogs = 1000 , target_update_rate = 3 , training_frequency = 2 , logger = logger , summary_writer = summary_writer ) logger: state space dim: 74 logger: action space dim: 9 logger: Gradient Clipping: 5.0 logger: Architecture: Dueling logger: Update: Double REPLAY MEMORY: NAIVE Prioritized ARCHITECTURE: Dueling User Simulator \u00b6 To train a reinforcement learning (RL) policy, it is often useful to use a simulated user instead of an actual one at least in the beginning. Simulations, while seldom accurate representations of real users, are substantially more patient. That is, to train or evaluate a dialog policy, thousands of dialogs are required. Having a real human sit through this many dialogs would be very expensive and tedious, especially since at the beginning of training, an RL policy will produce utterly nonsensical dialogs. In comparison, a simulated user is very cheap, will not be bored or annoyed, and can be called whenever you are ready to train or test a dialog. After initial training has been performed with a simulation, it is possible to conduct further training with human users. Since a user simulator is an important part of the training process for an RL policy, the next question becomes how to create one. In the ADVISER 2.0 Toolkit, we use an agenda based user simulator based off of work by Schatzmann et al. . The basic function of the simulator can be seen in the graphic below: To start a dialog, the user simulator will randomly generate a goal (shown in gray in panel 1). This goal will include all of the constraints (informs) the user wants to provide and all of the questions (requests) they will want to ask. Then this goal will be transformed into a series of user actions and placed in the action stack. On each turn after that, the user simulator will receive a system action as input and need to respond to it (panel 2) Here the user response is hardcoded in a similar way to the handcrafted policy. Any new actions generated to respond to the system will be placed at the top of the stack. Then a random number of actions between one and n will popped from the top of the stack and given out as the user acts for that turn. In addition to the hardcoded response for each type of system act, the user simulator's behavior is also controlled by configuration parameters. These control behavior such as the maximum number of actions the user takes per turn (default 3), and whether user goals should be possible to fulfill or if it doesn't matter. These additional parameters are stored in an external configuration file. Example: To create an instance of user simulator see below: user_sim = HandcraftedUserSimulator ( domain = super_domain ) Evaluation \u00b6 In order to train an RL policy, in addition to a user simulator, we need a reward signal. That is where the evaluation module comes in. There are two evaluation classes which are important to consider. The first one is teh ObjectiveReachedEvaluator which provides the reward signal to the RL policy. The second is the PolicyEvaluator which tracks the percent of successful dialogs, average reward, and average turn number for an epoch. As a note, while the ObjectiveReachedEvaluator is already part of the RLPolicy class, to log statistics, you need to explicitely add the PolicyEvaluator to the dialog graph. As a default, a reward of -1 for every turn is assigned to encourage the policy to keep dialogs as short as possible. For every dialog where the user goal is fulfilled, a reward of 20 is given. Example: A PolicyEvaluator can be created with the code below: evaluator = PolicyEvaluator ( domain = domain , use_tensorboard = True , experiment_name = \"tutorial\" , logger = logger , summary_writer = summary_writer ) Training a reinforcement learning policy \u00b6 Now that we have a user simulator and an evaluator, we can start using them to train our RL policy. This requires configuring the dialog graph slightly differently. Rather than the graph we saw in Tutorial One, we will only use the beliefstate tracker, RL Policy and a user simulator (optionally an evaluator as well to log statistics). This new graph can be seen below: To initialize training and evaluation, a random seed is generally used to set initial parameters. For the sake of reproducibility, it is recommended to store this seed along with the results so that in the future, you can initialize all random generators used in your pipeline with it. If you run the system twice with the same seed, you should get exactly the same results and dialogs. To train the system, a dialog graph is created with all necessary modules, then for the number of training epochs desired, dialogs are carried out and evaluated. During each training epoch, weights are updated for the RL policy and during each testing epoch, the success rate and length of dialogs are evaluated and recorded. The number of dialogs per epoch and the number of epochs can be determined by the user. As a note, when evaluating the overall performance of an RL policy, it is important to run the train/test loop multiple times with different random seeds to make sure that a good performance is not simply because of a good random seed. Example: Example training and evaluation code (warning takes some time to run): # SET CONSTANTS TRAIN_EPOCHS = 1 TRAIN_EPISODES = 1000 EVAL_EPISODES = 1000 MAX_TURNS = 25 # Choose how many repeated trials for i in range ( 1 ): common . init_random () # add seed here as a parameter to the init_random if wanted ds = DialogSystem ( services = [ user_sim , bst , policy , evaluator ], protocol = 'tcp' ) # Start train/eval loop for j in range ( TRAIN_EPOCHS ): # START TRAIN EPOCH evaluator . train () policy . train () evaluator . start_epoch () for episode in range ( TRAIN_EPISODES ): if episode % 100 == 0 : print ( \"DIALOG\" , episode ) logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () # START EVAL EPOCH evaluator . eval () policy . eval () evaluator . start_epoch () for episode in range ( EVAL_EPISODES ): logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () policy . save () ds . shutdown () Handcrafted Natural Language Generation (NLG) \u00b6 To transform system acts into natural language utterances, ADVISER 2.0 uses handcrafted templates. Although machine learning generated output can have more breadth of expression, using templates guarentees that all system utterances will be grammatic and sensical. Using templates additionally allows dialog systems for new domains to be created even when there is not sufficient data for those domains to train a machine learning based NLG. In ADVISER 2.0, rather than mapping every sys_act, slot, value combination to an individual utterance, templates are used to generalize this process by specifying placeholders for a system act's slots and/or values. An example of this can be seen below: inform(name={X}, ects={Y}) \u2192 \"The course {X} is worth {Y} ECTS.\" During the dialog, the system iterates through the templates and chooses the first one for which the system act fits the template's signature. Types of templates \u00b6 Under adviser/resources/templates the handcrafted templates are stored in text files. The templates have the format where the left side represents the system act and the right side represents the natural language utterance. Variables are used instead of concrete slot values so that the templates can be reused as much as possbile. Each file is divided into several sections, we add examples: Templates for system general acts template hello(): \"Welcome to the IMS courses chat bot. How may I help you?\" Templates for system requests template request(lang): \"In which language shall the course be held?\" Methods for system informs function info(slot, value) if slot = \"ects\": \"is worth {value} ECTS\" if slot = \"turn\" if value = \"sose\": \"is offered in the summer semester\" if value = \"wise\": \"is offered in the winter semester\" Templates for system informs template inform_byname(name): \"I found the course {name}. What do you want to know about it?\" template inform_byname(name, turn): \"The course {name} {info(\"turn\", turn)}.\" Similarly, templates for system confirm, request more and select are included. Example: If system act is: inform(name='computational linguistics team laboratory', turn='wise') The NLG output will be: \"The course computational linguistics team laboratory is offered in the winter semester.\" Notes: Each domain has a ist own YourDomainNameMessages.nlg file No python script needs to be modified (domain independent) Example: The code to create an NLG service for the superhero domain is shown below: nlg = HandcraftedNLG ( domain = super_domain ) Text to Speech (TTS) \u00b6 The TTS module is an options module which converts the text natural language output to speech. In the ADVISER 2.0 toolkit, this is done using the ESPnet-TTS toolkit , which is an extension of the ESPnet toolkit mentioned in the ASR section. We use FastSpeech as a synthesis model, which provides substantially faster voice generation. Additionally, we are able to provide a \"cleaner\" file to optimize the synthesizer for abbreviations, such as Prof., Univ., IMS, NLP, ECTS, and PhD , as well as for German proper names, such as street names. These optimizations can be easily extended by adding additional words and pronunciation mappings to services/speech/cleaners.py . Example: Similar to how the ASR module also required a SpeechInputRecorder module, the TTS module also requires a SpeechOutputPlayer in order for the user to be able to hear the generated sound file. The SpeechOutputPlayer and SpeechOutputGenerator can be instantiated with the code below. NOTE: This code will only work if you have installed the requirements for the multimodal dialog system (they are large, so this is not recommended if you will only be working with text) from services.hci.speech import SpeechOutputGenerator from services.hci.speech import SpeechOutputPlayer # (GPU: 0.4 s/per utterance, CPU: 11 s/per utterance) speech_out_generator = SpeechOutputGenerator ( use_cuda = False ) speech_out_player = SpeechOutputPlayer ( conversation_log_dir = conversation_log_dir ) Cleaning up TTS output \u00b6 To improve the speech generation, we also provide a file servicse/hci/speech/cleaners.py which allows you to manually correct pronunciation of words. This allows you to provide either phonetic transcriptions for acronyms, abbreviations, and foreign words or pronunciation patterns for things such as email addresses or telephone numbers. One of the methods from this file is shown below. def expand_abbreviations ( text ): \"\"\" Preprocesses a text to turn abbreviations into forms that the TTS can pronounce properly text (string): Text to be preprocessed \"\"\" for regex , replacement in _abbreviations : text = re . sub ( regex , replacement , text ) return text # List of (word, replacement) pairs for acronym or special words: _acronym = [ ( ' a ' , ' ae ' ), ( ' s ' , ' eh s, ' ), ( 'array' , 'ER RAY' ), ( 'API' , 'AE P I' ), ( 'distributional' , 'distributionall' ), ( 'ECTS' , 'E C T EH S,' ), ( 'Erasmus' , 'E RAS MOUS' ), ( 'ID' , 'I D' ), ( 'IMS' , 'I M EH S' ), ( 'NLP' , 'N L P' ), ( 'PhD' , 'P h D' ), ( 'PWR 05B' , 'Pfaffen vaald ring five B, ' ), ( 'psycholinguistics' , 'psycho linguistics' ), ( 'stuttgart' , 'stu gart' ), ( 'Stuttgart' , 'Stu gart' ), ( 'vegan' , 'viygan' ), ( 'Vegan' , 'Viygan' ), ( 'ImsLecturers' , 'I M EH S Lecturers' ), ( 'imsLecturers' , 'I M EH S Lecturers' ), ] Chatting with the dialog system \u00b6 There are two main ways to interact with an ADVISER 2.0 dialog system: 1) by typing input and reading system output from the terminal or 2) by speaking to the system and receiving audio responses. Since we've already taken a quick look at a spoken dialog system, let's quickly discuss a text based one. In short, a text based system is much simpler. Instead of needing multiple modules for input/output, the text based system just needs ConsoleInput and ConsoleOutput . When these are added to the dialog system, the system utterances will appear on the terminal and the user can give their response by typing and pressing enter to end an utterance. Example: An example of a dialog system constructed this way can be seen below. NOTE: There is a problem with the user input in Jupyter Notebook. To try the code out, copy the code below to a new file in the adviser folder and run it from the dialog system # Importing everything we need for a dialog system from utils.domain.jsonlookupdomain import JSONLookupDomain from utils.logger import DiasysLogger , LogLevel from services.hci import ConsoleInput , ConsoleOutput from services.nlu import HandcraftedNLU from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.nlg import HandcraftedNLG from services.domain_tracker import DomainTracker from services.service import DialogSystem # create domain super_domain = JSONLookupDomain ( name = \"superhero\" ) # create domain specific modules nlu = HandcraftedNLU ( domain = super_domain ) bst = HandcraftedBST ( domain = super_domain ) policy = HandcraftedPolicy ( domain = super_domain ) nlg = HandcraftedNLG ( domain = super_domain ) d_tracker = DomainTracker ( domains = [ super_domain ]) # Input modules (just allow access to terminal for text based dialog) user_in = ConsoleInput ( domain = \"\" ) user_out = ConsoleOutput ( domain = \"\" ) logger = DiasysLogger ( console_log_lvl = LogLevel . DIALOGS ) ds = DialogSystem ( services = [ d_tracker , user_in , user_out , nlu , bst , policy , nlg ], debug_logger = logger ) error_free = ds . is_error_free_messaging_pipeline () if not error_free : ds . print_inconsistencies () ds . draw_system_graph ( name = 'system' , show = False ) # start dialog for _ in range ( 1 ): ds . run_dialog ({ 'gen_user_utterance' : \"\" }) ds . shutdown () Check Your Understanding (Optional) \u00b6 Now that you have read how each of the basic modules provided by the ADVISER 2.0 Toolkit works, let's actually test it out. Run a Dialog System \u00b6 There should be a program in the tutorials folder called tutorial_chat.py All the modules you will need are already imported, but you will need to follow the comments and create a dialog system for yourself. Create a basic NLU \u00b6 Right now we have provided a very simple .nlu file for the superhero domain. Try adding more synonyms/more regexes to the rules in superhero.nlu so that it can capture more realistic user utterances. Update the NLG \u00b6 Try adding some more NLG templates to the superhero domain, or updating existing ones to sound more natural (the file is located in the resources/nlg_templates folder). Test out your new system \u00b6 Once you are satisfied with your NLU and NLG files, test out the system again. Make sure you remembered to recomile the regexes first though!","title":"Dialog System"},{"location":"tutorials/dialogsystem/#task-oriented-dialog-systems-with-adviser-20","text":"Already we have seen a little bit how services operate and the basic modules needed to make a dialog system. In this tutorial, we will introduce service based implementations of these modules which come as part of the ADVISER 2.0 toolkit. # FIRST SET UP ENVIRONMENT import sys import os from typing import List import time sys . path . append ( os . path . abspath ( '../..' )) from utils.topics import Topic from services.service import Service , PublishSubscribe , RemoteService from utils.domain.domain import Domain from utils.domain.jsonlookupdomain import JSONLookupDomain from utils.logger import DiasysLogger , LogLevel from services.hci import ConsoleInput , ConsoleOutput from services.nlu import HandcraftedNLU from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.nlg import HandcraftedNLG from services.domain_tracker import DomainTracker from services.service import DialogSystem from tensorboardX import SummaryWriter from services.policy.rl.experience_buffer import NaivePrioritizedBuffer from services.simulator import HandcraftedUserSimulator from services.policy import DQNPolicy from services.stats.evaluation import PolicyEvaluator","title":"Task Oriented Dialog Systems with ADVISER 2.0"},{"location":"tutorials/dialogsystem/#domains-ontologies-and-databasesapis","text":"Since this is a tutorial about task-oriented dialog systems, we need to introduce the concept of a domain here. A domain is a limited topic of conversation, e.g. weather, lecturers, or restaurants. In the context of the ADVISER 2.0 toolkit, a domain is defined by an ontology - which describes the types of entities which can be discussed in a domain, their properties, and which properties the user and system can ask after - and a database which can be queried by the system to anser user requests.","title":"Domains, Ontologies, and Databases/APIs"},{"location":"tutorials/dialogsystem/#databasesapis","text":"When we discuss task oriented dialog systems in this tutorial, we primarly mean systems where the user is trying to find an appropriate entity and/or information about a given entity's properties. For example, a user is trying to find the best superhero to save their city and once they find one, they want to know their last known location. To enable this, however, the system needs access to a database (or web API) which stores this kind of information. In the rest of this tutorial, we will work under the assumption that our dialog systems have access to an SQLite database which contains all relevant information for a domain.","title":"Databases/APIs"},{"location":"tutorials/dialogsystem/#ontology","text":"Ontologies are responsible for defining what things (entities and their properties) can be discussed in a given domain (topic). Below is an example ontology for a superhero domain. The entities in this domain are superheroes, and their properties include things such as name , primary_uniform_color , main_superpower , last_known_location . These properties are then split into three (overlapping) categories: * informables: Properties the user can tell the system to help accomplish their goal * eg. constraints to help them find the right superhero for their job * requestables: Properties the user can ask the system about to find out more information about an entity * eg. \"what is their secret identity?\" something that only makes sense after an entity has been suggested * system requestables: Properties the system can ask the user for to help it figure out what entity best fills the user's goal The general properties (loyalty, name, etc.) are called slots, and the specific instances of a slot (\"Avengers\", \"Aqua Man\", etc.) are called values. As one of the ontology's main purposes is to support in natural language understanding and a user will only provide values for informable slots, these are the only values which must be listed in the ontology. { \"informable\" : { \"loyality\" : [ \"Avengers\" , \"Justice League\" , \"Guardians of the Galaxy\" , ... ], \"main_superpower\" : [ \"Claws\" , \"Gadgets\" , \"Magic\" , ... ], \"name\" : [ \"Aqua Man\" , \"Batman\" , \"Black Widow\" , ... ], \"primary_uniform_color\" : [ \"Black\" , \"Blue\" , \"Yellow\" , ... ] }, \"key\" : \"name\" , \"requestable\" : [ \"name\" , \"primary_uniform_color\" , \"main_superpower\" , \"last_known_location\" , \"loyality\" , \"description\" , \"real_name\" ], \"system_requestable\" : [ \"primary_uniform_color\" , \"main_superpower\" , \"loyality\" ] } {'informable': {'loyality': ['Avengers', 'Justice League', 'Guardians of the Galaxy', Ellipsis], 'main_superpower': ['Claws', 'Gadgets', 'Magic', Ellipsis], 'name': ['Aqua Man', 'Batman', 'Black Widow', Ellipsis], 'primary_uniform_color': ['Black', 'Blue', 'Yellow', Ellipsis]}, 'key': 'name', 'requestable': ['name', 'primary_uniform_color', 'main_superpower', 'last_known_location', 'loyality', 'description', 'real_name'], 'system_requestable': ['primary_uniform_color', 'main_superpower', 'loyality']} This information is useful to e.g. NLU and Policy: A regular expression based NLU can match known values from the ontology against user input e.g. is italian in the user input or a synonym defined in the NLU? if so, this could be an inform act for the slot food A policy can construct actions based on the slots e.g. slot food is requestable and informable: inform_food request_food","title":"Ontology"},{"location":"tutorials/dialogsystem/#the-domain-class","text":"In the ADVISER 2.0 Toolkit, domains are subclasses of utils.domain.domain.Domain . The Domain class provides an abstract interface for connecting to the ontology and database associated with a domain e.g. the weather domain would provide means to contact a WebAPI for querying with user-specified constraints like location, day,... or could provide access to the ontology so the system would know what system requestable slots existed in the weather domain. In thins domain, rather than working with the abstract Domain class, we will primarily be using the class JSONLookupDomain , a subclass which provides querying functionality for an SQLite-Database (found in resources/databases ) and an access to an ontology file (found in resources/ontologies ) in JSON-format. Example: Let's create an example domain. For this we'll work with a domain called superhero which allows users to find the correct superhero for whatever problem is at hand. As mentioned domains require an ontology and a database to define them, therefore as we instantiate a new Domain, we will pass both of these is as files. If the ontology and database file names follow the following format: * ontologies/{domain_name}.json * databasees/{domain_name}.db then only the domain_name is necessary to pass to the JSONLookupDomain object to instantiate it as shown below: # Here we can instantiate the domain with just the name string since the paths to our ontology # and database files follow the correct pattern super_domain = JSONLookupDomain ( name = \"superhero\" )","title":"The Domain Class"},{"location":"tutorials/dialogsystem/#topics-and-domains","text":"If you're providing domain-objects (or, equivalently, domain name strings) to a Service constructor, all subscribed and published topics for this service will implicitly append the domain name to allow for multi domain dialog systems. The exact reasons for this will be explained in more depth in the mutlidomain section of the next tutorial. Let's alter our ConcatenateService to accept a domain name: class ConcatenateServiceWithDomain ( Service ): def __init__ ( self , domain : str = \"mydomain\" ): \"\"\" NEW: domain name! \"\"\" Service . __init__ ( self , domain = domain ) @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): \"\"\" NOTE: This function did not change at all \"\"\" print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } If we would run the dialog system example above again, replacing the old ConcatenateService with the new ConcatenateServiceWithDomain , we would observe no more prints from it and also no more messages to topics C and D . This is because the toics for ConcatenateServiceWithDomain were internally remapped to: * A -> A/mydomain * B -> B/mydomain * C -> C/mydomain * D -> D/mydomain Subscribers perform prefix-based topic matching , that means in this case: * A/mydomain is NOT a prefix of A - no messages are received when publishing to A (or B or any other topic here) * same argument holds for B However: if concatenate would be called, we would see output by PrintService ! This is because * concatenate publishes to C/mydomain and PrintService subscribes to C * C is a prefix of C/mydomain ! * same argument holds for D If you provide all your services with the same domain, you're not going to experience any problems and do nothave to worry about these internal details in any way. However, in case you want to * go multidomain (see next tutorial) * combine domain-agnostic with domain-aware services (eg. a console input module and an NLU module) * change routing behaviour of our provided services without subclassing them and overwriting the respective functions the Service constructor offers a way to overwrite topics on instantiation: class Service : __init__ ( domain : Union [ str , Domain ] = \"\" , sub_topic_domains : Dict [ str , str ] = {}, pub_topic_domains : Dict [ str , str ] = {}, ... ) sub_topic_domains is a mapping for overwriting subscriber topics key : the original topic as given in the PublishSubscribe -decorator value : the domain name to append if len(value) > 0 : the value will be appended to the subscribed topic and a slash: key/value if len(value) == 0 : empty domain, subscribed topic will be simply: key pub_topic_domains works analogously So, if we change our constructor as follows: class ConcatenateServiceWithDomain ( Service ): def __init__ ( self , domain : str = \"mydomain\" , sub_topic_domains = { 'A' : '' , 'B' : '' }): \"\"\" NEW: domain name! \"\"\" Service . __init__ ( self , domain = domain ) @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): \"\"\" NOTE: This function did not change at all \"\"\" print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } everything will work again, meaning we will observe prints and published messages to C and D again, because we explicitly overwrite the appended domain with the empty string * A/mydomain -> A * B/mydomain -> B Another way to alter topics is to append domain strings manually in the return dictionary of any Publish/Subscribe function (Note: This only works, if you didn't provide a domain (or domain name) to the service base class constructor .): You may append any domain string to the dictionary keys, seperated by a slash: / . We could e.g. alter our ConcatenateService class: class ConcatenateService ( Service ): @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D/domain1' : result } else : return { 'C/domain2' : result } Now, our ConcatenateService gets all messages for topics starting with A and B (and independent of their domain) again, like in our very first ConcatenateService example. But, on the publishing side, we're now publishing to two different domains, domain1 and domain2 , by appending these domain names explicitly after the topic followed by the topic-domain-seperator: C/domain2 and D/domain1 . Equipped with these insights into the inner workings of the message routing mechanism, we can evolve our tutorial system to a multi-domain dialog system in the following chapter.","title":"Topics and Domains"},{"location":"tutorials/dialogsystem/#automatic-speech-recognition-asr","text":"To support spoken dialog systems, the ADVISER 2.0 toolkit provides an ASR module which converts spoken user utterances to text. In the current implementation, we provide the end-to-end ASR model for English language based on Transformer neural network architecture, however the exact ASR system can be swapped out to support other languages or architectures. We use the end-to-end speech processing toolkit ESPnet and the IMS-speech English multi-dataset recipe . The system was trained on LibriSpeech, Switchboard, TED-LIUM\\~3, AMI, WSJ, Common\\~Voice\\~3, SWC, VoxForge, and M-AILABS datasets. The output of the ASR model is a sequence of subword units, which include single characters as well as combinations of several characters, making the model lexicon independent and thus able to handle unseen words.","title":"Automatic Speech Recognition (ASR)"},{"location":"tutorials/dialogsystem/#using-the-speech-recognition-module","text":"To use the ASR module, provided in the ADVISER 2.0 toolkit, requires three classes: * SpeechRecorder : responsible for recording the user input * SpeechInputFeatureExtracter : responsible for feature extraction from recorded input * SpeechInputDecoder : responsible for converting input features to subword units The functionality is broken up in this way to allow other modules, such as the emotion recognition and backchanneling modules which will be seen in Tutorial Four, to also make use of the data extracted at each step. Example: To create instances of each of these three classes, see the code below NOTE: This only works if you have installed the requirements for the multimodal system (they are large, so this is not recommended if you will only be working with text) from services.hci.speech import SpeechInputDecoder from services.hci.speech import SpeechInputFeatureExtractor from services.hci.speech import SpeechRecorder conversation_log_dir = \"tutorial_logs\" use_cuda = False recorder = SpeechRecorder ( conversation_log_dir = conversation_log_dir ) speech_in_feature_extractor = SpeechInputFeatureExtractor () speech_in_decoder = SpeechInputDecoder ( conversation_log_dir = conversation_log_dir , use_cuda = use_cuda )","title":"Using the Speech Recognition Module"},{"location":"tutorials/dialogsystem/#rule-based-natural-language-understanding-nlu","text":"","title":"Rule-based Natural Language Understanding (NLU)"},{"location":"tutorials/dialogsystem/#semantic-representation-of-user-actions","text":"The NLU module is responsible for mapping natural language user input to a machine readable semantic frame representation. In this case the semantic frame is composed of up to three parts: * intent : which determines what type of action the user wants to perform (eg. \"hello\", \"inform\", \"request\", etc.) * slot : type of entity property the user is talking about (eg. \"name\", \"super_power\", etc.) * value : exact value of the entity property (eg. \"Superman\", \"super strength\", etc.) For example the following utterances could be mapped to to the following intents: * \"Hello\" $\\rightarrow$ hello () * \"I want a superhero who wears blue\" $\\rightarrow$ inform ( primary_uniform_color = \"blue\" ) * \"What is the hero's secret identity? and their team affiliation?\" $\\rightarrow$ request ( secret_identity ), request ( loyalty ) Slots and values are defined in the ontology, but the intents which the system currently expects are found in the file utils/useract.py and listed here below: * Inform : user provides system with new constraints (eg. \"speed\" as a super power) * NegativeInform : user tells system new negative constraint (eg. \"not speed\" as a super power) * Request : user asks system for value of a slot (eg. \"what is the super power?\") * Hello : user greets system * Bye : user ends session * Thanks : user thanks system * Confirm : user confirms system utterance (eg. \"yes, I do want super speed\") * Deny : user denies system utterance (eg. \"No, I do not want super speed\") * RequestAlternatives : user asks for other entities that meet their constraints besides the one the system recommended (eg. \"is there anyone else besides the Flash?\") * Bad : user utterance couldn't be parsed to any other act * SelectDomain : only needed for multidomain systems, user says keyword associated with starting the domain These intents can be broken down into two sections: General and Domain-Specific. General acts are those like \"Hello\" and \"Bye\" which do not have slots or values associated with them and their regexes remain the same regardless of the domain.","title":"Semantic representation of user actions"},{"location":"tutorials/dialogsystem/#specifying-rules","text":"In order to understand the user input, one can specify a list of possible natural language realisations for each user act. For example, the user act hello() could be expressed by saying Hello , Hi or Hey . A common way of specifying these rules computationally is to create regular expressions. For example, the dialogue agent could recognise the user act hello() if the user input string matches the regular expression (hello|hi|hey) . However, it would be time-consuming and messy to write a list of natural language realisations for all possible combinations of intent, slot and value. Therefore, we created a new syntax which allows you to reduce the number of rules by creating functions so that you only need to write a rule once and can simply apply it to all needed slots/values. In the following subsections, we present the most important parts of this syntax.","title":"Specifying rules"},{"location":"tutorials/dialogsystem/#1-general-syntax","text":"The syntax uses keywords and indentation to create a nested command structure. This means that each line represents one command and each command starts with a keyword identifying the command type. Commands have a \"block level\" which is represented by their number of leading spaces (Level 0: 0 spaces, Level 1: 4 spaces, Level 2: 8 spaces, ...). Rule and function declaration commands are Level 0 commands. Any other commands can be inserted at any level >0 and are automatically added to their \"parent\" command. A parent command is the last command on the current level -1. In the following example, Command2 is added to Command1, Command4 is added to Command3 and Command5 is added to Command4. Command1 arguments Command2 arguments Command3 arguments Command4 arguments Command5 arguments","title":"1 General syntax"},{"location":"tutorials/dialogsystem/#2-commands","text":"Currently, the ADVISER NLU syntax provides five possible commands. Each command individually defines what its arguments have to look like and what kind of child commands it allows. This section will show how to use the commands.","title":"2 Commands"},{"location":"tutorials/dialogsystem/#21-rule","text":"The rule command specifies a mapping from a regular expression to a specific intent (user act). The command itself consists of the keyword \"rule\" and a description of the user act as intent() for user acts which do not contain slots (e.g. hello() , bye() ) or intent(slot) for user acts with a slot (e.g. inform(primary_uniform_color) . The regex command can later be used as a child command to add information about the user utterances accepted and the slot's value. Regarding the child commands, a rule can have an infinite amount of child commands. These include the regular expressions for each intent and below those, any exceptions and additions that need to be specified. Exceptions are checked in the order of their appearance. Note: General user acts (those which do not take slots or values as input) are not autogenerated. If you want to edit these, you may do so directly in the file resources/nlu_regexes/GeneralRules.json , however be careful as this file is currently shared across all domains.","title":"2.1 Rule"},{"location":"tutorials/dialogsystem/#22-regular-expression","text":"The regex command specifies a regular expression should map to the user act specified in the current rule block. RegEx commands consist of the keyword \"regex\" and a string wrapped by double quotes. They are usually added as child commands to rules, functions, exceptions and additions. The following example shows a valid rule specification, which matches to either \"hero\" or \"superhero\" and both the American and British spellings of the word \"color\". rule request(primary_uniform_color) regex \"what is the( super)?hero's uniform colo(u)?r\\?\" It is also possible to add multiple regular expressions to rule in order to account for more varied user utterances without making the regular expressions too complicated. rule request(primary_uniform_color) regex \"what is the( super)? hero's uniform colo(u)?r\\?\" regex \"what is the colo(u)?r of the( super)? hero's uniform\\?\" regex \"which colo(u)?r does the( super)? hero wear\\?\" For user acts such as inform , we also need to have access to the slot's value. The value can be accessed in the regular expression using curly braces. rule inform(primary_uniform_color) regex \"the colo(u)?r should be {primary_uniform_color}\" This way, we can specify one rule for each value of primary_uniform_color. For example, if the user types the colour should be green , the system detects the user act inform(primary_uniform_color=green) . Note: RegEx commands cannot take any child commands.","title":"2.2 Regular Expression"},{"location":"tutorials/dialogsystem/#23-exceptions","text":"Exceptions can be called inside a rule command/function to override the default messages. Exceptions consist of the keyword \"if\" and take a constraint as an argument. A constraint can currently only match the pattern variable = \"string\" . Exceptions particularly make sense in cases where you might prefer to use a synonym rather than the exact value in the database. For example, if a database stores colors by hexadecimal code, the user cannot be expected to know the code for each color. Therefore, we could add an exception using the common name for the color, so that instead of expecting colors in hexadecimal, the system will only recognize the common name for the color. rule inform(primary_uniform_color) regex \"the colo(u)?r should be {primary_uniform_color}\" if primary_uniform_color = \"#ff11bb\" regex \"the colo(u)?r should be pink\" Note : Exception commands can take the same child commands as rule commands.","title":"2.3 Exceptions"},{"location":"tutorials/dialogsystem/#24-additions","text":"Additions work very similar to exceptions. The only differences are that \"add_if\" is used as the keyword and that the messages of the addition command do not override the regexes of the parent command, but instead add possible regexes to the existing ones, this is in particularly well suited to adding synonyms. In the following example, we allow the user to also say cyan or blue as a way to map to the primary_uniform_color blue . rule inform(primary_uniform_color) regex \"(the colo(u)?r|) should be {primary_uniform_color}\" if primary_uniform_color = \"#ff11bb\" regex \"the colo(u)?r should be pink\" add_if primary_uniform_color = \"blue\" regex \"the colo(u)?r should be cyan\" Note: Addition commands can take the same child commands as rule commands.","title":"2.4 Additions"},{"location":"tutorials/dialogsystem/#25-functions","text":"Functions are similar to rules. They can take an arbitrary number of parameters and return a regular expression (or normal string). The function command consists of the keyword \"function\" and the function description using the pattern function_name(argument_one, argument_two) . Functions are particularly useful to improve the structure of your files. In the previous example, we have added cyan to be a synonym of blue . If we want to add synonyms for all colours, we would have to list all possible regexes for all possible synonyms which would result in a huge number of lines and would be quite a mess. Instead, we can specify a function which lists all synonymous colour strings for each possible colour value centrally at one point in the file and whenever we want to use synonyms, we can simply call this function: function color_synonym(generic_color) # the color itself is almost always a synonym regex \"{generic_color}\" # 'add_if' adds regular expressions to the existing ones (see above) add_if generic_color = \"red\" regex \"maroon\" add_if generic_color = \"green\" regex \"lime( green)?\" regex \"olive( green)?\" regex \"neon green\" add_if generic_color = \"blue\" regex \"cyan\" regex \"turquoise\" # 'if' overwrites (!) existing regular expressions if generic_color = \"#ff11bb\" \"pink\" \"magenta\" The inform rule would then be simplified to: rule inform(color) \"(the colo(u)?r|it) should be {color_synonym(color)}\" Note: Function commands can take the same child commands as rule commands.","title":"2.5 Functions"},{"location":"tutorials/dialogsystem/#3-shortcuts-and-syntax-simplification","text":"The examples in the previous section show the ADVISER NLU syntax, as it is read by the interpreter. For simplifications and for a better overview, we allow some simplifications and shortcuts. In a preprocessing step, the simplified template is automatically transformed into a valid syntax.","title":"3 Shortcuts and syntax simplification"},{"location":"tutorials/dialogsystem/#31-comments-and-empty-lines","text":"While the original syntax requires one command per line, in the preprocessing step, empty lines and lines that start with a hashtag (#) are removed from the template file. Please note that non-leading hashtags are not recognised as comments currently.","title":"3.1 Comments and empty lines"},{"location":"tutorials/dialogsystem/#32-tabs","text":"The original syntax requires four spaces per block level. Alternatively, you can also use a tab instead, since all tabs are replaced by four spaces in the preprocessing step.","title":"3.2 Tabs"},{"location":"tutorials/dialogsystem/#33-automatic-insertion-of-the-regex-keyword","text":"Regexes do not necessarily need to be marked via the regex keyword. For all lines that start with double quotes, a regex keyword is automatically inserted at the beginning of the line in the preprocessing step. Example: rule request(primary_uniform_color) \"what is the( super)? hero's uniform colo(u)?r\\?\"","title":"3.3 Automatic insertion of the regex keyword"},{"location":"tutorials/dialogsystem/#34-colons","text":"A neater version of the example in 3.3 would be the following example: rule request(primary_uniform_color): \"what is the( super)? hero's uniform colo(u)?r\\?\" To allow this syntax, the preprocessor interprets everything coming after a colon (that\u2019s not wrapped by double quotes) as a new child command and moves it to the next line with the respective block level + 1. This way, the above example and the example from 3.3 are exactly the same after preprocessing.","title":"3.4 Colons"},{"location":"tutorials/dialogsystem/#the-nlu-module","text":"ADVISER's NLU module makes use of NLU files which specify the regular expressions with the previously presented syntax. The pipeline for creating the NLU is the following: Make sure you have created a domain . Write your NLU file using the custom syntax (see above), name it {domain_name}.nlu and place it in the folder resources/nlu_regexes . Execute the script gen_regexes.py in the folder tools/regextemplates like this: python3 tools/regextemplates/gen_regexes.py {domain_name} {domain_name} Example: python3 tools/regextemplates/gen_regexes.py superhero superhero Check that the tool has created the files {domain_name}InformRules.json for inform acts and {domain_name}RequestRules.json for request acts inside the resources/nlu_regexes folder. Once you have all these files, you can use the HandcraftedNLU module in services/nlu by simply passing your domain object in the constructor. from utils.domain.jsonlookupdomain import JSONLookupDomain from services.nlu.nlu import HandcraftedNLU domain = JSONLookupDomain ( 'superhero' ) nlu = HandcraftedNLU ( domain = domain ) You can type some exemplary messages and see which user acts are recognised by the NLU, when you are done, type \"bye\" to exit the loop: user_input = input ( '>>> ' ) while user_input . strip () . lower () not in ( '' , 'exit' , 'bye' , 'goodbye' ): user_acts = nlu . extract_user_acts ( user_input )[ 'user_acts' ] print ( ' \\n ' . join ([ repr ( user_act ) for user_act in user_acts ])) user_input = input ( '>>> ' )","title":"The NLU module"},{"location":"tutorials/dialogsystem/#rule-based-belief-state-tracker-bst","text":"The BST maintains a representation of the current dialog state. It is responsible for tracking the constraints a user provides over the course of a dialog and their probabilities, recording any requests from the current turn, tracking the types of user acts in the current turn, and registering the number of database matches for the given constraints. In the ADVISER 2.0 Toolkit, this state is built up over the course of the dialog as the user expresses more constraints. Probabilities for each constraint are determined by the NLU and are not currently further processed. An example of the beliefstate for the second turn in a dialog about IMS Courses can be seen below. {'user_acts': {<UserActionType.Inform: 'inform'>} 'informs': {'bachelor': {'true': 1.0 } 'turn': {'sose': 1.0 } 'ects': {'6': 1.0 } } 'requests': {} 'num_matches': 4 'discriminable': True } As mentioned previously the beliefstate is built up over time. Therfore there are currently only three entries in the informs dictionary as up to this point the user has only provided three constraints. Each constraint has a probability of 1.0 because neither the NLU nor BST have complex rules for handling ambiguous user utterances, but this is an area we may look to improve in the future. Example: Instantiating a rules based BST is as simple as running the code below bst = HandcraftedBST ( domain = super_domain )","title":"Rule-based Belief State Tracker (BST)"},{"location":"tutorials/dialogsystem/#policy","text":"","title":"Policy"},{"location":"tutorials/dialogsystem/#rule-based-policy","text":"The policy is responsible for determining the system's next action. In ADVISER 2.0, we provide two implementations of a rule-based policy (one designed to work with a database and one designed to work with an API backend). Both follow a similar decision flow. Since we focus here on task oriented dialog systems, the policy tries to help a user fulfill their goal as quickly as possible. A simplified version of a the decision making process in a rules based policy can be seen below: On each turn, the policy is capable of choosing one next action, these actions include: Welcome : greet the user InformByName : tell the user some new information InformByAlternatives : provide the user an alternative entity Request : ask the user for more information Select : ask the user to select between a given set of choices RequestMore : ask the user if they need more help Bad : inform the user their last utterance couldn't be parsed Bye : end the dialog As seen in the above diagram, the policy first works to handle general (non-domain specific) user actions. It then queries the database and only asks the user for more information if there are too many entries and asking for more information will help narrow down results. In the actual policy the decisions are a bit more nuanced, but the graphic gives the general idea of the types of rules required for a rules-based policy. Example: Instantiating a rules-based policy can be done with the code below: policy = HandcraftedPolicy ( domain = super_domain )","title":"Rule-based Policy"},{"location":"tutorials/dialogsystem/#reinforcement-learning-policy","text":"","title":"Reinforcement Learning Policy"},{"location":"tutorials/dialogsystem/#why-reinforcement-learning","text":"Developing handcrafted policies can be quite time consuming and have difficulties adapting to unseen scenarios. Therefore a machine learning approach can be preferable. In this type of scenario, training a machine learning agent would circumvent needing to hard code a rule for every edge case scenario. Unfortunately this type of approach normally requires a lot of data to train the policy. Especially for new domains, this kind of data just does not exist, which is where reinforcement learning comes in. The basic idea behind reinforcement learning is that an agent has a certain set of actions which it can take and is placed in a certain environment (which it may either know in advance or must learn as it explores). The agent then tries out the different actions it can take, each action altering the state (or the agent's perception of the environment) and generating a reward. In this way, the agent learns how to navigate through it's environment and find the path which yields the highest reward. This process is shown in the graph below. As mentioned before, as an input the agent receives the current state and reward and it uses these to choose what it thinks the next best action will be. This action results in a new state and a new reward.","title":"Why reinforcement Learning"},{"location":"tutorials/dialogsystem/#rl-in-the-context-of-dialog-policy","text":"In the context of dialog systems, the RL agent is the dialog policy and the actions it can take are defined by the SysActs . The environment is a user (simulated or real) and the state is represented by the beliefstate. The reward for each action backpropagates and is inversely proportional to the number of turns it took to complete the dialog + a fixed reward for dialogs where a user was able to fulfill their goal. In ADVISER 2.0, the RL policy is implemented using deep reinforcement learning. Similar to the Deep Q-learning algorithm , an action-value function is approximated by a neural network which outputs a value for each possible system action, given the vectorised representation of a turn\u2019s beliefstate as input. The exact action set is defined in the class RLPolicy where a set of base actions templates is defined with slot names from the ontology e.g. if you have a slot food in your ontology, it will create actions like inform#food , request#food and select#food which are used to inform a user about a certain type of food, to request the food type from the user or to let the user choose between multiple food types. These action templates are the initial output of the reinforcement learning policy, which then get expanded using beliefstate and database information. For more information on the architecture see the ADVISER paper . Example: As there are so many parameters for the RL policy, instantiating it requires slightly more code than instantiating other modules. To instantiate an RL policy with the parameters used in the ADVISER paper, see the code below: # Allows you to track training progress using tensorboard summary_writer = SummaryWriter ( os . path . join ( 'logs' , \"tutorial\" )) # logs summary statistics for each train/test epoch logger = DiasysLogger ( console_log_lvl = LogLevel . RESULTS , file_log_lvl = LogLevel . DIALOGS ) # Create RL policy instance with parameters used in ADVISER paper policy = DQNPolicy ( domain = domain , lr = 0.0001 , eps_start = 0.3 , gradient_clipping = 5.0 , buffer_cls = NaivePrioritizedBuffer , replay_buffer_size = 8192 , shared_layer_sizes = [ 256 ], train_dialogs = 1000 , target_update_rate = 3 , training_frequency = 2 , logger = logger , summary_writer = summary_writer ) logger: state space dim: 74 logger: action space dim: 9 logger: Gradient Clipping: 5.0 logger: Architecture: Dueling logger: Update: Double REPLAY MEMORY: NAIVE Prioritized ARCHITECTURE: Dueling","title":"RL in the Context of Dialog Policy"},{"location":"tutorials/dialogsystem/#user-simulator","text":"To train a reinforcement learning (RL) policy, it is often useful to use a simulated user instead of an actual one at least in the beginning. Simulations, while seldom accurate representations of real users, are substantially more patient. That is, to train or evaluate a dialog policy, thousands of dialogs are required. Having a real human sit through this many dialogs would be very expensive and tedious, especially since at the beginning of training, an RL policy will produce utterly nonsensical dialogs. In comparison, a simulated user is very cheap, will not be bored or annoyed, and can be called whenever you are ready to train or test a dialog. After initial training has been performed with a simulation, it is possible to conduct further training with human users. Since a user simulator is an important part of the training process for an RL policy, the next question becomes how to create one. In the ADVISER 2.0 Toolkit, we use an agenda based user simulator based off of work by Schatzmann et al. . The basic function of the simulator can be seen in the graphic below: To start a dialog, the user simulator will randomly generate a goal (shown in gray in panel 1). This goal will include all of the constraints (informs) the user wants to provide and all of the questions (requests) they will want to ask. Then this goal will be transformed into a series of user actions and placed in the action stack. On each turn after that, the user simulator will receive a system action as input and need to respond to it (panel 2) Here the user response is hardcoded in a similar way to the handcrafted policy. Any new actions generated to respond to the system will be placed at the top of the stack. Then a random number of actions between one and n will popped from the top of the stack and given out as the user acts for that turn. In addition to the hardcoded response for each type of system act, the user simulator's behavior is also controlled by configuration parameters. These control behavior such as the maximum number of actions the user takes per turn (default 3), and whether user goals should be possible to fulfill or if it doesn't matter. These additional parameters are stored in an external configuration file. Example: To create an instance of user simulator see below: user_sim = HandcraftedUserSimulator ( domain = super_domain )","title":"User Simulator"},{"location":"tutorials/dialogsystem/#evaluation","text":"In order to train an RL policy, in addition to a user simulator, we need a reward signal. That is where the evaluation module comes in. There are two evaluation classes which are important to consider. The first one is teh ObjectiveReachedEvaluator which provides the reward signal to the RL policy. The second is the PolicyEvaluator which tracks the percent of successful dialogs, average reward, and average turn number for an epoch. As a note, while the ObjectiveReachedEvaluator is already part of the RLPolicy class, to log statistics, you need to explicitely add the PolicyEvaluator to the dialog graph. As a default, a reward of -1 for every turn is assigned to encourage the policy to keep dialogs as short as possible. For every dialog where the user goal is fulfilled, a reward of 20 is given. Example: A PolicyEvaluator can be created with the code below: evaluator = PolicyEvaluator ( domain = domain , use_tensorboard = True , experiment_name = \"tutorial\" , logger = logger , summary_writer = summary_writer )","title":"Evaluation"},{"location":"tutorials/dialogsystem/#training-a-reinforcement-learning-policy","text":"Now that we have a user simulator and an evaluator, we can start using them to train our RL policy. This requires configuring the dialog graph slightly differently. Rather than the graph we saw in Tutorial One, we will only use the beliefstate tracker, RL Policy and a user simulator (optionally an evaluator as well to log statistics). This new graph can be seen below: To initialize training and evaluation, a random seed is generally used to set initial parameters. For the sake of reproducibility, it is recommended to store this seed along with the results so that in the future, you can initialize all random generators used in your pipeline with it. If you run the system twice with the same seed, you should get exactly the same results and dialogs. To train the system, a dialog graph is created with all necessary modules, then for the number of training epochs desired, dialogs are carried out and evaluated. During each training epoch, weights are updated for the RL policy and during each testing epoch, the success rate and length of dialogs are evaluated and recorded. The number of dialogs per epoch and the number of epochs can be determined by the user. As a note, when evaluating the overall performance of an RL policy, it is important to run the train/test loop multiple times with different random seeds to make sure that a good performance is not simply because of a good random seed. Example: Example training and evaluation code (warning takes some time to run): # SET CONSTANTS TRAIN_EPOCHS = 1 TRAIN_EPISODES = 1000 EVAL_EPISODES = 1000 MAX_TURNS = 25 # Choose how many repeated trials for i in range ( 1 ): common . init_random () # add seed here as a parameter to the init_random if wanted ds = DialogSystem ( services = [ user_sim , bst , policy , evaluator ], protocol = 'tcp' ) # Start train/eval loop for j in range ( TRAIN_EPOCHS ): # START TRAIN EPOCH evaluator . train () policy . train () evaluator . start_epoch () for episode in range ( TRAIN_EPISODES ): if episode % 100 == 0 : print ( \"DIALOG\" , episode ) logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () # START EVAL EPOCH evaluator . eval () policy . eval () evaluator . start_epoch () for episode in range ( EVAL_EPISODES ): logger . dialog_turn ( \" \\n\\n !!!!!!!!!!!!!!!! NEW DIALOG !!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n \" ) ds . run_dialog ( start_signals = { f 'user_acts/ { domain . get_domain_name () } ' : []}) evaluator . end_epoch () policy . save () ds . shutdown ()","title":"Training a reinforcement learning policy"},{"location":"tutorials/dialogsystem/#handcrafted-natural-language-generation-nlg","text":"To transform system acts into natural language utterances, ADVISER 2.0 uses handcrafted templates. Although machine learning generated output can have more breadth of expression, using templates guarentees that all system utterances will be grammatic and sensical. Using templates additionally allows dialog systems for new domains to be created even when there is not sufficient data for those domains to train a machine learning based NLG. In ADVISER 2.0, rather than mapping every sys_act, slot, value combination to an individual utterance, templates are used to generalize this process by specifying placeholders for a system act's slots and/or values. An example of this can be seen below: inform(name={X}, ects={Y}) \u2192 \"The course {X} is worth {Y} ECTS.\" During the dialog, the system iterates through the templates and chooses the first one for which the system act fits the template's signature.","title":"Handcrafted Natural Language Generation (NLG)"},{"location":"tutorials/dialogsystem/#types-of-templates","text":"Under adviser/resources/templates the handcrafted templates are stored in text files. The templates have the format where the left side represents the system act and the right side represents the natural language utterance. Variables are used instead of concrete slot values so that the templates can be reused as much as possbile. Each file is divided into several sections, we add examples: Templates for system general acts template hello(): \"Welcome to the IMS courses chat bot. How may I help you?\" Templates for system requests template request(lang): \"In which language shall the course be held?\" Methods for system informs function info(slot, value) if slot = \"ects\": \"is worth {value} ECTS\" if slot = \"turn\" if value = \"sose\": \"is offered in the summer semester\" if value = \"wise\": \"is offered in the winter semester\" Templates for system informs template inform_byname(name): \"I found the course {name}. What do you want to know about it?\" template inform_byname(name, turn): \"The course {name} {info(\"turn\", turn)}.\" Similarly, templates for system confirm, request more and select are included. Example: If system act is: inform(name='computational linguistics team laboratory', turn='wise') The NLG output will be: \"The course computational linguistics team laboratory is offered in the winter semester.\" Notes: Each domain has a ist own YourDomainNameMessages.nlg file No python script needs to be modified (domain independent) Example: The code to create an NLG service for the superhero domain is shown below: nlg = HandcraftedNLG ( domain = super_domain )","title":"Types of templates"},{"location":"tutorials/dialogsystem/#text-to-speech-tts","text":"The TTS module is an options module which converts the text natural language output to speech. In the ADVISER 2.0 toolkit, this is done using the ESPnet-TTS toolkit , which is an extension of the ESPnet toolkit mentioned in the ASR section. We use FastSpeech as a synthesis model, which provides substantially faster voice generation. Additionally, we are able to provide a \"cleaner\" file to optimize the synthesizer for abbreviations, such as Prof., Univ., IMS, NLP, ECTS, and PhD , as well as for German proper names, such as street names. These optimizations can be easily extended by adding additional words and pronunciation mappings to services/speech/cleaners.py . Example: Similar to how the ASR module also required a SpeechInputRecorder module, the TTS module also requires a SpeechOutputPlayer in order for the user to be able to hear the generated sound file. The SpeechOutputPlayer and SpeechOutputGenerator can be instantiated with the code below. NOTE: This code will only work if you have installed the requirements for the multimodal dialog system (they are large, so this is not recommended if you will only be working with text) from services.hci.speech import SpeechOutputGenerator from services.hci.speech import SpeechOutputPlayer # (GPU: 0.4 s/per utterance, CPU: 11 s/per utterance) speech_out_generator = SpeechOutputGenerator ( use_cuda = False ) speech_out_player = SpeechOutputPlayer ( conversation_log_dir = conversation_log_dir )","title":"Text to Speech (TTS)"},{"location":"tutorials/dialogsystem/#cleaning-up-tts-output","text":"To improve the speech generation, we also provide a file servicse/hci/speech/cleaners.py which allows you to manually correct pronunciation of words. This allows you to provide either phonetic transcriptions for acronyms, abbreviations, and foreign words or pronunciation patterns for things such as email addresses or telephone numbers. One of the methods from this file is shown below. def expand_abbreviations ( text ): \"\"\" Preprocesses a text to turn abbreviations into forms that the TTS can pronounce properly text (string): Text to be preprocessed \"\"\" for regex , replacement in _abbreviations : text = re . sub ( regex , replacement , text ) return text # List of (word, replacement) pairs for acronym or special words: _acronym = [ ( ' a ' , ' ae ' ), ( ' s ' , ' eh s, ' ), ( 'array' , 'ER RAY' ), ( 'API' , 'AE P I' ), ( 'distributional' , 'distributionall' ), ( 'ECTS' , 'E C T EH S,' ), ( 'Erasmus' , 'E RAS MOUS' ), ( 'ID' , 'I D' ), ( 'IMS' , 'I M EH S' ), ( 'NLP' , 'N L P' ), ( 'PhD' , 'P h D' ), ( 'PWR 05B' , 'Pfaffen vaald ring five B, ' ), ( 'psycholinguistics' , 'psycho linguistics' ), ( 'stuttgart' , 'stu gart' ), ( 'Stuttgart' , 'Stu gart' ), ( 'vegan' , 'viygan' ), ( 'Vegan' , 'Viygan' ), ( 'ImsLecturers' , 'I M EH S Lecturers' ), ( 'imsLecturers' , 'I M EH S Lecturers' ), ]","title":"Cleaning up TTS output"},{"location":"tutorials/dialogsystem/#chatting-with-the-dialog-system","text":"There are two main ways to interact with an ADVISER 2.0 dialog system: 1) by typing input and reading system output from the terminal or 2) by speaking to the system and receiving audio responses. Since we've already taken a quick look at a spoken dialog system, let's quickly discuss a text based one. In short, a text based system is much simpler. Instead of needing multiple modules for input/output, the text based system just needs ConsoleInput and ConsoleOutput . When these are added to the dialog system, the system utterances will appear on the terminal and the user can give their response by typing and pressing enter to end an utterance. Example: An example of a dialog system constructed this way can be seen below. NOTE: There is a problem with the user input in Jupyter Notebook. To try the code out, copy the code below to a new file in the adviser folder and run it from the dialog system # Importing everything we need for a dialog system from utils.domain.jsonlookupdomain import JSONLookupDomain from utils.logger import DiasysLogger , LogLevel from services.hci import ConsoleInput , ConsoleOutput from services.nlu import HandcraftedNLU from services.bst import HandcraftedBST from services.policy import HandcraftedPolicy from services.nlg import HandcraftedNLG from services.domain_tracker import DomainTracker from services.service import DialogSystem # create domain super_domain = JSONLookupDomain ( name = \"superhero\" ) # create domain specific modules nlu = HandcraftedNLU ( domain = super_domain ) bst = HandcraftedBST ( domain = super_domain ) policy = HandcraftedPolicy ( domain = super_domain ) nlg = HandcraftedNLG ( domain = super_domain ) d_tracker = DomainTracker ( domains = [ super_domain ]) # Input modules (just allow access to terminal for text based dialog) user_in = ConsoleInput ( domain = \"\" ) user_out = ConsoleOutput ( domain = \"\" ) logger = DiasysLogger ( console_log_lvl = LogLevel . DIALOGS ) ds = DialogSystem ( services = [ d_tracker , user_in , user_out , nlu , bst , policy , nlg ], debug_logger = logger ) error_free = ds . is_error_free_messaging_pipeline () if not error_free : ds . print_inconsistencies () ds . draw_system_graph ( name = 'system' , show = False ) # start dialog for _ in range ( 1 ): ds . run_dialog ({ 'gen_user_utterance' : \"\" }) ds . shutdown ()","title":"Chatting with the dialog system"},{"location":"tutorials/dialogsystem/#check-your-understanding-optional","text":"Now that you have read how each of the basic modules provided by the ADVISER 2.0 Toolkit works, let's actually test it out.","title":"Check Your Understanding (Optional)"},{"location":"tutorials/dialogsystem/#run-a-dialog-system","text":"There should be a program in the tutorials folder called tutorial_chat.py All the modules you will need are already imported, but you will need to follow the comments and create a dialog system for yourself.","title":"Run a Dialog System"},{"location":"tutorials/dialogsystem/#create-a-basic-nlu","text":"Right now we have provided a very simple .nlu file for the superhero domain. Try adding more synonyms/more regexes to the rules in superhero.nlu so that it can capture more realistic user utterances.","title":"Create a basic NLU"},{"location":"tutorials/dialogsystem/#update-the-nlg","text":"Try adding some more NLG templates to the superhero domain, or updating existing ones to sound more natural (the file is located in the resources/nlg_templates folder).","title":"Update the NLG"},{"location":"tutorials/dialogsystem/#test-out-your-new-system","text":"Once you are satisfied with your NLU and NLG files, test out the system again. Make sure you remembered to recomile the regexes first though!","title":"Test out your new system"},{"location":"tutorials/introduction/","text":"You can find the Jupyter Notebook in our GitHub repository . Introduction to Dialog Systems \u00b6 Dialog Systems \u00b6 At it's simplest, a dialog system is a computer program with which humans can talk. As technology has improved, dialog systems have become more popular and more complex. They include everyday personal assistants such as Siri, Alexa, or Google Home, as well as more specialized systems like those you might encounter if you call your bank, or need to chat with an online customer service agent. Dialog systems even include some which try to mimic more natural human conversations and have the ability to discuss about \"open-world\" topics rather than being limited to specific domains. Task oriented v. Chatbot Systems \u00b6 Chatbot Systems \u00b6 Chatbot systems have been around since 1964 when Joseph Weizenbaum created the ELIZA chatbot system, a virtual therapist that could hold a conversation by mirroring back user utterances (eg. http://www.med-ai.com/models/eliza.html if you want to test it out). The main goal of chatbot systems is to create an experience that engages a user. Conversations do not need to be about any one topic in particular and can wander from topic to topic without trying to fulfill any type of concrete goal. Often, these systems work to form some type of emotional bond with the user and their success is measured by how long a user is willing to talk with the system (usually measured in turns). One of the most advanced chatbot systems currently available is Microsoft's XiaoIce with whom user's have kept a single conversation going for more than 24 hours (7,000 turns). Task Oriented Dialog Systems \u00b6 In comparison to chatbot systems, task oriented dialog systems seek to help the user accomplish a defined goal as efficiently as possible. These types of goals could include things such as finding a suitable restaurant, resolving a customer service complaint, or even just registering a new credit card. These systems are generally much more limited in scope than chatbots, but have been gaining popularity commercially as a way for companies to offer a more interactive/efficient/flexible interface for users than traditional websites/customer service telephone lines. Task oriented dialog systems are generally evaluated by how few turns it takes to complete a dialog and how frequently a user is able to accomplish their goal. The remainder of this tutorial will focus on task oriented dialog systems. Domains in Task Oriented Dialog \u00b6 A domain here refers to the specific type of task that a task oriented dialog system handles. For example, a \"restaurant\" domain system could help a user find a restaurant to have dinner at. A \"lecturer\" domain system might help a user locate which lecturer is in charge of eg. coordinating the Erasmus program and it could then provide information about that lecturer's office hours or email address. Normally the topics which can be discussed in a domain are defined in an ontology and the knowledge about entities in a domain is stored in a database. Modular Dialog Systems \u00b6 There are two main strategies when it comes to designing a dialog system: 1) using a neural model to directly map the sequence of words in the user utterance to a sequence of words for a system utterance. Or 2) breaking up dialog processessing functionality into a series of modules where each module is responsible for a step in the processing pipeline. The first approach is more common in chatbot type systems, while the second is more common in task oriented dialog systems. The general modules included in a task oriented dialog system are listed below and a brief overview of their functionality is provided: ASR: The Automatic Speech Recognition (ASR) module is optional. It converts a spoken user utterance to text in the case of a spoken dialog system. NLU: The Natural Language Understanding (NLU) module is responsible for mapping the natural language (text) user utterance to a semantic frame (machine readable) version. BST: The Belief State Tracker (BST) is responsible for keeping track of information the user has provided up to the current turn in the dialog. Policy: The Policy is responsible for deciding the next action the system should take based on the current system belief state. NLG: The Natural Language Generation (NLG) module is responsible for converting the machine readable (semantic frame) representation of the system's next action into a natural language representation. TTS: The Text To Speech (TTS) module is again optional and used in a spoke dialog system to convert a text based system utterance into speech. It is also important to note that the functionality of these modules can be combined. These modules may additionally be rule-based or machine learning-based and a dialog system can be made of a combination of rules based and machine learning based modules. In the case of machine learning-based modules, depending on the architectures, modules may be trained independently or jointly.","title":"Introduction"},{"location":"tutorials/introduction/#introduction-to-dialog-systems","text":"","title":"Introduction to Dialog Systems"},{"location":"tutorials/introduction/#dialog-systems","text":"At it's simplest, a dialog system is a computer program with which humans can talk. As technology has improved, dialog systems have become more popular and more complex. They include everyday personal assistants such as Siri, Alexa, or Google Home, as well as more specialized systems like those you might encounter if you call your bank, or need to chat with an online customer service agent. Dialog systems even include some which try to mimic more natural human conversations and have the ability to discuss about \"open-world\" topics rather than being limited to specific domains.","title":"Dialog Systems"},{"location":"tutorials/introduction/#task-oriented-v-chatbot-systems","text":"","title":"Task oriented v. Chatbot Systems"},{"location":"tutorials/introduction/#chatbot-systems","text":"Chatbot systems have been around since 1964 when Joseph Weizenbaum created the ELIZA chatbot system, a virtual therapist that could hold a conversation by mirroring back user utterances (eg. http://www.med-ai.com/models/eliza.html if you want to test it out). The main goal of chatbot systems is to create an experience that engages a user. Conversations do not need to be about any one topic in particular and can wander from topic to topic without trying to fulfill any type of concrete goal. Often, these systems work to form some type of emotional bond with the user and their success is measured by how long a user is willing to talk with the system (usually measured in turns). One of the most advanced chatbot systems currently available is Microsoft's XiaoIce with whom user's have kept a single conversation going for more than 24 hours (7,000 turns).","title":"Chatbot Systems"},{"location":"tutorials/introduction/#task-oriented-dialog-systems","text":"In comparison to chatbot systems, task oriented dialog systems seek to help the user accomplish a defined goal as efficiently as possible. These types of goals could include things such as finding a suitable restaurant, resolving a customer service complaint, or even just registering a new credit card. These systems are generally much more limited in scope than chatbots, but have been gaining popularity commercially as a way for companies to offer a more interactive/efficient/flexible interface for users than traditional websites/customer service telephone lines. Task oriented dialog systems are generally evaluated by how few turns it takes to complete a dialog and how frequently a user is able to accomplish their goal. The remainder of this tutorial will focus on task oriented dialog systems.","title":"Task Oriented Dialog Systems"},{"location":"tutorials/introduction/#domains-in-task-oriented-dialog","text":"A domain here refers to the specific type of task that a task oriented dialog system handles. For example, a \"restaurant\" domain system could help a user find a restaurant to have dinner at. A \"lecturer\" domain system might help a user locate which lecturer is in charge of eg. coordinating the Erasmus program and it could then provide information about that lecturer's office hours or email address. Normally the topics which can be discussed in a domain are defined in an ontology and the knowledge about entities in a domain is stored in a database.","title":"Domains in Task Oriented Dialog"},{"location":"tutorials/introduction/#modular-dialog-systems","text":"There are two main strategies when it comes to designing a dialog system: 1) using a neural model to directly map the sequence of words in the user utterance to a sequence of words for a system utterance. Or 2) breaking up dialog processessing functionality into a series of modules where each module is responsible for a step in the processing pipeline. The first approach is more common in chatbot type systems, while the second is more common in task oriented dialog systems. The general modules included in a task oriented dialog system are listed below and a brief overview of their functionality is provided: ASR: The Automatic Speech Recognition (ASR) module is optional. It converts a spoken user utterance to text in the case of a spoken dialog system. NLU: The Natural Language Understanding (NLU) module is responsible for mapping the natural language (text) user utterance to a semantic frame (machine readable) version. BST: The Belief State Tracker (BST) is responsible for keeping track of information the user has provided up to the current turn in the dialog. Policy: The Policy is responsible for deciding the next action the system should take based on the current system belief state. NLG: The Natural Language Generation (NLG) module is responsible for converting the machine readable (semantic frame) representation of the system's next action into a natural language representation. TTS: The Text To Speech (TTS) module is again optional and used in a spoke dialog system to convert a text based system utterance into speech. It is also important to note that the functionality of these modules can be combined. These modules may additionally be rule-based or machine learning-based and a dialog system can be made of a combination of rules based and machine learning based modules. In the case of machine learning-based modules, depending on the architectures, modules may be trained independently or jointly.","title":"Modular Dialog Systems"},{"location":"tutorials/services/","text":"You can find the Jupyter Notebook in our GitHub repository . Introduction to ADVISER 2.0 Services \u00b6 In this tutorial, we will discuss Services which are the backbone of the ADVISER 2.0 toolkit and allow for the creation of multi-modal dialog systems. What are Services? \u00b6 A dialog system created with ADVISER 2.0 is constructed from services , with a service for each module from the modular dialog system graph shown in the previous tutorial. Each service receives inputs from previous services, processes them, and then passes the results on to the next services. An example of this can be seen in the dialog system graph below: Each block represents one service (module) in the dialog system and each arrow represents the inputs/outputs of that service. To communicate with each other, services use a publisher/subscriber pattern. This will be explained later in this tutorial, but basically means, that a service defines a list of inputs it expects and that it outputs. The service is then asynchronously called once all the expected inputs are available. Example: Let's take the HandcraftedNLU service class as an example: * The service receives a string user_utterance as input * It is important to note: the source of this input is unknown to the service and does not matter. In this example, it comes from the console, but it could just as easily come from a GUI or an automatic speech recognition (ASR) service The service outputs a list of user acts extracted from the user utterance The receiver(s) of the outputs is not known to the service and does not matter In summary, a service does not posess nor require any knowledge about a dialog system - its purpose is to process a piece or stream of information and send the result out so other services might use it. This allows dialog systems to be created where it is very easy to swap out (or combine) different services. Publish Subscribe at a high level \u00b6 Information is passed between services using the publisher/subscriber pattern which enables for asynchronous communication between services: * A Publisher publishes messages to a certain topic (where a topic is just a string specifying the name of an information channel) * A Subscriber subscribes to a certain topic, and is notified everytime a new message is published to this topic If a method is a subscriber, it is automatically called as soon as it has recieved a message for each of the topics it subscribes to. This means that rather than relying on a traditional linear architecture, where each module in the dialog system must be called sequentially, we can break away and allow modules to run arbitrarily in parallel - which is critical for handling multimodal input which might need continuous processing. As a note, methods in a service class may act as both a publisher and a subscriber, only one, or neither. Implementing a Service \u00b6 With this terminology in mind, let's look at the way a service subscribes to and publishes messages. As a first step we'll handle all the imports needed for this tutorial. # FIRST SET UP ENVIRONMENT import sys import os from typing import List import time sys . path . append ( os . path . abspath ( '../..' )) from services.service import Service , PublishSubscribe , RemoteService from utils.topics import Topic from services.service import DialogSystem from utils.domain.domain import Domain from utils.logger import DiasysLogger , LogLevel Creating our First Service \u00b6 For our first service, we are going to make a simple class with a method that subscribes to two input topics, \"A\" and \"B\" and can publish to two output topics: \"C\" and \"D\". The code for this can be seen below: class ConcatenateService ( Service ): @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): \"\"\" A method to concatenate the content of two input topics and conditionally publish to either topic \"C\" or topic \"D\" Args: A (int): message for topic \"A\" B (str): message for topic \"B\" Return: (dict): dictionary where key is the topic to be published to (conditionally \"C\" or \"D\" depending on whether the value of A is 3) and the value is the concatenation of inputs A and B \"\"\" print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } Inheriting from the Service class: Any service that wants to send / receive messages in the ADVISER 2.0 system must inherit from services.service.Service . This class handles the communication mechanisms between services and makes sure the service is properly registered with the dialog system so that messages are properly delivered to and received by the appropriate services. Decorating Methods with PublishSubscribe: Each service method that should send / receive messages must be decorated with the services.service.PublishSubscribe decorator. Let's have a look at the decorator arguments: sub_topics : a list of topics the method needs as inputs. The method will be called as soon as at least 1 message for each subscribed topic has been received and in cases where multiple messages for a topic are received, only the most recent message will be used. Every topic in the sub_topics list, must be also be included as a method argument with the same name . This allows the system to map message content to method arguments. pub_topics : a list of topics the function wants to publish messages to. You don't have to publish anything during a function call, even if you include entries in the pub_topics list. But if you do want to publish a message, make sure: Only publish dictionaries! Otherwise, the system does not know what topic to send your return values to You are free to choose not to publish to all topics declared in pub_topics In our example, we have the option to publish to topic C and to topic D , but this method will only to publish to one or the other depending on the message content of A Dictionary keys must correspond to the topics in the pub_topics list Example: Assume the decorated function receives the following input as shown above: A: 1 A: 2 A: 3 B: \"dropped Messages\" How will the concatenate method be called: * The concatenate method will only be called after the fourth message, because this is the first time where at least one message per topic is available (3 messages for A , 1 for B ). * Since only the most recent messages will be used, the function will receive the following input: ( A=3 , B=\"dropped Messages\" ) * The variable result will thus contain the string 3 dropped Messages and this will be publised to topic D * As a note: There are mechanisms for collecting all messages instead of dropping them, but those will be introduced later. Creating a Second Service \u00b6 Finally, we need a second service which is responsible for generating the messages for topics A and B . This new service will also be responsible for shutting down the dialog loop when the example is over, by publishing True to the topic DIALOG_END . This topic is one of the default control message topics and is required to end a dialog. When we create our first dialog system, we will explain this in more detail. For now, let's take a look at our second service: class PrintService ( Service ): @PublishSubscribe ( sub_topics = [ \"D\" ], pub_topics = [ Topic . DIALOG_END ]) def print_d ( self , D : str ): \"\"\" A method which prints the content of topic D and then publishes the end dialog signal Args: D (str): content of topic D, represents the output of the method concatenate Return: (dict): key represents the topic DIALOG_END which should be publsihed to with the value True \"\"\" print ( f \"RECEIVED D= { D } \" ) return { Topic . DIALOG_END : True } @PublishSubscribe ( sub_topics = [ \"start\" ]) def turn_start ( self , start : bool = True ): \"\"\" A method to start the example communication, it waits for the signal to start the dialog and then calls the send_a method three times followed by the send_b method once Args: start (bool): The signal to start the dialog system (will be published by whatever DialogSystem object that this class is registered to) \"\"\" a = 1 while a < 4 : time . sleep ( 0.5 ) self . send_a ( a ) a += 1 time . sleep ( 0.5 ) self . send_b () @PublishSubscribe ( pub_topics = [ \"A\" ]) def send_a ( self , a : int ): \"\"\" A method to print a given integer a and then publish it to topic \"A\" Args: a (int): the integer to publish to topic \"A\" Return: (dict): where the key is \"A\" (topic to publish to) and the value is the given int a \"\"\" print ( \"SENDING A=\" , a ) return { 'A' : a } @PublishSubscribe ( pub_topics = [ \"B\" ]) def send_b ( self ): \"\"\" A method to publish \"messages dropped!\" to topic \"B\" Return: (dict): where the key is \"B\" (topic to publish to) and the value is \"messages dropped!\" \"\"\" print ( \"SENDING B\" ) return { 'B' : \"messages dropped!\" } print_d(): This method subscribes to the content of topic D and when it receives a message, prints the content and publishes a signal to DIALOG_END to finish the dialog. turn_start(): This method is called once it receives a message from the start topic. It then sends three messages to topic A (by calling send_a ) and one message to topic B (by calling send_b ). The sleep calls are inserted here so we can watch the messages in the correct order (which is otherwise not guaranteed, since this is a multithreaded system). In real applications, such sleep statements are not necessary send_a(): This method does not subscribe to anything, but publishes a given int a to topic A send_b(): This method does not subscribe to anything, but publishes a given string b to topic B Instantiating and Shutting Down a Dialog System \u00b6 With these two services, we can test the message behavior we just described. The first step is creating a dialog system and then registering these two services to it. A dialog system is important, because it is responsible for handling synchronization and message passing, registering remote services, and providing debugging functionality. Creating a New Dialog System \u00b6 To get started, we will create an instance of each of our services and register them with a dialog system by passing them in to the services parameter as a list: concatenate_service = ConcatenateService () print_service = PrintService () ds = DialogSystem ( services = [ concatenate_service , print_service ], debug_logger = None ) Note: Because a dialog system also sets up the communications pipelines for each of the services, it is important that you only have one dialog system active at a time. If you try to instantiate a new dialog system without shutting down the previous one, you will get an error as the ports are already in use. You can shut down the dialog system by typing: ds.shutdown() Debugging a Dialog System \u00b6 Checking for Inconsistencies \u00b6 To see if all of our required / offered service topics are actually connected to something, we can check the dialog system for inconsistencies: ds . print_inconsistencies () \u001b[91m (Potential) Errors (subscribed topics without publishers): topic: 'start', subscribed to in services: {'PrintService'} \u001b[0m \u001b[93m Warnings (published topics without subscribers): topic: 'C', published in services: {'ConcatenateService'} topic: 'dialog_end', published in services: {'PrintService'} \u001b[0m Here, we can see that PrintService has to receive an external start -topic message to be able to function - without that, turn_start will never be called. In this case, this is okay because the dialog system can provide this start message, however in a case where we would want start turn to be called multiple times, this would be a problem. Additionally, we publish to topic C but no service subscribes to this. As you can see, when your functions are not called as expected, this output might help you track down bugs in your system. If you want to get even more detailed debug output (print all messages and associated topics), you can provide a utils.logger.DiasysLogger instance to the debug_logger argument of the DialogSystem constructor and to the constructor for each service you want to log information from. Displaying the System Graph \u00b6 To get an overview of the system, we can also draw a graph of all services and their connections which is helpful to make sure that all services are connected in the ways that we thought they were. The code for drawing this graph can be seen below: ds . draw_system_graph ( name = 'tutorialgraph' , show = False ) # render image to tutorials/tutorialgraph.gv.png # render image in jupyter notebook from IPython.display import Image display ( Image ( filename = 'tutorialgraph.gv.png' )) You can see in the system graph that PrintService needs an outside message to a start topic (which in turn triggers the counter for $a=1,\\dots,3$). Both topics, A and B , are published by PrintService and subscribed to by ConcatenateService . Similarly, ConcatenateService publishes to topic C and D , where D is subscribed to by PrintService . C however is published but never subscribed to - which is shown by the arrow from ConcatenateService to the UNCONNECTED SERVICES node in the dialog graph. Logging \u00b6 In order to debug more effectively, ADVISER 2.0 also comes with logging functionality through the class utils.logger.DiasysLogger . The logger has the option to log to the console and/or to a file with the following log levels: * NONE: No information will be logged * RESULTS: Summary information will be logged about the success rate/# of turns after an epoch of training/testing * DIALOGS: All user/system utterances and summary statistics The logger can be passed to any of the modules, to log that module's output, however when passed to the dialog system itself, the logger will additionally be able to log any messages sent through and all received by the dialog system - including the message's topic and content. A DiasysLogger can be instantiated as below: # create logger to log everything to a file logger = DiasysLogger ( console_log_lvl = LogLevel . NONE , file_log_lvl = LogLevel . DIALOGS ) And passed to a dialog system to log all message communication by passing it in with the debug_logger parameter on instantiation. #ds = DialogSystem(services=[concatenate_service, print_service], debug_logger=logger) Running a Dialog \u00b6 To start a new dialog you can call the DialogSystem object's run_dialog method. When doing this, it is important to specify a start_signal in the form of a dictionary where keys are the topics the signal will be published to and values are the message content to be published. We need an external signal because the run_dialog method is blocking and no function calls after run_dialog will be executed until the dialog sends an exit signal. Therefore, the start signal is an external one time message (or multiple messages) which kickstart the normal dialog loop. In our case, the start signal is to publish to the start topic and since the value of this is never used, we simply set it to True . ds . run_dialog ( start_signals = { 'start' : True }) ds . shutdown () SENDING A= 1 SENDING A= 2 SENDING A= 3 SENDING B CONCATENATING 3 AND messages dropped! RECEIVED D=3 messages dropped! ... And we get exactly the output described in the previous section! concatenate was only called after a message from both topics, A and B , arrived - and since A received multiple messages before B , those messages were dropped so only one concatenated string is printed. Since the dialog system loop is blocking for a whole dialog, it is very important to make sure there is an end condition to close the loop (i.e. publishing True to the topic DIALOG_END ). If none of our services did this, this notebook code would be stuck after calling the run_dialog and no new code could be executed. To prove this is not the case, try running the code cell below :) print ( \"Not stuck in a dialog loop!\" ) Not stuck in a dialog loop!","title":"Services"},{"location":"tutorials/services/#introduction-to-adviser-20-services","text":"In this tutorial, we will discuss Services which are the backbone of the ADVISER 2.0 toolkit and allow for the creation of multi-modal dialog systems.","title":"Introduction to ADVISER 2.0 Services"},{"location":"tutorials/services/#what-are-services","text":"A dialog system created with ADVISER 2.0 is constructed from services , with a service for each module from the modular dialog system graph shown in the previous tutorial. Each service receives inputs from previous services, processes them, and then passes the results on to the next services. An example of this can be seen in the dialog system graph below: Each block represents one service (module) in the dialog system and each arrow represents the inputs/outputs of that service. To communicate with each other, services use a publisher/subscriber pattern. This will be explained later in this tutorial, but basically means, that a service defines a list of inputs it expects and that it outputs. The service is then asynchronously called once all the expected inputs are available. Example: Let's take the HandcraftedNLU service class as an example: * The service receives a string user_utterance as input * It is important to note: the source of this input is unknown to the service and does not matter. In this example, it comes from the console, but it could just as easily come from a GUI or an automatic speech recognition (ASR) service The service outputs a list of user acts extracted from the user utterance The receiver(s) of the outputs is not known to the service and does not matter In summary, a service does not posess nor require any knowledge about a dialog system - its purpose is to process a piece or stream of information and send the result out so other services might use it. This allows dialog systems to be created where it is very easy to swap out (or combine) different services.","title":"What are Services?"},{"location":"tutorials/services/#publish-subscribe-at-a-high-level","text":"Information is passed between services using the publisher/subscriber pattern which enables for asynchronous communication between services: * A Publisher publishes messages to a certain topic (where a topic is just a string specifying the name of an information channel) * A Subscriber subscribes to a certain topic, and is notified everytime a new message is published to this topic If a method is a subscriber, it is automatically called as soon as it has recieved a message for each of the topics it subscribes to. This means that rather than relying on a traditional linear architecture, where each module in the dialog system must be called sequentially, we can break away and allow modules to run arbitrarily in parallel - which is critical for handling multimodal input which might need continuous processing. As a note, methods in a service class may act as both a publisher and a subscriber, only one, or neither.","title":"Publish Subscribe at a high level"},{"location":"tutorials/services/#implementing-a-service","text":"With this terminology in mind, let's look at the way a service subscribes to and publishes messages. As a first step we'll handle all the imports needed for this tutorial. # FIRST SET UP ENVIRONMENT import sys import os from typing import List import time sys . path . append ( os . path . abspath ( '../..' )) from services.service import Service , PublishSubscribe , RemoteService from utils.topics import Topic from services.service import DialogSystem from utils.domain.domain import Domain from utils.logger import DiasysLogger , LogLevel","title":"Implementing a Service"},{"location":"tutorials/services/#creating-our-first-service","text":"For our first service, we are going to make a simple class with a method that subscribes to two input topics, \"A\" and \"B\" and can publish to two output topics: \"C\" and \"D\". The code for this can be seen below: class ConcatenateService ( Service ): @PublishSubscribe ( sub_topics = [ \"A\" , \"B\" ], pub_topics = [ \"C\" , \"D\" ]) def concatenate ( self , A : int = None , B : str = None ) -> dict ( C = str , D = str ): \"\"\" A method to concatenate the content of two input topics and conditionally publish to either topic \"C\" or topic \"D\" Args: A (int): message for topic \"A\" B (str): message for topic \"B\" Return: (dict): dictionary where key is the topic to be published to (conditionally \"C\" or \"D\" depending on whether the value of A is 3) and the value is the concatenation of inputs A and B \"\"\" print ( \"CONCATENATING \" , A , \"AND \" , B ) result = str ( A ) + \" \" + B if A == 3 : return { 'D' : result } else : return { 'C' : result } Inheriting from the Service class: Any service that wants to send / receive messages in the ADVISER 2.0 system must inherit from services.service.Service . This class handles the communication mechanisms between services and makes sure the service is properly registered with the dialog system so that messages are properly delivered to and received by the appropriate services. Decorating Methods with PublishSubscribe: Each service method that should send / receive messages must be decorated with the services.service.PublishSubscribe decorator. Let's have a look at the decorator arguments: sub_topics : a list of topics the method needs as inputs. The method will be called as soon as at least 1 message for each subscribed topic has been received and in cases where multiple messages for a topic are received, only the most recent message will be used. Every topic in the sub_topics list, must be also be included as a method argument with the same name . This allows the system to map message content to method arguments. pub_topics : a list of topics the function wants to publish messages to. You don't have to publish anything during a function call, even if you include entries in the pub_topics list. But if you do want to publish a message, make sure: Only publish dictionaries! Otherwise, the system does not know what topic to send your return values to You are free to choose not to publish to all topics declared in pub_topics In our example, we have the option to publish to topic C and to topic D , but this method will only to publish to one or the other depending on the message content of A Dictionary keys must correspond to the topics in the pub_topics list Example: Assume the decorated function receives the following input as shown above: A: 1 A: 2 A: 3 B: \"dropped Messages\" How will the concatenate method be called: * The concatenate method will only be called after the fourth message, because this is the first time where at least one message per topic is available (3 messages for A , 1 for B ). * Since only the most recent messages will be used, the function will receive the following input: ( A=3 , B=\"dropped Messages\" ) * The variable result will thus contain the string 3 dropped Messages and this will be publised to topic D * As a note: There are mechanisms for collecting all messages instead of dropping them, but those will be introduced later.","title":"Creating our First Service"},{"location":"tutorials/services/#creating-a-second-service","text":"Finally, we need a second service which is responsible for generating the messages for topics A and B . This new service will also be responsible for shutting down the dialog loop when the example is over, by publishing True to the topic DIALOG_END . This topic is one of the default control message topics and is required to end a dialog. When we create our first dialog system, we will explain this in more detail. For now, let's take a look at our second service: class PrintService ( Service ): @PublishSubscribe ( sub_topics = [ \"D\" ], pub_topics = [ Topic . DIALOG_END ]) def print_d ( self , D : str ): \"\"\" A method which prints the content of topic D and then publishes the end dialog signal Args: D (str): content of topic D, represents the output of the method concatenate Return: (dict): key represents the topic DIALOG_END which should be publsihed to with the value True \"\"\" print ( f \"RECEIVED D= { D } \" ) return { Topic . DIALOG_END : True } @PublishSubscribe ( sub_topics = [ \"start\" ]) def turn_start ( self , start : bool = True ): \"\"\" A method to start the example communication, it waits for the signal to start the dialog and then calls the send_a method three times followed by the send_b method once Args: start (bool): The signal to start the dialog system (will be published by whatever DialogSystem object that this class is registered to) \"\"\" a = 1 while a < 4 : time . sleep ( 0.5 ) self . send_a ( a ) a += 1 time . sleep ( 0.5 ) self . send_b () @PublishSubscribe ( pub_topics = [ \"A\" ]) def send_a ( self , a : int ): \"\"\" A method to print a given integer a and then publish it to topic \"A\" Args: a (int): the integer to publish to topic \"A\" Return: (dict): where the key is \"A\" (topic to publish to) and the value is the given int a \"\"\" print ( \"SENDING A=\" , a ) return { 'A' : a } @PublishSubscribe ( pub_topics = [ \"B\" ]) def send_b ( self ): \"\"\" A method to publish \"messages dropped!\" to topic \"B\" Return: (dict): where the key is \"B\" (topic to publish to) and the value is \"messages dropped!\" \"\"\" print ( \"SENDING B\" ) return { 'B' : \"messages dropped!\" } print_d(): This method subscribes to the content of topic D and when it receives a message, prints the content and publishes a signal to DIALOG_END to finish the dialog. turn_start(): This method is called once it receives a message from the start topic. It then sends three messages to topic A (by calling send_a ) and one message to topic B (by calling send_b ). The sleep calls are inserted here so we can watch the messages in the correct order (which is otherwise not guaranteed, since this is a multithreaded system). In real applications, such sleep statements are not necessary send_a(): This method does not subscribe to anything, but publishes a given int a to topic A send_b(): This method does not subscribe to anything, but publishes a given string b to topic B","title":"Creating a Second Service"},{"location":"tutorials/services/#instantiating-and-shutting-down-a-dialog-system","text":"With these two services, we can test the message behavior we just described. The first step is creating a dialog system and then registering these two services to it. A dialog system is important, because it is responsible for handling synchronization and message passing, registering remote services, and providing debugging functionality.","title":"Instantiating and Shutting Down a Dialog System"},{"location":"tutorials/services/#creating-a-new-dialog-system","text":"To get started, we will create an instance of each of our services and register them with a dialog system by passing them in to the services parameter as a list: concatenate_service = ConcatenateService () print_service = PrintService () ds = DialogSystem ( services = [ concatenate_service , print_service ], debug_logger = None ) Note: Because a dialog system also sets up the communications pipelines for each of the services, it is important that you only have one dialog system active at a time. If you try to instantiate a new dialog system without shutting down the previous one, you will get an error as the ports are already in use. You can shut down the dialog system by typing: ds.shutdown()","title":"Creating a New Dialog System"},{"location":"tutorials/services/#debugging-a-dialog-system","text":"","title":"Debugging a Dialog System"},{"location":"tutorials/services/#checking-for-inconsistencies","text":"To see if all of our required / offered service topics are actually connected to something, we can check the dialog system for inconsistencies: ds . print_inconsistencies () \u001b[91m (Potential) Errors (subscribed topics without publishers): topic: 'start', subscribed to in services: {'PrintService'} \u001b[0m \u001b[93m Warnings (published topics without subscribers): topic: 'C', published in services: {'ConcatenateService'} topic: 'dialog_end', published in services: {'PrintService'} \u001b[0m Here, we can see that PrintService has to receive an external start -topic message to be able to function - without that, turn_start will never be called. In this case, this is okay because the dialog system can provide this start message, however in a case where we would want start turn to be called multiple times, this would be a problem. Additionally, we publish to topic C but no service subscribes to this. As you can see, when your functions are not called as expected, this output might help you track down bugs in your system. If you want to get even more detailed debug output (print all messages and associated topics), you can provide a utils.logger.DiasysLogger instance to the debug_logger argument of the DialogSystem constructor and to the constructor for each service you want to log information from.","title":"Checking for Inconsistencies"},{"location":"tutorials/services/#displaying-the-system-graph","text":"To get an overview of the system, we can also draw a graph of all services and their connections which is helpful to make sure that all services are connected in the ways that we thought they were. The code for drawing this graph can be seen below: ds . draw_system_graph ( name = 'tutorialgraph' , show = False ) # render image to tutorials/tutorialgraph.gv.png # render image in jupyter notebook from IPython.display import Image display ( Image ( filename = 'tutorialgraph.gv.png' )) You can see in the system graph that PrintService needs an outside message to a start topic (which in turn triggers the counter for $a=1,\\dots,3$). Both topics, A and B , are published by PrintService and subscribed to by ConcatenateService . Similarly, ConcatenateService publishes to topic C and D , where D is subscribed to by PrintService . C however is published but never subscribed to - which is shown by the arrow from ConcatenateService to the UNCONNECTED SERVICES node in the dialog graph.","title":"Displaying the System Graph"},{"location":"tutorials/services/#logging","text":"In order to debug more effectively, ADVISER 2.0 also comes with logging functionality through the class utils.logger.DiasysLogger . The logger has the option to log to the console and/or to a file with the following log levels: * NONE: No information will be logged * RESULTS: Summary information will be logged about the success rate/# of turns after an epoch of training/testing * DIALOGS: All user/system utterances and summary statistics The logger can be passed to any of the modules, to log that module's output, however when passed to the dialog system itself, the logger will additionally be able to log any messages sent through and all received by the dialog system - including the message's topic and content. A DiasysLogger can be instantiated as below: # create logger to log everything to a file logger = DiasysLogger ( console_log_lvl = LogLevel . NONE , file_log_lvl = LogLevel . DIALOGS ) And passed to a dialog system to log all message communication by passing it in with the debug_logger parameter on instantiation. #ds = DialogSystem(services=[concatenate_service, print_service], debug_logger=logger)","title":"Logging"},{"location":"tutorials/services/#running-a-dialog","text":"To start a new dialog you can call the DialogSystem object's run_dialog method. When doing this, it is important to specify a start_signal in the form of a dictionary where keys are the topics the signal will be published to and values are the message content to be published. We need an external signal because the run_dialog method is blocking and no function calls after run_dialog will be executed until the dialog sends an exit signal. Therefore, the start signal is an external one time message (or multiple messages) which kickstart the normal dialog loop. In our case, the start signal is to publish to the start topic and since the value of this is never used, we simply set it to True . ds . run_dialog ( start_signals = { 'start' : True }) ds . shutdown () SENDING A= 1 SENDING A= 2 SENDING A= 3 SENDING B CONCATENATING 3 AND messages dropped! RECEIVED D=3 messages dropped! ... And we get exactly the output described in the previous section! concatenate was only called after a message from both topics, A and B , arrived - and since A received multiple messages before B , those messages were dropped so only one concatenated string is printed. Since the dialog system loop is blocking for a whole dialog, it is very important to make sure there is an end condition to close the loop (i.e. publishing True to the topic DIALOG_END ). If none of our services did this, this notebook code would be stuck after calling the run_dialog and no new code could be executed. To prove this is not the case, try running the code cell below :) print ( \"Not stuck in a dialog loop!\" ) Not stuck in a dialog loop!","title":"Running a Dialog"}]}